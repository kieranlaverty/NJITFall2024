{"docstore/metadata": {"718eff5e-0d3f-43ac-8226-5232381f4af0": {"doc_hash": "f2af3fe376f5b62fb5f2eb636867457b6c77d5e24e7457afd63dd8853e682c41"}, "68413b82-f021-4b71-91fe-e2bb6ca0eba6": {"doc_hash": "5078d9da3d977f63e97a759e44688c64192e2dd217feb532f1ad57f801b64f3e"}, "84480dbe-b03c-42bf-9b51-3e044e5d48a4": {"doc_hash": "a39e157b68b0eac36d25a445661f4fcadee7d8c77134f2b4d3a6c4ecdbacea31"}, "9829986e-2155-4196-a42f-1c6e40ac130f": {"doc_hash": "2f18301b716ab11ee3a2a63c654beace0569538952d6ea742f3f9052cd85de40"}, "79806eb3-8406-4f54-bdbd-95a12dcf08ed": {"doc_hash": "98ebab5d8e342b5765eb38b149a57216a83303f20a3312e896774205f44bbc7c"}, "4e48a309-2246-45e8-802d-53cf0501fb06": {"doc_hash": "e053f3006acc572062bc177daa3afd491c7f759d4cb66017c67c6bd2f2775e5b"}, "f9478ab6-7eaf-4ce6-bbbc-d5258f31b157": {"doc_hash": "7946e42c1fcf99918efd14a87628b0b935fe79561c05ecf3168009d43481cf04"}, "bc9a497f-1214-4008-a29e-fad59826e416": {"doc_hash": "c90d1c3344fa8c700815edf6e02e69af037548f25ad0ac57d283f87b3e669154"}, "4fdae7f2-4ed0-41ad-9971-a8272a8fd4b8": {"doc_hash": "c777671c11cc83215ac5a373a036287779c722ee66dd7ab5d3736476e4e2c4bd"}, "020cc405-8012-470b-8ca8-192e424019a4": {"doc_hash": "59e8200309dc7c91e942d18995d4097c003a7cbe08b9c98935e62164ed05c1f4"}, "2e12f008-0c37-4675-9d4c-25703a56040a": {"doc_hash": "4d1626c64d76473ae4f51ca8b7bf94f21ea43d2e7e28ef0878999ae43f8322ed"}, "625db00f-e7c2-4132-8c75-cc7160e87160": {"doc_hash": "f9da88544b4e6759c75e3a84d4d259e4fe09b5fdc8dbf21ed7ecb1f30642b3bb"}, "1eb5e137-c72c-4e63-b7c2-b7153801d13a": {"doc_hash": "9d81c29b9d56da32881634b81709ed6bb2d4e2753e1959e6daf94fe71b019891"}, "e50abfaf-1789-47b9-8d95-1af68955910b": {"doc_hash": "155cff4aed96f4242ca043d81e97adcdcd36bd36dd7465c5bf52c9700f968aff"}, "7956e872-9719-427d-b09e-502e92e1afb8": {"doc_hash": "7526e1fc679e96f972a57d3a94ba320e5fe4d09b9f918589e8efd8fd4d332a70"}, "ca9742ea-0801-4e54-93a2-2aa9863274ad": {"doc_hash": "7c02ce6a5802ad26cbf7ae7235086cb7ed66e4ad1f4f398990f4eb5926654d28"}, "5937be6b-7e42-440c-a403-e75af91ccc9c": {"doc_hash": "b074b294f1509adc613585960be93f7c5699feeb11791799af939fd3cb4ab0fc"}, "f131e00b-603d-4242-a728-fc6ddcd552aa": {"doc_hash": "6dfd7d6d87e025cb9074979f31eb4e416ba59a2a569f61efd99952140e9a74cc"}, "5c3379b8-6495-43da-81f6-82621a32bb47": {"doc_hash": "85ddefbcfc39ae8007ff23299f259a7a56c481d02c93aa8de539dc3d1306a9f5"}, "23adfc25-5664-4baf-ab8a-1beac1691f15": {"doc_hash": "d7ae9fd6809b2abde79258cd5c8db89e29753643179f6a4effe1ea6d78006784"}, "3a86e926-5a74-4ed2-a224-6ffc6784f2ba": {"doc_hash": "7a630ab7568b450a7e4fcaf4cfe12be37aedd210711ba679d7235f554488a91b"}, "755050d1-ae98-4329-a2ac-a91dadf0a1a2": {"doc_hash": "edfb1df63eda93fd907ed65b6a41f954a95676d1eb8f991306974122adf8d458"}, "8cc005f2-60fa-4609-acec-094babedb29b": {"doc_hash": "b0251315c325fce33ee1317e2e47bb27efb7c2e90bab0b9a85d50e533a73ac97"}, "f3e6c603-b7d7-464c-87f7-68992ff22852": {"doc_hash": "caced76ebce96314ab03a82db9f97fb66f88d098bc1e810a8bd05b7e67a867d4"}, "b39b720a-aba3-401e-9e2c-2097e339cc6f": {"doc_hash": "a198b87397acb6716787234770e42d2dc14744c1c29b8f4dc20b25b03107578d"}, "16bc8577-167b-416d-b8de-db9bd3ebc3aa": {"doc_hash": "ddd90571006a76786c903cad98317c23ccd398b9203b757c7d5e9e1545c32efd"}, "a036a6c9-2a57-4805-92bb-a42b6ebc9dc4": {"doc_hash": "9c3a3d56e84e4dfa271c40d741df0408aa290829a9978356ac7f92b73e7d27c4"}, "50d12424-e819-4a2f-8b84-b71f94cc56ef": {"doc_hash": "bf30a669f76684622b1294feeca415c4c875c90370f06f77a75c8d2d7240aa5f"}, "f30241e7-7fbc-40a1-be80-fd5120d627ce": {"doc_hash": "71188e290e56ded64e98836f960eb76216eeb5646e1700e65d071f2202b23fa2"}, "ad5b7a7e-e541-4670-b0cf-ceb0f3fc9c15": {"doc_hash": "bd282e50e04a4d5a75146b862c135ac515f1c463e440e58f83dbc8e4520e7d26"}, "4f2fadff-692c-4d4e-b1ed-8c90d1920f2a": {"doc_hash": "0abd985f2083f6a1028bac48023e0b88b201f2e158cdf95771ec8e5e2fe6dcd6"}, "f270e715-0e54-40e6-8bdb-4fc61767b8ab": {"doc_hash": "f3a955e1ab62b5551cefba3d1a289f1815358d1d0dd7ac388462a59962913e2e"}, "d1893d1a-6cd6-4d84-9764-db71eeaf4acb": {"doc_hash": "c51b0903064e1b053ed805b5fa5a6cbbad61dc3084b4bf2190bf1718c94fbcd0"}, "18327ec2-e894-42cc-9a56-5eadcd04dc1c": {"doc_hash": "6d18a427353a4eb106b5745ecaf5d2bac50f695e361be985fbfe48e9b3a0a810"}, "33c1f82e-ff9a-413a-b81f-487249871b1d": {"doc_hash": "7778101e92d7d20b12a86db7b9c836e8b9c1ade604465f153916469f55483a51"}, "725b4510-4255-4142-95f1-94093d6ec3cb": {"doc_hash": "7be90c932fcdc75d114d7c13e60ae5c78fc1637066db49e41a601893cebdc18f"}, "6afe5b7b-8993-4a70-9982-e59a7f594cbf": {"doc_hash": "18dddadeb0109af1039bc4ffdeec8411c4cc0f18e7e6ffb811f2849e88cfa745"}, "faeec7e9-a99b-43a5-9bc7-90765ce8fa36": {"doc_hash": "84b7deb121d04bf52781293623118bf18d01483c97a05a21aabbfceae52488cc"}, "4b1341be-2b39-48fe-81c3-6c4688f41ba4": {"doc_hash": "d74308c478e3e8593c003ea9944e0a34a88b9b88b262b05284bf9d0b6b9956a1"}, "87af99ee-1cc2-4516-bed4-556292d32e37": {"doc_hash": "f09dfe863fe94e1b2aa734bd0ef8213c043999cbebcf8cdafe8d9542084cbe06"}, "d707ff2b-acdc-47e9-b94e-baf71a1ec631": {"doc_hash": "2080e75b80031701df6531060639713fa8fdcdd520feda628dcdd004846d9e38"}, "9ada9682-d106-4b7e-9d4f-0e8990038a59": {"doc_hash": "eaa1298194f7af79069d48dee019f97606e7033c11ea8412fd1fc81978303789"}, "ab7fa45c-4770-4bbd-99c6-0160ce218d38": {"doc_hash": "4a4e0a9ec6f356263768e81c050e32a1215d7d476472f6ad75ad2df522064b59"}, "6e1e3990-cfa5-4bbc-980f-a8e3a7a2b9e9": {"doc_hash": "102c647f8d075859720dd48e290f6e95055e00f4ae893da18d6ff1ba6ce5967d"}, "268eabe5-f4e3-4f49-9e16-049d211c0a4f": {"doc_hash": "53f5049e32174e452b6c028d12b65cb625207509055479134122d7e20f59f236"}, "7a6ceddd-0d6a-4d15-89d0-3ea9d8c02b9a": {"doc_hash": "78d221c01b0de52b1c013e6491f9b96ec4768363ff504dccc1f9f1f04102f10e"}, "90f29465-ad78-4161-bdd4-4ae10781de61": {"doc_hash": "337e815769710cdc0d0e3d51cb555bdc9617850948aa3048a66268191caa68b2"}, "9b4aba06-fc20-41e8-b5d2-f09aa3342cc2": {"doc_hash": "09bee5fd9653dbe96e4f0282ddffa012ed8864750d72c18eff1cc9222c0d8548"}, "4f3ac8ae-3b5c-4aad-aea4-d16630952fae": {"doc_hash": "0866519932f70c8e415e3990f6f6abe8de86a0a9668b807592ee63b04a733ea3"}, "3f851b04-1eb8-4853-8cb2-6522a9935683": {"doc_hash": "c88e4e1a9f30d0fea618c9486f3f331d0409455ceeb85d03ed9b677a6397a100"}, "2d9331a7-06bc-4156-a1cf-c028d1b8f952": {"doc_hash": "c4100b1e7cafd8901049c2084f374eb9f12a81d4c215052b02bc3fcd0643ac55"}, "993ecd16-3dbb-4ea7-8382-601df0df9a73": {"doc_hash": "4c6ee53f4aa6a6e79c24c351f95e78ec456526ee3baf4ea3ddda9db5612c1e90"}, "d3353383-62e7-460c-9725-5c306b9e0579": {"doc_hash": "8fa555a58d0247396d0b7455ecbb3539681ed7589d5316778f1cdebe365b88e0"}, "e26e1194-279a-474f-bf68-a760cc5f5fe2": {"doc_hash": "7762bda0ade3dc820abdc887038014479a903c2d7a5ee858e62ee5e4a5574566"}, "30aa02eb-ec59-4612-b2fe-c41afbf31f05": {"doc_hash": "6b7f3484c9d503b715b52509559468ab48199a21a4db1b87d1cd269826aa1075"}, "1b3e409d-a989-46f2-a0e8-e68eeb5bed44": {"doc_hash": "2fec4b4ac47827ed5e37284ce596aa3b0481805f6d87665f61492c25c06f19c9"}, "7f893caf-6c66-4b59-bbd6-636fb8686bea": {"doc_hash": "3d1e6674c9d4ce58d6eb7af9e275670d2307daeb026145afc848f44280fe3e08"}, "0b059111-7d3f-4f17-a0d4-e5c40ca3df67": {"doc_hash": "b1e1a7046b882addc1aa3e6a3274e0476227baae5abadbe9907ea9df998ac470"}, "71d356a3-d694-40ad-9d24-75e98adb1162": {"doc_hash": "f73a2859013fc6e3c96d88e9114e18381e4870e017ddd3eb7ce36ea401a66736"}, "f9dcc3c3-a559-4304-9591-3304f007e38e": {"doc_hash": "7ec41309c977af2255428246816d81ec46c91dc5d1994a8b43b2979e7019d365"}, "80fde06b-e558-4d1c-8b95-96f704a0c8ed": {"doc_hash": "836f758eb0bbcec20507ef14e4530496ec7ae83513533d40ac2d2f98cbec8407"}, "faa5f3df-ccc3-4ce9-9ab7-76660c24116b": {"doc_hash": "0520f42c2fada15749a0347c14348e69a7c052a569503a47e656cf56ba929c5b"}, "193bd3b5-7552-444a-b940-2d54c3606127": {"doc_hash": "e6545b30b6ac5362eeb7756d8e0ecd4961de79fabb1ee2c96de333dcecb8ac5f"}, "d48f4050-f2ea-43ff-9c61-14a2b05dc1cf": {"doc_hash": "f3013835dc44245e05b8e71c8ef78d6824bf2933dedf84f4d0fe686a7f436501"}, "8741c4dc-bfa3-4852-b7b1-832e8a24b54f": {"doc_hash": "da3bbc77ea3ecd8989b6b9e81a31db60c701855a50facc5ee5b8dcfea1adfa33"}, "adede142-f86b-49fc-b002-f2f3a5c50505": {"doc_hash": "291461fd2916ff9b307180c7d7694e8eaca8ae5c06a8c66b6775481c9b4ed0a0"}, "ed028e55-e520-470c-beea-6aafeae3f4a6": {"doc_hash": "51ecbe638d67be7c383d9e1e64b94fd963ff96a2e2643b8882559efbb8386c83"}, "6d1b6549-22b7-4e24-ada5-6a9502eaca5f": {"doc_hash": "c0f02a632fb6d54adf4ab1170dd7592351ea0ec0acfbc219d3cc780d08a46754"}, "a07592f2-ca99-4d0c-aaaa-90ab9fbfa598": {"doc_hash": "9ca3cd51715be1b0cf397c1d130ca240a1d5918d1dafb6a43df9c74d00f377bf"}, "ff1812b8-79cf-4da5-982e-7ee198ff9407": {"doc_hash": "00afbc61aa60e1886e52cb318cfc248b5ec414a75b57fa4b7001bb30d4c41836"}, "9734d0af-7af2-4e4d-a5b3-19545f934a24": {"doc_hash": "c65ed7c8845b4e122567f1d8a97cbcfad68503db43a318cfed63d2896c25cd13"}, "2612028b-7dd1-4ec1-a359-3a3870825446": {"doc_hash": "38576ef6d81072bc41479ee51df56431f155b06a9bd926db2f681c2d79e47299"}, "f02f67e8-15b6-4701-8f7f-672b35b4b2da": {"doc_hash": "7083c4c8f64f14c4abd59d7e53471f88b8cd1feaaa0290c18245757e4a3b7dcd"}, "4008b542-5c38-412d-a8b2-0ec2b53e812d": {"doc_hash": "430de36e1a3e28031e4b3f2a4a0546f30a4bc4da12e84cb58b8c17e661925071"}, "66ec205c-ca01-487c-a425-779b8385ce03": {"doc_hash": "9234f2a5d840f2d956fd98a24f6e32ea52ea577dd8466357b27468c957052269"}, "6f81479c-1371-4697-ad90-042521adbb4d": {"doc_hash": "b78a5982fccb80be0955de289109d10daf1ed27461bfed3170a750a8a0679a3a"}, "bba7f5ba-dc84-4b60-83c7-eb40367fc866": {"doc_hash": "579f60f7a1529fb8c982df78c013f99cdbe711a8f37fd6e6169cb23e7778a0b9"}, "bd52669e-6b5a-49e8-af5f-d8d98e7cdedc": {"doc_hash": "fdbc15df3b24ae5bdbf1d2c58e53c4da9dadbab83d7ea488b8afd448af40133f"}, "c6150d04-f7c4-4dcf-8c21-87caeda2ea2f": {"doc_hash": "f3ab48189a9d4b2f5285703c5ab8ee1f023de37fd29785a478e1dd7beb226a24"}, "ee15e2dc-1078-4e4f-a25b-31828a338d6c": {"doc_hash": "dfe0b84a9fffd48fe26a99626c08405cb53e4de184bed0c7a0ff6be4fc758b0e"}, "cb7f96df-28be-4cdd-bbec-203a49fc8c6e": {"doc_hash": "de2016e2db6c9fab2465116000f1d0bfb0d48322ecc3384badee61542e4ce528"}, "0a8981be-7da6-461d-a7a7-fcf1249dce5d": {"doc_hash": "a94a7aae5330cb5ec10270a2e18b02b06032e1cbf117e4e0312b85d371ce8ae7"}, "79e09c87-7906-4c1a-9b88-d93cc0ef91dd": {"doc_hash": "d1c80860a6a983a908d2338ca311774e343f32531fd6720f8480887849c5fa35"}, "ed575f07-71c8-4abc-8fcb-b626e8a8e240": {"doc_hash": "bc96b40c095808c653180ddabcb2ecba35aa3568162a271a0bb9bf44477badcb"}, "42c4ac4b-cef1-4d0a-8fce-d4e40198749b": {"doc_hash": "bd33d778978a6983c57fe4854e3fb192c8aaf26086abff6bbf9a65eea04d4cc0"}, "b2399847-6c4d-41d4-bcd1-9721215e597e": {"doc_hash": "4c2bc640160d8f45ca1a6f499d92fcc4cbf828c8ef257c71c72b308bc165ce99"}, "11d4ef51-b189-4038-9718-c28622a03dbf": {"doc_hash": "cedd72b3672baf82b4408f725dd7165213c13ce6d2449dc8c82167a291d581ab"}, "787d7e78-b7e6-4afc-a43b-b08a2208eca7": {"doc_hash": "e0a12d914b9c128f7f4ce203856e30d56d7a13822f8ec1fa378f5f836d4deee6"}, "e5a07e84-b24d-48e3-bdc4-869410dbe674": {"doc_hash": "7f6e786aa42ff252125ba78eef39b0717ac3c808d3530206e016f2790b0bf4bf"}, "b1e526fc-e769-4f49-99e4-3e1247d91071": {"doc_hash": "d4d65f82976188270fe272e3cb253dd786f3a9087990af2387eff8fdfb57c8a6"}, "93848c46-d556-4e4e-8e0a-0241df0fa7fc": {"doc_hash": "f5b1737bf3e4fb47d42c9c0fb1cb402f1f8cbea85a51244f7dbdb4310c6ca2af"}, "39fc460d-a7b7-4bd7-8ee1-459e7470c5df": {"doc_hash": "f0ebdc34ba4873b7d05ba4f09199056dd05558e4480a2b6680dbd394e08dc3db"}, "e7968dcd-5cd1-405b-8bf7-0d5323556670": {"doc_hash": "53ec3ec33b8526db323a6f05e6972b8f859b1198b29d911e625fdb2d1e0eec53"}, "d1bb0de0-db2a-4655-af02-fb158c4efc55": {"doc_hash": "ea601a4fbd71a74c8c30c8d3df6e2c64fd2ce20da3fe3d3f830f72ec9a11139b"}, "1a032f26-cfb9-483a-b87b-d2fe24205f08": {"doc_hash": "09675834fd07bf2c300d0b42125958690cf034733c23023d4422d26febb98964"}, "6b4abbce-a384-41fa-8fae-4031aeca7d2a": {"doc_hash": "90f7ff2d9c10273b32dac06be88f04afb97b40211d96861791a08c92f1393277"}, "f8abe648-a4f2-4069-a5f8-4a6f68112cf1": {"doc_hash": "6fad08d98b89dc6db3f4ed279d3e6e038f573ec9456332ac7e85399a5ee31399"}, "e1cda2b2-4e87-4c41-8b20-d709a342f985": {"doc_hash": "99061047515944ce8fb308bba820bb1b482be783bf575105f5a3ab597ad71639"}, "c29adde6-2d1a-4aab-bc2a-c6cad078aafa": {"doc_hash": "a2a88acf8acb8a750c1a4da110ddbccddbc4354facc4d1a3821e6eefc3da414f"}, "1f3e80c2-dfac-4a3a-acdc-bf2563ffc259": {"doc_hash": "bbbfd9bb666c7af60b15dc693f594f0f3ae67a6a49c410832cbd5dc0d196766f"}, "27e666d4-ce96-485a-b671-d3281a6b8c4f": {"doc_hash": "c0e77a208e0df099dec0dcaa14c67a24f4e8ee3a518c567ade53dec00728526e"}, "6d2a4fc0-a7b2-45b4-8cc4-a0e55285a204": {"doc_hash": "7c8bbe17e3785c33b254c392b9bf2c89ab09f38567092c759837af2b01f7607f"}, "e8f043b9-8820-458e-aba2-87c03b8ab06e": {"doc_hash": "4d9088f9345afa59fe3b3d5b30cc581d16b806249bde190fd2575455b054fa7f"}, "a6d4ada6-5953-431d-954c-d724b8e51203": {"doc_hash": "95ef466bbb241be89fca2d19ac25e844b570b2523b0524d7e53810c23f956e6f"}, "56155bc6-0e0f-4be7-b8ac-3b92c68e2770": {"doc_hash": "6c5e4bc6219cc700a0329f18bb9362f8498f3223f422bb00ac875251dbbcba49"}, "28ceeab6-898e-4fc2-9a56-bb198d4cf3b2": {"doc_hash": "b344b9b442c786ec3914b4606b35a721d50b5045f702b63338688276ee9172b8"}, "803921da-05a0-4faa-ba0f-829ad3c86c23": {"doc_hash": "7b0c1152567a18bc33e1e2d056013e9b6b32993e7b964a54d4f218e2455d0b88"}, "cf91f90a-a791-4855-9d9c-a3a341d4ac7d": {"doc_hash": "6e32dcd183e5e1f48a0b988477e371c5df6670ad0868e6bcc34c0982da31cede"}, "5e2f61ef-8c36-4930-a332-3d35c3b57010": {"doc_hash": "5f89acae03b1c25722b20c20e3221c39a1af56f81a204999a9415039f7bb095d"}, "25f44172-c490-46ae-be42-a56b3b190c37": {"doc_hash": "37462e96eeb013e7c40470c5e8d1a55efde97fb6583b18b0b3e857d2d7a4434a"}, "9d28a70d-bf62-4b79-9a26-c305d73f9dcb": {"doc_hash": "7dfbbbfac5fd80600618e6331fe1e308ca905feb55c933391a64d19feea80f31"}, "cac64a08-aa40-4215-b4b4-d4913d584a9c": {"doc_hash": "61b865a5c542051bc76e365fb0d591852b46de7c950da5f1221a8ab65bf5abea"}, "97a80c91-b707-48ee-ab1e-5d5278ecc716": {"doc_hash": "0d44af07b2498fd03d6598b87fc830bb0b90d76e3e0541b2e47b2f159c307fcf"}, "69ede226-5ed6-4900-8d9f-28e503e72a09": {"doc_hash": "65c126ea5e81b0163feef1054c13a9fafd232f2c010257d13973d6dfdf21bc1a"}, "848d32f0-ee97-43bf-aabd-06716aa69d77": {"doc_hash": "feaab0f300f0da5eaffa61d4f400a4cd0d67e11b56d4996199054ba4a5cea8b1"}, "3a8aeaa5-ec81-4fa6-b412-56143a889c20": {"doc_hash": "95298a9a6508e64ff8302d9cd89b0e57db5a2d2621b7cf4f7d52e74199e20ea4"}, "9552ac9c-417b-4475-bb3e-88ab9a6ad5bd": {"doc_hash": "f1b6d7a783d439c7fe12ec1880210d1a712d64cf52eed228a90928654d44dce2"}, "d11e5500-70e5-4394-960b-8f754e82f6ba": {"doc_hash": "181aed24d7fa52175bcff9a1d65ddfd1a9247dafd41178c356296b4bdaf64ca2"}, "2bae5020-5188-4ec2-ac64-e013c7128923": {"doc_hash": "7a925fa77f164af66ae5f65adc2f2a35583a21ceff7530957fffc3316d3b8b47"}, "45f1c665-9493-4da1-b1d5-86c00104e08e": {"doc_hash": "a87595be22640d83e4129b26258723b491defb224b695f2493f9df4a15d65f9d"}, "9e55930c-82a8-4720-977e-8af40392cc43": {"doc_hash": "43e3839f57399f1e47fd6f4140e2c79ad02aed508febd1f6d1189fd89cfd5713"}, "0a6174e1-6ee8-4abe-b476-7a563e647b47": {"doc_hash": "2978a6a3687b31a39d771c8dd79dd83899fe19f1d347cfdfb861507eb8e97644"}, "2700a2ec-d2c4-42b3-af73-4dce9696eb4a": {"doc_hash": "4de6e7234726fdc61901af2e55b37050539e1d170d4fd7c9c871edd7b18d3c39"}, "0366dec6-afdd-45a9-922a-34422fc937c7": {"doc_hash": "d326aab69c54d4867518150ba24140570b07d4e7f29dfae6d264c2bf40e7cd5b"}, "d28071a5-dc63-4768-9fa2-42a47d76b49a": {"doc_hash": "dcc16b769ba1507a4e3940d3af00c3329b5e75958ab21ebc74fb5c1e05420401"}, "54691a2f-89cb-4fbc-8a2d-6d716e7c6e19": {"doc_hash": "05c57366cb9f8680e54b8d0e5b8f710f274d555066dcadcf8a15a130e23ba6a4"}, "d623e7b1-09ec-4870-9c38-647ac09d03f1": {"doc_hash": "ff7968fdd250482bac31991b095708806c2a31db9d9a5cbbb1768271a9239549"}, "30969199-6085-4578-9221-f1f440c1ac40": {"doc_hash": "9ca8d32c9d8bd6f1b61d331def999dcac6ae0e809c7e3a5c11722ce1903a1225"}, "97858353-81ad-429d-b869-b24eca3c982d": {"doc_hash": "d2e0a87cc4c0e3d564402a62205d362102da0f7e2f6aa73cec7e8fc21c962fd6"}, "28b424a1-5b01-4b13-81c1-c0cd1f6ae853": {"doc_hash": "f19a05deee7ac2431f49365fa41bcebb377af41cd55e6e4208f09eb497b87294"}, "864e8da8-17d3-4d13-aa5a-b3a70fa7a20c": {"doc_hash": "4d3a6a3337339bdc7d09580df4172dade398c71a319cb6dd19f9eafb1605b6e3"}, "f5a0d173-0573-4744-97b4-f39dd4091f5d": {"doc_hash": "96e470c6dd3275c68944e8aab7e7374fd79cdd3de2eeb5b50e4c98775510e3d4"}, "9bb7a012-7163-443f-ab82-2ad62999d93a": {"doc_hash": "2e3cc52d5ccc7f0b64d0153018381ee7d2b93bfb0aba6d395a590ee0c1408971"}, "b0790221-ca80-4f64-b6ea-69e125aa6c0c": {"doc_hash": "07e37db990f1a1ca89758d97a8f1a8cf1cfcc3def23e6f62a17ff82bbec2b1e2"}, "a9cb6792-fd5e-4a72-8726-52d9d4173f2b": {"doc_hash": "03f7d432ed69b0f432dd3d62cf69d70a5a33480d1512d652e793c159cbd572fe"}, "6ceb790f-eaa2-4f07-bd38-bb4386e9833b": {"doc_hash": "e4eb9532108080deb81cd2d363392ff8d3478f4a98f465c52db50338db821584"}, "0e6ee11b-09c9-4283-beb3-dab424a3b7e5": {"doc_hash": "f74d86942e980f90baf6cff6bdabb6ecc00a942780aab69723271bc63a54d8c0"}, "569b404b-52eb-45bc-8e55-42687ac31a94": {"doc_hash": "e72b28cac7c88b159453227a4e17e7070996c23e571f913d32ee077f7d76e624"}, "6be64de5-ba13-4db8-9654-11e5f88ef360": {"doc_hash": "f82645c247c2a56e41d3ef500bf4d55ecded316123fbec2478fe50152012691b"}, "1bde34fa-52f4-4540-903a-12463f03952f": {"doc_hash": "1a7dbf1ef51d566ee6add7788c3f781cbd98b47e21ff4acf1d33b4585c0129f4"}, "a4c1b0ff-79d1-4310-953d-e4bf3b9c6b66": {"doc_hash": "94280836e0177c86b6468e70b25298c5349acef7765ffd6ead859e94e7a5ebec"}, "56dde7c9-f983-4dcf-86f0-c60ad35facb4": {"doc_hash": "4f1066da63c9d25beb7db6ffa5039320e9d5b1ac9875d410e63a578de98894e1"}, "971983bf-9542-4e21-8260-63bff691c029": {"doc_hash": "c66c62e367edffcd92ebb9094722d511981c3eaf640a016a6ef86ffeb067a61a"}, "c5cdc269-d563-481b-8d71-1a0cfcfb3e0f": {"doc_hash": "32398bb07d51015cce033576f5e33a104c5dd071c80d8ae5f5ef44f266ab6711"}, "d1e6b0a2-cdda-4bc3-86e0-e204b8181237": {"doc_hash": "61248c59140b6c9454b5e1adaefa74976f4a355a32fa1ca411ebd04e0cf15132"}, "84050f23-07dc-4fe5-a089-277cd68a87bc": {"doc_hash": "718844ebf9d6c48fe8400b6bff6e6939e5599d8fdc330402761a32e578e86b50"}, "99126150-4bad-4350-8ce9-920e7cbd7884": {"doc_hash": "25c617a74baa54fc4c8641685d701d68949db0706333db1cdfc583d9e22aad1e"}, "75c67326-a331-4e8d-8832-f162f63b444e": {"doc_hash": "5f5b8f08a9d5c6437dcd75e4737a8994660da043bc64acec9c5cef3a67ca6bb3"}, "1ca6cce4-e72a-444f-ac2a-43deff0205ad": {"doc_hash": "c0f36ae1a74357026f0e675fee47f23f8c6f017218df6dbb6cd757248b637297"}, "82bc8d08-c1d9-4a3c-aa47-14524138935f": {"doc_hash": "ea7aa181edb54c12a8bb6bed459afe8d4bf442226e76722d2740241774c11538"}, "d2d3d345-8fa6-4374-b0c3-288706de72bc": {"doc_hash": "a0e9cd64ca849577e73c54a2e0b932d2dda9becc80b9510908d344d1a5ae9c78"}, "faba9fee-404d-4c59-874f-f57f0d4703d8": {"doc_hash": "72021d015fbbaa11ca40a78b8076e730f8d1b729c17f67e03bc5f9aad97366c8"}, "804d66b5-09ea-46e4-9561-6ad0cca74770": {"doc_hash": "1dc416d0a1c3d4db54d0472fdbb363b70d99447c133eb47ddb1040413382ac69"}, "2fafc2ac-86cb-4ea6-bae7-ace0563ccf3a": {"doc_hash": "2bf9d2716d6129dee6a4c77b62ceb1b13cdac1cf45101243aa956dc28573fab6"}, "fba1c0eb-4ac1-4887-8978-dcddbf45a5c1": {"doc_hash": "da30f5075eddd5a1e5ee316be2dce3c28b0bb67bd6a81b39949d53bb1b761153"}, "dcc7dafd-c7b5-43a1-90d1-057288eaac32": {"doc_hash": "5dd72531d63886c42c582dd97dbe4aa258198090423393b9e4f634e587b88c6d"}, "be20631e-77f0-4d7b-9fbc-f9a796bdb73c": {"doc_hash": "a5ff9d345db74894b5a266e51a3667535faacd39b671ac2b30050a650aa406c6"}, "a983d791-6e5b-408b-9c31-46bfd637b3bb": {"doc_hash": "b2107c83fa23a846330afaafd172298ab202e6c97602dca7af9f5d9be92be436"}, "6bd754d5-a894-4bfc-aa05-9debd265d1e0": {"doc_hash": "0f1e25472273e8b7fd64ffe64f3080b3dab5b40cf6c50dfc7543dbceb90642a1"}, "2493d4d1-7e7c-45b6-b8b0-2733e1314d1e": {"doc_hash": "5176bb83e517665ca7744d7341b6d3de1d9f1e5565fc32de7db5958972c624d6"}, "cf099bf1-4332-4afb-ad82-a6275a7eb773": {"doc_hash": "8ef39840a5b268d0a6cb5d33cbf8dc44290251bf339416c2ce7f71a488f5e379"}, "b7274fe8-3501-49ce-8f1c-0bdb2aa36caf": {"doc_hash": "e0dc9173818bcd8c58f0827c362d964c1c9c1e810b9952726f6ace3624738ed1"}, "b4fd42a2-63c0-436a-95f4-2a50666a1fcf": {"doc_hash": "fe9364649e2ce64c0563ad0eb75ea9c364b504b5b1b416800deb7e6460562244"}, "0e0dded5-3b0c-44a5-b22b-7fa94b52cbba": {"doc_hash": "9f064d8ecb7fe54ed10a25a6c24929c1c64c504a3a863291504f592c447f05fe"}, "2b0f7f56-b153-4d06-8ed2-6ea9deb16314": {"doc_hash": "5988b9bf06e1cbec360e08e434b22d3561ef03b04ffb0bede6abe8fe9c764a00"}, "ae499a2b-cb67-4cf1-9be7-4e4a76a3e268": {"doc_hash": "1fc38b55cf43928a8853762bf452b9d0f0a09dd9389bf2efc63dd4cdf87597fe"}, "dafbb433-ef7d-40eb-a3d1-4ebb61517bc5": {"doc_hash": "1d39c4a54ffa43f74f02d4051306e6410018b718ad2fa9ac36b0f17b8e87f768"}, "987ba5bb-fb6c-438d-80c0-d289953ccd14": {"doc_hash": "aed95c484f0c2aa50747d2b45fbb91a18d839bd351131f3acac3a11286e86f2b"}, "a2441bad-0800-4101-8281-e9ad2a57ebfe": {"doc_hash": "bd37b4e865447a45d0e7ec6925969b333055cd4c014fb834d44b2aa45f97e6dc"}, "3cf8e927-ead8-4e2c-b8e2-936210b5079a": {"doc_hash": "eb74ab8b8fedff8e9142ffd65012cb69c023336e2408c3d57fb471e6496dcc66"}, "c7616701-0aa2-4f09-b512-aa7d5aa87759": {"doc_hash": "754401126dda8e47602e78f41d6b6ad4e11fe73db0b1aa33905483c02cff28ab"}, "1b0baee7-0595-43b6-ad90-0ea07d2c0955": {"doc_hash": "62486071a9cb423e2138358c9330986c0114b404041193e7468fd28f4b492c82"}, "31d46ea6-d267-4628-9924-c721964817ef": {"doc_hash": "8d06ab6cfeeae2ddcbd4a1b2c91bb7cd59bffa2709517141b3fec08f1957c62a"}, "282005e1-8b2e-400f-907e-6911914aebee": {"doc_hash": "bade993eb23b174f18e24b88eb1819f4ba35b7fcae8a5f995146455f87b33c8f"}, "d6b007eb-8da9-464c-8176-6de8cd446cb6": {"doc_hash": "6ec5af79dd74a59f0f41dd408562bf45b210c91270e90b08778adc863b5541fa"}, "ff910460-6fad-4b43-8c95-f24a6fe9f6b6": {"doc_hash": "0648f7d3b9fd69e2ef9a589233c4dffb1c12facc4167830a352e94a6c47b13a4"}, "e9f842b8-ad30-4479-98c5-3845cc22a343": {"doc_hash": "450cbe89bea1ac8f7255baf51f94b28cccf0e404d6dc81cbe45cf65651b02594"}, "9097fd4f-f17d-43e5-9f3b-05d277ea6ff0": {"doc_hash": "41535aa3c802e65530499d199da9c00383a6b59e1871b6dadee4636ade5d7985"}, "7f889c2b-dbee-4f51-accc-8ba45db25e5a": {"doc_hash": "cbbf557da387ece132c43846a40dc48369716aeef5a492f9eb089e9b3ec5c099"}, "d176f800-d9c2-4502-8c4f-4a8ba4b98455": {"doc_hash": "a29c8b4e6b23affb8a403f38813cc556d930cb5c068fe2dcd345747442e70656"}, "426b8838-6dbd-4ce1-9e62-4a7dd5d25754": {"doc_hash": "a491cdc182559ca187ed84987c5c4da3df39d1291b4f66b735c8b08b976a3eb8"}, "51052632-3339-4f15-8a1b-d50e5d13eb34": {"doc_hash": "98e812e5e00a1a49cdafde468a41085538968ec57b108958bbef94d47eacc3a7"}, "ab13f966-da7f-4b06-b8b9-f67d9caa8262": {"doc_hash": "33414b5598c06b718dd27f11674bb308790fd5c6e9263fcc875bb72e955a721a"}, "2060e13e-9074-422c-9308-ecbec79611c0": {"doc_hash": "df4dc4c99fe3443bde0cf30d60f6fd7be5c768a4d3bd61652b111240b6d5b8a0"}, "4460bf1d-a326-4c70-927b-fd9e381a6eea": {"doc_hash": "54ae49ce944a4b2502cb85ae31989f01b78a450b49db1356e0cd4f10e410342c"}, "d4296d53-305e-42c1-9629-0491912b9fe0": {"doc_hash": "36253feb61319b2ee70b4040330e6e54546910a9ad1232a30d8789863b1fa114"}, "d5c0917c-aaa2-4077-bda8-bd5e838abc85": {"doc_hash": "2d58f2a362b6b6713746dc7c0d2664d731046632baa85a993ee40fde6ab95226"}, "3dc3e4e8-6507-42bd-ad99-1914b4e246c6": {"doc_hash": "59620f14fb201d3b6aebb54a13a826a72fda8b162a71b38d9fec646202b8cf5a"}, "71b68203-775b-420b-9a36-4a04646551ce": {"doc_hash": "0e6a9d266ad564e9ad781505620af70ceced879a7d6af4b385b147d6db9310dd"}, "43b2b1d8-83c4-48cb-9866-0023673d0dd9": {"doc_hash": "4f5c7058c15eb554804f512901713d14f2987908fb044b2b7bba7bc81e39179f"}, "ca7fe14a-71f1-4eaf-92a6-9bec7f3e72b2": {"doc_hash": "3acfda5a5d8910d72cfe3096e1d59c89b56f28c94f6b18bf5f5c1d5f3109ef0b"}, "cddbd3f0-bfbd-4f3f-aef0-876e8aeac72d": {"doc_hash": "b7435681b59445726d58066fd5fa6a8d503e7e921531c3a3c609553382598fed"}, "020a85a8-2d42-4824-abfc-d2f43e8d9c90": {"doc_hash": "d42273a9c53c2f7fedf14c671ba5f8c861be99a77e1b96e86eb869b13be53200"}, "68613df9-b843-4633-85b6-6322b0fdf762": {"doc_hash": "d912ae3d150143047b25db187e95b4c868af4669158fabcb5f1fc1c76bfe0370"}, "32d46f67-6973-434f-aa47-448cacbbfc7c": {"doc_hash": "9a3b038b4701c144fe10bd43473ef26f253d7c3f91ab99af231d8381a45f25f7"}, "a0270026-978b-4cd9-9469-10c9a2783bbc": {"doc_hash": "4d3bc3eab52d2f764406fdec2144eb36c15c2165cd124b0abb536eb9191de4d1"}, "130ca9bf-f780-4912-9725-094e49f23144": {"doc_hash": "9f124504bbe48f9c99cdb8fff0871c4418db08c1fb08c374a6736e23e208a146"}, "eefd56df-5adc-4e44-9a3b-cf43bb829d9c": {"doc_hash": "46b18eb434e9e8ae79bee8dd843f2a719a57ccfe9c31c93cb11496b76017d5cf"}, "1288119e-1daf-4d9f-bd44-30f7b889d380": {"doc_hash": "04e4016a49213e6a94d860fe96c8ee70b268106d6e332e662a1267ae77096c73"}, "109b9958-d9cc-4932-a62c-aa178e3a8102": {"doc_hash": "cd7690c08b5846703afe7673b377f5473e6a9217d3d3cabb193c721f88c19fe0"}, "b3d735f8-b4b2-44a0-b67b-2acb36a81124": {"doc_hash": "aeffded2bab3cbb35a9db64492b914ae85538221ccac838267592ee2c9957294"}, "ff465812-ef9b-474c-a26d-9b496e5a3f69": {"doc_hash": "c5becc275826e62710730a3ce03ce547915d8a922a18b5a3184fc388483f9d2f"}, "64efa05b-1227-45cf-b1e0-f985f402d138": {"doc_hash": "6bffa68e22e1fdf14745284eb709619deabfe434f72f0ab68528c218ffacc724"}, "74d213ba-51ce-43d9-9c4d-b33502bebad6": {"doc_hash": "c5c1fc56b818b290bec43f84f25d312d2fec8fb2fb1bfb416ef19b5844910f58"}, "bf5d0c5b-d9fd-497d-ae87-93c4bf9b7db2": {"doc_hash": "19f19a0b24916e2d7129470e0ccc535d6485576afab227d18f2437a98583bb75"}, "160b30d5-c10b-4dbb-81f1-dbddc080128a": {"doc_hash": "2a74012ca966f4770605263029566055d5de9408634fffcf8b2e82928dcb00b3"}, "a684543c-2b4e-402c-8e26-134a0476151d": {"doc_hash": "ea220c02397f4d8fafc92d8052dd8ad6226d7aa2b861d4a3816ecd9034e092eb"}, "f421a9b2-8b56-49da-9723-c20ca39e00d8": {"doc_hash": "f7e1371dfe37872668c0443f531f869431636c28a2b728176d28f8f63b04b01d"}, "4d0945b2-4bc9-4368-8a51-331d967ac990": {"doc_hash": "2f57afa437cdc20c53c3e862330cf9a8088ab20c0d89c747072dd38665198b4d"}, "e56e9c4c-7541-4cd0-9af1-0a4d62942f41": {"doc_hash": "7158d24854e6805708726824a48b4903af5683fd9de8df53b4fa767dde232a60"}, "8a07d81d-6212-4c73-a92c-c0d2c08bfdcc": {"doc_hash": "f81002099d79d3ca91534bc5bc2e0ee7211177b2ee406aeccf2f9920b37839b9"}, "0c1b7c52-752b-4294-9cc2-7a5f135261e2": {"doc_hash": "07074c67a899da39e8fb227a0d847a6168f2e1181c4c0374c498dc7a9cc343d2"}, "8c4f8a63-5980-41d6-b3f0-979752b4e046": {"doc_hash": "b0ccc6a8f81d5a2a78f1f2a511a0cb0fd1da0258d2e56784b6b3fe216d992d4d"}, "ced0416e-6f27-44db-850c-26d825932625": {"doc_hash": "21716a0a240155d51afe481593f0b2e1e1636efda9b6c06e6c928e0ef64498dd"}, "9789ce45-2c2e-478f-b6aa-e448fd1f89d7": {"doc_hash": "9e28dd364b586cb9a05b7e24305a31b76beb6c0103bd422a50ec7423ee7ac69d"}, "55ee68cb-4244-49e7-b6d0-a9019bd95f3c": {"doc_hash": "0e1fae52a3c86a5fca6cb05482747e76783f9b14d12fcb39892058f1a7fab23b"}, "b55f1c7f-c0bb-43f5-890b-1b4d4c815996": {"doc_hash": "7650ba475b4339fb3a9df26cb0daf5c6a0d30b0e98e711ca0e93c112576a2c2d"}, "7706171d-8a61-4005-8196-ff562817410a": {"doc_hash": "adf6961557ba03ce41226cec063d49cc91bf11c3534ba6140ac247a6a1034f3e"}, "fadc733f-1d1a-49d1-9d24-b20379f32cbb": {"doc_hash": "49cf486d9ab603507082a165a1d2d7804a981fff94bd7dce122f56b121fb99fe"}, "e84242c7-f9ba-4b13-84d3-28d371e8cbe7": {"doc_hash": "90655e4e67fc6fe9832fe06535952d142b0f5bdf21c81c1d805c6d4deeed5948"}, "28e368dd-8028-423a-a280-5c48f639af16": {"doc_hash": "00117ddf49fbacf553433b4debcfb038fb1d72659399cfb58e82b663dd30b30d"}, "a699639e-bca0-4cd5-adb6-417d24d647b0": {"doc_hash": "298ecfe2d450dff08a3548b4090bbf34354d2d528ec2cbd7e4dae436852c580f"}, "6de7b98d-ff13-4fdd-ab73-f114d214d532": {"doc_hash": "1fed88e353d5e154210392cbde5eeee7408131a037a51feb77bc948b3ca52560"}, "d0942196-c0f1-4ea6-9fcc-8b32033e34f0": {"doc_hash": "0c01de6c358476f4d28231290105e7b868c95f208bd9d1b471e2aff15222f1cd"}, "cfeac3a4-8310-4bff-863f-1eab5428c119": {"doc_hash": "6f7cb40f60e3d954a2e3ad9ee63a65db58d2a460a56eb0c780f22964d5f0974e"}, "4679f4f6-fa1c-4b8b-a31c-3aef76b616fb": {"doc_hash": "d5a3ff44a830bea999cc6d39b2562c2ce1770c3b0af858770e4df767f4c8cac1"}, "f3096a67-e85c-4426-9183-9bce6c436400": {"doc_hash": "c54ebbc3d847e03cd2df80eb8c1a5f1a0e5e56b59bde6343ac0d4e1a0354597d"}, "b01295f9-fbd5-4257-81f8-9f8e0e0d9761": {"doc_hash": "4c65084f62fe22ee1ccd5a084b4da9e52d599164bc7d4f1841862217a1c101c2"}, "6a7cf54a-5c1b-47b2-a818-a8e69ad6c359": {"doc_hash": "54fb3bcf9907fd04ce029e8b3bd07c97e35a7676cba8d24d0f64431d67dc6afa"}, "f33f6653-5afd-4ff4-aef0-85f290e93a03": {"doc_hash": "826de5980d031a59fc2e5ed707719d0bceed3604c69714eb589d8e83d6f31263"}, "d374bf29-cec2-436d-9cff-ea1d908c4bee": {"doc_hash": "620b7530e71f3ee7713442c45ca9612695fccf2bc65c76203e50d16e4906ffdb"}, "b2c2a386-64fc-4042-bd59-0a7e627a16ab": {"doc_hash": "de5ca051438a8c6cd1d1eb2ba0f070bc65f34645b9cff59047a63def4e072b02"}, "f4a210c9-ca68-4289-8ba6-61a4979a31a1": {"doc_hash": "c01f2f8a4bbcbf8a211c4f66aebf802e96229a66c316689cc02790921a0592a6"}, "9b2696fa-239a-48e2-b1f3-36374ca3ab48": {"doc_hash": "5f0ac042063cd908d07ac5084b71050930ced9c7c7e75ce9e8929fe5e07f6d9e"}, "73bc6c90-2e8b-4c4b-9735-fadfe29b721e": {"doc_hash": "2072fac4a31fc7665ff31d10ff630cc51703d35a71e85f76987eff946a47e63f"}, "33646249-555b-4a63-88db-42b1698676d3": {"doc_hash": "063426fef8ad3f460790d067e0201d418052c4a24dfacdc96cf1008e42789400"}, "1a519a03-f2c8-4903-9701-574a501fc006": {"doc_hash": "6e383e5ef3872ed07e0931884ce68738dc08d54ab2db93a0dfe53eff70120eb2"}, "c6a1889b-6223-4696-bf0d-fb89b957e063": {"doc_hash": "b310af6c430a72c3d245979cbb0d9ff6cfebca5d7aada8c42272916bbca0706f"}, "cf477d16-c49a-414c-92b7-85d8f83e9791": {"doc_hash": "ca7ed030aca2717d8acf3134ea891b5c01eb339bca46ab3dcb69407b737d6648"}, "6c882f15-c630-4b3f-a944-5637186abcac": {"doc_hash": "3346a8d525f739dd5454375f245a17f4d87bfe6e2d15fe1e4f53b8f490ceb6be"}, "e0934978-9309-40ff-96f4-5834ca9c6eef": {"doc_hash": "20ca4cb6914b5f408c7021c2f652d990c2bddb2b0363724c6cdbdeab77dac80a"}, "93cb5ba1-df53-493d-91bf-add9213e7d5c": {"doc_hash": "59a5adc882ebbf18dc6ce2205772e3c4c234c5f48eeffb47304e9ed83af5d9a8"}, "d1ba9f54-2dd8-4428-b80d-9ec40e9e8ed6": {"doc_hash": "1a440c2f73d763abb17c5ce5dc58f59a591dd2047034faf62a71982d93c7c44a"}, "07904c0b-1395-4ece-9f39-e95cb1463312": {"doc_hash": "b48fc1f4b3f2ff2cb749028e9283bf38e460f7fd41aa862dec3c380aa19f280b"}, "61c47e37-f84a-49d8-9ca2-cce1a57b1e9b": {"doc_hash": "495044425bf22919c69c1359b2fb92de53fb17f0b8f62c9ebbbfc0d4e61f8a70"}, "9521e2d2-600c-45f3-a51e-17d2e0a66468": {"doc_hash": "73dba350894726552f5eea5b371eb282583ff64338cf2c640e0a1451db6e9e4b"}, "3a01a01d-316a-4056-abbe-5c923357eb64": {"doc_hash": "023e16f135c4a8ffca80605d94a45c9a7f065c4c1373d372ade6c925acb22e6c"}, "ec9af7d3-c0ff-4bad-89cd-db63a60b8a37": {"doc_hash": "9bac179451dbb7d9c60c6ff0addfb3285ac2305aec95da08d64c38aebd3f8953"}, "63251683-4b8a-408b-9bb8-2c2476718a2a": {"doc_hash": "3e939303721b9469fc6c1112779f25bc3112f3dac73eede29611b9320e46d96d"}, "91f81465-71ad-4397-8fe4-b85beac190d2": {"doc_hash": "a5d39921cf70cdf68368518f742b50b39ec7965c87bca131f4339432e0bfd363"}, "0e25104d-afc3-4ddb-b038-99e72e084c99": {"doc_hash": "0f9206cf0d1f199b9ead7e71a6c0f1151e30045d2c73677c21d32adf2fc63863"}, "caef289a-0146-450e-8579-d593be74aa61": {"doc_hash": "701ace2d3867f752ecbf30b95e19a8f617b85bf5e9c0d25e983bf92b4998604d"}, "96c46d1c-f132-422c-ba42-ec1cc9d4a37d": {"doc_hash": "e19952c8d59fc66a7b88f5005fd082c338c80441bf93ee91b7d9c54c3723fc3a"}, "254f834a-a45a-4c20-b76a-8dbd1bc5b0e3": {"doc_hash": "cc3f813d9603980095020937eae54414d0cecf1cd4efb0b13852a80827b5304b"}, "e0ec40fb-9a7d-4fd2-924a-7c6b40e7a484": {"doc_hash": "52bc8b92d347070f9f77d8456d9f5274de601bc180ddf05080af67789b42e387"}, "5f38b593-f275-492b-9c6e-13c767e4a895": {"doc_hash": "2992916c6036a0ede557b3c7a4435fb7937de28e7f1da05584048ada050c878b"}, "dcd94cf8-f137-4f97-8fd9-7477d7a91e15": {"doc_hash": "cd343975793d4ee8d1a091ad403213ef344a51d293b2bd2a26b0f6d57101974b"}, "68b9aa22-13f6-494e-b3a8-6335e09829eb": {"doc_hash": "2f1116be3d5707d46d76d029aa3f408782eba0513f4b3fada0aa19a8e635c2d8"}, "0c85331b-8854-4dab-bdf8-3280c407e252": {"doc_hash": "e13517f692eab5fd57b9a312085fa020cbd0c145afd86a398b576c045c09681d"}, "ab439d72-3acb-4a1f-9886-3b8916fa44b3": {"doc_hash": "ce4a4f09489569c353541f493451ccdff3f826992776efcb36a23272080e09f6"}, "f789e0a7-0e28-47ae-ab59-3d0d9f179465": {"doc_hash": "1b48eb1a5f4974a39a267e1a0c1dbb044e003096db6a6f3d2eb56d00291dc469"}, "d1f46888-af6a-478a-9d73-a6bf77e9220b": {"doc_hash": "b925996b4cc2810c719ff7ebe87b1227ef359a8b8814651c9425546b0f8f5db0"}, "450ca569-6d9f-47c5-9a23-8f041807a11c": {"doc_hash": "a945d6cfb5184b7ebe2a7bd400885f96e2625ce9ecd0c35185c0a2ad91df9382"}, "3b6a2885-1200-4887-83db-4313e1e406c1": {"doc_hash": "4d2272d0d9bbf0a7ab0a3e7978e5742b4c4a3163b7d2ac39972ffb85bd58e50e"}, "fc955964-ce62-4dbc-b97c-8ffe8153c8ea": {"doc_hash": "2dfb3e53ddc3515ea241428aec761f9c26ff04189f33b0f4e50d1ce56c4328e7"}, "4f924cb6-2415-4d94-89ca-55397bc16c6a": {"doc_hash": "f1684704a6151b5f0943ab359c242dc9cc1e9dbb48c8691d0af4ebfeac607159"}, "94b14313-2a41-4826-adab-0e98b7b93609": {"doc_hash": "6f9a3e4f40609fbf54aab6d9eb66c2b3fa91505e19172cd7172edef26d01ccf1"}, "cba11a72-fbbc-4353-a8e7-5295c2e84400": {"doc_hash": "229c330f6a177d43735bf7d8f426b820d5b4d67a7fce4cb76285c156836d5ea5"}, "5e83c011-99f1-4220-ba0a-f14481afd823": {"doc_hash": "33a177f2f69fda97c92d59978341423c620ea1665517f19bc1fb7f72c8f64d27"}, "e657479d-ed7e-4588-9e02-315b87303f97": {"doc_hash": "585d3e6d6877518f9aa01e00df210faaff828e2eb2937e63b3939a6218c9eebd"}, "88cf75b4-4341-4b39-add8-71c8b8d85465": {"doc_hash": "cb1787ff8e61d404e9d9b1844b2635ca96b0fa137d0f78adc35b6faa25837f67"}, "18da970b-7bec-43c3-ae49-20589d503edf": {"doc_hash": "9af4d0af3eef045cc039a3415d7745d314ffd871beaee6604d65517cd762206e"}, "e2481236-722f-4a2f-986d-8f9df3fcf96c": {"doc_hash": "de376cf61e02ac46eebd9ca16e903a8c9242810c0b087241e008cc5e29f380ac"}, "1af1e19a-d55f-40a9-b23f-73e4f9fbff71": {"doc_hash": "a92463e0533bcf933f55d938e026eee379f252066aaacd340e1a6c989ccc4254"}, "ee2c0684-bb02-4c30-ae8b-20050ba6dd1d": {"doc_hash": "68a27f03ae0fe62fdb686e3b84c8f14e57c53c3d574038ea018deb5a7cae0eed"}, "1d9dee5a-cb80-437e-800c-51aeedbbee79": {"doc_hash": "c549d61cf0a14cc9e7f8fff5dfb43c2e56d954b36be996e73eafc0c399695f5d"}, "10274fe2-f588-4446-9cc7-85f785c0bb2c": {"doc_hash": "448d5db7ad24f572e57fe77e5205a8e35251c7b462d47dd365323388d2de4727"}, "d4699e30-18f2-48c7-a579-769ec7cae559": {"doc_hash": "c4eec2dc41090c0e392362a25a0b8b925443a35b740b6d248fca8d8009711ea6"}, "9e4faa9c-a637-4b60-af72-fc6e78cff3d9": {"doc_hash": "48cd989e73e0e6009dd91393f854f7077c061894b4c78e5d3ef7e017dac1ca64"}, "d56510d5-914f-41ac-a9b9-2801aef9772f": {"doc_hash": "0ab1f8018f19d136a6bb2764886c0e85a7e009d062e6d5746aec33d7876549e6"}, "c8d32618-dd77-46de-88d9-0b268f7f5b89": {"doc_hash": "06302bfafb3fcee27b7461d6fdb91a1f08fad03503683792e86d516f3eaa4abc"}, "5b07e079-d31d-4270-882e-316e48ee7913": {"doc_hash": "6e308fd6aa4155779eb88aa700280ebe0eac5f76be0c1691ab0c1398da041961"}, "9008f41e-992e-42e7-ac6d-7e25b65e5f43": {"doc_hash": "747d5f746c4b7f040bfff46b17ff7d82c92169db0e69e2acd38107edd1ad2a7f"}, "6bf7cb86-07a3-4796-840a-ce9ae4ef2cda": {"doc_hash": "a61aac690443733b1eb315bd3c8075c8c863c2e2e59855bdfd036d73244040b0"}, "be66d327-b470-4998-8cda-08a4400075a8": {"doc_hash": "ebece465b7ad97eea93b1b9afd55256f88babe8afe13897836c4de824831798e"}, "f5d2b179-533c-412a-9b70-ee3360290f75": {"doc_hash": "f72d8ccf5714bdd35bf2adc97ab2502d2795aa86c6ced08a5fa7e957788a65bc"}, "6578f30c-79e5-4de7-a6fd-cfaeda163f10": {"doc_hash": "3551ce9eb6023691f8d560a29ee3fee7061fae9dc79523c78dddeb47dde4c63c"}, "6e9595d2-b7e6-4fac-89ce-23ae16b31517": {"doc_hash": "fa5a5fe45c4890b4c124035b99f9829afa10bab917cb30243c15fd265dd5d2f0"}, "2fa136e7-1ad9-4a97-8732-fdba735eca2a": {"doc_hash": "dc14a57a6da7a62c0d9c47b1b1f3156ac0d596ff79cd65205d3d8930d86ed3cf"}, "219bc4cb-e111-4b42-90d4-bb088057caff": {"doc_hash": "7e84b8b2ac30b8535a8fc4d613773020cbe265a0931c660c8471d0e33f23ebc0"}, "6e70b6cc-9731-41dd-a673-82eb52d6204a": {"doc_hash": "68f3b436276758cc494d4a32fc1ec51199a2c9930293af29d0b05f1104b124fc"}, "9d115533-87a8-4a49-bcde-d0698f01f95a": {"doc_hash": "9003d0cd9c1f855bceec1013b0ca6acbbe379a9cb20688beda0c2ab989944998"}, "4742d777-1323-4c6c-8a39-d544997464ff": {"doc_hash": "e58993576cc5ebbefc988e977780929c29b03e45072df323b726bc99de288185"}, "0eede7f1-dbdd-4084-a6ae-3edec85ca30c": {"doc_hash": "d53d0d2aa2b8ca2cce09d3e42118b858af234ddde1f6817a89ea1ff75a99deb3"}, "7d29ebef-9e59-4f32-83e3-952018e16349": {"doc_hash": "0989ca6e9fbb2fec97067f9af2bdd5ce11a94a5b8140d0cfa1a4b008a85a8fe4"}, "5d9e1f79-327b-4349-98ae-0909f1d20eaa": {"doc_hash": "ac98c175c6eeac291d3fe497bd9fe4ec454d75382052eed36669e7f29eaccca5"}, "b1c1a9b5-7673-4e63-9261-279a897f5773": {"doc_hash": "0c45e8c7a8d8c780fa9165b14b92aad651e990e59d7f7a294f019ff6cd1bc14f"}, "f7132beb-df11-4b51-8a84-b39cc2c557e3": {"doc_hash": "421affbfde9f3e39a4022a93db8c31e29ede8d6b3b8a63448a86d98b70a3ece8"}, "fe8ebbc1-5be8-46af-b5c8-d9ba47012550": {"doc_hash": "b7949bceaf9df0f1dc9b064d24da1d72bbf7d453295ad3e0b1faeffb62d5c816"}, "20335c3f-0b91-447b-939e-af2e36e22355": {"doc_hash": "f9498d614c927e77c63c2a3d5a5bb2149cd0e1734e2b26eeb6662e07cd8bdf38"}, "fb3cca11-1b34-438b-a98d-8f55202472af": {"doc_hash": "598127a403b6813e440881eed00008a450dfd238a341dcee58537d6565ba5e73"}, "066828ac-6f9d-4493-8586-5ab0ab80fca2": {"doc_hash": "37d297d49a3e87ca9a09ccabaeb8eafab3e3e69ddf39e343fa40d34b3754bd15"}, "a66eadd1-079f-4afc-8483-fbdd56f3312c": {"doc_hash": "ee4b7cc2a1cc8447559f422abdc6b1ab2b9caddb81d7aa1dd766dc3b2178870f"}, "551d0ed5-7629-4421-8a4f-41ce62de51fb": {"doc_hash": "e9fe242a05a0291107e080aa559da2752198526423cca4a6683d5760258e9594"}, "d3a9e826-5924-4523-b678-f5a8926c79ed": {"doc_hash": "86e661d1b7c76751ee2d4fe00f4a3cfc69a963a6acc4fbb949de0a07d41f8a56"}, "d16353fc-f3c9-4b37-9757-265c6993b23e": {"doc_hash": "cf3539c4eea6fdc62d2247ad902c402786b78d1545218d4283df1281e4c18cca"}, "40271db9-e37b-4768-9a86-07238d1bd624": {"doc_hash": "fc0b3c1f362a6b4ee533b3a15d133a5a99ba445f814dc03cf37c8f87efe05dab"}, "9fb68c1d-a2bd-47b8-be16-f8e32ffab8f2": {"doc_hash": "ab3bcf008bcabf6b14b796bff127d0773b19303932ec85928e406d13ab811711"}, "afce9df1-82b9-439a-91c2-585166713ccd": {"doc_hash": "c12264155b28f6303a5e2af7add60ed85d1e219ba9e763c5feb50254abc71b76"}, "385d68ac-6a22-410b-8b61-4f411d2142d2": {"doc_hash": "2153a56a123dc1107889b1c2022b64746f27a7d917491bef9b22fe21c89676de"}, "bb998b89-ef7a-4209-8d93-d4330e4498d2": {"doc_hash": "56ff89392c9e97caaa5b9ffa778f4a5385eb1a790258f8db37187cd8f79bfcab"}, "200df845-02fb-424d-80ab-40a2bffa97f3": {"doc_hash": "57b147d94c035118b54f1c0081cc51603c76dceeeaf823cf173c11b3796a7857"}, "a9af46be-9a60-4040-ac59-7ed70a33a7b2": {"doc_hash": "59d90cde8e46f8a75edfdf2d06015d009701050f4372763709f3b6424eeb9cc2"}, "607f1e33-2605-491b-a23c-b8d81f574279": {"doc_hash": "b1da95bd8030e8b7eafc6fee6a05a3b268697e62ab3a66bc8ea1a8aad240e483"}, "a744686d-1b9a-4f9d-aff9-2de55375798f": {"doc_hash": "969f17ce901dfa1da22cb5b00141c29348a019319f79f9cc0a72a184bfde2cfe"}, "1dcefb27-b1a2-4c71-a453-bfb30aafe29a": {"doc_hash": "38cbaff4a8ded4052c0aab919e6af708317c6f69cffbec235f83e21448786444"}, "864edd93-b1d8-4b2b-b4e9-3213cad3ef49": {"doc_hash": "d889ceb71478f27f510e5007f2c7229f186d5c71ac76ae2cbcec8a35a513ad10"}, "29aee9ab-d5b9-42e1-89b7-4c02e450513c": {"doc_hash": "49ba3da76dff00fae7eebec4d3c4dfa2fa460ef148aa17410061b9f08465c26c"}, "016b6917-a164-4316-b09c-425ed49c2f2c": {"doc_hash": "d531ab8b4e8278dc48a22eb8b522ff52c4de2b18486fd2a227c15935405a0f76"}, "f29676bf-e0a2-465a-9166-f13f8e78abba": {"doc_hash": "b762dd5ce2406d6099a65397b169cace25f6e1ca7c1357732df443b15bad78a7"}, "a69dcd48-f575-4887-b361-345ad010b03c": {"doc_hash": "997d16a63b317468166e7ae7b0e09b39de0bb722a2ac7fa990f20ca1334f5b93"}, "31db41e1-2fad-485c-9e9f-55c0cab8bd8e": {"doc_hash": "48eac30281b105341559a5b430435037169ab2d876bfb5f726622abac6bf3cf8"}, "1042402b-bbcc-407b-a10e-233a280892e3": {"doc_hash": "7b210c3d76c0f78c9db14d25755dc425543064d25556998b66cebc08e643ad3a"}, "6cc5711f-be69-4bd5-ad67-7e64f9cc7552": {"doc_hash": "1232d83eefb186de7167adcaaa462d52cf44d457b68a6be5bde993ea759ce225"}, "defd9b5f-5f24-4de5-aabe-73c2fe626903": {"doc_hash": "9cf1ef06b3310ce78357890df19368ab2ca287a9db90ad938af9379a579b0dc1"}, "e9c09b94-0484-43e5-86ea-a619d09dff35": {"doc_hash": "450f57eea0f6bfb184e71581ca0de4daf36b8a3deed863e6a75f11982aa5105b"}, "e2c03e3b-0410-4789-856b-2fd854d05840": {"doc_hash": "2020e8e82464804368de9ed6fcb99bf715c1d15c86fd08b5be8c62f9b94650d7"}, "1e45ce4f-4856-4da2-8c64-fbeb77e6bf20": {"doc_hash": "760aac54e5dc23de254f491d70e41ca83524b2e0f49e220b7849a0308d71cf2b"}, "73d64630-11d0-4470-8951-80633c25e02b": {"doc_hash": "116e44aee401c902e43ac75679bb7cc0793238bb490fc7badb6a055098bda993"}, "2224b39a-bf57-4926-9b66-eab46b49fcd7": {"doc_hash": "f099aa2300e5646c6adac33e75ab2eb98d7272065c8f6fbfcda934ab6fb5765a"}, "830f7bff-012f-4563-824e-ee7b19174305": {"doc_hash": "b6be63fa1a57cdf1472797cf75f2ebfae518fc290d7273a4f811319460d94419"}, "0c9329fb-05a7-4a0a-b157-6b2e1a6301fa": {"doc_hash": "523c8a799342c6f96c53d10ebec3a94ccc64abe15d68dd3939699065eef9a8c1"}, "5d5e0954-8283-40bf-95f3-4151f71086c5": {"doc_hash": "16ea10e181f6e61ddafece5feef1943b50da813ad427144504e210a17b8b1bb5"}, "ab2c582c-4fcb-44f8-8b1d-b004a4f60230": {"doc_hash": "130567dd9177c11baf593c3e662d092a95272ed9c49725b0155f2f765a1f6ea0"}, "be85e1f2-50de-45b6-abb1-53da29a52fa1": {"doc_hash": "c96ea26ff9a9110cd523c7355f059f716f406d3291760f945e3fea48cef18447"}, "2e3ccea7-4ea6-4b80-85b1-0192b485113c": {"doc_hash": "853dda841eca1d9a1321b9e976c87ce3dc7b9d06428497f316b460976a3db243"}, "d66172d5-5c87-4774-b04b-9d75d2acd12b": {"doc_hash": "26c46ce086ae4acfbd3c5fb78010fd07e8f2b4b2fc9b161d03b94b4aed1aca18"}, "9f4cf67f-16c1-4c00-8f21-148004e01c72": {"doc_hash": "f7515b7823e8f828d2191fb6145384e7be4c88838bc86547de9552cb9c37f210"}, "f54e4bc0-7397-43c5-9a9c-c2c90f82faa9": {"doc_hash": "f06a117e1295bc66ae6c2e80f1711ee1095db0d65cbb8410765b40414691c1b2"}, "a82127f1-11be-4b79-ac97-22056bd3b524": {"doc_hash": "c150436e753eeb133a7e8f0629b42073e30607c3d597649e2c5cf32d428df94c"}, "a398fe64-1b5b-4eb4-8e1d-a57663a2f83f": {"doc_hash": "97572c9f995c4036632661b7960a6795c533e8b40507af9d5b20f5ee607e03db"}, "7dbb6d46-82bb-4b56-aa33-d78c00d7ece2": {"doc_hash": "82ff1c0bd4fea2b3d1f4c5bba023194d9c216610af01a6483f78780ad8036fec"}, "5651aed3-a2a3-4170-866c-3f61cdeb7a63": {"doc_hash": "20fe5fd2cb3133530722a82491b0e262ea75ea16ffc7f6ec75c75b05da3f9c60"}, "798cae7e-23d0-4662-816e-c08708bcdb07": {"doc_hash": "ee8ef931184f338b5028ac06189c32d3232406fc134abdf6bd97a2ec4e1eaf50"}, "890f675c-9b03-48e6-8dfa-faaa19476c18": {"doc_hash": "9de41ceabfb8fa65356260b49f87f552019df1272194746e6d91ea615ff20ea0"}, "cd796fd0-bd66-4d36-bad0-d6092c6791fb": {"doc_hash": "7748b0f857c98567dd2b0ce3ab900eb7d5d91407b26d6d4ef3bac0d9818b9e22"}, "6a044a89-72a3-4676-88e5-e6830a53ce30": {"doc_hash": "2d508ccbeead53d2e54a133a5aae03ed41319baa81f0e61362c1a9aaf1070a70"}, "c043c329-0236-4773-b612-e98b3aacce30": {"doc_hash": "e880cbbef6cdb048ea3eb54cb3ee849a5e318f71d9877f849c9391544e76d984"}, "081ac93b-bc28-46c8-ab99-3d21f40c5b25": {"doc_hash": "84cd6561a88a5193c549c109aff02469ca4d39c7e913dbf98f714ee9f5685817"}, "bf47a3af-25e3-4f13-82b1-e119012a29e3": {"doc_hash": "0a1275231078217e9829d44a53db4013a2e2f46556603385e87df12db03e93cf"}, "1ee68b7f-2c5b-4dc6-8311-460a38e5977c": {"doc_hash": "ea1b12a7b432786d35eab7c09f1e5c0eaa7bfc1ace49b24c9c33de50c87427a0"}, "79586157-f9ae-4ec8-8314-1e7bfe7d5c50": {"doc_hash": "340093cb69a9da782a9479603abfdd2132d7d47045376fbbc69317ab9ebd7420"}, "9ff34026-3b3b-4004-8dad-7c89304e7d83": {"doc_hash": "c044ac6bd664cd21428b5c463426d142cf19af1d57e7e90429630d5093917c6b"}, "8f55f959-7b4f-4e17-a63d-1675484569b3": {"doc_hash": "d5d0bf632b94de4b92c8004947497b1cbb4ec1f7ae54c2b0f994f6f33dde2e93"}, "bcbc1947-aee9-4644-860f-2aa81a9f826d": {"doc_hash": "3050609df59d58b9a78336016aee7271c344db1e70f4305bb06686bbaf54910d"}, "9480d53f-c80b-42a3-8379-cc06459760cf": {"doc_hash": "4b395a72f9f6e92ec18ef165b696a9e6121a0c93b594d1374f8a9989de83c02a"}, "a6767625-e211-40d1-af1c-bcb465387b0f": {"doc_hash": "3a3f98c677fd2abab38e6f59c7f37e2511b8329ac00427c4685a53f1774bb421"}, "14ba5022-b35c-402e-b3d1-c3051c1c3597": {"doc_hash": "cc4dd73b2a73c188aacabd8779de5aba61a13020441485ff661f28a62cd974ae"}, "d0ab9f07-aa64-4025-b27f-055690ac78de": {"doc_hash": "5290b59890e3da04e42ebfd50fa7149d9b4efd5e8e851042739ebc77a2efc684"}, "b9f6313e-d215-4074-b111-872baac079ca": {"doc_hash": "eac4532ac9d4d9ea83b8fd1a7d6d520f0afe06f236dbe85223dc54abf49077b8"}, "b3468217-d6e9-4c03-b2b4-589b7ef1b3db": {"doc_hash": "180378df904567ff6dd2e0540ead3c543b52521d91a95fdac9d5d944904e33cd"}, "eb3f896c-a985-4cea-9e1f-a097a70bf163": {"doc_hash": "81712ef9705b7ef7d660f0678db6f807c8b642fb80be0d2974c2eab64ab2fea6"}, "0b4b7c84-2096-4124-8fea-b55407a030f7": {"doc_hash": "a6b6ab8c7ed37999622840cbf9b45bd336f6e561e9398821f71b9f9b29b1889d"}, "2cf87c05-9062-4a49-8c5a-f3b4c730098a": {"doc_hash": "b2873e9432892721c4baa650d7f192795fcbfd7da1535df96720c4a1132b9a47"}, "3bb34d36-b517-472c-9f1b-553bd41084ac": {"doc_hash": "a251e1526d6f7d9008682da6a8cf7fca7b9f726be15552aeb94e16398fb5d167"}, "1df2bddc-fcf9-47a8-95a1-abc273c6aaa7": {"doc_hash": "5eb01b4592a4753c33462265e508c968d3e70b27e8e53e2ac88cf9340eff1884"}, "42a3a1f4-72f2-4ba0-a117-29b27c3bf0f6": {"doc_hash": "02721c783b78c3ce7ce6177ee32c8c9c3c46e3faf39de6a682979a22f36750d7"}, "476a6102-fafe-4a30-a441-61d1d688233a": {"doc_hash": "da4de8177306533092bc114f41b049fafae5b0b2c372bee5fbc40baa135a672d"}, "46cbdadf-bf54-4e41-a3d5-9b23f1ae87ce": {"doc_hash": "c982d719a649378f1fe92db6399ae56d7f082929758f8b38fb140310a96d57c7"}, "0737b0b9-8d92-4191-ba95-1486ef0ba9d2": {"doc_hash": "7a4fdf00b7e4b08b150a2088265c1553577b6582dc3347f245fb6afa7d35985e"}, "c24ad952-0f84-445b-aac9-5d1a63fd9a4d": {"doc_hash": "392f83f7cc5c8b22942bc1f2ce1d4f8041fde3a0a7f97b34363cd55abfa765bb"}, "2236f0eb-2457-449a-a621-f767bac4099b": {"doc_hash": "ec1c00a44089a1fb90b0b2680225d8301b5162e608ae4a3a2010ff512a58e601"}, "4f8fee51-edb8-4586-abf9-6df2181f773b": {"doc_hash": "394f6294d42bcc84e3cbf4296b27403c6bbfc072942b12ea700858d9195cbfd9"}, "fd63404f-c603-47d4-9230-c8b6f2009b14": {"doc_hash": "4d5bc4b96eab9112ed0b86fede4cf8c06b1bd612e8655fe88b67ac5524241b11"}, "896a69bc-3255-4a14-9393-63518a7b9693": {"doc_hash": "9907f14e4abe5c6270d89d7359f1ebc40036ebeeda53acbad05657bd6080e074"}, "646b85ff-6473-4fbf-84f9-709631949fa5": {"doc_hash": "3d92e8952e84f52f7566b761c65c9ca910ea18ecfbf7191fd30181bca3c4aef3"}, "a29d51f8-8480-48b6-91f6-7840d05d0343": {"doc_hash": "28ca52985e69c91acd78387645cddfd9e4afaadcbd0d5de395e053f9f637a44c"}, "bd967916-493b-46fc-b39d-e9d4041bd6c7": {"doc_hash": "795ff3f868037dab48bf79e99801bba53a05425e8955b90d1da4e7023634161f"}, "f1fc008e-65cf-4c40-b84d-509f5ca6d561": {"doc_hash": "3531b39cfd0e56e31df0d4a28ef759391522c5f5f3573e3989efd0b74429596f"}, "633c1e8d-115b-4768-ade8-e94550ad85ce": {"doc_hash": "9f1f8387ec1ad992e3adf2f1c9ddeeb062aacca213208aba447dc3127521c772"}, "567e0dc5-5d02-4532-89fe-b97ba870ab92": {"doc_hash": "256c4eefa62a4e810a793c0ce89b6f04e1ae51d683473efffdbef06ea41b0353"}, "58a8000e-a877-4b02-9119-46cc25b53a20": {"doc_hash": "37865c4d0c524490d61f704fe625d483d6fecd9674159bce06206aa9e1ec48cf"}, "c3769e97-8951-465f-a04f-14d1296e1c02": {"doc_hash": "b5e7cfa73894b14ff2e3d8c86b4404a89647a3d637c0df2d0d0e6158d62b174c"}, "86f48024-32fe-41d4-992f-18d4fcfc9b94": {"doc_hash": "9f8e5d142d95158080c9f9341a47b8b11d67381ec4ce0324043183e15a3dfa94"}, "c8227386-cadc-4d1d-a2d0-721ddcfb0fe0": {"doc_hash": "710d7e697f76422302fa9acefa88be281db783e2c922425a47430e672ffa5e2e"}, "4cd4b3cd-2e73-487e-929e-abe9947605bb": {"doc_hash": "5b3a271921db8777ef810a24c3c259faac54869a1c955a7963813601ad2f6423"}, "390b40e6-70d1-49a7-800e-d2330f7716c7": {"doc_hash": "3fb3033d9b51c433961beb0cdc4c9ed222618f3f3a921b59152402c90bbaf785"}, "243aafdc-431b-47e3-bd00-1033a755c66d": {"doc_hash": "6d642c6bd8d4661e763d1e9b6e115cebc2e11e6a17db8d58b0f76e35943501c0"}, "765a9e98-524c-48ed-aae1-2548635bd96a": {"doc_hash": "466b85d44958b1419fefc9d3b82284b4d98b91ef727cc880d41b6f5f4ddd7ff3"}, "c1c14f6f-9881-4305-8e47-fffafdf986a7": {"doc_hash": "06410292f70743add8f7e5e1a5a672d933654b49362c46f4767e33033e281532"}, "907c869d-6af4-400b-a7e2-8902a75233ef": {"doc_hash": "4178e5a562f76249d5a1e5464c30676f5a02fe18c6ad7d092da43eb1c9d2ab69"}, "76565ab8-6625-49bc-b791-b8217bd0dde6": {"doc_hash": "a67f7146b6c22dbebd0d332d30e65fd79043cca08fb4c8a5677271c1ee21ff8b"}, "3e5ee014-1bc0-4736-a3bc-09a7810bc635": {"doc_hash": "489f6ffc0983e4530be78081dc0b79905fd9b8a42a53256f2751f5e736e29c38"}, "683f59d0-02c0-4b64-9db2-44bf0799aaf6": {"doc_hash": "78252e63c3248017ce2978400764f152793cb4bf435743057bbed051559a525b"}, "6f7a0035-bd79-4070-8cbe-58855794f9d7": {"doc_hash": "3a9bf8d31faa8a5709b2d4569fff2b270a3ac5b4dd85d3eeec37f08a09f9109c"}, "9aadcb44-f588-4a6f-b607-186acb4c376d": {"doc_hash": "15f714b5c421463c1d97f2612c0fd8fabc5ad9d7611d973ddd96707a00a2d505"}, "cd890bb9-9be8-40fc-a6b8-f335e77bfe44": {"doc_hash": "b7c2a181e5c47d8f890aeb184cd4677e7271b8bf8d227ecb23a1d8331292e0c1"}, "9a353861-0601-4f06-b39c-57dabd68afa0": {"doc_hash": "7310f7f6e9c3ddbe0b327cc47322deaf140b3f1d82c06f23425176e7fe534f5b"}, "44cbd1a7-7346-4ff7-a99d-920665f9c0c4": {"doc_hash": "7eb06ab85d1a478346942b81e46366040401adb7105d301a25bd324e292f0673"}, "1dd2a4d5-7dfd-4099-bc2d-7415cc52d2ec": {"doc_hash": "9474bc3502d312baf82addca02327eedceca6e648277a3af3d629415c3f7903b"}, "515e522d-cb4d-461b-8716-ebc1f88fc1fc": {"doc_hash": "0463a71f6bd6803e5a09a1b1c34a715fdc59cec21a2fd5e53fd1f0bf0975fd50"}, "c7d9801e-b41b-4bb0-b55c-3b6f41645198": {"doc_hash": "3b67aa1848c5ba202535e8da12cc5d40e82d616ff1ac4de069a0e140b16c1acf"}, "ad9de203-421b-4667-8507-5dbde850858c": {"doc_hash": "9e6e1ba7882d331f561cb612c9ebbdb3747c0dde270c26aef033aa66c62584ef"}, "ed82fd56-ca7e-43e4-9ab3-9c44657e6ea2": {"doc_hash": "752ee09434f0b71f314c7fc4b1b13c84b7acd1b6c9be1e08027da44eaac61b06"}, "0491b36f-d847-4dd7-a9c3-58a8f2f61a89": {"doc_hash": "be769dec39e5f8467245ba3bf2580f27cd03255f3fc2e782866425edd1301797"}, "51d5fa42-8f2c-491d-8e22-1735a0e5455e": {"doc_hash": "0ca08223fec45706eefcc607d600630952cfeb11c038424bfc6d7bf3097530b6"}, "eae95560-c74c-4697-9694-e3ff2e326153": {"doc_hash": "ecf21ba94ffc040e22e7b710b31e7d67e2216bd81435d76621e44512362c75c0"}, "ea044575-f8a7-4b4b-829d-9477ae6e577f": {"doc_hash": "2e358fe2a2fa4f6d0156520b0082ade824612fb36a7a406dd7eb96b93e9c5f79"}, "15f7c676-5147-48af-aec6-20fad312d9d1": {"doc_hash": "ceb8eb9571fc5459bcaa1cf4941fc1a1418ffa4be54b5c6f2ae7835c7389da83"}, "84f5c419-ad8a-4b08-8346-075f0c1a199f": {"doc_hash": "2d97892ecd7e7ba0b26c289068589a080c00d0205435c977203be63c113b543a"}, "2cca7d5d-5e82-4eb3-99d4-b6e8293cb28e": {"doc_hash": "ad3a44ddf290d21219de8501c4e1d9b406a0aa155dbda9f2b16325d12fd32665"}, "b2b8b953-f39b-4755-bfd3-4368039bedc9": {"doc_hash": "344325ba2401ef946f3b57b8163cbeb07becc295bb13b6eba222b775bd378129"}, "1ec923d1-3aae-400a-8605-4f388253f05c": {"doc_hash": "02087869016e38ad98b8daa782d7b022234783be6e282b44d418acaf67781bdf"}, "d09f6409-d387-4d39-a3d0-a5338ac8566e": {"doc_hash": "b281e93e86930d84cc86ff704062a9516cde58c18665a22f2c631b7ab9297c91"}, "119ca212-8a8e-478e-b555-3ac829c68b3d": {"doc_hash": "2b8452a68809bb6cc9924a94e801337501cc67df308053f09d679809b90e7aa8"}, "430adb4b-bec2-4541-bfac-186e84f3dcc5": {"doc_hash": "c9a5a8d6b5ccd6cdbde5badca5d5a54e8cd60bc105ec39fb1caec3f1329fda54"}, "dc830c54-edde-4c6c-9f3a-cfeb0515e07b": {"doc_hash": "a5086d481162956d3351d524a8b3badbbc4bb3e7a2ad3299beb370ef14284b76"}, "b6c05785-88af-499d-bba4-5818ebced598": {"doc_hash": "73e89c72fb19c7d91bbd9d5a779a075de3da446efdf3c75117d884bdc0fe62ed"}, "b1506c16-4cac-401a-8980-028809d6ebdd": {"doc_hash": "31a690b6ae2db765293ea2ebf8c803d19672b23918ab711cc75c068913e46a74"}, "c4159930-e9bc-4684-9f2d-df448e8b3142": {"doc_hash": "0fbe3db782f19d78db73064425b61b6d7e2ce3d81ff6475a5129e2232948c868"}, "744c407b-0a6c-467c-bbda-66f0f68ea438": {"doc_hash": "930330cf86177acde292f0c0975e0ebab295e88c829444307609d7137ecba4d1"}, "15893b6e-9588-4d95-8e60-3c0ade826740": {"doc_hash": "8ced53a9bac64621f48d36f00c11cf61f4a93c55f199b3aefb7bfd665d69b747"}, "45c18173-4b74-4433-abd3-d5342778adab": {"doc_hash": "77a1d3d1e98e53f17be1eb22c5006bbaff12d539a70cb00af376f34c149a7a27"}, "05bbdacd-b517-4cba-8e22-b263bfb39b04": {"doc_hash": "c545a9a3d6ac09e259d789b36e9e82d1de04e6ed475d5ca596d31592d2c0b4e6"}, "9353f800-dccf-4804-81c6-4a7e0b728082": {"doc_hash": "1889c6494967c161696d4023d124904905a359481d5afddc4f5042f56e3f5651"}, "3cf4579f-765a-48d9-b24f-84257403da46": {"doc_hash": "2559f8b41ea7c4c42addedc8b72e0e90f36e127011943a60e1a9ae1982da9069"}, "890d97e6-77c5-4b56-93b5-8dc5c23be368": {"doc_hash": "219e4250a5a02d8e190012e214b90221502cd0526362242340ba989c6c6f0b98"}, "587c35bf-d49e-453b-bd1d-101813bc3803": {"doc_hash": "36448b7ff669af5d9178d6019c7cc2d30eeb7b4a9750ac6d29306156b654bc46"}, "65ba714e-d0b1-40a8-b862-70dada25177e": {"doc_hash": "e9100f30748797b3bd434deefbcc9c8257362f1b6bdc544139e45269e5d4d9e3"}, "7780c01d-32d4-4b9e-b885-cde44be1fe18": {"doc_hash": "e9c93e89b933fbd7870e1c6624d5ffd8ef83122e292f947cbe02129dc310e59a"}, "16e17059-437f-4dc2-86a6-e56c5bec37e6": {"doc_hash": "e8260b4b806f56eb064dd36d1b9a977dfabaf2d66c26deb33f282b0b88354e01"}, "364ea9bd-3a81-4c52-9a02-8d00ad590cb0": {"doc_hash": "b3211600978d55645c5f2562c21c8f0db047d385fed0ebcd5c1b5e0fc73174b8"}, "24e42174-6850-4ffa-880e-2838eeede7c7": {"doc_hash": "0e0143ba6c0ca40599f73ce6aa6dc6487727bf44b31a6808abba279674c7c0a5"}, "8850da52-324e-4616-9b75-40111487b6e4": {"doc_hash": "82e942ed212b7fbd7ada956d1f3e9d574a56db3dcec02d349a317497813f0d59"}, "67a22e51-2b6d-442b-be97-c90f437a1f3f": {"doc_hash": "37b0b67987af508afad6832b3f58009cc91276ccaa59785540b8a89bbe060cd5"}, "620bc0bb-832c-483a-99ec-7851e6154b33": {"doc_hash": "9e6adc02c10b99eae78290ba9c595b674b3ac7f6c1b66ed7ea26817d9c14195b"}, "7f1341be-80db-4de4-bff7-636d6c7cd6fc": {"doc_hash": "a2390b30a776989674d37c75dffcd2d5e885e628846ec04b91346203d47e8888"}, "f1a841b7-bd18-40ad-9be6-310d9c74470a": {"doc_hash": "6a3f53ee722267863b984905ad88ef2be9d163fd5907a318449eac748a988c27"}, "d51c34ff-d274-434d-a6b6-109da4fff6ae": {"doc_hash": "29c532059e53dd4e02d42d66d449c3b904c78403d26687211643d80c04c1fa2a"}, "2ef67385-f61d-47e6-87bd-29fc4c775555": {"doc_hash": "53ba486ef092b753f627c5aec960617b13eb94f0688c804f87f9f5f80240fc9e"}, "27f77c74-98cd-4841-8e75-0482ac84a378": {"doc_hash": "b05578733aa2f8d6cff4a68975b6097a049985b54a4d4425402fa326347da4ec"}, "26394e48-a8b2-469d-bd93-f350850ce6e4": {"doc_hash": "df5d470a0e9510d407a9799f113bdb1407d66049ff1cd37014e581a2d0927b7c"}, "dc93ac9b-5e8f-40fb-8223-a1bfcc3478e7": {"doc_hash": "d26b125b13734501be95db882989bd18d310c9ba4743be5ff8e4e7dba5f414a9"}, "20522f53-d63a-488f-9e79-8b40bfc00b25": {"doc_hash": "a27d97b23b06eeb94c6685ed12c5024fc61fc0dacb0c75a813b6c317f6803b24"}, "835f5ae4-f5d1-48f3-a9d5-fd1464fcfca9": {"doc_hash": "953336e4f1b623710b9ab0c8ec92ad1771cd56ff0143c265a16e3f4c1ad9d223"}, "d6cabf4f-e2cf-4834-8c91-54fba7ddb801": {"doc_hash": "cb54dfbaad0f868094dbda5a274014bcdfeb6b1fb0c5ed05f5b623b207347505"}, "78c4bf7a-6919-42ae-ade7-f5478d491901": {"doc_hash": "4e5c26aea978c0865a1398c4c0754a97382d44056abff23c9cfb26ea7b7d6b73"}, "38af870f-8f73-4f26-b2b3-a615dcac6c2b": {"doc_hash": "4a1a9813ca6fd88c92cd10f5042551f2b3ea87aa2c592f33e0f2001c38ef3b9d"}, "26ba1eb2-5253-4e94-aace-aef0d559431b": {"doc_hash": "0ff10e25b11950ee8220c4ffd7e723f156e54c73165770db5a611378010cf854"}, "2fd6f813-a6fa-4055-9eee-c05395734085": {"doc_hash": "f114c21a514d68f17b74dffe4fa8f8d5c0742b514b02d7dd0724214e2e8c7a89"}, "9eb559bb-ca03-4dc1-9501-f02b7c9c169d": {"doc_hash": "3a42bf95e727a3a8a6b4636a36556fde79c8010926c428ca76c590809e58b5bc"}, "1732a6f8-01c1-4218-b4f3-99d11cbe957e": {"doc_hash": "f191277827ce79d895ff56a5268e59a8d6df6d12385a4895ee9b83e5de277559"}, "507ccc98-5fd9-47ca-ac07-25d22df75645": {"doc_hash": "7530dfceb7fdef2215125c723908b28d8ca3cf25df36cebc958d0fb100b04c1a"}, "af1b8f7a-4bdf-4406-b185-4b44a6ccb98a": {"doc_hash": "fd7f8d6716aab2c6fe79e798b292435c2f6f940ee2e969fba716d0d4fe00eb2d"}, "c9f3c7a6-ddd1-47dc-8859-a70e4a4b2be9": {"doc_hash": "8155a34c3b71da35e238b867a9507311f952777733d262fbe5d37543647ab40f"}, "4dda1b94-0c0d-4778-98a9-201045855444": {"doc_hash": "b6185536542cb3501e59e944a305dae0631342008b7c7e1207c8cb8b0e25681d"}, "2469be29-7572-4342-bb35-7cd3db8c8fbf": {"doc_hash": "02a31395e8be123834d9b55cf0512aa39a92e31f9e0754cc032364ba7693ced7"}, "1d072a0b-ba58-4673-8370-1fed05b9976b": {"doc_hash": "d62eb83c3c54c51669a612f1ab150c4c44f8b85a64fc2509196b8aa4143bbf2f"}, "0edd96fd-f414-486e-a804-a331afee648c": {"doc_hash": "5b419963e2cf42ea10ba7dd65976e654071a7db5cf100279a5dd827f264e64ad"}, "a56e6206-a570-4259-b7be-2abce519d5b0": {"doc_hash": "b306db5ca0e28e77203b70b6ed5b744ee8a9bc6c0fbd553ef9850c9995646003"}, "3b063e1e-4251-447a-99b0-7879a71304ac": {"doc_hash": "709499ac380a95190d6f9b431e183070b5bbcbd0727c2af9ea51c79e0e636b6b"}, "a0832e99-ac1e-4648-a4ef-e3c891af5858": {"doc_hash": "6c6da734f670852292000e5e2dbcfc509fa4bb7d09b9800110fcabd4834c0341"}, "19804c2d-938c-4f07-abdc-f027568795fb": {"doc_hash": "95bd616d25e84a1f736b735a6c34a1f9c68fa4c5cfd1a7cbc922a26ce489ebfa"}, "26880900-60fc-497e-ba27-1ea178ebade4": {"doc_hash": "dedd4c983d527801110d15911635467a1b07bd339f640a0d153102d8923b13ba"}, "9df74982-31d5-4727-a3a2-d2727fc481e9": {"doc_hash": "80eb87c2a89158353844e3381c7f3f422ffb356900c19c9975fad06ba6ce3f09"}, "2f63fe20-0c8e-4263-902f-dfbe65a01215": {"doc_hash": "04d6e253c0a7584fb5db38946bc42938d6c581876ee9f7705dc13e229b209e62"}, "e579ddb6-8214-4a51-9332-5ef11f1d8a63": {"doc_hash": "d457641f58b764e81a40088b92422d41e299686def12ce1bf88452f1b8302a90"}, "212411d6-20b1-464b-9afd-f7eca501b16d": {"doc_hash": "db93ae28129b2ad2fc004df57680dd4e07bd1c5fa99dae17000923f1c4b5d933"}, "24e9b62a-5dec-4e3d-8c08-de41e64ae5f5": {"doc_hash": "65e087565dac6cff768bc8178fc8aeff08bc3e10b85c91b164b6985e1e29f663"}, "2c83ff71-abea-42b8-a0f1-746e89ce16a1": {"doc_hash": "e3c94d1b55065368d4b88c47404a5cf73696a65080b4d65a3521af657e652a49"}, "87b341da-065a-483a-a356-e7f7c6a64394": {"doc_hash": "589f7b99a29ddc1b7e3358e690fbeff9824aa86fb983ff43fa3bd24f820bb1d6"}, "544eee7c-ffc6-4d3b-8b7b-ad2d93fbd41b": {"doc_hash": "a74f3b46db2982241f6f389839d7271f8ae53e214bca7b71c270a220ed577074"}, "2887db5d-bcf8-48e5-84c9-5a90aa2e0bad": {"doc_hash": "eab545a9e0657dff0ad7e0577e398a57e42c4b6db4f937d5e7ae53ba1f957142"}, "5c85362d-aad5-4e70-8c34-bf5ad5a43e05": {"doc_hash": "0c729256e51d101fb2ece7062d4a2d80ce79ccedd1c0acc5c46bb9c408cbe46a"}, "9e3fceca-e435-43ec-93ed-9ae6e3f9f9f0": {"doc_hash": "e4f1b13a7b50d7ea99a45da03d0808c5f454479fb57d6e0f342af7d432f027c4"}, "859c6b72-a850-4e6a-82c1-88f7c26b1ea0": {"doc_hash": "3d940f428aa96630602eee810fdf0746d25fd429fa9da9de5f233288685a693d"}, "307c75f8-8385-40d1-83c1-ca74f5093f70": {"doc_hash": "c41ede649d9055a3139acb09e8980a686a1e6e36ddf2f80f6a4ffeac2d8f6fa4"}, "75e9e62b-1d76-48d5-9964-53d572d0924f": {"doc_hash": "89da3ec63955c9290eec2b5c15c528b169f9642d402d179bba7cb612dfe2d150"}, "c76e44ec-7081-45d2-bebd-c2e5317f9c34": {"doc_hash": "703784217290c98de873511d8d0ce514d726d5c506149f2b7a00a5b3434fb6ef"}, "8d57d29f-acc1-4b6d-b012-d95f788a42b7": {"doc_hash": "80099b66151c9a93c961fd7fbb538f7d72dbd0457efe0f14e8a86181b2cc8c8b"}, "e4bd2813-2a5e-476b-940d-e368a7c7a507": {"doc_hash": "9a92ae2ed628d2e3fac6a9ac52cad76487cc28661835f09d0159df6e74152a0b"}, "fd36819f-b150-4935-8771-0538b9592b9b": {"doc_hash": "b48e2073667345b0199ec7a57ba5b6e99f0e61e98a52cca6faaf670ae6dddd1f"}, "1424a284-25c9-4e1e-9aca-046d0fa037e5": {"doc_hash": "e2b8f6fa0ae11eca3ada18d5d3558f55133d940004b62d79a8379c47567afa22"}, "b450e992-5a88-4002-b156-6276ca33f482": {"doc_hash": "bd92b291f7f2f37390d87222289531e3349b1cc3e4f121c23985c297769d7c5c"}, "17e872b1-0bcd-42e6-9180-c671fb9402ab": {"doc_hash": "c699fa6db2958797f275953e1b78622d5885fec8d56f6f0155120c02df31d409"}, "0f8cd6a3-733b-4aef-bf06-55d41d0ebb3d": {"doc_hash": "954db1bd563429c3d4cc7c32d84915d0471238dd236953bc38678094a802f87a"}, "3a86abe0-878d-4dab-b123-24b3e4e41de4": {"doc_hash": "feacc7255de591afbfc1e8564f3cd2a8c58941e96deea6d54ae5b038be7f8725"}, "8789bf31-3874-4aff-9f1d-e93894d2f417": {"doc_hash": "4b2e1567d73fe7b620ab93359c5e589e7e10634fba191de6a68858da94bb13e7"}, "0f724f9a-261a-45dc-868f-4fb6ce8d4b12": {"doc_hash": "34826069f1c363b35c30855d9a879d05f640419e7a39ea2929521a12a0e0aea9"}, "b6742c90-e8dd-4046-b535-899db847730e": {"doc_hash": "d5d45aa0ea8833c7c9b277f902b7ace8f5ec2ceb94fc60f673afc26279fe0e29"}, "a853a1be-9ead-4e6d-8faa-19baf84a27b4": {"doc_hash": "e8afeb2dd4a2cd542f35469f2836f1d89f60dfc289a6af0292d085fdb309268e"}, "14112a47-fb6f-4de7-b1ee-d396a86d0133": {"doc_hash": "5830f15703e2fad89c0b926cce5a545d92285b5102bcff6ba737518650cde315"}, "66270d69-ecd3-4a9e-8ab0-b0be00805298": {"doc_hash": "4edeabfd16f68e76d9bc089d43e60daa2bb7a7bfdb25ddc7677edf2f3270ff45"}, "045d5eb6-8866-4894-9a1b-58923350de99": {"doc_hash": "075040ba3acea3dd36f4290f213cf7194533525c270f98cda24813439906d8ba"}, "017e18ec-3413-4de4-b398-f372140c8c70": {"doc_hash": "7641c967001e2f25e6c0c31595cf641aa0035c31bfd58d861618b7b5d0a1d4c8"}, "70b0aac5-bae7-416a-a552-31ae317f7130": {"doc_hash": "b456560b558cebcf69d4340bb336e60f2695e9f9395783fd589764eaa48e64f3"}, "66b6c0f4-7420-44e9-a216-4ee86ed8772a": {"doc_hash": "85acde8f5ff8757afee09e425765def839415ec65161bcb5a9080b4c724beb0a"}, "5ed33ea0-a67f-4b43-ba36-9245588c1218": {"doc_hash": "42a92bb82b80fe408e9226b8e709261e374dbd249bf16ce10c04502b761129de"}, "fa3ffb3e-e43d-4cf6-86da-90269b57abe2": {"doc_hash": "7978978f98aefdfff6d432c3f3367859f38a6e9484e9b91a68ef8903c2820bb7"}, "45f33713-b6aa-4935-a278-d1a8f94b7a86": {"doc_hash": "82cab7fdf35d7599b5ef19b869f97bc4d963143a96f786007b14895223b82dcd"}, "468dbc05-4ddf-4c75-a967-3fd1d3c4a3b7": {"doc_hash": "33e2eab26a0b822e303705f119ddf2f5f4553275485a6440427cd9230ff79179"}, "17ed5d61-6115-4619-ad62-d83f972d4364": {"doc_hash": "9c6ba6f27fecab8807f65a8f0ce2c2cf6b5a96da14537904d731cb4f1d1315b8"}, "1d478bcf-346a-4f4c-befd-66ccde15f541": {"doc_hash": "470ea01bb19552c7d01c15fc1cac192979d99463ba0b3145647408f67ffd181c"}, "332edaba-6fc6-45f0-97cd-58fab188a06d": {"doc_hash": "39c7bf034c8b28abc87eea50ede258592e3b671a8a0d64791cf5deae4914da72"}, "38e8a702-13cb-4772-b4dc-c87cf9faed3f": {"doc_hash": "eec2a187b64f12552c29f10c7b2926c5705349a608724b3095ca812d4bcb23de"}, "cc49a2ad-5616-4ea4-97de-2711e179ac7c": {"doc_hash": "5acf0ebd8ceabdb0e71f62106f0c8e75c71b309c40730016d9f0b86a65c2abeb"}, "e757524d-4f28-45b0-9838-4cca885d8c50": {"doc_hash": "363840265af16199dc228636ce0e631740094180f4d4cbc2bc7e42822777ea7a"}, "ceb9deb8-479f-476b-ab84-1b940b71e966": {"doc_hash": "23687d03d6688b0630874cc94b3ce341b9521a02f5443cbf532cd2c11622312a"}, "3b57089e-e78f-464a-8192-4bb3fcec53ab": {"doc_hash": "b290c0883ca99427a7bce5f1adaeb69a7bb09b205f896e7ec53bdbf1bafb8dc2"}, "0fa3631d-408b-4c3e-b940-357df38437ef": {"doc_hash": "8a7cdac76912eb0f5c91a70568967361b1adc31058b22313df82aa40f3b8ee51"}, "ac183fc0-5bad-410d-9e2e-3a168f217883": {"doc_hash": "576cbf5ee7ddb01f171640a37966839b33be94325b98371aa96c78bafd3d7134"}, "8e2ec2e5-1475-4ce7-af5c-cf243f8e56b3": {"doc_hash": "083eb8f42f8f3dd423c51f51bcb842e5428a118958d0d63bba086858c1ecd74e"}, "de96f9f4-c76c-49fd-bb9b-9889bf521be9": {"doc_hash": "bca2237d968d778fd49363db06ab2714efa37ebfa96d28d364944229f25ee1cc"}, "cd6e8b1f-a414-49bc-bf58-61f9ef93468b": {"doc_hash": "6e7f8fc66eae2a9828acf50b098d6c31fc562250257ff08e54d038f0c86660fd"}, "139abc9b-b189-4da5-983b-4102e9712266": {"doc_hash": "c6d2517aa272174c58ae056d0fa49996c85d1293f9eb1037f75dcbd51e91d90d"}, "e68ce08a-c3ae-41ab-bc5d-d0d2513130c8": {"doc_hash": "0ba8a091311ab0396d012596bc38de1ef3d8f10d618ebbe02ea19ca346079d44"}, "7f4b7344-c284-4406-8be1-4a68d79537ab": {"doc_hash": "a455a60d61d6d351e642e8483402e00de1fdb267e7f52208ca14758f6fcc2d5d"}, "7c365f04-87d9-448b-8255-403363fd00ef": {"doc_hash": "a83aaf35f56462599b9c9681598061d55526e8e76f471eb9bfa122b7f8a548e9"}, "dd49bda7-f961-472d-bb42-8b127c3103bf": {"doc_hash": "aaf93aa6b6e24812affe18ffff679a31717f13d55d8de83d84bf9b269295fccd"}, "1953d877-8a1e-4b1b-b308-ab5674c29ff3": {"doc_hash": "f5cbcb83d61bf816110704ab9b3fb91c410ed48390392766ecf5a4b6b8db4022"}, "1f2579c0-fe51-4401-8f2b-58cc64ff6c23": {"doc_hash": "3d7810bebfaa51db685d3c93a0611a78667756e86f0b7968b7e4e06008cccffc"}, "cfab5cc6-7a03-4d26-968a-6d8c016655c7": {"doc_hash": "8b70af1852e852cd78d442d251dd5bc7c9f508579c19fdf4a1c22dc98800c798"}, "c9af52b2-d04f-4895-83e1-21ac849d1688": {"doc_hash": "89bd511d57f327baa1aa87fd2a7a95e680a81bf2265fa8375056009c77ac326a"}, "426cde84-425e-4b61-99e9-023042bbdd19": {"doc_hash": "dde33cac57f1d43b3ae34edab677559547a0921ae62847d5e9da40d30ee4f5aa"}, "516e1e8f-6204-437f-991a-62c2cb08575d": {"doc_hash": "4da1cdb4f2805ef2082399ced3cbf6fe04d2dec95090485847b80459ba3a3f1e"}, "67e5da83-7725-4af6-a4c1-049aed2f8151": {"doc_hash": "2fb4ac5f118b627f27f74cd1afb24ebf8e1e34d8923197f44a5d0ac906a8f0f2"}, "e3cf8860-12a1-4ebe-8c91-5087a2184271": {"doc_hash": "753a23c0c2dec2effbfddfd181f770c2fbc593ee9a120a0cc730e96465329fa8"}, "a7fd15b6-c60a-438c-a6c9-e72f78887474": {"doc_hash": "74dd800366f9cb554cf9536adbea8dee69280557b06a732863dcbb0a88af5bdc"}, "f41b4ef6-160b-43a3-832f-453466286a7b": {"doc_hash": "3724fe21391eebdf000c7f77ef12dc23217b42c15ce70b89827dc53aac532788"}, "5657d12b-0a6a-47c5-b788-f0e2307bdc50": {"doc_hash": "93923462b27dcde9ae2cf410efddd2f210506d13c8d87e4669fa8327f6d99ab7"}, "f0047b98-d84e-4550-ac43-6a50499d5610": {"doc_hash": "6f815c09f0afd3cd827ecbf990d0ffdde84d4e9b57d733db82a579272c7f7e6f"}, "d9ba1f8c-c362-4d03-9377-4a31f2b61b7c": {"doc_hash": "6b3f01ae12a042e5c127e17358dc82bdab393f01430ca79fe54a206e0fbe1a91"}, "a9c76e60-c040-4486-9465-08123f8aa914": {"doc_hash": "e4dbce6891907bb349697578e59baa65e7a1bfd25e22e4b7b2b307295a5d3c9e"}, "ec7ede54-4847-4bae-9dc5-590886236728": {"doc_hash": "efa764b9656a4cd126ceb2f54a1ca4957812fb0ee71fdfd24eafc45ca1dde389"}, "0625b5ef-df5b-4459-a8d7-913314f6d6b8": {"doc_hash": "ad46841c76b337bce949e422748efe8444a70b0e893dbef0f8627159a0e1313d"}, "e32cebb8-df3b-49fa-82a9-3c4700c241fc": {"doc_hash": "912ad0575a6d86d98792dd2cebc18d674023d434d5728b68f68371b657c86879"}, "6a268334-8417-4230-95d0-3bd574353cca": {"doc_hash": "8f343657a2d62a463923d55614e6042064b6cd3cac94c54b43a3491e501f9df2"}, "448358b8-c6d4-4253-9ec9-79e3f79da6df": {"doc_hash": "f716552491eb6743e97994b96c850093a22d8c33416db370d9804a034fa653eb"}, "db93a9b8-712c-4c28-947d-25cd3b704785": {"doc_hash": "b3f927119627ead562fa3b9fffbd7bd7a30f7de0fd98b56a074c29e66f96e1a7"}, "2244ed92-81c4-4379-a67e-377749487e92": {"doc_hash": "4bfb08a437ee8368d25b40dd1c567900c1130b8700d0fbdde00d5890a03e188c"}, "b7aa6b92-9474-47de-89ee-f40f48c31bb0": {"doc_hash": "49de3197af80453f68df09384a47d813f279ca6be5218896f93bb22ea1d016b6"}, "50cf0158-ef10-401b-ba59-5729777cdfef": {"doc_hash": "1bfdfe032d32b3185cf076ed7f54819e7888d2dde46e86296c06bfb2346d14f0"}, "186458d7-dbad-4a6e-a6a6-e6ac1949dd0a": {"doc_hash": "6da59c999a273ff50c1f45eae752a5999a8c2c53a90ddcb0428e9b9b9463a6c4"}, "d9476070-4add-4a0c-9c6d-80b1e3f6ade4": {"doc_hash": "504663995cdef60be89ea4135591d4997a5065dd24c5bc5829df1da81164d94e"}, "826de6dd-2bc5-4667-9e86-98f47b9db291": {"doc_hash": "07e7007fdfc94f592d3c207f634d3bff3b4e4c21d015065ac9f6cb8bad6b9e65"}, "0fc3d9ed-ea21-4eb6-a4ce-13eeb347714f": {"doc_hash": "bede2f24331abe3cf2b6e6303b9014f3dfbec5312413a9ba4ca3cae509333de4"}, "ad54a376-98d7-4605-8568-c5baa0b63502": {"doc_hash": "a5aedb9aadcda012bb2e3b0a5cf9a91581c9c593dd33aaf7f3d23f284320e8bc"}, "e6a52ceb-fbdf-4c17-87ec-424a832afab4": {"doc_hash": "03cf50a8e44c348faf1c46296e041a65043beb642cb2d0b24d104dab44918ca0"}, "8a4dba34-81d2-43f2-8785-c1faf3602d18": {"doc_hash": "001e2fa1dd4f6ac9daa9d0d1ed97b18c2911438c4006d9983ab09c1f841098cf"}, "48bb094f-fbf1-4362-85cd-3dc306d3905e": {"doc_hash": "326c8794d65923350c0809d9fc993b34d600eea6db56984931f9accfbd83c37d"}, "37e60bff-86b3-4035-9306-6324a11ab1d4": {"doc_hash": "f2916e3f979826af8682ff68c9597535569942810f01d30576ab20eac04df362"}, "508e586f-4ed6-4ad7-bca8-6914181a85ee": {"doc_hash": "b422aa094be38edee280d1bdf59b43a2c6dffcc77d765cb071faad857b9ed346"}, "4aaa9779-ea69-4ec3-b054-8befabc9664d": {"doc_hash": "ec5d57134db330e2b003f7cfd61a5ad2dcc394f636282033c432ab106ad14667"}, "e8ccf932-3def-4315-983f-c35f7a8be0b1": {"doc_hash": "44aed356b57ec2710ef9350cca4e8a3c8d8c3e0626b0e4247020696cc17f2827"}, "b17d234b-3de3-4989-9f6c-21e935cbaffd": {"doc_hash": "f3ad92c2f5cdebfdbdde8444d661061bced5ac166155cdf17c3ad11cb11e69bc"}, "5c92d7a3-073d-4feb-b557-efdafb37d046": {"doc_hash": "77bc97a0986fe7f1d7c2230e957bfd892e43941bc151dd0dbfba510d4b4640f1"}, "fe38761d-cdcd-4257-999c-78d7448f087c": {"doc_hash": "99cb26c395be2c3cfa989d12aad77d83b48122162cbe092c66dc2b0c0e212c69"}, "ac932970-4b56-4655-8609-fcad93ff4662": {"doc_hash": "113ef7c48334773cf20b8cb0e1ce7ca030c6c82c7811a87f04e7fac010421c74"}, "b864ceb3-a0dd-4467-8acf-6f7b3e6d0f81": {"doc_hash": "e3c7d491b5ab0ff77f570f49cd56bf9af99988e97e125b64405054712efbfda1"}, "da6bdb74-491c-4e3d-9c6c-94a00ec7ca51": {"doc_hash": "da0bf040d609e47b0accf07cb0fd311f47ebf9ca4f52eb01083395eb8639ec85"}, "ee113b5d-88b8-4936-8585-e0ca52f133f5": {"doc_hash": "888f3338055afc18e1913901e1a7219972880b2118a75d79042d96317d952b2e"}, "8d5b8c89-77d8-46de-b6e8-4d36809e55bb": {"doc_hash": "ad2e3e19cc84a488dd327fd20b41b92eec4b1916addb796cc6bd233fc4e5ccca"}, "b794af8a-d3da-4f99-a825-497c8d7c8895": {"doc_hash": "ff5a358effb75b139dcfa47bb7051eec884ee4a00eba1ed979a54adbfa6af565"}, "eddb19a5-9349-4aca-afeb-d4184a87cf76": {"doc_hash": "2c5ac3aff039d062c67ef8481abc4480aeb71caddb7f0cdedfc399fd02cb1c56"}, "c7d62a26-3881-4daa-a410-139ba68ebbe5": {"doc_hash": "1efb1e8020fa80365271c9357c226fb73a0d2cfbbcbbae7192cd5b7977720e24"}, "fc6b2b00-0c49-4eb6-8516-3ee5ef7950bd": {"doc_hash": "058f7dca99dcc20b399a4f76f92d1546871900563d7d674bc7e105364ecc4443"}, "b1c69d01-cee3-43fe-bac1-f6a736c9653b": {"doc_hash": "ebd7ae0923deed352ec580b7a22af366b1b737c2f775273f76829dc0ccea51e2"}, "9b724465-a69c-4c19-b8c7-d2b9c33329ea": {"doc_hash": "0cdf165f12f8de6502a5ee62eaf31adc31e06428f32895bd9078713f87c8758e"}, "a1c726c9-27a8-46ab-8672-52c69a2b5b76": {"doc_hash": "1d9384eff52c70cc089d65df30ac1eef9dadd10ddd0634cd47f5c3579524cf12"}, "7274af2d-02a5-4f48-a3a7-0d7b7050b6a3": {"doc_hash": "c6ce819fd903ac5b0980c8567570a160f7cbc04ec904b61afa413d5670900b2d"}, "78abfc42-d53c-4bec-bd06-ed1412506650": {"doc_hash": "6242185c22e889a93d1c653533b45c5c7490e01a3889b98d99677476ca403bf4"}, "5bc8225c-2bb1-4d0d-a78e-36abfda4fc71": {"doc_hash": "0507b9b3168b5d99b2234ed74a096fb6a480596f48ec281d6034c537099d4ee0"}, "8e26418c-902e-4c72-aa2d-02d45af7e4c8": {"doc_hash": "a66882ceb4f8b7924c5750e26e0a8048755fb330f28a49dba075630a500c1e3a"}, "5679fb97-eb15-4f02-bb8d-7c4628433ecb": {"doc_hash": "65b99933f6878c0a57fc8d73025f00ae01d178a07f605fd5a76509ea35e9ef99"}, "de9c11ea-b086-4ac9-9dce-68e826004ffe": {"doc_hash": "07ff0b95f33eba542394a7b13ef46af9ca169951f23417bff650ce9c1a89a54b"}, "35482ce9-2813-4334-aeb4-ed2a6cd74a16": {"doc_hash": "8c64b77e9d144ccedfd586d40389988d139b2761868a608e3c105a3397a38cf3"}, "509661a8-4ef8-495d-bbd9-be671f4a0185": {"doc_hash": "1eced8778df561fc3fd53bcef1a194c5ce71acde288d3fde0f7ce0cbfd4d4746"}, "f621cc47-5312-418c-b270-1151e347880d": {"doc_hash": "9d8a9f9a0e14bab80981e2cf88d4f8a3700426041434f3b530ad27b1ce7c43f9"}, "63e2ed12-14f8-480e-ae4a-2f100494ac8b": {"doc_hash": "dab7dee573bdc2e664832e3db12dc55c557ccd5caec761c8ab67cee01ac8e17b"}, "68700776-4698-41dd-b88f-c3d7b77893be": {"doc_hash": "628d1bd2181b23291731a107b235433c02407ff8232e32582424ce35540ed583"}, "04719050-31d9-4f6f-b79f-cdf83f9eb875": {"doc_hash": "90c6f9c6c528601d0189c39aa2ad65bf68679b1ed57335fee5fd8f480a127b6a"}, "45604538-8ec0-4b22-8efe-e581291900fd": {"doc_hash": "4a11240852c05b75ff0e10551041809ccfc9146fda946f4ec50e548bbc15d8bb"}, "7706c506-7aab-4131-b8fd-7cc38fb6d9b9": {"doc_hash": "bc398e83d48269f482513b3272d775e239966e711c5bb1dfceee143215476504"}, "6b770969-928a-4557-91cc-3b75cf385d6c": {"doc_hash": "cc9766e702723f1caed402b89b647d8c4127e5775c99af72eea32fc4e6851b70"}, "1615bf7e-867c-4039-9a78-65cbede6fa37": {"doc_hash": "c4cc4c9610e822b4a76e0f9f0fbd5c0ce173b66994f33caa066d471e0b048da4"}, "e5bc341a-b7d8-4df2-b0bf-2b6f04d8114d": {"doc_hash": "6352cdf8171f2e18a2d62e91c73fb374eac865dcd633584351a5dbc668bf10dd"}, "690ad7bd-6c9c-4e1d-a8ef-3911b671bf0a": {"doc_hash": "c2cd9c90302b4e2299b99ef56eea0244024451eb2f1b7dae05914c991141d52e"}, "fac7fc97-7b4d-4e5f-88ba-33534d065572": {"doc_hash": "de63025aba993188dc8d5dc954a32776f7520817f0ac4e333f2a2be3fdb9c09d"}, "c4bf5345-77f6-43e9-b79d-21a790aa1ebd": {"doc_hash": "bcc1e9236f05decc456f9c2563533609fe0d01b85c6a82f7c3f8c053068c23fd"}, "ce46e09e-73ba-438d-b8bf-95a85710187a": {"doc_hash": "83045cbd2aa971ce5883155bac7ad9f1904fcc2db11aefd56ab2ea72590153ba"}, "37c84395-2870-4773-8d1e-8e0bd03df537": {"doc_hash": "63898d3283ce547158f443b1016f1fca3cf41fe7fb83af245f96bbc1c6b88e70"}, "b706f7a6-6818-471b-8e59-a68955d784a0": {"doc_hash": "a371b06d5522e7364e2c84a19b2440101b3a6f6dfd6c6dd28f52a8963147fae7"}, "73bd739d-87cc-4c9a-a1db-05fdd5fedaf2": {"doc_hash": "34bcfcfbc62d3792c832d3ccebd3fc4d1cb23fde3e8842606f4dc7f4d2e3ba50"}, "edd683e8-36dd-437d-b3d1-210e8e63af37": {"doc_hash": "5a1703f6c29197e6465bef6bb8b999a02885affcaa6dc71a069c189fdfa007a1"}, "c2dba794-2510-4452-b49c-c8653cc43b56": {"doc_hash": "6a523074d23b5aa4699090ef41f618a8e007edb4cc26abb3afe98fe91c14d135"}, "d8d02010-ef0d-4d79-a340-710361d24a40": {"doc_hash": "c22073ce3e1932361cd703d48024055ba4440a17b63715868b2e3395562e83e7"}, "6f6cfdc5-d3c9-47f6-9435-b1066c34f7ce": {"doc_hash": "29cb082ff5e4c2bb4cf01d3fbddd8f1c19dcb4e7821d696596923830a97d84a2"}, "98f68874-6f42-498c-9919-9a2cd3809441": {"doc_hash": "fdc0226da651949bc5b2138e0d0b9f623d769244c8d9fe07dde134d45bb5277b"}, "b8b14827-cf2c-4046-ac29-ad251910988d": {"doc_hash": "74ce644fda97fe25c370386d769b9fa3721fef5120eda66a9994d551e7870090"}, "5ac5b7a1-2d51-4b10-95df-633045b4f0e5": {"doc_hash": "15a170525e61dac95658b5d0ef9f2d691bc9801ff94b163b98a752824889cfc4"}, "9c6af9c4-15a0-4449-9e92-5d81a4dad94e": {"doc_hash": "4fa7ed4197a60379cbb17e2a2315dce164214bfc3bc32904a21bba448be5be17"}, "fd2a8623-68d2-4d97-96cb-8a119015e1ae": {"doc_hash": "949ee6ce814da9833f038ce2a4d16014d32b3353fdc89a04686866ef91de4010"}, "d5ca777d-e9f3-4fe7-9439-d1199975d918": {"doc_hash": "36527b36ff46df7b75df9f82d0c78d3c5c33de60406e7032ba26a23c00d749a2"}, "7b022788-9596-4a0f-8a8c-a1b63a64704b": {"doc_hash": "750abd1ecc6f7a924dcca6d8ca49ed53043ef317fb004ab3183e52fa7649232b"}, "68b29de0-926b-462c-b958-f686ee61e541": {"doc_hash": "3145de1b06e903b5582a11b137945559625a30f880c54dcfd95f567e30ce39ef"}, "c82822c6-d9d4-4013-8ebd-a1bb97c792ca": {"doc_hash": "d553a98d41c8edbfc6894c8805424f37f5b9b8ef7f9a668dc1fc8efde2676a54"}, "7e7f80ed-82dc-4ca0-b969-b4240c770c45": {"doc_hash": "d9c75a7a9416c3671c9c2d455a98b095999f155c9834980ea090f46327fa62ac"}, "f4485722-3856-4f6d-ae7f-46363560e2fb": {"doc_hash": "3f339a6de51fd1efb0fcc51b00a88ab9e7de79ae3c3634890a3cbddec091dbb5"}, "1f605cfc-0e3e-4581-aaee-17390f20445a": {"doc_hash": "0882b7ffbe3aadd1cbb6c08c086afe5461c9c761a4e83ab626800f86b898bb91"}, "27b568b7-1fc0-46f2-bf96-8650bda62a52": {"doc_hash": "54347e55841bf74339c03d9bf16e03d9986718a8bae17d40d4346335c2659b8e"}, "f72d1ce6-1097-4635-9923-744aa6d44144": {"doc_hash": "14d2d0b7db6644e6e204249bcd856e59dc5ce322e398fb24b8c37b1a514fff99"}, "794f82f1-88db-47ab-b47c-42079e9292cd": {"doc_hash": "c4f638baaa430f754a23444aa186af40dc617509c8dcad246207f7ba27f81a32"}, "81d8bfe7-6bc6-4eec-a784-5f6d073b01ae": {"doc_hash": "86ef680b056f89ceb98d1093c974a486793d04df28d2938023728c5e6deb8c0b"}, "a45869e4-360d-451d-8b34-6ead2327eb4f": {"doc_hash": "e471ccc1d998cac56ccbe7c63e50b7224190436c38a843badd228729bb55f25d"}, "7e1838c6-de90-41ec-bce8-046b2fade43d": {"doc_hash": "5e71e7828f9d6ca2542a7441778d6a4540563488896860169b051bc46ba61970"}, "02bdeffd-9cf4-4f2d-9bbb-641e7ba856f6": {"doc_hash": "21db92751d01feed0da2e300acb2eb2afa793e4bfb5ab7de2252166600e66ef7"}, "29efb7e4-d9ea-4eb8-bcfb-285aa7fa0115": {"doc_hash": "0fb6f69c086790aa36be1af025dc5348e72622c903589f872a0e0f9d03cc847f"}, "16eff6d4-b728-40cb-9d17-5492a4d3db27": {"doc_hash": "75135b56feb6094b76ed53bd88d59a158ff58431a686f6b88467de96b2fe00cd"}, "2b197b28-e3f9-45ed-bced-90d2a1d64e87": {"doc_hash": "a4ba771e87e38fd216e66fc23a969b2fee02ef516b0cfb580de4010967a52cb3"}, "568eab24-2c2a-4de1-af32-850ff3baba0b": {"doc_hash": "ee54dd1704432de150a84539b0e4d2f0a55731f04b8b7a6f02fb4be3d57bda89"}, "e6e6c0c9-4fd3-4aa7-9ddc-a997aa5a136a": {"doc_hash": "ab62b1986ef05711395b7c80493290496803faba8203b335670f4d4d14e62d74"}, "7d3978ba-fbef-479f-8fb9-f735eb335f0f": {"doc_hash": "5614611257882709bad5fb00689b8f1e84480eeb2fb923a2182af6d961bf8a9b"}, "7ee356cf-f18d-4658-be1d-3eecd39517a0": {"doc_hash": "6d9955092f0afd7429099428ce7fbd9458f17a5f1086ff71c3f7f84acd502365"}, "62d9fc53-f44c-4af1-9459-75ac958c1f4a": {"doc_hash": "a175f885f21f38f21e7d9cd27906213e24a8d96a1567cec54d65c3457dd7011c"}, "9d8bcc7d-99ce-4225-b038-e4d772436273": {"doc_hash": "62d4c2e0de32b51234f3ec354abb26c82ee1ae421c2bc7cafdd2075c780788e8"}, "f52bea60-2849-4f3c-bd0b-75b27307e96f": {"doc_hash": "f0e67ff5835e869520a00937729a44a7f958085e1e6fd19e7aca67fe1ad0c552"}, "f5385cf2-6400-4ad0-930e-d3759c81f28f": {"doc_hash": "193a7c268d7e7868a5404f1502b564e91e0ace95632618d3417f78028659010b"}, "a1038074-afb3-4d44-86d9-0bd27efe70e2": {"doc_hash": "ca05582ef66d7b4dbab83d2563063d944210eda9cc5592cec1fdbb54d8923de9"}, "6542d896-6b73-473b-a9ec-7253daf085bf": {"doc_hash": "4f59d11b1c4be6086b72e04e9a78aabd7310fcadd21c018c0d3755b6440b871e"}, "b85c8ed2-ac2f-493d-8710-e2bf7fe1c5db": {"doc_hash": "044ecdd8d8adc20f21afe28530319b2eb0e1a4db6e099998a94db81bffacf860"}, "afc6bf40-77f4-485f-b417-a6df5031ca7b": {"doc_hash": "6b9f424faa689f472e3a43f7298a77f44632323da4be7bf815b0e4e9e5dc5190"}, "9a32da5a-e4b0-43f8-97e0-7c0a2cf4a8e6": {"doc_hash": "f96e5aae772373fbcb5ea07e76ec29369f3d93d47639b0b1ca67862d4fb09482"}, "c3285bfd-8ddb-4c86-af0e-d32bd0d6c98d": {"doc_hash": "b8fb7b10921392fb10e01e3f2eefccc371344ebe4aeb20a0e63b97de579d0fec"}, "5201ede0-37a0-4b28-9a30-6a619a089421": {"doc_hash": "3f1d8ccb52701b81b6cb29f5fb16164baaddfef8458e4f068cbe291b9797d03d"}, "02f2515f-45a9-49ea-8ccc-9abfa8f60b54": {"doc_hash": "e7d371c2e2273077f80021deab97e923b35edd1292907a16745b1a9935b96d49"}, "cd571c8b-3d36-42b6-977d-a6763e091468": {"doc_hash": "77b44a08182da788593e6efd28b45058488a1d4d213168a83e823bf8108be159"}, "6d92b267-b467-4321-8bf1-7a35a3dd2511": {"doc_hash": "6e39d6945506e6ffd9d401d814684765321f90aea51bbce2de29832b2371a3c2"}, "4adbe36d-d504-4a00-bfc9-7edef0d7580b": {"doc_hash": "246a05f7dc6f4103f827badcaf33a4259ac25cf0d75ef1c745478272d2f47a95"}, "7079f221-2dd2-40e8-b574-f0ba821db952": {"doc_hash": "75c8546a215b67cec7e17009eb671af69ea4e890b0eabc14d8a4d0438d76c8c4"}, "91d3978d-23ed-4116-921c-a8045e65fd37": {"doc_hash": "ba612094df0713f0a6951b1554e42282448a3fa8d28c9113da2ad0e6f8af6e26"}, "727a5a3b-4138-41ae-911b-743af3f007bd": {"doc_hash": "948a4a86dec0262b358bd6254da5a98fe80ba7edcecea97290bca156092acda6"}, "0dda1ba1-7dc8-4a51-bdd0-373731d9da18": {"doc_hash": "66a514b02bc91c2deec1196ac34b24b3020dd3a6140f7a31835dfbe4d74f1301"}, "46e6e8bb-f018-4f19-be2d-74344601ef18": {"doc_hash": "0eb0404456ff56d1011984aeecb0027b83be647cf61596b6a911e610624e60eb"}, "dec8a953-54e9-4870-8a75-ca890ceae60c": {"doc_hash": "ce01ac741ba9b990c59b6397a49e9f42ad6d8fa893580940eb496789c6b3b1eb"}, "ae1446e9-16aa-4ed9-bcd8-3662ba9e36a8": {"doc_hash": "8d63e5927a09b165f4c19d3504d02a6a7921779b63e0c94c719105e456f94103"}, "5ff1bc0f-9812-4b4f-9e0d-d1504c786d94": {"doc_hash": "9eaa2f72bebd1b8411416fba8a589694489451daa0aa4c26a99e5591cf57a1e7"}, "23ca34a3-6f46-4ffb-8aab-b38b43d770cc": {"doc_hash": "3db66786bb80f33d6e6f4917e8603547e42810793c1285138e7c882c50f684c7"}, "44292833-a1c8-4799-b97e-bed602ff59c5": {"doc_hash": "5acce0ffa6d87998dbc3dbba2aafb3892373dc1304822638de7640fba1e2d43b"}, "d1878d8d-8751-48aa-9f6d-4b42cf115000": {"doc_hash": "faf85df3e636d1ca8acdfb76d502b4162c61017337e2f3e62ed59c91d370c6f6"}, "a2bb13cc-55a1-482b-a220-e4038c6857cf": {"doc_hash": "31c9af26c6dc2c9a335773b31563076d6335456ba5b4b17fb6544104be1b4898"}, "4a4aea1f-f7ab-428a-8644-31751fda865e": {"doc_hash": "d58c7aef7bb7906482aed3b4475710e6115c70185ca48be69f91fcd6c84dac44"}, "262c00e1-9ce0-455b-a193-6dbe6b572095": {"doc_hash": "c95ed356326711567e38db8b61ea1c101fe4ce2f10a3772f9b46c4a40703d62c"}, "359bd711-5c5d-4d8d-8daa-b337116a5316": {"doc_hash": "76029cab683774c95148716caaa69d35171a0f003b2d03b31a378e3f5ac6e3d9"}, "d41e905d-f1c5-45a5-8149-92963e0f5e0b": {"doc_hash": "728826b4ea3e301c092dd0241b7955ce07b950a8fc5827161e6d14f65cba34a5"}, "edf96e66-a450-4cfe-8470-b59f90525f58": {"doc_hash": "79110be5aa410fda2ba248412101ed692e7fa046051235d929423f21efe11c15"}, "3b4a7024-577d-48f9-b074-3a56bfe3f453": {"doc_hash": "90aedc90c8ed17a9961508b42820a62aa9d6c84094b5f98d020d3bce096445c9"}, "fb4b9232-e0fd-4ca7-bf8c-3b5550024f88": {"doc_hash": "d663ad90706fecd983f50af73feac5d9704b80c8d6b752e34bdb73979d476a55"}, "07556098-21e0-4d69-9ee8-34a69a059a42": {"doc_hash": "a270fef6cd748d053b2e83acd42a24272bd6a8fa21d275feb1f49fe981915e0d"}, "1aa3c3c3-b099-4721-9ede-af0e77295c21": {"doc_hash": "24f91d75fb58824154ee9b6835599dafb58a60bf74dcf6662030f47caa984310"}, "933df950-0564-4583-98e1-16f9361c55f3": {"doc_hash": "19d431574614c70c66d4fbba1221674316e921ca2dbab5b8d4c175c6e3526578"}, "be88b7d8-26c4-4c50-aca0-baae8e319692": {"doc_hash": "f5d3bacfd5985fe5d14b129b6ba375b93d8ef83b2a1399052948bbffe52cbc2a"}, "6167ee3f-81a2-4b5d-8fa1-486501fd4fc7": {"doc_hash": "2fffa77046580ecf36e48efcf8d4d93c75723dc87ce52444a39224c440b7b5e0"}, "bd59607a-61ff-4d76-be0e-acf057d46d33": {"doc_hash": "bc8cf08ddfc0d1bd38e0c36bb4f8b5607ced42cd9860fb2a52780464311f4a44"}, "53bfea96-22e7-447a-a2a5-ea7f4db3fb44": {"doc_hash": "43526046997cf2dd79a3657aa23196763ae87d480e2e2a47e500eca29b5f081f"}, "27479ff1-2c3e-4715-b37b-914fc1a1e01e": {"doc_hash": "a17b0d567dd5b4a071e9669146474f34f5e8e065a3ce15284fb7b976f9c30f94"}, "966f48ab-c8e6-45d3-9941-45aa91d8449f": {"doc_hash": "335083d9e96390971925ed825182d27ee9e264bb7b156dd3050de0abc98517b9"}, "264c24cc-2c56-4f38-a1dd-96b261b31ad6": {"doc_hash": "e77459679316af92b3ca2ce0d10a134c0675d9df32dd97c8fc6bff2aa1658175"}, "49e51f27-125c-41f4-ac7b-fc8997c40b9d": {"doc_hash": "b48cf6552491ea3ef732a66b44da0e6b09725e07fe930fc6f67395ce8eced02a"}, "a980f5a8-1f58-4c64-b94d-4a51beb0c9cc": {"doc_hash": "c6a75358f657082811c526763692c11b38087e6247b87db950b3e8514aa076e6"}, "ca7ee332-c272-494a-8d1a-a4fbb41d91c4": {"doc_hash": "9642db9ac8497dc46e132a6d3f120cdca305c91ee8d68d105156f715bf34fb3f"}, "55a84ef4-aeba-4c8b-a610-78cf28f6a425": {"doc_hash": "2b682ff981b86771dbec261e8c1e7fbb66d12e301995c8e475c70d447b443c7c"}, "d146a532-2f3c-49dc-85d3-b25e34d6a592": {"doc_hash": "58b9aa90a3f6ad100732dda0b215d0e6346776950d587a5164119bce9c2c62ff"}, "1fe4e962-cd2d-4960-ab8e-f097c1d203c4": {"doc_hash": "bd43e7ab02bb58616363eefedad88c97dda04433ac5ec93a7209b9ecda6a7b51"}, "31484385-bc46-421c-9c38-baeae26bd2b4": {"doc_hash": "d4d60760e1341d5e0d99fcdfe779468efacc11bed58142d40636d2b5ec35be72"}, "31601c6f-72a7-41be-be94-382d9fc8b16f": {"doc_hash": "10779f461aac69ff81fc81f7a77adc06b5830510861f2e544f2213927b1f7edd"}, "3d794c4e-8b37-4952-b371-8471e23c54cc": {"doc_hash": "0e72653f11d3e15c77c617cce27e3cc57c1485420d260153821de0ff9119b3c0"}, "6cec1e5e-b4cb-4c62-aca7-10654095f57a": {"doc_hash": "f9abcfa16535d7c72ab32c78fb807b7702aa030d2bcd4c63d908cc74f558b263"}, "95884c02-e73a-45dd-a107-37f379deb437": {"doc_hash": "ecbbc7016f6120dbc3e44978c6c376968db8967f14da8f60e5dd3835835ba281"}, "abfce39d-eb99-4e71-88a0-ceb5722cef89": {"doc_hash": "a16b1de259b82572c5f08611151d7ba738d2218736a0b58abaf832d450a0e100"}, "b8296b80-18fe-4a90-a4e6-0d05ab4481e6": {"doc_hash": "fead4917e9638b6f967726e44549cc1ce809741dab7e6d3d16bf747dc04ab81d"}, "de5c0c3b-b9b7-4b29-b428-67b61e26490a": {"doc_hash": "ef59ffc2e5c921ec25175cc6aca17efd102c7c1523b6ffa066f7f4e8247bc34a"}, "2d4e2e04-ae67-4b1e-9fd0-6de65f487f74": {"doc_hash": "4d6462cafe3f9ebcaa06d1ac1a552d04c48fb7027f58fc5d6ee57dc4b0298db4"}, "f47de9d4-04d6-4492-8f9e-fada8643d096": {"doc_hash": "1bd0713c256b6dce77da9885ec8f5e671433c3754186533f6306a1bcad4a514a"}, "448cb869-e223-48a6-9466-f465b6af6234": {"doc_hash": "5207b83108523771fa57dad5655afc69c4ab2b601bf0a59533059223d454a967"}, "6c8b6da6-1589-449e-ad61-3103f05c0bc9": {"doc_hash": "6e68015906bd3e9136fe35e5a4db3294178b3dd5074efb372f699e655e988cd0"}, "ddf90a3c-417a-4f55-9e18-7c210bd23161": {"doc_hash": "d251088e555db0ab7322cf89526fe5e4f9b0326780e0e08704551212cb47754a"}, "927e5ccc-9654-4efa-b67d-336e58e045a0": {"doc_hash": "5a845cefc2aec2d8a42ebb4921a7d66a5154f2aead6c0a3c8bf72265e0545b9d"}, "adff3466-6057-4fc7-af37-89d64ecc8d78": {"doc_hash": "c2db4014d0c1927812e08c4f3110fe30a20596d5a57b245e8107ba9611a85dc5"}, "cf288c58-6c63-41c0-b8e9-5923b61e79e7": {"doc_hash": "b53e7447d7f57000b8b2135568b41e900bc3fdd018feb85e7ee56cf43444f3d1"}, "ad17cb25-0e69-4f5a-a218-697c597f70db": {"doc_hash": "23cba1fc556d191a63da13e23ac070f59f7a80715ee17850f070c9b3c19aa962"}, "ee24f0f3-22e0-4115-ba51-42484dc95d81": {"doc_hash": "6e9e85b404f08493dfd6739ef2830c27038d5c857f44e56f03ec899ede2fca91"}, "ee8ce06d-e9fb-4f43-87c2-e25b8f16142a": {"doc_hash": "f453ada22bc3d7c41ee8bc21c052f4f0a77372a7d8997cf39a7920a9c0f2cc39"}, "f6433652-fc14-40b7-ae20-cf5bd618af00": {"doc_hash": "453a52ff8e52fe646a538259c27021a0bb8229a7a4ac05051d4e5ea995104f24"}, "7b466cfa-d99a-46a1-83ee-6d3c084760f2": {"doc_hash": "d0a6756446bbabfc61cb46e20da96a031561d20dddc8378d137bd50bb384609a"}, "2c05c809-20e5-4c2c-8f55-119d4bc65ddf": {"doc_hash": "fd0834fdd1790b069359fb921008a62bd9b064187af69dc46e8e5486171caa86"}, "cad1de93-88ca-4e08-8174-8fb3f4ac4477": {"doc_hash": "972f049eff383c776f6a2f42d175761b78bafdead608c751e035caef8b34ff37"}, "75b5f002-9500-448f-9455-87f3442f42ed": {"doc_hash": "de8eaa39ea60583f18448eeeb7bb4bfe232e85e58c7da2f357a3ef3a701ebbc9"}, "5155a425-bbba-4e0d-b81c-bd73509a22e9": {"doc_hash": "b368f0a0a02a64f177698913efb6544f81574afa2b1c124b9056680983836e05"}, "c9b3c287-44aa-4e4f-b1b2-e4708b5f00d5": {"doc_hash": "839304d5147c44c65e3ed0ba1755af0b87815314fe46dcfb4a6a1adb842d3f82"}, "e2d42b5d-8155-42e0-a13b-da6a6d12d2e3": {"doc_hash": "02c25eb40b55aca228d639d9e444c629856bba92b4ebd4e6539d39fe2fc70e6a"}, "cb244484-4e89-46d9-843c-d9357ffe6105": {"doc_hash": "c3c933a67387c97cdbbb5c7fa4dcd57debeab92845a38a4444830e288e518550"}, "a91fe56b-a7a9-48b3-9dda-24383d6d1ff2": {"doc_hash": "43ae52504862d55b7d3e77f5f7134da86ac401f4b65b460401257be61b02cbcd"}, "2ead6c59-9aad-4c3c-853b-5977a71b2f6a": {"doc_hash": "9553833f18559f2abc0c10eab1233914d2e9246800427cce90884e4d63f70a5a"}, "16e4b73a-02cb-45de-b908-9428488e8b02": {"doc_hash": "0a46739da1b5140e43dfb3ce0b44239d4c690b3202068d5117ca73869db4e4e1"}, "2b5e44a5-e136-438a-b707-13c9677b9d52": {"doc_hash": "7e442ce0c8e3b9e812d488cf6c85b3dcffd0babadd8cf9fdd6573fdd2bdb8b00"}, "3000855a-a40f-42dd-b782-0e2f718f848d": {"doc_hash": "6b0e5a904da95e7d01bc1fc161fcab3689cfab499b654e1d5ea52f8dad79053c"}, "38742736-69f8-4178-929a-21d9f6fcb7c7": {"doc_hash": "80f701e555788677bc669fa8f4c61723ce938f23327f716b4152220490a7160f"}, "42e697c4-af99-42a5-be6f-8b715043aeaf": {"doc_hash": "919dd89eab7038b5a3a5f2329ab46c4d1d430b98193c40460f804c03bd11edf8"}, "2c34cf53-539e-4fe6-bc12-8744b8ab45b1": {"doc_hash": "3aab5143b17a9b288501f573ab2f46e011c010457374aac529a3c19d4bf01b85"}, "4ba56652-f7fa-436e-bac1-929f49062fb6": {"doc_hash": "0c891520d8fbed6288c7953c4708ba891dcbcc0fb52340bf8f0da7240d20d926"}, "8291bf50-0ba7-49a6-8c13-6c06e12e8af6": {"doc_hash": "22292e08b62d693334de1ce910247bf2ff52b2d1baf4360d8b87cf4d53fa7c0c"}, "df2bc6c0-5550-4a18-98d1-ca0f8acd5c09": {"doc_hash": "5f22ac09d293e01ff5557ad26ca09413fc32318b7da30764a11fd651c19f3c4a"}, "47485790-4ffe-4c22-af58-6e528b760ead": {"doc_hash": "ecd9306c45d6b2913f0137d5f62bb7cabf28f76b1d7f270d3b41baefd388ca0a"}, "11215cf3-b9d2-4731-b9fe-7f036853eb80": {"doc_hash": "3352954e2928462e21782a396e3502b9db4c447aa0d94f1739bf1a01a7941bd4"}, "c8e817ba-e86b-48d4-ac4e-a1aeffbd1d71": {"doc_hash": "59a0fc56d85812992712321fb1bd12ebbc7730254433a30e6dbc1455cb6bc529"}, "b787ca16-3662-4fc8-be72-4eecca88405f": {"doc_hash": "907485a36c918b8da77c9cf5015ebbc4aa391bca0fef73d7d49d034939a21b12"}, "7c28d313-2532-4c9f-8243-daf2347b7c11": {"doc_hash": "4d119c32f234b98d4f520a665facaab4007cb7e543f763a5cc006693a35d1216"}, "267bec77-a109-448a-9ce2-c9dab2a59cf0": {"doc_hash": "34dd1fcbcc457a3fe0b462e8539a1579a632871b02324b13649a8712112a8c4b"}, "a299c54e-0947-4772-a46f-d5d9a57a719a": {"doc_hash": "e8074d918756cb6dd157a8d581da35bec774890ec11720eb2bd119c7790f2bc7"}, "424a7c3f-4cbe-4b51-873f-cb880a91533f": {"doc_hash": "687ee93feb9ab64121223d24ae6425b3942aeb1c01e420a3b324ced90a94e69e"}, "a9644e0d-7967-4a02-aca2-837028f12e99": {"doc_hash": "cb7e9599a77db782e4b3c296d5ef55e0b89365373db6d3dc3bf7b2163f1fd8f4"}, "51214602-82da-427b-af0c-be8b40887e07": {"doc_hash": "c632a3cfab4b948e861619d2e8c6a46ec1653fc81ae402125ab461890f6ff8b5"}, "7c805a67-0a63-4054-9ca6-d9cef8ab4d91": {"doc_hash": "7c3e735d907bfcab4d13aeb74b5867021410e9485ddf8a9a8605083937543488"}, "153f8743-c109-4227-8d0e-3cb67514c743": {"doc_hash": "4823f4978780b0f69f3f0527eaa8d364273f591d726afaf3b4c8c9f4f6c7f19a"}, "ce71b5d1-0b1b-40a9-a00a-26fba5fb8b4b": {"doc_hash": "fd8b46b1674151bcd3095dd33fc3e9ac2bf00c294bd69c93830d7c271bf90d64"}, "4f88ba83-e254-4fe7-8620-46afd37b9966": {"doc_hash": "9eea84aab677304cb1fa646581dce8ca964fbd428b6f81d526f31e89ce15326b"}, "85b8a2aa-3b2a-416f-8590-f7f3959d1cf8": {"doc_hash": "f0c1789b97bc2ff1470b60dcb69db07b66bfd185629768ef578eee708820858e"}, "8176dbfb-831d-43b7-bddc-dd2a10529fd8": {"doc_hash": "1077cfacf5e1682429ecb6b24dcbc49b0bab6e6b4eb144482676bed81c2430db"}, "be5a3c9b-efc9-4b99-a413-3d2c72c0ce96": {"doc_hash": "34b6e1d951e257d010e4cbea714021118b7aec3c683fa47729c9ff451b5a23fc"}, "9b69b97b-be75-40ee-a84d-807ffa98292b": {"doc_hash": "d3a3af1f3619a15502d5557ce5426a9d9c821fcddeb5d981745a21d76af842ed"}, "88c57ea9-bd30-4bef-8e83-b2b5a37ab8d8": {"doc_hash": "ebdd2aa49a3cdd0719ffbb73558c706e7be6f7a8ebe38f44b986dd38808fba5f"}, "be8fba07-60f5-4216-8222-bd8a3c56d0e9": {"doc_hash": "3f21562262110e8de4d7ae28a977d12835253bf1959658cdce59baeb6114abe3"}, "e9697af5-4d69-4aff-a3f5-538429ef7550": {"doc_hash": "3d5551b5cd75d33ee4d763fa525d27f2cc181e4447d703971271d898019421d7"}, "aafd4e9e-d5fa-43c2-9eca-b06a98e65060": {"doc_hash": "93b9bb06fdd9bd22c074875c841bbd60d1cb5cfceb28d3cc1b5537fb57444a4e"}, "4b87256a-30e7-44a9-91ff-f96bf683ae27": {"doc_hash": "b4d298338c0cc9b4d9a0c35e374d085249de4230d3267e18da48739337381dd7"}, "ed46322f-6583-4922-9e4a-01e75987ef46": {"doc_hash": "8365022676f8c001f89e44c0db0ff9cb88b97ab8b6c9640522bb087bcddadd1e"}, "dbde8267-f37d-4e57-a99b-5ef58957fcfd": {"doc_hash": "d0b1d69692658ba1c2a630824a6a3cf6f104a55d2eb2c473d8ff0a19257cb19c"}, "cf3aec92-d71a-4240-afee-bcae965b15d0": {"doc_hash": "fa1abbb201bbe557879503f557427bd23ea22633955ed4ce63ad4ee85105a0ae"}, "764f70f0-b2b0-4ce2-9594-5cc7e4b35650": {"doc_hash": "6dd41ea2d14a63bbabb2512d9623f697d7785db9b88b4788641d955fcfe4440b"}, "c1e645a1-7aac-412a-b4c7-0d1e163e8716": {"doc_hash": "a35fb655af732752b4c2e575704bfda2717c9b85ddef8e0d05b5634c1b1abb16"}, "e29889e9-a394-44d3-b666-4cc65390941e": {"doc_hash": "809bce1c554a7eea989a9685fb70ed9e1154075967fd34cad4643581e01841f4"}, "d635aefd-96b4-49b6-8dfe-b7e606bb749f": {"doc_hash": "cc3db62fef8ed762d34a42320df1cfcc099cfb2ca5adebda609d0d57dfb83379"}, "2cbd9874-e1d4-423b-992c-8b2db529cd7f": {"doc_hash": "b609a3d9b3ed52f3a97ad90d3d42c0b54a835c9f6289af740b5a43e6cee4d551"}, "188863b3-daac-4441-a607-fe109b0766dd": {"doc_hash": "f9372bcd79152e549d8fa712faa997a17ec704f5ea7ed053ef92126b91eb6f82"}, "bf955db7-1e7a-45a0-aef1-39cf22c1baea": {"doc_hash": "116ab48de408c73e83b8fd61a88bda5301b62fd89923f1b0b2ae335e3d8dbe8a"}, "3f8bbbc3-3ad3-42b7-bc4d-fd4f73f1e647": {"doc_hash": "1cc637421f9257dd61cb9173800cd5653e250d6000da4df1ea2d7e163f45e8d3"}, "a2b965a9-1a4f-4ee9-a1a5-21e15307ae2d": {"doc_hash": "750ee11242e6d331fd704031631cd14f2f08c13755c6d9e0e0198f614c366813"}, "8b394317-0275-45f1-a6f5-ef641760a114": {"doc_hash": "2896b159e497e92496e4c0c785334293154081336ff573ea066702e52a094714"}, "6395cf6a-89c4-485e-8465-5c8f2253d8b7": {"doc_hash": "93d2127f3a5989415abc276bb772e2ee4da2d522515adc3667bfcef89c920344"}, "434751c2-f69d-4511-99ae-c9a2ffd49eb2": {"doc_hash": "732f94880753b5caa77435fc5ad871b991cdcb49c1d5a47fe5214659585010c9"}, "da05a50d-7e02-4334-b061-2bdfe03432b9": {"doc_hash": "e0bc8b7da9e9d1270351ce7213fd6ba4e24e6eb057814c0a74f31c614ac3e4f3"}, "bd801881-de1c-4f9f-8585-8503650f0e76": {"doc_hash": "177cf72496507da8d95505722343c89c76a0cc09d2b5788fc2dea4084a084491"}, "6c5a2a59-6761-45d6-9e72-91a950acd176": {"doc_hash": "354e87389a1f1a9107bf1bbe562f05387c467bdd8b112144ee30ed08b02888f7"}, "c85bc06f-c06a-4756-9e85-6ae6bd17e2a7": {"doc_hash": "940ae905a284904fc4db82286210f165a7764f3ad9f26d7e94ac26f1baf3bf5b"}, "dee01888-6c66-454e-ac69-9613290d207f": {"doc_hash": "173da62f5abec82f7a1660c22efd580447e9a0999fd306f74f72ff26eb6df2b7"}, "043e995c-ebc6-41aa-963e-b051cfb39023": {"doc_hash": "c3b389310493f58bce65da6344514f2f62cacbf5ed0584a2d87e2793494647dc"}, "9c88e89d-1572-498c-a162-4b2399be03e4": {"doc_hash": "d9b9b2c559f05b15d823a2103ed5c46dbe76ca9cf0b293e5adfdcca14be61675"}, "18e53e85-30c9-4f79-a6ac-cbb67c90bb20": {"doc_hash": "a712f4193f69a6567e12facddc9a47fc73759fa1c71dd9f38dd1a32cb47523c1"}, "bf3a0ad1-69a9-4a77-a836-004f10a7feb1": {"doc_hash": "b6eef9bd818859b3affbaed5a4f7aa0663293fde7b9745b756c6669c2c5ea67a"}, "f51033c6-0578-46b9-80c0-b8b9a7736f85": {"doc_hash": "4adbcfc41a7e7f64d0b9290a52696444660a3103c6f693d527d1d10981a12b1d"}, "f7be9653-0593-489d-abc6-4164e0769f78": {"doc_hash": "74aa0260ba29f98b2e241fe581d19141657cf21aac2720d63131e1eeda864df2"}, "2b49817c-3133-4373-a1c8-9d4f5f8d908f": {"doc_hash": "f819a38c2fcab070d347bb4e461116d176cfda6c38a9042cb4cc446f1fa710b9"}, "b56ba533-2859-4fc1-8842-0876c945c994": {"doc_hash": "a501221594e64a0df68f9f80e85ce95ec7b0c1f90291ea389f153682c5e8223c"}, "c70ebe84-e40e-4547-b42f-69e1036482e8": {"doc_hash": "91b16744a6e4cd9d837aee1c11bbdcd88ee862244db06654ed1c2cd1d49632b8"}, "c1921b58-16da-468f-afbd-15aa161c8103": {"doc_hash": "f347b99d66c6792c4974d7c6909c8f1f6ccd8ef12af2349da1990e9172925853"}, "711bd499-e9af-47bf-af3d-d868d7ae49bb": {"doc_hash": "63b066061120b52f232b50632ba7ea5e835c0ac139461ba1e0760653c0334787"}, "a691f552-13c7-470e-91b4-835659309171": {"doc_hash": "76f4b030f405295c125c7e820901edf1996d71d10497dff06d9fe8dbdb913653"}, "24ae88e3-bf25-43d3-868d-11372ed0bb7e": {"doc_hash": "187c0485612334a7d07d12c808eaf7835eb08d49352da6dbb86092c989fc110b"}, "5ea7bc45-6623-4d87-b6ae-d410bf58478f": {"doc_hash": "aa488a83c92f57b2f40ca86346fd7a8ab1e1a1ce5f215ba7b8b337a0687a6d8e"}, "84dc5fb7-ecba-479b-9c9d-b186d8d29622": {"doc_hash": "76a11f432a66c9deec7296fa1b020443126efa8cfa48d32b008bbf5b4eb1d95d"}, "9791e908-04ad-48b0-a9ea-7d90973d0f83": {"doc_hash": "4c912603d6663a1b1491fe737c26e441a01c4790503d2a0e222668fc03c533e6"}, "2d3f32ca-e39a-4bbb-b52c-b489c4653ef7": {"doc_hash": "2930f5fc75f50278452a50760ea57935ae66fece65d58a1e17dc7f5397427249"}, "3474da39-a95e-4fd0-a2ab-6b7569485ea6": {"doc_hash": "21027e6056488ec67dbc7b21d9e3ca642c268e0bc1fb0e644423a7fbe76f254e"}, "cf62ba83-137b-42a0-8ae8-6e29cae4d7d5": {"doc_hash": "dab2cffeb60706d6d1fb6fa574bae05dc14d713dccacc47e1ab2ef45c084df6f"}, "5ca994dd-546a-4d4e-a130-976f64eaa4ee": {"doc_hash": "dcd7e5086cbfb383a5ecb5200c90dcddd18a862f7a60a41bca6278adde45676c"}, "030089ec-2105-4886-ae34-1daa00b518b4": {"doc_hash": "c92f73f6caf6cbb584ddf749a99a84eccaf6907ccfc9145ae4ecf939d79a28e3"}, "fe686b58-d837-432e-b3cd-a6f918c6893f": {"doc_hash": "770abc58c1b1c4998b8e1c753c93f5668c82d71bf55a6595a7e99706a56dc421"}, "84472535-029a-4fe2-85b3-6aff6c5030d6": {"doc_hash": "55b5b9abd705f24f076196e40cb43ba7d3b7a402f4d00ea96e6f1e11a963323e"}, "7eaf12b8-d3f1-4122-a030-afb3c65c122c": {"doc_hash": "8442a57c109682cbae8109203337f76200b0eabc162b109377a63c09bd134a0c"}, "3bac9c96-5aa6-4031-b055-e7d034ca1ec2": {"doc_hash": "a4404f6655bffe25079718c7ad39068734cffa55359744466d00e36017f863dc"}, "0bed854b-0d52-4e3d-a9cc-215a62cf62b8": {"doc_hash": "3f02c7d6873a128335bd792433593654d400961fbdb08c1ee639ec541bd1421e"}, "f776f761-4dab-423a-9127-59af5ac54afb": {"doc_hash": "6f8c095b5670a28518ade2871a451b63308bc641cc302c96f9e642d56f19726a"}, "85ef6810-78f9-4ee3-803b-1832f959b972": {"doc_hash": "dc89deea8dd47e6d61c45aea838e37f9f75c0479ba0322d18aedf943ae2291e4"}, "cec06a64-d579-43f5-8eeb-7d566eefbb09": {"doc_hash": "ef70f5e17449113441f7b8b286ae9911266bcabe9eac30de1c9818a057087546"}, "0432f765-52ac-46b7-8a64-46993358215e": {"doc_hash": "da886a912f5ebb8586cbc1dd8672dd18a191f4db790281d2b1030aa1428eb7a1"}, "0d3372c6-1931-4ee3-bad5-ef141ce3abd2": {"doc_hash": "e55127825a53217afdb668ea870a9526179dfcff2d5c8316895fa620f24e3e88"}, "95426591-5709-442f-8cd2-8abf65a52984": {"doc_hash": "00b724ecae40c65efd5afce777a30d46770bbde3f167e1e713073ca399d145ab"}, "6065b453-9eba-4f13-916f-941a767417b9": {"doc_hash": "c47d064788f40d346c14ef5ae171ebfb0eabefc7d41e417991cbc51570471aa9"}, "b356567d-d8f2-4cfd-b2dd-9c9113b3039f": {"doc_hash": "e712048932ec9754e82d434558d5cb32db1e348a784e57487631da5813515e61"}, "11703f22-745c-463d-b53e-eb05835c97bc": {"doc_hash": "d295defebef381773e24283dc5a8fac5ef1417bff2e975dd8e9fcafab8995d0d"}, "39c69c99-8687-4d50-a2a8-b4a5d3b50782": {"doc_hash": "2346899a9ab4adbcfd2b5d6b2863662befcd5cbdee19f087fe6fd490220b8e29"}, "b109ff1f-c244-414c-b811-ceeec2cccaad": {"doc_hash": "498769fae0bccb292f62dceaa71180c6e2f090c332309bffc08118964d49becd"}, "50b099ff-a2fb-4ee8-b292-12c29a73ef64": {"doc_hash": "1e4aafeec66cf2e7ebef3bbede47b1788d3ff20967d7d0355c24542a30bd860d"}, "b5a44a25-85bf-411b-8bf2-5e06cac2476e": {"doc_hash": "5dd5e49b6d38ddac021c650519652b5258308ebbb60c8e73d4003ace7e1937d3"}, "b86d7efe-4a9e-4ee8-a6fa-b700dd73f112": {"doc_hash": "230ecdf2d2cbfa68313e2205cee44f686e76b9d98260a34620b34cbc8182107b"}, "49cb956b-4ce2-4d7b-a872-8b8f03de9b63": {"doc_hash": "608c05552435c36963724aa84c548464b7e7acbe8f5a54773175f585563b4991"}, "0e6cc344-e889-44d2-8e98-166083dd04ab": {"doc_hash": "2c9a434ea8fc6b9cb707a6b7c277ab1fb70fd42372a44d38df0268fbc6c69bdd"}, "c61b74a1-7b19-490b-ab97-65784629777e": {"doc_hash": "eadbc5acb38c6e4c8e39c87a6cbd00410e24b575d75d40e0f15fe01e126fb800"}, "7839cb72-b817-44da-b6b0-15003fd7aead": {"doc_hash": "fef3fee44136ed6deb083824a9b793cb70a3715ef75d96af388bfb88ee57442e"}, "5db7d381-2daa-4348-ab88-dd1911f16400": {"doc_hash": "6d3ba003041d9ae8d346862ce0c1d43571c205f1785ba99ab1901131705947fb"}, "c031c70d-2e20-48ef-bbf2-03e0041c4461": {"doc_hash": "abc5cf4ebe9dd8c4a569cb52eaf26f2558e31f9b905ab9332a7382566a569b1b"}, "03ba42ed-82a1-4213-b845-2f336a9ec302": {"doc_hash": "35540881d7ea99313b5f70894b4ebbfdc9eaf200933e8e886096b2001e7bf4ad"}, "51c66e7f-b309-4cfb-9c21-3335b3889c8c": {"doc_hash": "c37a7274b6068689c784674827d88fe6da1334e90c3b8ec5e860dee32f0f9934"}, "588f1e82-afec-4918-b42a-2ff948c3cc10": {"doc_hash": "70c6d4b544f2b5765829e8480d7a34afbb1ca5de0ee9843e889130d0756b0ded"}, "8872df19-acec-436c-b456-b8d0c5d4b250": {"doc_hash": "89d307586269d866c626b07876fa4e62b157f401114872ef33b6dae2fac17fd7"}, "27cd9a90-cb1d-4ee4-9bb4-ef3c74eacb17": {"doc_hash": "326bf693c8ad9d0f86c6d7fe5a5f5eaf1981d7147a26e9b05c914544395dfd50"}, "ce9258da-39c2-4b61-83f2-89cc5f8ca5e9": {"doc_hash": "dc651d04d586bac40135ef84a002ed802163e6d35371fd163dd923cacc5cbeb5"}, "e309113d-9eb5-4b40-8f10-c43fe9c54ef2": {"doc_hash": "196707a05643236700bf188754858d333da6ee290d667d1fdeb8a4c786230fa5"}, "602464f7-80eb-4e19-8327-f59b328d6d78": {"doc_hash": "d80fbcb8bbe4ef4434bed6a999d73df9f677cba85db53646a07ce61e9adc91dd"}, "991697e3-7fe4-4267-9fa4-feab4a729ea9": {"doc_hash": "15d063f1f424cc96d9da7b0ca07080e4de7865a9d519edf45297eefd4bc7c298"}, "ecff123b-7552-4248-93e0-8c289547f96d": {"doc_hash": "53113eec8113ff2d1582c851b5d7579711705bb7c5304c8f8c7d329ea9c1c8f6"}, "aaa0a4d8-6001-4888-b687-1dd9816c32d6": {"doc_hash": "44e0b50ec4fb8c317d37c392d7baa115e48183ecc246341d70f57ec6aa98a7d7"}, "90d22edc-2f3c-4d2d-8950-77d96e519f7a": {"doc_hash": "2bb10dc1f5398749342d34a450e2eced0d005a8788f4c37dd2c62bcadbfe3dbe"}, "3fbcd913-a890-4651-b833-e753bca1427b": {"doc_hash": "fe31bcaf7670cdfd4ed7855b4b5010d09de662ffd069fedf13b9272133071636"}, "249417f9-7762-49e9-aec1-a4a04edb652b": {"doc_hash": "a16a43c2261b6c62858e01f9f6407640a7599a5ffef5f60a0db029e3a5d63940"}, "2ee32b91-20d2-46d9-b8ae-688f57246466": {"doc_hash": "322cd297328a52a4f50d459e245bd0e3ba1143f7b013ac2c8a25b39998e2797f"}, "d5557427-3952-46cd-9d61-f5600faad9a4": {"doc_hash": "9b77fbde06a9ccb7121f39135947653a8f3c5a53630fba597e9cfbfc196c8190"}, "beec423b-6d55-4e96-bcb3-b6d24cd12f22": {"doc_hash": "196d0239d75c0214db4dab42d1f19cfffd43110f64503a7df06ad96d2a744d19"}, "b3ddb4ab-7699-4279-a54e-ef343a82216b": {"doc_hash": "5593952bc124c4d7a9c55dd5e9164f294be5b9310b542737b53605785a39dac9"}, "371143be-00ad-41e1-9555-6c54db6b5d70": {"doc_hash": "80aa9c69ba6ece3225a013722c57023a4f49da80f3c340cf0430b22ef9ecf735"}, "a1f5bd7d-8c87-40c0-a736-1cbb566acfcf": {"doc_hash": "0ce47b8be5fe2d72aff175da6cae5af3e06a4b2252b9c3e7a96e5b29cfd90732"}, "75f6d77b-b5fb-4a98-9181-63c2c8d6a0a0": {"doc_hash": "325cc4d595bc3ea029945559dee9bba5149915c73fcd1e1e87e5210e663304b8"}, "cdc66615-e8f7-454d-adde-c492ab98fec4": {"doc_hash": "7eee99941d2e635bad1d176ebd73e1cd93347aea91ca65f28c5cd132ebf68ec3"}, "70d270e3-7220-4e0f-94c9-a7e8051cfc2d": {"doc_hash": "6d6348a203698730e869a1a4e768b7941a4df4b90e49926f880c5d65cd502b51"}, "9faf1f0c-f20e-45c4-a5fb-f00d2b379b39": {"doc_hash": "ff130dc4172cda8bea9c2f4ff70513f298554168c8f870ce0f0a8ae69bc2c9c2"}, "b8bbc4af-4a25-4e78-913b-68a40eb2da3f": {"doc_hash": "413366662f63ce29d9e57a1db5cd6afa46adcff12a53238ce936ac14c6f1c029"}, "fda93fcd-fd9b-4570-b8c8-4b013c4ffbf7": {"doc_hash": "dd467cdac7fc625ccf33dd72288a91f1f640233e3a90b46bf866fe225521232c"}, "e333f705-a029-434e-b76f-96a180a3ec34": {"doc_hash": "4725d77b8b515f6ee551f6bc9fd595e08bbf608d1865de80dc5ff676b64da693"}, "e4bf679b-758e-4449-9934-9decdcd64c53": {"doc_hash": "ca37e2b893d6fb28dbacccc36990aca39279bae641b4a13836eca0bb943b05ae"}, "b557cdc2-8574-43f1-a84b-ceae39550074": {"doc_hash": "d6b9f30b7b7474315582af24d3264a057c7f82d42cd3fb332302b92738bcb550"}, "475a5953-2fbc-485d-9a76-a275325c2dd3": {"doc_hash": "d1a786bef4a9f86a4c677c83fcb1963f7cf7facea4da072c3104acf360ef7775"}, "eeabf41f-7fa0-4681-ba2a-3676bd7f9385": {"doc_hash": "c2572bf0ca9fe8bb6f9f9ceb9e2dc66ec13c2888372539ff2073034b0fa91d3d"}, "d2f2c5a3-3ef3-4208-9c97-8b6f48180a05": {"doc_hash": "6cd039d78a6a2e8fd67a7de50b33c47b7872e8fddd80c0d900bd0b6bfcb953b9"}, "7773128b-4279-4bd5-8144-65c100fd4481": {"doc_hash": "de5fcc0f96aab71a651436374f4c585db9b7cd9d73ef838bf52df57489a016c1"}, "0b53fdd7-cf28-4b6d-9a44-65a9f35ec628": {"doc_hash": "84cb7d4c97d8f5a95e39493b496f7ef0083776db84df80856ba094b17abee7de"}, "8a9e960d-a1d1-4c7e-b7af-2d9cd6a85549": {"doc_hash": "7a45d7bb8ea39d1e4a08af579a626ecbb6cbbe4413a92c32cf351410cf51d1c9"}, "b9708d7b-2cbb-4ce0-aff5-31fe4187ccbd": {"doc_hash": "38314fff6703450545f6b16203fd7540c0ff092f7d6c85e630db4df68e13af8b"}, "406a13f3-24ae-4bef-b893-d053451e7b5a": {"doc_hash": "1d5a55732af0efc221291e860cbb5c7f7e3cf66164a0ba8b336cc46651157f7a"}, "7cf12f6b-4a01-4d4c-a1ec-a4527afd1d5b": {"doc_hash": "4141047bc33e13ba94b16a5aa73f003fb059e15c8de8ddc5ef070b86c7e17d02"}, "4f03000c-d65f-41f7-96a0-619d946bedd3": {"doc_hash": "2cb0c39c36784af15de7974fea9ee7076fa84e311e5757480ced38533fb3a59a"}, "784be029-23c4-47e2-bdf8-5090491c901e": {"doc_hash": "c983f4aedcae9e1629a3887000f54a53f121225b7422504293394c14b77abc6b"}, "9a8c1c69-39cb-4151-882f-cbb0291d5719": {"doc_hash": "f72cf0963f1031358ff916b191e75e5b5ce2f4f5f53e36b8928eea39b187e80f"}, "64175e74-07a9-49a0-8e56-3aa026a55bb2": {"doc_hash": "d9e09df2d20495fb731160d0a2f896e437f0c1417eed068786f935c1302b67b3"}, "beec1148-fd1f-46bb-a681-21bb55fea26c": {"doc_hash": "1af34d267d062b6e65a720e7313f830e1ef13180ff824754533277b4e4e30352"}, "48bae3a8-01e6-4887-abb1-ae54676b3cf6": {"doc_hash": "90f9530408ae425d1c706dd24df2e9152c709c982ffb6421076da26d0579648d"}, "246062dc-ef95-400b-abac-45eb0bf6b60d": {"doc_hash": "2e06976ab4c02a536b22d8009c651b13b7807aa60d5a363c59abe84c08504d2f"}, "a19c1116-e34d-4ae1-9ef6-d954e12bc1b3": {"doc_hash": "023971e0d5523038d75684dc09546b33ad9e43ba919161226f4dfd2b2e0589ac"}, "6ce8bdbb-b31c-405b-a012-6c4c5db48cad": {"doc_hash": "262cd9c694fbc8f725db7b8f91591c0d34bab6cdb268e251d07de8a1b4416e0e"}, "8e5eab7f-1012-4e31-a6c0-4f897ce5eee2": {"doc_hash": "2f49e50546110dbcfe1ddfe0de44f805927e9b022e28926af5024da4b715984e"}, "2f5cbeb6-935c-4c7e-a439-edeef28d34c3": {"doc_hash": "a054df4d136d7842a9fde534c69d80dd830c0cb80157cd76550443894a6898ee"}, "9ee5bb24-c120-43a3-9528-a452917cf703": {"doc_hash": "cc934c74379e0ff8219cd6f924e7f972585671f6209f735eb3aa5a1b5b769daa"}, "51605275-0f9b-4bc1-b9af-171f8522911f": {"doc_hash": "5049e0e8ea1e113c7bdeea57bd329bb915f39d86f68906af7bc357b8de5f9863"}, "124870af-5fb8-4ebe-b88c-db7bbd87dab2": {"doc_hash": "f2e38b800582279b18e4417241ba91be6dbde4aeed2b288815db1c16c6baf99b"}, "359bcd3f-673d-4481-9332-631ed382ddae": {"doc_hash": "335c9847007a529108981a80387d11597a9ac2768aff29ee82bc630c44def549"}, "6644cc7b-7f6c-49f8-a567-a9032fc20b74": {"doc_hash": "790a47b2afcf596daf2d0b3d4142fa8d50bc9406effee7ffa5cfbbd7ce2a83a5"}, "1c8a088f-907f-4cd3-a9da-7279e4125309": {"doc_hash": "836568ac48996398018d7abc3e1ce07d8b4074fa88241c666e7f85b4b8067bf6"}, "a8962609-5222-4c18-b050-d24755f4c61f": {"doc_hash": "677326068a03a9285af8351ce72440728615800b6ad4e0246533d36d96d58492"}, "bd8f1cba-b5d9-4b89-8fc0-bc7fbff84e63": {"doc_hash": "ea92a647f56c4ae81e5beea2bf07b07f837a4afa4174d222bca35f3653ee443e"}, "a078d418-b645-42b0-bc41-d0b160fbd004": {"doc_hash": "b46212d9d5d0610ad87cb536f4b104a0024c6b1ed2510374aea6f0b537adc9f8"}, "604fc341-6e0c-4e9e-9b25-38c0ca29f3d9": {"doc_hash": "a27ea3375fbf379086724d4442f4f7628e3307e73075f0d4a9738bcf9cc89550"}, "2ec51096-01d3-4ed2-88a1-83e80259a523": {"doc_hash": "2e3d58e1c7ba7bd392d53428d4003fd75e27c5d4ad6a2e77b8b319a84c370403"}, "0a41b72f-eb21-4a70-9f48-fa7efccc8866": {"doc_hash": "e7e0eba99c4371e2ceab26dbb491056abae938e29fbfd348404466aaf1c3d001"}, "4ffe1006-dd87-4936-a453-7e81e59b4232": {"doc_hash": "7d441da4380b630caa32d3ef94189e5ef3ed325f8e5b18c8c67ca130c5396cfa"}, "1b0a366f-eac6-4b24-bd1e-f195267bc162": {"doc_hash": "1659d14d05ab700db6d8d3257bb0a9c3a0453d200be99e2d8c6ae3d065126185"}, "32d20442-5c93-413d-adba-c9221154560f": {"doc_hash": "162d039a2c46ca4e621c8f8dafe25d9d6446e1ccfb20550161c23f6f4837fb95"}, "39da3daf-dac6-4def-bedb-9bcecdf5e61c": {"doc_hash": "d857ce277f451ab26ccd8188b383af9f93e93107d111ec4b43451f39abff20cd"}, "a36e428a-710a-47dc-8317-81cf415ab194": {"doc_hash": "90ac6af1921a519cccbfdb9dde1bc37dac04bc63c9581b0fc9a54a5dc1d0f652"}, "6b1235b0-ce4f-4239-80ed-bf49f662714e": {"doc_hash": "c325891e0c4dcc545a296cd478a07502701b1d393cf2ad1546ccc513cf412395"}, "2cf657d8-c137-40af-b472-0306010a0b08": {"doc_hash": "d0b93cdc97ad78f5dcb0b529721cffa325bd0fec1ce7c113b40e1fd85f904683"}, "f5e7b054-13fb-4818-8455-0aa0a204501b": {"doc_hash": "95f60a16765942456aecdd540a71e3a3be1ca99a46c9a1cf999d3dfeae207fc9"}, "cf1d45f7-7a98-4cfd-9fc5-cc9c550f4904": {"doc_hash": "cbaf3f9b5de783c6e807a0f25ec453afe1836fceb161552a58da524205291880"}, "94c21bec-196b-40a1-892d-f12d241167ea": {"doc_hash": "0ffa6964b4bd20fa9b38472a2c9a0bde8b18f59d963f6ca98bb785207689c229"}, "31fac131-303b-483f-a23b-4479b48169d4": {"doc_hash": "e271de6ec6a62be1516ffe29d5f148a2fa5d5875d12fe2938f6f80685d36600a"}, "a6b0afe5-5335-45b7-870b-94c517864f57": {"doc_hash": "f9b9fa111f2d068da69ee1b84d7c2bc38ec3f503bde5a3daa2c9b92d6d0dd122"}, "5e1d98cd-dda5-4e38-912e-9dfa1afbe47a": {"doc_hash": "0396785a104ac1dc31d853ac03e0a8970d0169e64c74a74e8ca2f842ebb30c7b"}, "65db84e1-f92a-45fb-bc28-27bd55d563cd": {"doc_hash": "41f68b14086997ae049d62414477b3d9a8fd942775941436586de4b4893603a7"}, "7338dfe4-7ca9-4526-bcc8-bfb199e92eb5": {"doc_hash": "c00312cd908dc1b1f27f116a2741038dbe212116c205ade8cf8725c8710caeaf"}, "7f2e7113-15ed-4db4-981e-09bbc754cd89": {"doc_hash": "c28787a99e7946e63c4834d2458b717a88f5ecb06d47b5c8281f8436546aea8f"}, "f1ac8b69-9df5-4e79-9a21-4dae840f7a17": {"doc_hash": "9d100472900dd5d09f132c2a593d53183ca493e1427c27d79079c902a4910508"}, "110cc1fe-ac50-4d5c-bc9e-3e94486485a4": {"doc_hash": "327a43e294d1fa7c4b1d3d0fb5209ff7c1da940a7f8830a4a1a216c4f80d4f1e"}, "ed270164-359f-487f-a02d-56f657461b19": {"doc_hash": "8e56841da3154572160321723ea03ee29cec5187de17e2f0c4155bb7899f17f2"}, "56c0033d-29cc-40fe-9cbb-e7b585840e06": {"doc_hash": "12f94990c481c5ec8cf2e8bc996f3d3f17e908b707e92463dd813cc3a34c2b77"}, "3725b24f-89e2-4b7c-ad3b-1bfffdb29df5": {"doc_hash": "8feb4960f1a83c12b0692dce4888c7dbda516ec375f21ab61a075a21c6c964ee"}, "6e927557-037e-4583-9ac7-66db4dc513ba": {"doc_hash": "0c100aa52e76b7f6655eef00a92310afddd0503f1eb5f9672289a8bd3143f2ec"}, "530608eb-cdb1-4fce-872b-6a58f7bb0466": {"doc_hash": "6cf5b78215f8768e845182c8a20e6089498fb6f1d203975fed481ab34bbbea12"}, "9d8a0107-2c78-4811-b35a-71a77dd713ef": {"doc_hash": "231c1f1bd61813882c1df451bffefb2e792545b6df524658f5f67bdc677cbddb"}, "7d3bb5f7-7610-443f-af36-1cc6c11d6b5f": {"doc_hash": "94a743a709d330854cab2130d3f457edc73c50ae2d555e2a403feb6d24329f28"}, "779ba5bd-e08f-4714-b0c5-43aa1106be38": {"doc_hash": "0dd96e195ab1143d5004d029a5fb1d7ccd01d9d0bae57e6c41173a6bdd50d07c"}, "e17272c6-7085-4124-be4d-f078d58a8ef6": {"doc_hash": "c5ade7a1963842cb82360f8db38d30d787e48a9d17802174b4b3cb69cd50b30b"}, "531f88b7-b6a2-4ccc-ac79-064f19d29c1b": {"doc_hash": "a08031dcd156774919bd237047db56262daf26052c7d6834b1044550e47afaf7"}, "1e037851-65ab-4083-b330-ba70f831a7ce": {"doc_hash": "aed87672488def860e4338a7814f10882d980dd360f9e86d730d5903d66473bc"}, "9d288575-1054-4d06-a7e4-482c794c123b": {"doc_hash": "cf2116dda4a95a2a1b0d9b78610b6ca151b2eea32547832a14c97de8370c3ef9"}, "f035cf80-3eec-4d97-8d95-78b2eebc897b": {"doc_hash": "553109d3db360685d55eefd1b80f222200e4b653880f398da4de0455cfe721a6"}, "e61ece3c-0d5c-4cd7-889c-24d278806d15": {"doc_hash": "1e86ca7af8078c84740c48bbe7151fc56ad59c8509fdc8a9363d0aebb54344ce"}, "2d3ac2a7-90f8-40ad-8b83-c33860f25f7f": {"doc_hash": "6e5b30337bc5758ec1aae13d8ae549b1af58f284504caaebf02a1bc07bd8f6ae"}, "b6d65c86-b497-4c6f-8d4b-8f8e3d78fb52": {"doc_hash": "8e5ba93a8b77a59f6b6dcfaca4abd9e90deb06ddd9e692f05ac8e9862a368597"}, "588c75df-6df1-4243-bbcf-ec4da15cbaa2": {"doc_hash": "5d505e63134ef85023eb5e6d9c2a146fc4e16e9d0b9976f5d7e2eb9153c7ae07"}, "d151f6aa-271d-46c3-acdd-bd7172340711": {"doc_hash": "8b5a7be1386366fa4536206cb428f8d7a049add59c1b2d895de60ec5b53eb13f"}, "d3aebd53-d2df-4f54-9f60-d8e8ca347b15": {"doc_hash": "d329265cbdd14bcb08486b1d9e739c5c28902a2aa7deab9def789db2449c0aed"}, "4cd67c24-fbe5-41bc-a187-15fc5f828cfc": {"doc_hash": "d5930538dead730f274c224c5f0c56218a570c250ce744c97b9e6b747a7a840f"}, "54e355fc-6c2d-4861-9d88-808af8e9725f": {"doc_hash": "c45ef97f7022374cef85cb566336718ae869405c4957aa0c672ee64006439976"}, "0768b5a9-5cb4-48d4-88ec-ed9a34ed203a": {"doc_hash": "f599cd07b6b3434991201848e0f19ea09e23ce9bf4240a54417bf6af43dcacc7"}, "9790a890-7e97-49eb-922b-537b4dc0c023": {"doc_hash": "24959526c60e6dbb6e6aca2e4a45acb9e68b619bcfa1f21df7b0b0060fae9999"}, "f9a0f2bf-83b4-4fcb-bff8-bd785e32047e": {"doc_hash": "877c7eac6c6902ffd4397d0a9cdd33df34e07584e4c8d0a9bec9a96dd9fd8155"}, "31798e22-345d-431a-96cb-701d39cd51f1": {"doc_hash": "9d2296e7753775e9b4528335330f760cdb29210ba083ec161331257423f0d29d"}, "698003fc-d2b5-49f0-a816-431af7f5337d": {"doc_hash": "8b0f4e89624a5185d37bc73c43d7bea730715ef5c99f6e7c2549d96171a04d22"}, "0effb730-ac3e-43fe-9627-820435850fa1": {"doc_hash": "ebf68b9d5044589046f0bc3733749b72c2cd534dbeb593cf6b541d38a2c59947"}, "e6e68ab4-27fd-45ae-ad8a-e064ed829306": {"doc_hash": "2bd7f60fc55da5a1df04a8de87f4acf3b19e5d301713faaefd5256e92dd687ee"}, "a2c4ecac-c16e-459d-95e5-0ccd9444b343": {"doc_hash": "7d95bb90f2bbccecdb4e2730768c67488dcf5eb1d25571990550b2573c5806f6"}, "f2a23db9-93b8-4661-a0f1-37a4faafc8f4": {"doc_hash": "bb21964bb2734536b92a37cde4d8c428a16f657aae4101c90c822454e46ae9be"}, "cc824a43-68f6-49d9-93eb-6014ba1848b3": {"doc_hash": "85bdb95457dd298c2d68b9a49f69a8aa0cfab467ab862dd7dbc3a01db99fb776"}, "e36eff1a-6947-4f1b-b943-f7acecdc28e2": {"doc_hash": "798a837b2fc1a84347d2e16eb55dd205b659a7fc6ef130de1d9daa93bfd2847c"}, "0a47d563-871b-40b8-bfe0-71af3042e0c7": {"doc_hash": "5e829da2df56941d67203b3b882c7b1e99b138f1e0bb92d2e9b7246b7c377ef4"}, "dc43ca0c-bd26-4be6-a8a5-c8f2102a86ee": {"doc_hash": "c1646dc77694ada68d08feda43f66e7c8aa71d631c826438d9c922b6f9469168"}, "6bc13c2a-8305-4858-bb25-259f6f58e99d": {"doc_hash": "073bdf552f081e40a561633765d0249b14fa91b70be30544b2e17495893525cd"}, "8158715b-cef2-4015-9562-b06aaaa04497": {"doc_hash": "c6459ffde1c75d41395bbe9accf38a67765638b53386b7b04f05ca4d47c664d3"}, "11672e81-c2ce-4630-90f3-30681ed5d73d": {"doc_hash": "cb7ab1d8c48391e002ad9fe32ef6b32b6db7e2d97165b991a211e0007553ceb3"}, "5204c427-8fd3-4b4d-9141-7e97fb38a56d": {"doc_hash": "aee0c8827a07d2f094ea2a2f9abf450a3ea262f0a845ed52d4b4889fd4eb37b6"}, "64fa15ac-c027-4f53-89c9-0c1b5583502f": {"doc_hash": "d317ba36266ddcd9ef53b373fceb87dcd9eaf662966e35470b5acaf3b4c8a7df"}, "27541c14-f639-42c6-bd2b-3e1bfe34bee3": {"doc_hash": "9de9426356fb7f6ca2c4adb53f34561f35b957491ac0e8b225283c3c77469f2e"}, "664e8370-87bf-4cbe-9936-e1a4a96f7d10": {"doc_hash": "ead6ec7bc30d5f9169a9e1ba6d5545ca9f821cc6bfc1126266c9ea2cbf81ad30"}, "db8c9948-9ca5-4ae9-b2ac-12da3acc3447": {"doc_hash": "97ece45bce89e795093e8c05fd230ae155cfde5e433da3d1bf432993944a0b51"}, "dc775fd5-8062-4090-98a0-b2e3300b9514": {"doc_hash": "2d7a570fc89a5b852f8c0dcaefd9c4391fa4de0144fa6aa9502161e36dcce678"}, "ace730b1-1f39-463d-b1eb-7fd401550204": {"doc_hash": "9c8c7ab8fe33291d6630932c6c25f8cb5f55e8fa76bc548000702e1695bf0ab1"}, "ef979d4f-4980-46d6-8414-dfeaa0eb1be8": {"doc_hash": "64cef1bef8886aac40c516a61ce579c5726ce2d81b84de5fe77378ad7b18c2a1"}, "c3757882-d8e2-4e62-bcfc-a2f5f3ac06bd": {"doc_hash": "e061e672d8b77245ea91fdee4d253b08756d8159c143f328f84f83c2199ca152"}, "3a85ae34-00d2-4d2d-aca3-0d894760a221": {"doc_hash": "cc66ce9b0d3d298c96c3d5cfee73c133df2ed5a323067ba47efd3e19285d3130"}, "8e5339f8-1724-4380-8282-3d6031fcd974": {"doc_hash": "1351b2e3b61f71f86857e2e80b494de10e2ef71572c2a35060d5732406a16507"}, "96edd470-d0a4-4e33-894b-30ca769bc094": {"doc_hash": "0c5a89ae6173532bba210ff413dbb449f81f0fe15f69769250c35b5aa51e6aa5"}, "9d8e97e3-178f-42e9-a209-d5ddb5972831": {"doc_hash": "ef8719745ce2e30b0093e12c3a21c39be58aa06923a317f238de9ea3e4d0c25b"}, "01db6d96-bc82-490d-8823-3e677c2905c1": {"doc_hash": "568a83ddbaeea349f1f77061e08cf3950a46f5c8b8fc2083955126a00ac2713e"}, "a54b8dbe-4d11-4bc0-bd73-f1fdef0876ad": {"doc_hash": "78e218f731a4682f3d50f8da5ec8569aa5a4c2836eafb216ef21a3676d0c95a1"}, "1067cad0-218b-4145-92a1-ef05d72d02f1": {"doc_hash": "12afb85affb94a450868e2f0462da7afbcae52832b51705694f9dd5d7da30527"}, "0c8f2a83-1d10-4f97-9763-e6a7db2e948f": {"doc_hash": "f77adea67ed4e13253f75cfc9f5d1a67e12913d9918d720ec11dadd6bb3ffb7a"}, "2e48ac36-3b63-4c9f-b356-0eba1c400395": {"doc_hash": "71ab450c396dcd435f61e22fa916152f097cbb2b5bf907bf8ba5ad1d8531779f"}, "4fbbfba3-46fc-46a7-9c7b-69f5140e7c79": {"doc_hash": "8f0efcf5629f560ee9477e7d70f4830fdcf656c75873f58b41578da71faac608"}, "7000771f-fa4f-4b34-a811-d9b8ded08d68": {"doc_hash": "b7b417b2f6ef3d096a6f868d051e5e1374c0309afe894ecde83aec4119cfa280"}, "bfd95a86-26e2-4148-b364-865713db1d51": {"doc_hash": "2dab82ec78cfeef38b79d579ef1a860e3e9ad7829d7725b5f186155af0f51f9a"}, "25447448-a2e5-4b64-bfe2-63be4984a1af": {"doc_hash": "bc0afc73eaa749c4421d1fce7af41f139f5869ba702aecba1227a4c40db8f266"}, "17a136d1-14dd-4305-924f-c5a3579e5699": {"doc_hash": "aa5505502623106e4c1aaacbb1a56e46cebf8fbfa58c7c269cc95e62bea1e787"}, "88084533-c476-427c-8d28-250771cfefc1": {"doc_hash": "2ab7987ae099146ff90b470e65ed11a031440fd5f96009ba8554c1111b7d2ef2"}, "f19184bc-33fe-4325-a440-1dc077fc720c": {"doc_hash": "cd13c5ac48017245582a15c6acdbcbaf0defe7672055dfef69314634bc5c3377"}, "b0df48ad-3cb4-4ea4-a6d1-cc4211fab607": {"doc_hash": "e92739c1f3ffae7f9d1455b281fc33903e164d638c1602043baac422cc56bc89"}, "bb6cea9e-2448-4dfe-acaa-9532e1e68cf2": {"doc_hash": "1e86c0a6d74ed381fbc47d9326002b46cf3500bbd259b9500e4f1b288fb9517c"}, "655ebe1c-6034-4a99-9087-9f55318f1adc": {"doc_hash": "f1f55a5f5bd435dd1d15232aba69d1234175f8f1d372018191e7d6a60c453905"}, "cf6c459c-f5d8-48a3-bb57-9a1a481da80f": {"doc_hash": "9c3143b5dc9063218a8ecfa11c265dc76839dbda4b4744f3516e1aafe4dfeee0"}, "1d42eace-8e1d-4d0a-af6c-e3a8da4b7f8c": {"doc_hash": "c9dc328b6bf112d9f88337c60eff31018622590508fe5b3b815a01785a124663"}, "bec61c2e-e6de-4e2b-833d-06eb5deae25b": {"doc_hash": "15ca51e90ec9bb38e0d3642f9eb2d6a0f25787ba4a5e234638c4de9a8bcf93be"}, "dcf74ee0-abdb-45c0-9e06-35a3b586e2cc": {"doc_hash": "8c1a6202c07a6c0b9b9855712154acfc3bde33357544e67ef5f6e8edc3b2a6d5"}, "3bfa176a-1f95-4e51-9b3a-316ce84a22c7": {"doc_hash": "c58575831cb8ae8fde683c24d0aa2739a69d0f619fef5c323a47df082a846ef5"}, "3f8c2413-171e-4f98-8d19-12346440980a": {"doc_hash": "38083a0701724d1d73c7aa05ab836c005680d165710f8b53d063df6c3582b047"}, "c56c00d1-d371-46a7-b3ec-e569cbaa408d": {"doc_hash": "8eb57986a71ed87037488c659fdef020c9cd011ea95519448f9d5194bb839fd7"}, "da85d089-01fd-40c4-8a45-99c788db6dc7": {"doc_hash": "d5fb4e8b9a74a590fe18ff3b3d15de55c0be589f3262aa4c0c3a0878ae65c37c"}, "58ddc913-545a-4626-88bb-0dab82093ead": {"doc_hash": "8cbe35ea5880ef97278178c0ba1502f0f082a9878d5ef44a00b6c479ee1248db"}, "8bf92c09-77c6-4be5-8579-06705122c6f1": {"doc_hash": "47c82363c03da7b4bec699a34464a828be37c17270517d6bcb6286978bc8c8df"}, "a350b4db-e07a-41f3-b2f3-a47e9fa9d6b1": {"doc_hash": "9eefb1f4db37da3de4938d81dae60cc011bd60ebf525ec8ca0d146e1b3da14ad"}, "c87b58e4-1df5-48a4-aa2b-4127c47028ac": {"doc_hash": "940996b0b57cc7615fcc1f31063f9336f155d71c2b979895b577fbe21cf733a2"}, "78753eb2-0c5c-4969-ac3a-542bf1993ec3": {"doc_hash": "b9702b4270fe6e062daa0841ee98ebe1d7314ea92c544d54153eb2d53599a674"}, "fb00c216-81ba-4c6e-bc60-747457efe92f": {"doc_hash": "b8e10794055cf1ac65ec57494f106277345607a341e882a08d4983dd37cfa591"}, "508d3bd4-596c-461f-9c79-52a21b0565e3": {"doc_hash": "80997b55431da1bb00308f32dcd73d93fa3fe6e111a6281112c082d8849abd9f"}, "dacd34ff-0234-4b63-99dd-4e662e8506b5": {"doc_hash": "241713b2f418a0c2ad7a09bb832bd22c6b8a35c2bf5611822f2eb14e1ff1ab9a"}, "5dd81624-a8cb-4f8b-8721-a22720e67c6c": {"doc_hash": "69c1613a5723864e0a76da66fad999367989a61f4da48fc8705a2e23e7423c57"}, "bcc98809-e744-4dbe-9d47-643307221871": {"doc_hash": "74317011b87e0288d515f248d816af066c7655f79db61d71ff3280a887e14d98"}, "a84966a9-6e4c-4a5a-84b5-57a368c79c36": {"doc_hash": "427c143a8dfa4bd0cf7c2f51e9842fce945127e5f2899b97669484e105397c45"}, "e6629f19-b56d-4506-8c48-e9ad2de85b52": {"doc_hash": "b9b43189cb65af9afcae5cd94bd5efb48ab714d31314c5488b2d13903a5f83b2"}, "926f1fcf-b6ae-48e5-9a1c-1032cb961b1d": {"doc_hash": "7b8df688f16dfa0bfda637ef2e75c11dca32458d74a967ec029060af21696287"}, "81487b45-8413-4208-8041-c8aa1f1e063e": {"doc_hash": "9bceda43a9a10888ef2bf20784a5e1d7a756ef90e695bdf41ac0a98d46962756"}, "51c128ec-97f0-48f8-a988-147c654b51d1": {"doc_hash": "809c8d27b870cd02e40602c9ede9ac842581b490952040af5386457fa3e3527d"}, "3731a957-0fe7-4560-994c-e56c8892a4aa": {"doc_hash": "16fd53adb29e83a66895f00c36442d6d6c49b754e1673045a95b413dca780c9e"}, "a6886040-9fb9-4382-a229-a5d5cfcf8528": {"doc_hash": "f74835aa53b2d4064d76c0f575af9bc20b7440e0a0eb5276b37bfb8594925aa6"}, "2bb89aa9-f4a4-4aee-b358-eb0db25c2f54": {"doc_hash": "01d5ef2183763f304709d01b71e77bf96c500fbe7b502ca842d6bf5734fd023e"}, "834756d3-064a-45f1-a3e8-55f68788240a": {"doc_hash": "cd75374e4e64cc5924543586c591f86cca39b72974fa454553cd9192b0b3beda"}, "a1aeb747-1ec0-4b4f-a55b-8e841ae01abc": {"doc_hash": "5b7cd5223129c2b6ff00379f9eb669663e7baa9d7e41b004479268c657505e42"}, "7027798f-38c2-413d-bc7a-ebea06b4da2e": {"doc_hash": "96a6c5c8a99d9dd9060a96169c8d4b1511bd0a3aab4f41d7d0c7e768449d1b95"}, "98e00c25-ecb9-470d-bef3-bea7c8fafd91": {"doc_hash": "4ad942abb307cfe882eabbf16cdcdca227cd7e7c722c85fc32ef3015bf3a7abd"}, "a4463b90-911f-495e-afa2-52228065ed4d": {"doc_hash": "780a634bc6e25eba3acd2e9e43a721959ea3e2564cfe04eeec77b015cae26d8d"}, "438eab76-7f80-4a5d-9f52-a8cc82a88881": {"doc_hash": "6a642aec4609ac927243c1f93d5c0630ab242b37ff6d1cb285638f537aca674a"}, "618b35e2-f2b2-4622-87aa-6a58f48b1d3a": {"doc_hash": "839b68373776a64586a6d00f6c8c99331cbe5df74d8f2f21890f6ac9dd98e30b"}, "124a7aed-b02e-47bc-876b-63637c59811a": {"doc_hash": "3f63b713068c5a37dd7eb58bad7c11f63b334bca0bc50ae18dd0656ac38e33a2"}, "e1669e80-5bf3-4b94-925c-9d6557068806": {"doc_hash": "e387c1c41dd54aa81c6b3150d6f05d10c180518fae9d7bc47a7a33cde5e09a4b"}, "4a2e1732-fd51-4c0a-8340-1b06f8dc74e7": {"doc_hash": "39345caa381c7ff9dcf7493324f920de6b4f02aac057f3b9ac405c1f7b1bda70"}, "f342970e-0e28-4f0c-b773-4d31759ae4b3": {"doc_hash": "3bef34185d24d2b91e06192cb9abe8b866b4635a74253d2ff4374854b358b253"}, "5d5f4a4e-86da-463d-9d7b-18e22090acf4": {"doc_hash": "2009258bbc99e9353e06fc0c506ac345a99dc6827c167f06bda63a3750268365"}, "3131d840-5cb3-4056-972b-9ed0f6588974": {"doc_hash": "e04e2c16092d276689fbaaae675520ccae63efc5f57f3ec3ce58605110055157"}, "9c60d7e0-5ce9-4d9b-b874-642cd0715938": {"doc_hash": "fdd2f792de1e6a0031a5a345151ba444fbeac733adf2bf7c38ffbd69cd75032e"}, "6593fb55-8dcc-41d1-af99-c1ff1d2685ee": {"doc_hash": "0b42772738b01f04f69e52bd043636db5ffe65157557b7b0cf9f33866021b04b"}, "5fe9c554-fae6-43a5-b274-712b68c702b3": {"doc_hash": "2265796f112673896decf436cdcdfc2b3c326ebdfcbbdcf1dc20372e7b558785"}, "aa20d37c-ddf9-4710-ae59-64d7253f0e77": {"doc_hash": "a598999fe0c8d1de2a974269aba1876b31c0f3a9db149a7a3ffa6caee9805706"}, "c410adb9-1403-47d3-859b-c4ad48a30e8f": {"doc_hash": "98b3c8d02287914cf3b246689f667591847fb87659adeb9dd58ca96071c37f30"}, "16692cac-39ed-4da8-85cc-43d5493b1090": {"doc_hash": "00efeec21b168714d4c40aae89b22678f383d848b3763e1b6dfe7422eb604d9b"}, "69ddaa41-fbd5-4111-8888-fde555a1cd99": {"doc_hash": "1543fd248b7c8c965ea72e0c02e2123edb720cdbdbdff2bec10618d341729e49"}, "c64dd00c-5a71-461b-a7aa-24a07610b74c": {"doc_hash": "72ac52b147fe59b16d64c59db120b40131bc0c31156cab3815cccb9c5e00b434"}, "43b0e7df-4912-4def-90b2-60604db225b6": {"doc_hash": "2715870278225e82a82d7198708b61a855fc948e087817577b10d2b7e469fbea"}, "ca28fa14-4437-48e2-ae23-9f732ed14586": {"doc_hash": "66210e949e62e954a2331fe12a24409069e188cf94b6897efc883c4a41fe0659"}, "bacddf08-1901-4cbc-8828-930cbabc11fd": {"doc_hash": "5d090d085223a1dc883bdec77a93a6f97c4a7a3f8697156e2e542fe8f46218a6"}, "c97ea8d4-f522-4bed-a453-9eb1cb8cfa85": {"doc_hash": "49662ccd6a43d4a7ccaf0b33cf748e8addd02bd6e2a60720d489f53dc42b0ff5"}, "ec216030-44b3-4a56-8685-bb7fc93f4d2d": {"doc_hash": "19b489abeae1ffe4c22c9d7a7f6c05110f8b3f113232a3351790881caad8b9ad"}, "c1d2415a-60aa-4f68-9d93-aa266d0da7be": {"doc_hash": "334a09935ed7b6c72ff6336bd3716ce924bb427f669e2eefa1eb6e887275da9e"}, "c0669d3f-69d2-4384-af77-e2384c7f6750": {"doc_hash": "523aeddd430ff923315f16cfcaf314aae9d0651b401007a7c6a5ed40ba5af64a"}, "71ef1115-d0e5-44e1-a28c-a8a65590dc07": {"doc_hash": "1951ecaec9484876443ba333cc594979aebd70e8c4ce6c1c2cd13bb986b71ed1"}, "225e3686-c73a-4a0c-8918-a60289e2da04": {"doc_hash": "f985bcb1090ae974b38aaa56c476f8b39efed59cebd6238458b1e1e3e9396c20"}, "df079df9-43ca-4eb6-8a5f-ee9118bda107": {"doc_hash": "61008047f86df26c369e0e7198b912fbf760238f705407011347a049c8e7ca83"}, "c0ae27a9-c425-4183-ada0-09aea59be0ba": {"doc_hash": "b4a091700e8121139cc016eaff89d9521daa08d83218c0f13bdebb23411123af"}, "a0651f0e-0784-437d-ae8c-c7bec54e1752": {"doc_hash": "c2f0d3001363d1b09da2d7896b95563bad511c637f27140dfe237dac5921339d"}, "b52f1475-6b60-455a-86f1-a3d49ec46177": {"doc_hash": "5bb4ab51ee40afc9e4820d81432ffbc427d792ee53aebdad41dcf05e655617cc"}, "3bba584d-9758-4f6e-a11c-5595973d85b3": {"doc_hash": "5e505a5f18abfc99335f50b6f4cb56494b81fc87a5ae8a54c66451a3ce930760"}, "7cf99aae-4b90-4823-a1af-72e4dd60279d": {"doc_hash": "78748dd70cbc145530fc414e48a628d14d0dd4d76d2d5175658e6bbb31cfe300"}, "d67dcd29-2774-498e-95d5-b544c108feae": {"doc_hash": "6da374008bcde4c3e0453a4af28f31fd687d58daaa5d9e55200dc3b47e073bb0"}, "84273060-b30d-4298-8b78-dff3d59d10aa": {"doc_hash": "617e9bf876a7b721fc4f24de2ae3649ae61817070dc3ad3f0ed88bd3e6ecfa9c"}, "b3a560e9-2cf1-47e8-9c38-b793af3d8272": {"doc_hash": "ce4d52b1e86ddf75da0cd371abe40db93af3938146a85983ab53a91793c46a4b"}, "df9d599a-bb9c-47ec-ba1f-07091b411110": {"doc_hash": "d7f0a918410c39fc0201eec40ab7f983c328f6e236189bfa37389a60d5e66a30"}, "b5492367-ff90-4338-a4bd-6e51c8408b41": {"doc_hash": "f5d31977c3d016f9b60ab1fe20bb735fecbe0d310472acebceef7bfdb77bd4a0"}, "0c120afc-9622-4ac3-8db5-de67672b4743": {"doc_hash": "0d31b487ba96c4536a6cddb3ebe0c42073d829026740d78b3e5b409f97dd70ba"}, "b2ab75a7-5582-45bd-b8d6-8da6c499d32e": {"doc_hash": "7902e25ee8574803302568c190746e3a3a638b22c6aea489916d96aee1a770a4"}, "7b9a7816-9df1-4087-b9d3-fce837bbb5f0": {"doc_hash": "f4dcfbe811e6c5556e3248d61b877cd43df3432b43a6ba5308d7fa6f45631e9a"}, "3700e140-0b5b-49f5-84c0-5188fe804335": {"doc_hash": "117a07a9dabe2b78fc19a63c475b4cba25b80bdafe742ab621e3fecda8670761"}, "0a4b4fa7-907d-42d2-9f07-aae78a62ca34": {"doc_hash": "55ab602c4c144d174a6e6b0e7fcb94af773b9fdf3c96924b4307f6a738cdfe44"}, "b3d53c93-3816-4a3d-b988-89de6678d2a9": {"doc_hash": "943d7a8653cffb69326bca16df9f8e15a4c202c171f3cf7050ac9874e8cdbeaf"}, "43291d5d-62f5-499d-8c07-ffdeee836c43": {"doc_hash": "bc2ab1a853b33bec4071028f50e420b17781df7e80582e2cf89ccf5a75643c1d"}, "e468b855-cc21-4d80-b909-8200bf17080b": {"doc_hash": "b3232eefb92bff68d7d76c13afbce4c4b1e9f71ef214f0786b393af5f12fb9ec"}, "83f797ec-4ce8-44b5-89eb-850eb5133e1c": {"doc_hash": "5bdf0763965385c8e2d01a7013b1f527120bbea126439c9874cbafaaa32e716f"}, "58b60538-3f44-4246-b039-bfa7e6c89fb6": {"doc_hash": "4b9995f7831bae9dbaf36769ed303488df381b0afedfd6e2686d00e0d448bf11"}, "5be7398b-a424-4fe8-b370-8b3f29de85f7": {"doc_hash": "3902df11775474e652080cc6c95c1a06e0753d528c4d4792d75560271de480ab"}, "4a264752-044e-40fe-b85f-31e60aad4b47": {"doc_hash": "8d8aff2e9d7ed8176c8b5dd329b0b7ce2ebb119857fba47fe886c50d0606b136"}, "4ef10fae-1db0-4059-a410-4269445e3742": {"doc_hash": "b7e18c6c34c6808a0dff52ba655a4bc671579600cd5e7947f6e65d1eb1e46419"}, "8e8ba577-26c2-482b-8626-160edc2666b8": {"doc_hash": "75e9c15e6b1cc33b6d1cb602fd86a33fd6bcbb4ab5e6c69279de7800917821e9"}, "19737886-8029-4703-9ad1-9dbe0c77a189": {"doc_hash": "45e71524d3c371acf3402cba05b6ee5053b501b102ce026f5ea5752ed41bed0b"}, "fdd10dda-4495-45d7-b626-205c73b0e350": {"doc_hash": "8ce1ea7dd3a87a1ff6478a128a22b15a0c9e34aa73400ea4ab260a510b003651"}, "3fe14965-f01f-4843-8c32-5de48b389f24": {"doc_hash": "eeeeda2d0d53611ef4ba49b5df2915cfdb57f6c20be7f241301acc2127de09aa"}, "369c3d17-e714-4d75-96df-0da9758a69c0": {"doc_hash": "f8cccf234465e1f3439ba38967b061ba53d7b47c96b9fdeeaf123c72e4a07ce8"}, "455ce185-23a1-4116-8a54-50d7c7a90082": {"doc_hash": "974c1b52b59d512508258dae444377c7df9c4401c7eab0991993f912fd69b153"}, "2411fb60-fa01-4b22-afeb-bfb127e8e749": {"doc_hash": "dab1c0f373eeca38be5ea29768fb650cb75f7e9f6ce5db544450342c16bddb42"}, "6e7f2396-0cac-40bd-a590-95885dd01ad1": {"doc_hash": "ff6894d3c14b0fdc1b1dd69e882221494b2f30f30b7b592b6dc414a1b2a1140a"}, "24aeb13b-8341-4cb5-86a3-2367cf7d611a": {"doc_hash": "aad8be9a827925ffccba733c741a6b5c706d8a40da3f390403676deab9a98e7f"}, "030e0f32-38fe-48b1-8cfc-88be95ab76fa": {"doc_hash": "1e0411a36d2a447ca462fbeb35678d6c614b24caa1046f053334875ac5d07bc7"}, "2de137e7-5a91-4e95-bc4a-fc3825196fe7": {"doc_hash": "559ee142e0924181bcd0f61513fcef58a7d2e48a278c266b188999721f12b7f1"}, "31ef0ef6-9099-4b04-9d31-09356ddf80c4": {"doc_hash": "750414a45f6cedb79766c3cdbd0dc149ee522105d5141b89e3f833f0e6801328"}, "9ca373e6-9500-4c0f-bc2c-c7c376545751": {"doc_hash": "38bffad8a367c358bc986dbdff9486ced628ebda75120b1afd9239b50f4654eb"}, "1f14d2b8-2692-425e-a7af-117aa00dc560": {"doc_hash": "ff3c6bcffba9e92befe699d21b4e27af99f0a868b014a5ef4004acd532f5fc73"}, "a9d95332-2e83-4b70-b3ef-13d46c8da8bc": {"doc_hash": "b93167433a0e43274a9622abe57243fd5ebe9720bbfb23a6b5ac642745eb6818"}, "f78015cb-5822-4aba-9634-dc6813c5f4f2": {"doc_hash": "bccc8690f2962d0d2ac5ccccda122d686c44328d278ef670f1ee1abd67920d5f"}, "88ace39e-f3cb-446f-9cf1-629e1172ce31": {"doc_hash": "4b75ac2db30c4e603728b91e6ae6755d59ad72b651ce61185f72881996234d41"}, "68b30d78-5beb-4b1b-b8c8-a3eaf7779648": {"doc_hash": "55d29c39e6ab1933a04adee0615d9c4eb533d4ca7803ff96a47f708b681f4959"}, "5d26d6f2-7afc-468b-b9c5-2bf5394a9a98": {"doc_hash": "86dfaaf2dc6ff8af17ca937933aa2b7770d8f350b0ab29f9cd2e715ba5bcc07b"}, "6e8c0b4b-529c-44de-8d1d-061d4eb83c4c": {"doc_hash": "4ac69e63b95b39227d3904f39616702eb055dbf7bbd3d57bfde65ee7fd537492"}, "9f3e69c2-0fbc-4c49-999a-c1f32ee6b5bb": {"doc_hash": "216c93afa98e1f94ccad1bcf434f53cb4327a625ef94a39c47329070a4f164dd"}, "ae9e144a-b356-4fd1-a5f2-3cfca09816e7": {"doc_hash": "7a002eec7220044779f9cdd112939defa047622e88a69db2897f028ddfb0acc2"}, "8f59441c-7148-4bb4-82e4-00d48a4b9bac": {"doc_hash": "94c04c0db3d933faa9e0f937b6af61b72ed15c254aaac8c23a082ef295fe9581"}, "4787e665-1654-4fba-9717-88d6e6213c24": {"doc_hash": "34e219f88d7258d6928f02c9ad006f2f29f1818a0e7007a5ff79bd97c49a6a92"}, "6098143c-9292-4f57-a499-bd881f4210f5": {"doc_hash": "34e16248b2dab7ea31e5633e4cbb242fb89ebe513dde270506e0bec06b21820a"}, "174a8d1b-05e5-410d-b995-1b5f7f446daa": {"doc_hash": "6360967f76da8cdf1019b0e97db6906637b7454f38cc8ec12db469c816057ef7"}, "5aaef3ed-c7a0-4b55-b679-5fad8959b283": {"doc_hash": "67282deda1230d30b4afaf5196282265843c646241a1508c8dc9934095441925"}, "e0f20ab2-cf4e-43f6-bb0d-a998c38e63f6": {"doc_hash": "14d960fa3b2d1a54905892df98dab4a40fc7dffb502d7b6c46ab0c8481d48824"}, "3b4be816-716c-487b-8cb8-1fa3eaf70060": {"doc_hash": "61612158bcdf4de2dfb2c80ceba981f2a30aa07b88f2f303d89a8b45cfa7ae81"}, "eab24963-c5f1-4db8-b5e4-9a9dcc78bbeb": {"doc_hash": "cd47079844ad944ea5ce3d03c1d96ee64c320cbd38ac1183c684d1db1035b1bd"}, "fd558d3a-bbbd-4469-a5d9-37407de68fbf": {"doc_hash": "e175d57bea188593921a5ef12a744fc44ca077f70d58b813ad06d98bb97949dc"}, "1e1692a1-aab8-46fd-870b-ca0de5950f7a": {"doc_hash": "ba95756cb7345dd339577086c12d1abcb11c4f7f2b291fee83dd18783d20d2be"}, "492f26e7-34b3-45ba-801a-531824a6f307": {"doc_hash": "bc461bf7c45f69ba3e2e16656f25737ff40d79a6a1be103e7ca751a7b1ea073c"}, "717eefd6-75f1-44ff-8817-5aa70f56b554": {"doc_hash": "8c5bff2473a32ebc39d71c9a7e617675133e67f244f215a648318c311e7bade4"}, "e0cd431e-ad0c-45f0-a6e5-207c087fe494": {"doc_hash": "81ce9a22903e77e640843efb0b7d6bd86e37c85cfe74e7508ea00ed6a7d61777"}, "983ef592-fc28-44b2-a9a8-2ef46879f1ce": {"doc_hash": "180cadfd33648924deb402eb5e45f06cb7ef2fef3af9444c92eabe01a78b607f"}, "86b3dac2-9179-40d3-b464-e76178b124bf": {"doc_hash": "e11e18e14a2bfa01ffd68ddbaadf32173f817a629b115f13fea2d5df9809bca9"}, "f08388fc-5680-40a0-9504-66e18baaaada": {"doc_hash": "f980abf2308a039bc716e6c0dbb08bc650c53539b8e754135b0a6ad3a87c0baa"}, "126b16cd-8866-4519-979a-fbb79c99a932": {"doc_hash": "f7bab55cd3bd034027e97cf05f778a3dbd80650890d019ab2fe6cbe5d9e06e50"}, "61895bb2-23d8-47cc-b4b7-0c05bcfaeecd": {"doc_hash": "9631d36492aeaa2509e3ed5eaef6422f5e91b5f5f3c8806c5dd2a5db5ba21d54"}, "c5da67f7-cb8a-4822-9aed-5886d8404d15": {"doc_hash": "c2b2acdc808bd07c7735d5aea133cf5517e7b3312f5d5a24994a77f23503a158"}, "bd41028f-590a-49f1-88df-bd062f17fd4a": {"doc_hash": "39a23a1c3dfb45f32bf777154c2c3da047f7851196abc2fe9820fb1f67e8ca5a"}, "f7dc9c37-f551-4a6e-a623-4c2e336a84a7": {"doc_hash": "e3fbec89f67fe1c6fe2e3efccf52b0f05205e85ac50a6b3cfdc6893475fbb067"}, "5276800f-96e3-452e-aa87-d9c96a332856": {"doc_hash": "85b2fea6f9d476762b737539ca3101fa234fe43ca3214ca2282cd22d88242ca1"}, "02b4f4f3-3929-4107-bcc9-dccc2f6fde59": {"doc_hash": "e74c53c702e28ff8301cccff1df0cf054efd0569de31ce46bd9d12da916680a1"}, "304a7d0f-3c3e-4362-9359-c3d5b5777dd3": {"doc_hash": "d83f4b61e413f96c7dabb2eb3400a26464593285cc24916a107ead88ce2a34b3"}, "8537139b-95f0-4aa1-82e6-e6c0686c7b93": {"doc_hash": "b4b0a7a62cbe5b36c1395cd2d6fa4df001cd84d7f82dc9ea8c28a9a98fc6c5ca"}, "6d2e232e-c67b-4d67-818a-ca81529e7f92": {"doc_hash": "ce3d85a3c612b2a25c46525ca2b61ff59cb4f18d7f9b65d4bec4e15fb59aa328"}, "ad734571-2657-4907-8fc1-22e12393e69b": {"doc_hash": "c3ac5092bb334e4d4911239a4ec5ef73b22f78f97ecdaa9e823f7be863f0d017"}, "a9c17bef-cfe1-4da0-b79a-9043fa967d4f": {"doc_hash": "e94697bb69d35a7ac46a1be84dbb3460e22b6952351bb7c853cbff41393cb9b4"}, "5b72fad3-1ac5-426c-8a67-0b062ac2aa24": {"doc_hash": "c67a689a09b348a9f25d63ad45facccfe63cf102ec8e2a1b90f9501eb05f9ffc"}, "42d8241e-c9a5-4cae-9ebd-7c5168d2703f": {"doc_hash": "5eb67d7548a92cf602506c39ae37d274dab8ad0a692074f32064d2f537703299"}, "6de9d35f-0e37-457b-bf6e-b621a00458c9": {"doc_hash": "8650e9f6a927333136c959061b12a4bb1e1e1b306d5a18bf0d243a67108fc47c"}, "5fb4be06-fa68-46d2-bd2a-60c3500df870": {"doc_hash": "c2e7ab14ed4235967c879365d0e032b8df99ead3d1eab8e0c6ca45c75900a55b"}, "097a64d0-a747-4251-8a56-663b7c99f7b9": {"doc_hash": "872e59bc6c09d785aecbe6c50d67fccadff01de7d9ccc241ea08e8d60157bc96"}, "f1adb9f6-d60d-4cf8-9866-cbedc1ef30d1": {"doc_hash": "cb6721817aa7be9fa612ab7c34dab15afda88949c4f2fb9bad3bfc11c6fc63b2"}, "15f67db8-59ae-4e03-8fa8-a32d43fb4897": {"doc_hash": "1962c4b51f887d5be754056fb67dd6653fd1e2d7d8957efd626fcddc4d72cc38"}, "5f6a8165-b518-43f9-98f7-cbae8632931d": {"doc_hash": "a282aaac9317a4327bca58a3550123acf19a038810f51ef02d8be85a4d42599d"}, "48fc486b-3e45-4eb8-9cd3-7abf312194f1": {"doc_hash": "1a95685545f0a549970197b9d350a92027947e72f4bc1fdfc20249f8986419cd"}, "ce3d2ef9-72fa-4372-bd47-e4c270b094c6": {"doc_hash": "95b39f3e98df56b4956433342f33f0c800a8956f3544e0008bb215de47ba9fed"}, "80fd7197-223c-41be-92ac-82cff791983c": {"doc_hash": "5aff8f791de302963dbc49b1da74e45d2ee9c358bbea9e5d4f540e13287f7cb6"}, "94629437-77de-469e-b198-a264957172ba": {"doc_hash": "20f91eb075bec1d916d2b1f3174813a602b1a48a5b260178e93aea00e7642e2a"}, "9710fcf5-be7b-4307-995a-a044827c7eae": {"doc_hash": "f185038db90a3503f0340ea4de55e04b5c175902b8efafeff990f62ecef2c49a"}, "9a26dbff-c0b5-4f07-8e19-057c9c37ab10": {"doc_hash": "daebeed5c3604438408fe556ff04951875d8362a2ed6fd8e0281e4770fb9756c"}, "effc38b6-67cc-4a2d-8120-0c0df34121d4": {"doc_hash": "c8ca12db44f0f1c4479c4be6828af6a225c3e8e8e2e96cccaf4ec3cfec579eb8"}, "05a6157c-cb63-4311-8c4b-5842260982e4": {"doc_hash": "8a1aa155d20660bc9b67ca0ca6aff68f500f003be6806470323fb49ba7f1c517"}, "560d8476-0eb4-4264-b22a-3fa5bfb876a5": {"doc_hash": "96022139b37b12f33683474f10a15578a7b01496081533295dfd449f2f7c5557"}, "b5277ad7-283f-41e5-9995-d49586d4cb92": {"doc_hash": "4d827a267ba9e8a0cbab0eef03962ccd10463f74197a93cc9d36787ec5ce29f7"}, "77c7181f-a596-4142-a757-d270eaccf56b": {"doc_hash": "4eb3164083e8348f5a61e1c12398ff98d94d719f923e1ece4061c38739b2623f"}, "c767897f-b55a-4b94-bd50-cd45f5603ca8": {"doc_hash": "2acf72419621e91385d95791ee2daf08f9f19fdb8f16f4af8046a15810b1a39f"}, "bfad893a-dda3-472a-bfd9-5e154c9dc397": {"doc_hash": "c88207180bb11b546d38049ba650aa028d69a8b7dad6659671803b3e8fc1d1e3"}, "5cce5e56-9480-46e8-9baf-d6d0e1f634f3": {"doc_hash": "ed0126187ba01314a5c14c39c0471bd5a1fdbfc015e20950c56f175702b6bd9a"}, "6836bd8c-15dc-4239-864a-2d93e1461e3b": {"doc_hash": "a33dfaa21de4590aa47cb799383987570640ab3b138cfcd68822af2b22900d60"}, "3d432e47-783f-4545-b1bd-5139c3c15059": {"doc_hash": "8bc6b27998b0df400aaf460262c0730e226ee837005717c70e934cf0eaf0a584"}, "853908f1-595d-4280-882e-879d1228ca47": {"doc_hash": "a36b0f08c79d6986577a79155b7d92fdcae4521b3c40b5533749237d4176e373"}, "c5877935-558a-4b3d-90f8-5066af04aed0": {"doc_hash": "bf6674b93570289947cb73bfdbab40bb7f585ff5015db477469d9afd51c7eaaa"}, "c06221d7-0038-4909-a32a-10beb9601cc5": {"doc_hash": "1b347e867c646890b66609b33f82d48a8fd509f1ccc875ed31e78dfe735c20a5"}, "ee500273-a3d5-49d8-9acc-091f39ada3a5": {"doc_hash": "19862c81aeb6f0c9b5b41791ec40b64445a6e3eed167aadff0bda35c1c628628"}, "ac812169-2dbe-4ae8-8f5e-4db944318afa": {"doc_hash": "caac5c01a759b6357daaf2f2261ac982af5bcf9237066927956a0f6e8edb7605"}, "928cc9ae-35f5-4bf8-a84b-6147914d4e8a": {"doc_hash": "d8a0a71a4ad58f7f1126161c385a7690a827e9e46f84a1dd35a4c945c3a7faf0"}, "8d16451d-7b2d-4127-a41f-d1430e17378a": {"doc_hash": "7506fe549f0a8c1220b979102738ef2cb1e6c494e6d89266f7608e18cc2e1b70"}, "e2a745d1-c965-4707-b4ab-55713cec3b46": {"doc_hash": "15762110ace6a9e3ffe68b215b1b6efc87c77ba9a728ea882d61818ed9c6301d"}, "e822d38c-1118-46b7-89ed-6fa36726c5d6": {"doc_hash": "67fc01042763abdc3c1c8c8165f6575b319c4a248abca6158c6d54a66ceb5c76"}, "0f92aa44-848f-46db-ace6-2f909b451715": {"doc_hash": "f607480237c010b4a9ada644895eb6f8d7a34dac8bf18ef7d7922b7cf8d52eb9"}, "33c3afa9-7d09-49dc-b043-fd7549e02697": {"doc_hash": "79a591a4b256e2c8d4132ff90bfaed2609ff8c25b2e3352d405f55f08d489f34"}, "ffc8974e-cefa-4a8b-8532-60a43124722f": {"doc_hash": "81aed450a867b6484a370f5913ef7fb016c449e45fdd1d5950b41edf300853c3"}, "5fbf7d35-020c-4161-bbd9-282f30ff0783": {"doc_hash": "cf6a35c9a0681c6037bcd530df0ea919064abb8c374897c2eef2de383bfa667b"}, "b3d05ef5-22b1-4476-9f6b-29d23d41a99c": {"doc_hash": "e957b5d33dae9f834455330b6e0af3716fbef9413a7956b8789f39df3676b7e7"}, "c385ae98-d182-49d1-af2d-babe1a090f6f": {"doc_hash": "00466577c290820f6bbb1af8c1eebe643405be9eaaed9133a853c05d1c64426a"}, "32211d49-2073-48b4-8ff7-1f734f10a6b3": {"doc_hash": "83291c231d6b65b58857b885caaddf4979941aefa879dfd05b64b1faeb840be7"}, "3e62bcc8-9676-48c7-b5e7-23c67f8ca9cb": {"doc_hash": "802dcd4ec88df1aaf5c290ed328a1ea4a49b95a68a80c89cb0729ab49328eaff"}, "07fd9c5c-57e0-4e10-bed5-2745789d0e70": {"doc_hash": "0ac65e6d249bd77ffa3dd5c5833656c2798b30f15bc1cc7626df852dcb8b4ea1"}, "2c9f70ba-b429-4425-a7b0-faf45f8f87be": {"doc_hash": "af4aca06489239cf57cf3221fe591db7937a9870cdbc54f0c6b830b56adf28e5"}, "18ef0941-93ff-42dc-a734-660d9b930f7c": {"doc_hash": "b4b4441ba9ce1035ede5d907ab5d69bf14bdd63a0bc5d33b5f527593f158aff1"}, "656daad1-1fe0-4de6-84cf-c83fc5bb6f24": {"doc_hash": "161c9d9a211d95175333225e02f1b25e6defd2b510ea7f8f7880716e07650755"}, "8955b9b8-b58a-41a4-bcfc-55eb3bfd1b93": {"doc_hash": "ec04ab60eec8f7256056ba573f6b7c36e28352571a8f7f9fd8eeea7a51f87bcd"}, "397457e8-308a-4713-94ab-883b715a2a67": {"doc_hash": "389b21b2401786972d63add03cef083c33e5fc97eb05ceffca3eeaf60f9a5ce6"}, "ab540cf0-82ba-494d-ab73-f5c4e3a97bd9": {"doc_hash": "eb0f636261a00d9d245c3386963f3ae294f9379fcf428efeae322361874057bb"}, "b9b9a2f5-a75e-492a-90dd-6617e54b0b80": {"doc_hash": "4d9fd8526e0399352c0e2307687a80f18a8e7f7b30c940f9c4b13d2985e450d9"}, "a51a6fa6-02c9-4e5d-859f-1021e93cabbe": {"doc_hash": "8b50045fba70743300f1d53f8fb91e17bfb78b79ca4d13c9cd27ac85a9fc3286"}, "f62047b8-29c1-4507-aff8-886f2f84387b": {"doc_hash": "424e3fe94a2906db67de6b6464d1440c271cd2f29fa9072c1691db4cdc82e966"}, "7c9250d3-ade2-46d3-b651-687b499fad8f": {"doc_hash": "3aace01c6714f6c702ff8bb0f5cdd46d16e2409dfb39ac55842e3ff620dfe8f5"}, "613e48f1-6a4e-45a9-bd81-6a4cf41dbcb7": {"doc_hash": "21d1cfae5e660bc56fc759bb48918a6282f069bb06d693910b8d4b3382c05c32"}, "d10b4bba-644b-4f1e-8905-26ff299c52c4": {"doc_hash": "22a45228bb85255af912584389f369868288b642913d0ed6dadf339f860d2426"}, "e4e1e00a-dc67-4746-9699-3add70bf247c": {"doc_hash": "6cc3b9f0beb7c0e25844d02fa710da40b0d3c99ade196a8a129afa0fff7575ed"}, "68d3b583-4b4b-4cf9-9661-d40e639c7d84": {"doc_hash": "dc55251eba7072f27054878f8e5acfb3cf8892cee4e37581758939e5848f5ccc"}, "fefecc41-9720-411d-9e18-8b2087abd80c": {"doc_hash": "452f8708d463e462cf2e6389c6847aa33d132ba52ee925a23980ec5906d1a33e"}, "2e2552ba-c063-48e5-9d9d-03148eff3553": {"doc_hash": "5d58554dc590b29c36a7eab63ddf3742dfa00d5ef1734621060e9872a3e0444f"}, "3b4af78f-dfbb-4a17-ba98-4c9db7e74835": {"doc_hash": "a910154b4623e0968274e33ae83675a4c7322aecbc9cfe9524733454263b9792"}, "5b66a0c3-0d2b-4040-b753-4ca068f46d45": {"doc_hash": "60ecf0bd845b695d04ca75db3e22ddca88d26b4575b207765d464da7afb05534"}, "b955caf2-228a-4001-ace2-365da4d78617": {"doc_hash": "e89d25ad03ef86e5cfd27e1046d9dec6fcf0439cc69d1ab71157f4f051cfd963"}, "063e8cf1-f44a-4f0a-a7d6-da2b1b7fdca8": {"doc_hash": "b12ce2dcf7cad2508fa35ce79d5ca23f6196d8de951ef5e88282801b9acc88cf"}, "4aaf6edd-5198-4cd2-964a-7b0955556f5d": {"doc_hash": "336217e241d253b8cd4df3b26a40ec7fcc7a79d952be095e867d060418887d48"}, "64608d2b-9958-42f8-b7d1-d31f57b2a7f6": {"doc_hash": "687e71a4841a81dd208882a0125d8bf50128e5ce3aa45b66d6d9eeab23144973"}, "55645ccd-a764-4c82-8a5b-d5c1bdb4977f": {"doc_hash": "b08505d4cffb9acf5a39f8cce903f04a9cb6c3b637722d63e84734f96a70369e"}, "6c9c85b2-4210-453f-ae49-4804109196de": {"doc_hash": "2e19fbdf93a8b6e2745164186d2e50cfad5241cd6675aea89f9e426aac417f18"}, "e8f083ac-d9bc-4dff-8791-3514d3f1221e": {"doc_hash": "cdd294f65a1a422970bac3f85e48631410bd17065f7f4d92c7ceef2feda58d5f"}, "57a0300a-28b0-4de2-bf3d-482f16b49a6d": {"doc_hash": "8efe150eefeeba082bd9ab056ae5086d46fa29c3f4fbb736c50ca945dfa67129"}, "fec5a5df-e972-44a8-9721-4a6a5bd71f4c": {"doc_hash": "1730dedf34b2ec2bb422501f4404a236a512732c2ed794f971482db39723e667"}, "512837ee-867d-4121-8de8-ba4ac5dd4ddb": {"doc_hash": "15669350a121c93ff0dfbb8eaa666c4c922fa78410d718fec145daf14acb8f79"}, "afd9f82c-3eca-4281-b9aa-3d0696d81217": {"doc_hash": "826d8ca86009ce75fe962b403a0072bd681f9db4590e752f82294c19324e328b"}, "7536d56a-0f50-4193-99b2-5211a824a7af": {"doc_hash": "90d089f638924a9499637480b8f22d7b9c8785c1b040f0e25108e3c6e93b88b5"}, "f514bc10-07f7-4289-b367-16ad3e66bea9": {"doc_hash": "b9458fcb2f3bbab6a61b5b34c34d546826a0a9028cc17e0328e9fc467525934a"}, "641b07f9-8b08-4fc6-bb19-5fb76f470409": {"doc_hash": "2ff9389e4e0e7a163179cf4de15982d6052119193a3cad72f68c48bf94bc3861"}, "2277f387-c738-46eb-8fd3-5261f944ae81": {"doc_hash": "cab4521cc8ed0b6a5a198ecef801a3f54eec41d68c86abc8fbceca644ca3aa1d"}, "cc9b6c1e-7161-463f-86cd-c63038729c4c": {"doc_hash": "a54f43cba9aaa5a8016a7de10c9aaac6254a83e1625f8c9b59a31a527df46eab"}, "d824f03e-7afe-4bae-bc25-fccfbb343a7e": {"doc_hash": "f2af3fe376f5b62fb5f2eb636867457b6c77d5e24e7457afd63dd8853e682c41", "ref_doc_id": "718eff5e-0d3f-43ac-8226-5232381f4af0"}, "733d022d-092e-45d9-95a6-aacded131edf": {"doc_hash": "5078d9da3d977f63e97a759e44688c64192e2dd217feb532f1ad57f801b64f3e", "ref_doc_id": "68413b82-f021-4b71-91fe-e2bb6ca0eba6"}, "4ac3d829-791e-493b-8b5e-6a90dfdc8c82": {"doc_hash": "a39e157b68b0eac36d25a445661f4fcadee7d8c77134f2b4d3a6c4ecdbacea31", "ref_doc_id": "84480dbe-b03c-42bf-9b51-3e044e5d48a4"}, "9071df74-4cbd-46d6-af21-f7fd4b3fc58a": {"doc_hash": "2f18301b716ab11ee3a2a63c654beace0569538952d6ea742f3f9052cd85de40", "ref_doc_id": "9829986e-2155-4196-a42f-1c6e40ac130f"}, "e88c2583-26da-4339-9d3a-0ab76027e03b": {"doc_hash": "98ebab5d8e342b5765eb38b149a57216a83303f20a3312e896774205f44bbc7c", "ref_doc_id": "79806eb3-8406-4f54-bdbd-95a12dcf08ed"}, "51b54416-e865-40dd-b796-b3ae2fa4bd0d": {"doc_hash": "e053f3006acc572062bc177daa3afd491c7f759d4cb66017c67c6bd2f2775e5b", "ref_doc_id": "4e48a309-2246-45e8-802d-53cf0501fb06"}, "3fbce58b-8128-4ce4-b322-170528063ddc": {"doc_hash": "7946e42c1fcf99918efd14a87628b0b935fe79561c05ecf3168009d43481cf04", "ref_doc_id": "f9478ab6-7eaf-4ce6-bbbc-d5258f31b157"}, "eadb352f-c537-4734-81b2-392af6f7014a": {"doc_hash": "c90d1c3344fa8c700815edf6e02e69af037548f25ad0ac57d283f87b3e669154", "ref_doc_id": "bc9a497f-1214-4008-a29e-fad59826e416"}, "3bd3cd9f-ea0a-4096-aa21-0f6881fa2b2a": {"doc_hash": "c777671c11cc83215ac5a373a036287779c722ee66dd7ab5d3736476e4e2c4bd", "ref_doc_id": "4fdae7f2-4ed0-41ad-9971-a8272a8fd4b8"}, "1f5db6d4-b49a-49e3-a339-d4253926c36d": {"doc_hash": "59e8200309dc7c91e942d18995d4097c003a7cbe08b9c98935e62164ed05c1f4", "ref_doc_id": "020cc405-8012-470b-8ca8-192e424019a4"}, "4f2b435c-1f9f-4244-8865-f0ecbcf51166": {"doc_hash": "4d1626c64d76473ae4f51ca8b7bf94f21ea43d2e7e28ef0878999ae43f8322ed", "ref_doc_id": "2e12f008-0c37-4675-9d4c-25703a56040a"}, "a659beeb-9513-49c8-b514-9eccae8ad32b": {"doc_hash": "f9da88544b4e6759c75e3a84d4d259e4fe09b5fdc8dbf21ed7ecb1f30642b3bb", "ref_doc_id": "625db00f-e7c2-4132-8c75-cc7160e87160"}, "10279548-9563-4846-9cfe-9b14b61999d9": {"doc_hash": "9d81c29b9d56da32881634b81709ed6bb2d4e2753e1959e6daf94fe71b019891", "ref_doc_id": "1eb5e137-c72c-4e63-b7c2-b7153801d13a"}, "cd5b61ac-21c6-4a55-a278-1974e0e29e92": {"doc_hash": "155cff4aed96f4242ca043d81e97adcdcd36bd36dd7465c5bf52c9700f968aff", "ref_doc_id": "e50abfaf-1789-47b9-8d95-1af68955910b"}, "bb04e546-3cfa-40e9-8be5-2a80074711c5": {"doc_hash": "7526e1fc679e96f972a57d3a94ba320e5fe4d09b9f918589e8efd8fd4d332a70", "ref_doc_id": "7956e872-9719-427d-b09e-502e92e1afb8"}, "d3c30157-ab36-4772-bfd4-7366d3b329b6": {"doc_hash": "7c02ce6a5802ad26cbf7ae7235086cb7ed66e4ad1f4f398990f4eb5926654d28", "ref_doc_id": "ca9742ea-0801-4e54-93a2-2aa9863274ad"}, "9f0ee467-7666-47f5-8fb4-cd7c3d1d2c66": {"doc_hash": "b074b294f1509adc613585960be93f7c5699feeb11791799af939fd3cb4ab0fc", "ref_doc_id": "5937be6b-7e42-440c-a403-e75af91ccc9c"}, "50240af0-e371-4bd2-bd53-7293a9cad051": {"doc_hash": "6dfd7d6d87e025cb9074979f31eb4e416ba59a2a569f61efd99952140e9a74cc", "ref_doc_id": "f131e00b-603d-4242-a728-fc6ddcd552aa"}, "590d9183-af52-409b-99e2-28a524a7582b": {"doc_hash": "85ddefbcfc39ae8007ff23299f259a7a56c481d02c93aa8de539dc3d1306a9f5", "ref_doc_id": "5c3379b8-6495-43da-81f6-82621a32bb47"}, "7931a6d6-a775-4d34-bbe4-5f6d0822ba06": {"doc_hash": "d7ae9fd6809b2abde79258cd5c8db89e29753643179f6a4effe1ea6d78006784", "ref_doc_id": "23adfc25-5664-4baf-ab8a-1beac1691f15"}, "8d6e4149-8f6a-471e-a4e2-58b010c887b7": {"doc_hash": "7a630ab7568b450a7e4fcaf4cfe12be37aedd210711ba679d7235f554488a91b", "ref_doc_id": "3a86e926-5a74-4ed2-a224-6ffc6784f2ba"}, "4ddb7105-bfe1-41b8-a8fd-a999a33c51f4": {"doc_hash": "edfb1df63eda93fd907ed65b6a41f954a95676d1eb8f991306974122adf8d458", "ref_doc_id": "755050d1-ae98-4329-a2ac-a91dadf0a1a2"}, "54e8887f-8f0d-47ed-8caf-11d09abfe87e": {"doc_hash": "b0251315c325fce33ee1317e2e47bb27efb7c2e90bab0b9a85d50e533a73ac97", "ref_doc_id": "8cc005f2-60fa-4609-acec-094babedb29b"}, "1496c6c7-7bef-4855-a34a-225211f5e342": {"doc_hash": "caced76ebce96314ab03a82db9f97fb66f88d098bc1e810a8bd05b7e67a867d4", "ref_doc_id": "f3e6c603-b7d7-464c-87f7-68992ff22852"}, "3eab7099-37aa-4f51-a768-aba28481320f": {"doc_hash": "a198b87397acb6716787234770e42d2dc14744c1c29b8f4dc20b25b03107578d", "ref_doc_id": "b39b720a-aba3-401e-9e2c-2097e339cc6f"}, "c524c6d5-3fdd-4a1f-a53c-85daa3af1204": {"doc_hash": "ddd90571006a76786c903cad98317c23ccd398b9203b757c7d5e9e1545c32efd", "ref_doc_id": "16bc8577-167b-416d-b8de-db9bd3ebc3aa"}, "3e435bd9-f4b8-4863-bb5d-666584c289dd": {"doc_hash": "9c3a3d56e84e4dfa271c40d741df0408aa290829a9978356ac7f92b73e7d27c4", "ref_doc_id": "a036a6c9-2a57-4805-92bb-a42b6ebc9dc4"}, "6abc4124-71a4-4bde-b930-e176e8118d05": {"doc_hash": "bf30a669f76684622b1294feeca415c4c875c90370f06f77a75c8d2d7240aa5f", "ref_doc_id": "50d12424-e819-4a2f-8b84-b71f94cc56ef"}, "416fb828-8a58-461a-837c-24e2a6d72663": {"doc_hash": "71188e290e56ded64e98836f960eb76216eeb5646e1700e65d071f2202b23fa2", "ref_doc_id": "f30241e7-7fbc-40a1-be80-fd5120d627ce"}, "35f3d01c-a53c-44f9-ae06-c5ff6aa73251": {"doc_hash": "bd282e50e04a4d5a75146b862c135ac515f1c463e440e58f83dbc8e4520e7d26", "ref_doc_id": "ad5b7a7e-e541-4670-b0cf-ceb0f3fc9c15"}, "40b49747-41b5-4b32-bf96-12bddc89bf79": {"doc_hash": "e6665d3cea809160e452cf676dc0f4f6d50920ed0f5f5f40c9058ea5d47274da", "ref_doc_id": "4f2fadff-692c-4d4e-b1ed-8c90d1920f2a"}, "9bd548fe-e6a6-4f90-a147-50f50e795611": {"doc_hash": "6c3f70b1160097ec5a8f798205229917d28b234c619d643c8a1d5ffbc7e61d8c", "ref_doc_id": "4f2fadff-692c-4d4e-b1ed-8c90d1920f2a"}, "0b2a8efa-0b87-436a-b681-bddfafed6913": {"doc_hash": "f3a955e1ab62b5551cefba3d1a289f1815358d1d0dd7ac388462a59962913e2e", "ref_doc_id": "f270e715-0e54-40e6-8bdb-4fc61767b8ab"}, "dacbf4f4-304e-4819-95fb-2f135c59e6de": {"doc_hash": "c51b0903064e1b053ed805b5fa5a6cbbad61dc3084b4bf2190bf1718c94fbcd0", "ref_doc_id": "d1893d1a-6cd6-4d84-9764-db71eeaf4acb"}, "641f9f46-a463-457f-93b5-2dd0c1394831": {"doc_hash": "6d18a427353a4eb106b5745ecaf5d2bac50f695e361be985fbfe48e9b3a0a810", "ref_doc_id": "18327ec2-e894-42cc-9a56-5eadcd04dc1c"}, "a9ec3348-8024-4dcb-9e06-2bce7509ad71": {"doc_hash": "7778101e92d7d20b12a86db7b9c836e8b9c1ade604465f153916469f55483a51", "ref_doc_id": "33c1f82e-ff9a-413a-b81f-487249871b1d"}, "d19c769a-2083-46cb-af7d-b21ae6794916": {"doc_hash": "7be90c932fcdc75d114d7c13e60ae5c78fc1637066db49e41a601893cebdc18f", "ref_doc_id": "725b4510-4255-4142-95f1-94093d6ec3cb"}, "f379a3e0-3b49-4107-ab83-75744809f415": {"doc_hash": "18dddadeb0109af1039bc4ffdeec8411c4cc0f18e7e6ffb811f2849e88cfa745", "ref_doc_id": "6afe5b7b-8993-4a70-9982-e59a7f594cbf"}, "a5033a77-aeca-4b6e-ac2a-c4892ae75f89": {"doc_hash": "84b7deb121d04bf52781293623118bf18d01483c97a05a21aabbfceae52488cc", "ref_doc_id": "faeec7e9-a99b-43a5-9bc7-90765ce8fa36"}, "6d7fcf72-05fc-4066-8d12-7d7ac24c04a8": {"doc_hash": "d74308c478e3e8593c003ea9944e0a34a88b9b88b262b05284bf9d0b6b9956a1", "ref_doc_id": "4b1341be-2b39-48fe-81c3-6c4688f41ba4"}, "db467600-1227-47c6-9134-4b3e30cc1a4b": {"doc_hash": "f09dfe863fe94e1b2aa734bd0ef8213c043999cbebcf8cdafe8d9542084cbe06", "ref_doc_id": "87af99ee-1cc2-4516-bed4-556292d32e37"}, "bb8e34ab-e7a6-4b6b-b1c5-9a3fb1acefc6": {"doc_hash": "2080e75b80031701df6531060639713fa8fdcdd520feda628dcdd004846d9e38", "ref_doc_id": "d707ff2b-acdc-47e9-b94e-baf71a1ec631"}, "b647dc54-6834-49b6-83b1-cb969bb077b3": {"doc_hash": "eaa1298194f7af79069d48dee019f97606e7033c11ea8412fd1fc81978303789", "ref_doc_id": "9ada9682-d106-4b7e-9d4f-0e8990038a59"}, "0a2b2869-b579-4b93-a420-73182e14a780": {"doc_hash": "4a4e0a9ec6f356263768e81c050e32a1215d7d476472f6ad75ad2df522064b59", "ref_doc_id": "ab7fa45c-4770-4bbd-99c6-0160ce218d38"}, "bf459a42-d58d-4a62-9379-7e579bba9c57": {"doc_hash": "102c647f8d075859720dd48e290f6e95055e00f4ae893da18d6ff1ba6ce5967d", "ref_doc_id": "6e1e3990-cfa5-4bbc-980f-a8e3a7a2b9e9"}, "fea18bae-690f-454d-9427-bd3039f7c5be": {"doc_hash": "53f5049e32174e452b6c028d12b65cb625207509055479134122d7e20f59f236", "ref_doc_id": "268eabe5-f4e3-4f49-9e16-049d211c0a4f"}, "1f6a5995-e723-4227-a57a-89c267b10c9a": {"doc_hash": "78d221c01b0de52b1c013e6491f9b96ec4768363ff504dccc1f9f1f04102f10e", "ref_doc_id": "7a6ceddd-0d6a-4d15-89d0-3ea9d8c02b9a"}, "aa6d42ac-4c18-4dde-b74c-06217b84e8b4": {"doc_hash": "337e815769710cdc0d0e3d51cb555bdc9617850948aa3048a66268191caa68b2", "ref_doc_id": "90f29465-ad78-4161-bdd4-4ae10781de61"}, "c36801fb-e233-40ea-b740-562992b9cff5": {"doc_hash": "09bee5fd9653dbe96e4f0282ddffa012ed8864750d72c18eff1cc9222c0d8548", "ref_doc_id": "9b4aba06-fc20-41e8-b5d2-f09aa3342cc2"}, "3f110e45-85d8-4ee6-8215-31ef759867a5": {"doc_hash": "0866519932f70c8e415e3990f6f6abe8de86a0a9668b807592ee63b04a733ea3", "ref_doc_id": "4f3ac8ae-3b5c-4aad-aea4-d16630952fae"}, "5881026a-a66b-4449-93df-a2eb9e9fd38f": {"doc_hash": "c88e4e1a9f30d0fea618c9486f3f331d0409455ceeb85d03ed9b677a6397a100", "ref_doc_id": "3f851b04-1eb8-4853-8cb2-6522a9935683"}, "01a51abe-c613-4a2f-8b18-eaa5607a0c0f": {"doc_hash": "c4100b1e7cafd8901049c2084f374eb9f12a81d4c215052b02bc3fcd0643ac55", "ref_doc_id": "2d9331a7-06bc-4156-a1cf-c028d1b8f952"}, "93f3b5b6-01ac-4cd1-8473-ac16d244aa27": {"doc_hash": "4c6ee53f4aa6a6e79c24c351f95e78ec456526ee3baf4ea3ddda9db5612c1e90", "ref_doc_id": "993ecd16-3dbb-4ea7-8382-601df0df9a73"}, "13975d75-bf86-4362-8056-afadc898ab21": {"doc_hash": "8fa555a58d0247396d0b7455ecbb3539681ed7589d5316778f1cdebe365b88e0", "ref_doc_id": "d3353383-62e7-460c-9725-5c306b9e0579"}, "3137bc4c-ddf3-466c-8443-480352a6fc6f": {"doc_hash": "7762bda0ade3dc820abdc887038014479a903c2d7a5ee858e62ee5e4a5574566", "ref_doc_id": "e26e1194-279a-474f-bf68-a760cc5f5fe2"}, "7df582c1-602c-4ae5-9472-b8cdd17100be": {"doc_hash": "6b7f3484c9d503b715b52509559468ab48199a21a4db1b87d1cd269826aa1075", "ref_doc_id": "30aa02eb-ec59-4612-b2fe-c41afbf31f05"}, "f892c4cf-8c0e-4278-bd9c-95a8c6225667": {"doc_hash": "2fec4b4ac47827ed5e37284ce596aa3b0481805f6d87665f61492c25c06f19c9", "ref_doc_id": "1b3e409d-a989-46f2-a0e8-e68eeb5bed44"}, "a586481f-c25e-4ad9-bc02-65108fb45c8c": {"doc_hash": "3d1e6674c9d4ce58d6eb7af9e275670d2307daeb026145afc848f44280fe3e08", "ref_doc_id": "7f893caf-6c66-4b59-bbd6-636fb8686bea"}, "78b2ab0b-58ff-4e42-b704-d156302e243e": {"doc_hash": "b1e1a7046b882addc1aa3e6a3274e0476227baae5abadbe9907ea9df998ac470", "ref_doc_id": "0b059111-7d3f-4f17-a0d4-e5c40ca3df67"}, "599dbd15-a868-4dfa-81ce-1175022507a9": {"doc_hash": "f73a2859013fc6e3c96d88e9114e18381e4870e017ddd3eb7ce36ea401a66736", "ref_doc_id": "71d356a3-d694-40ad-9d24-75e98adb1162"}, "2bfac92d-e15a-47b8-bbad-cef729e947e9": {"doc_hash": "7ec41309c977af2255428246816d81ec46c91dc5d1994a8b43b2979e7019d365", "ref_doc_id": "f9dcc3c3-a559-4304-9591-3304f007e38e"}, "a66fb34c-e373-4216-90d6-cef2b90fcbed": {"doc_hash": "836f758eb0bbcec20507ef14e4530496ec7ae83513533d40ac2d2f98cbec8407", "ref_doc_id": "80fde06b-e558-4d1c-8b95-96f704a0c8ed"}, "c8ad36c6-143f-4a88-b578-a7a3ceaf3953": {"doc_hash": "0520f42c2fada15749a0347c14348e69a7c052a569503a47e656cf56ba929c5b", "ref_doc_id": "faa5f3df-ccc3-4ce9-9ab7-76660c24116b"}, "e9da1c07-f300-461e-84cd-c6f7f470907d": {"doc_hash": "e6545b30b6ac5362eeb7756d8e0ecd4961de79fabb1ee2c96de333dcecb8ac5f", "ref_doc_id": "193bd3b5-7552-444a-b940-2d54c3606127"}, "e724e379-5f22-4552-aa6d-c09866d2414d": {"doc_hash": "f3013835dc44245e05b8e71c8ef78d6824bf2933dedf84f4d0fe686a7f436501", "ref_doc_id": "d48f4050-f2ea-43ff-9c61-14a2b05dc1cf"}, "32f0698d-a8d9-4a9d-a2a4-d60be47a111e": {"doc_hash": "da3bbc77ea3ecd8989b6b9e81a31db60c701855a50facc5ee5b8dcfea1adfa33", "ref_doc_id": "8741c4dc-bfa3-4852-b7b1-832e8a24b54f"}, "2e1a139e-eb22-4214-ac35-fd7f1ea73c92": {"doc_hash": "291461fd2916ff9b307180c7d7694e8eaca8ae5c06a8c66b6775481c9b4ed0a0", "ref_doc_id": "adede142-f86b-49fc-b002-f2f3a5c50505"}, "6a4afd83-d4e5-4abc-8e56-138e285a0439": {"doc_hash": "51ecbe638d67be7c383d9e1e64b94fd963ff96a2e2643b8882559efbb8386c83", "ref_doc_id": "ed028e55-e520-470c-beea-6aafeae3f4a6"}, "15b531d9-d868-42ef-972c-9f35b91b8797": {"doc_hash": "c0f02a632fb6d54adf4ab1170dd7592351ea0ec0acfbc219d3cc780d08a46754", "ref_doc_id": "6d1b6549-22b7-4e24-ada5-6a9502eaca5f"}, "70443837-b621-4d2d-9952-16940a6111d0": {"doc_hash": "9ca3cd51715be1b0cf397c1d130ca240a1d5918d1dafb6a43df9c74d00f377bf", "ref_doc_id": "a07592f2-ca99-4d0c-aaaa-90ab9fbfa598"}, "81f4ac52-a5bf-4771-a1a8-676eb94a99a1": {"doc_hash": "00afbc61aa60e1886e52cb318cfc248b5ec414a75b57fa4b7001bb30d4c41836", "ref_doc_id": "ff1812b8-79cf-4da5-982e-7ee198ff9407"}, "52a0de85-65fb-4c08-9bce-2569ffeae690": {"doc_hash": "c65ed7c8845b4e122567f1d8a97cbcfad68503db43a318cfed63d2896c25cd13", "ref_doc_id": "9734d0af-7af2-4e4d-a5b3-19545f934a24"}, "bfafcf4f-5410-47a3-b048-8ef5223f6654": {"doc_hash": "38576ef6d81072bc41479ee51df56431f155b06a9bd926db2f681c2d79e47299", "ref_doc_id": "2612028b-7dd1-4ec1-a359-3a3870825446"}, "bc4f408b-8989-401c-ad64-a9ef412fd064": {"doc_hash": "7083c4c8f64f14c4abd59d7e53471f88b8cd1feaaa0290c18245757e4a3b7dcd", "ref_doc_id": "f02f67e8-15b6-4701-8f7f-672b35b4b2da"}, "33ce1c7d-dc1a-4e46-857d-8ea2cd3cf2b6": {"doc_hash": "430de36e1a3e28031e4b3f2a4a0546f30a4bc4da12e84cb58b8c17e661925071", "ref_doc_id": "4008b542-5c38-412d-a8b2-0ec2b53e812d"}, "f5e2a343-170a-4139-8156-ce12a9577b72": {"doc_hash": "9234f2a5d840f2d956fd98a24f6e32ea52ea577dd8466357b27468c957052269", "ref_doc_id": "66ec205c-ca01-487c-a425-779b8385ce03"}, "4fd92611-7fab-4441-8794-92a5910b24d5": {"doc_hash": "b78a5982fccb80be0955de289109d10daf1ed27461bfed3170a750a8a0679a3a", "ref_doc_id": "6f81479c-1371-4697-ad90-042521adbb4d"}, "0ab73145-2116-4c2e-b60e-f86a33226e5e": {"doc_hash": "579f60f7a1529fb8c982df78c013f99cdbe711a8f37fd6e6169cb23e7778a0b9", "ref_doc_id": "bba7f5ba-dc84-4b60-83c7-eb40367fc866"}, "59e90f1d-11f6-4fa6-be21-de0d8e7da99d": {"doc_hash": "fdbc15df3b24ae5bdbf1d2c58e53c4da9dadbab83d7ea488b8afd448af40133f", "ref_doc_id": "bd52669e-6b5a-49e8-af5f-d8d98e7cdedc"}, "69fb3b61-5fe3-436b-bb81-e079607b99e7": {"doc_hash": "f3ab48189a9d4b2f5285703c5ab8ee1f023de37fd29785a478e1dd7beb226a24", "ref_doc_id": "c6150d04-f7c4-4dcf-8c21-87caeda2ea2f"}, "b17234be-52da-4b46-a51a-1cb5333a1d5a": {"doc_hash": "dfe0b84a9fffd48fe26a99626c08405cb53e4de184bed0c7a0ff6be4fc758b0e", "ref_doc_id": "ee15e2dc-1078-4e4f-a25b-31828a338d6c"}, "0144706d-f33b-4f05-af65-36d213bf4dc2": {"doc_hash": "de2016e2db6c9fab2465116000f1d0bfb0d48322ecc3384badee61542e4ce528", "ref_doc_id": "cb7f96df-28be-4cdd-bbec-203a49fc8c6e"}, "f21c069b-d7e1-413d-9e34-146fab6365b0": {"doc_hash": "a94a7aae5330cb5ec10270a2e18b02b06032e1cbf117e4e0312b85d371ce8ae7", "ref_doc_id": "0a8981be-7da6-461d-a7a7-fcf1249dce5d"}, "05075055-abf5-447f-b648-c7ee9dfadc97": {"doc_hash": "d1c80860a6a983a908d2338ca311774e343f32531fd6720f8480887849c5fa35", "ref_doc_id": "79e09c87-7906-4c1a-9b88-d93cc0ef91dd"}, "c77e64e3-6c4a-420f-b00a-3edd2cb474f1": {"doc_hash": "bc96b40c095808c653180ddabcb2ecba35aa3568162a271a0bb9bf44477badcb", "ref_doc_id": "ed575f07-71c8-4abc-8fcb-b626e8a8e240"}, "f863320a-4fc3-4296-8916-c7da2406d2fa": {"doc_hash": "bd33d778978a6983c57fe4854e3fb192c8aaf26086abff6bbf9a65eea04d4cc0", "ref_doc_id": "42c4ac4b-cef1-4d0a-8fce-d4e40198749b"}, "c2ed04a1-98db-46e5-9381-634f0f2565ba": {"doc_hash": "4c2bc640160d8f45ca1a6f499d92fcc4cbf828c8ef257c71c72b308bc165ce99", "ref_doc_id": "b2399847-6c4d-41d4-bcd1-9721215e597e"}, "a9a90cea-6a16-4cff-a549-447cee2b9d0a": {"doc_hash": "cedd72b3672baf82b4408f725dd7165213c13ce6d2449dc8c82167a291d581ab", "ref_doc_id": "11d4ef51-b189-4038-9718-c28622a03dbf"}, "7148b9e3-1f6c-4e4a-b15e-94d4daf221a3": {"doc_hash": "e0a12d914b9c128f7f4ce203856e30d56d7a13822f8ec1fa378f5f836d4deee6", "ref_doc_id": "787d7e78-b7e6-4afc-a43b-b08a2208eca7"}, "60ffbdef-f205-4c67-8592-b086ddcdb97f": {"doc_hash": "7f6e786aa42ff252125ba78eef39b0717ac3c808d3530206e016f2790b0bf4bf", "ref_doc_id": "e5a07e84-b24d-48e3-bdc4-869410dbe674"}, "cf3209ba-d448-4555-880d-53162fe979f9": {"doc_hash": "d4d65f82976188270fe272e3cb253dd786f3a9087990af2387eff8fdfb57c8a6", "ref_doc_id": "b1e526fc-e769-4f49-99e4-3e1247d91071"}, "617bf87d-2e1e-4115-867f-cbae3746b6e4": {"doc_hash": "f5b1737bf3e4fb47d42c9c0fb1cb402f1f8cbea85a51244f7dbdb4310c6ca2af", "ref_doc_id": "93848c46-d556-4e4e-8e0a-0241df0fa7fc"}, "b9933bd6-f123-4258-95b8-f385b5513fc1": {"doc_hash": "f0ebdc34ba4873b7d05ba4f09199056dd05558e4480a2b6680dbd394e08dc3db", "ref_doc_id": "39fc460d-a7b7-4bd7-8ee1-459e7470c5df"}, "7927626e-d9a0-4bbe-bbf8-81369eb40728": {"doc_hash": "53ec3ec33b8526db323a6f05e6972b8f859b1198b29d911e625fdb2d1e0eec53", "ref_doc_id": "e7968dcd-5cd1-405b-8bf7-0d5323556670"}, "bb2d9984-67b7-4d8a-82b8-91da3e8d5148": {"doc_hash": "ea601a4fbd71a74c8c30c8d3df6e2c64fd2ce20da3fe3d3f830f72ec9a11139b", "ref_doc_id": "d1bb0de0-db2a-4655-af02-fb158c4efc55"}, "0cad6b4c-576d-4218-8ec2-fa461df72811": {"doc_hash": "09675834fd07bf2c300d0b42125958690cf034733c23023d4422d26febb98964", "ref_doc_id": "1a032f26-cfb9-483a-b87b-d2fe24205f08"}, "e421b952-d4f7-48da-9fbd-bcb756e1b7ca": {"doc_hash": "90f7ff2d9c10273b32dac06be88f04afb97b40211d96861791a08c92f1393277", "ref_doc_id": "6b4abbce-a384-41fa-8fae-4031aeca7d2a"}, "12ad2c3f-fd69-4ab4-b29d-3d46696cbb8e": {"doc_hash": "e98f7699a4eabf1fdc2b69aeac34c165447218e2ef4b5e5d922513631e094475", "ref_doc_id": "f8abe648-a4f2-4069-a5f8-4a6f68112cf1"}, "ed4790d1-f0ea-4240-ba29-ee1f4c0b5a7f": {"doc_hash": "a39d24b15b918f668e9f3fedd051ba900fb38cd5ac44e4b0c265c24508bbeb3d", "ref_doc_id": "e1cda2b2-4e87-4c41-8b20-d709a342f985"}, "f40f765b-1d64-4843-9d95-effd0747b318": {"doc_hash": "d1d21d3bc3f93c5c55be1947efb2e67da24eb37b093b93db360f1bfcb007ebf6", "ref_doc_id": "e1cda2b2-4e87-4c41-8b20-d709a342f985"}, "f1727c6a-636b-45ca-8997-946b790d86ca": {"doc_hash": "a2a88acf8acb8a750c1a4da110ddbccddbc4354facc4d1a3821e6eefc3da414f", "ref_doc_id": "c29adde6-2d1a-4aab-bc2a-c6cad078aafa"}, "05b4a805-7dea-4c47-928e-8aeed6583602": {"doc_hash": "bbbfd9bb666c7af60b15dc693f594f0f3ae67a6a49c410832cbd5dc0d196766f", "ref_doc_id": "1f3e80c2-dfac-4a3a-acdc-bf2563ffc259"}, "4b4ddc73-e2c4-427d-a387-a1fb531a33ed": {"doc_hash": "c0e77a208e0df099dec0dcaa14c67a24f4e8ee3a518c567ade53dec00728526e", "ref_doc_id": "27e666d4-ce96-485a-b671-d3281a6b8c4f"}, "d79ded35-b431-4d0d-b983-885f8e797e06": {"doc_hash": "7c8bbe17e3785c33b254c392b9bf2c89ab09f38567092c759837af2b01f7607f", "ref_doc_id": "6d2a4fc0-a7b2-45b4-8cc4-a0e55285a204"}, "ed3a7d16-b2a6-453a-b0b8-d93bd8855ec8": {"doc_hash": "4d9088f9345afa59fe3b3d5b30cc581d16b806249bde190fd2575455b054fa7f", "ref_doc_id": "e8f043b9-8820-458e-aba2-87c03b8ab06e"}, "575e57ca-c73c-4351-b8e9-f2e0bbf09631": {"doc_hash": "95ef466bbb241be89fca2d19ac25e844b570b2523b0524d7e53810c23f956e6f", "ref_doc_id": "a6d4ada6-5953-431d-954c-d724b8e51203"}, "2106326d-5ce7-4f94-af00-bcab066aa183": {"doc_hash": "6c5e4bc6219cc700a0329f18bb9362f8498f3223f422bb00ac875251dbbcba49", "ref_doc_id": "56155bc6-0e0f-4be7-b8ac-3b92c68e2770"}, "736add59-f22f-4992-a026-4f582564cff0": {"doc_hash": "b344b9b442c786ec3914b4606b35a721d50b5045f702b63338688276ee9172b8", "ref_doc_id": "28ceeab6-898e-4fc2-9a56-bb198d4cf3b2"}, "4f0428e4-b33d-410f-ad9f-7361b3ebd78d": {"doc_hash": "7b0c1152567a18bc33e1e2d056013e9b6b32993e7b964a54d4f218e2455d0b88", "ref_doc_id": "803921da-05a0-4faa-ba0f-829ad3c86c23"}, "e0d298e3-4d87-4d4c-949d-783d564177eb": {"doc_hash": "6e32dcd183e5e1f48a0b988477e371c5df6670ad0868e6bcc34c0982da31cede", "ref_doc_id": "cf91f90a-a791-4855-9d9c-a3a341d4ac7d"}, "bffa3e25-34bd-48b2-826e-eb11dd21685f": {"doc_hash": "5f89acae03b1c25722b20c20e3221c39a1af56f81a204999a9415039f7bb095d", "ref_doc_id": "5e2f61ef-8c36-4930-a332-3d35c3b57010"}, "2c3848f2-be86-48a8-b9f2-de60fb766da9": {"doc_hash": "37462e96eeb013e7c40470c5e8d1a55efde97fb6583b18b0b3e857d2d7a4434a", "ref_doc_id": "25f44172-c490-46ae-be42-a56b3b190c37"}, "29a3574e-bef1-4177-8feb-021d55c608a2": {"doc_hash": "620091fb1e6a5409bb5e42dd88b261ff0415637cd7b9ac407f4f30b0566bf7a7", "ref_doc_id": "9d28a70d-bf62-4b79-9a26-c305d73f9dcb"}, "9f0aa0db-2a4f-4c48-80fc-150e8d50b22a": {"doc_hash": "fe9885e4def4babca1577aa9a64fe052b9efedd72540c0261b761d1a2585cfff", "ref_doc_id": "9d28a70d-bf62-4b79-9a26-c305d73f9dcb"}, "51117917-f541-4cc7-8ecd-4a4e0657e508": {"doc_hash": "5381a035539638c8d7935f63ca30f4630ffe2afcdf38930ac88d9ef59dd674d9", "ref_doc_id": "cac64a08-aa40-4215-b4b4-d4913d584a9c"}, "8ba4b377-fb2e-467f-acf2-01cf80f810f8": {"doc_hash": "417ad03b3e6cc3960dec8390b07242a391d45bd8b56ff003706861297c7f137e", "ref_doc_id": "cac64a08-aa40-4215-b4b4-d4913d584a9c"}, "e6edaedb-a003-4b8d-bab2-a55c8ac57c39": {"doc_hash": "0d44af07b2498fd03d6598b87fc830bb0b90d76e3e0541b2e47b2f159c307fcf", "ref_doc_id": "97a80c91-b707-48ee-ab1e-5d5278ecc716"}, "a9a05212-276c-47b7-bffe-24e43ae969c2": {"doc_hash": "65c126ea5e81b0163feef1054c13a9fafd232f2c010257d13973d6dfdf21bc1a", "ref_doc_id": "69ede226-5ed6-4900-8d9f-28e503e72a09"}, "969b1d9c-a631-40ec-b3a1-ce660e929dee": {"doc_hash": "539aab6d66d34fa844d04947de3a7608f54a0a391a099d744d00075ab8331ede", "ref_doc_id": "848d32f0-ee97-43bf-aabd-06716aa69d77"}, "8666de96-70ca-4cfd-8440-d0f5c0a7f153": {"doc_hash": "8f603ffe22372370cb4abcf7d32a4f283ade083a50dfb0d2456b2d066a78280e", "ref_doc_id": "848d32f0-ee97-43bf-aabd-06716aa69d77"}, "06f2257d-a580-4a11-a926-8e5f7ec6a48e": {"doc_hash": "95298a9a6508e64ff8302d9cd89b0e57db5a2d2621b7cf4f7d52e74199e20ea4", "ref_doc_id": "3a8aeaa5-ec81-4fa6-b412-56143a889c20"}, "eac017ef-b37f-400a-b548-37df4aff5052": {"doc_hash": "f1b6d7a783d439c7fe12ec1880210d1a712d64cf52eed228a90928654d44dce2", "ref_doc_id": "9552ac9c-417b-4475-bb3e-88ab9a6ad5bd"}, "fb5ccd20-5864-46d7-b36d-e3439f394294": {"doc_hash": "181aed24d7fa52175bcff9a1d65ddfd1a9247dafd41178c356296b4bdaf64ca2", "ref_doc_id": "d11e5500-70e5-4394-960b-8f754e82f6ba"}, "09aefb07-ba76-4618-8084-06f8842fbcf7": {"doc_hash": "7a925fa77f164af66ae5f65adc2f2a35583a21ceff7530957fffc3316d3b8b47", "ref_doc_id": "2bae5020-5188-4ec2-ac64-e013c7128923"}, "7974e510-bc8c-4e80-839a-0461fce8a316": {"doc_hash": "a87595be22640d83e4129b26258723b491defb224b695f2493f9df4a15d65f9d", "ref_doc_id": "45f1c665-9493-4da1-b1d5-86c00104e08e"}, "6470f9c7-42a3-4a6b-ace0-c105308081dc": {"doc_hash": "43e3839f57399f1e47fd6f4140e2c79ad02aed508febd1f6d1189fd89cfd5713", "ref_doc_id": "9e55930c-82a8-4720-977e-8af40392cc43"}, "d6e8128e-529b-4793-a39d-26ac11e59d8b": {"doc_hash": "2978a6a3687b31a39d771c8dd79dd83899fe19f1d347cfdfb861507eb8e97644", "ref_doc_id": "0a6174e1-6ee8-4abe-b476-7a563e647b47"}, "4c4dd99f-c5cc-42b3-925d-206818b37ba3": {"doc_hash": "4de6e7234726fdc61901af2e55b37050539e1d170d4fd7c9c871edd7b18d3c39", "ref_doc_id": "2700a2ec-d2c4-42b3-af73-4dce9696eb4a"}, "c8521df9-447c-4cf0-b5f7-9be268d98b2d": {"doc_hash": "d326aab69c54d4867518150ba24140570b07d4e7f29dfae6d264c2bf40e7cd5b", "ref_doc_id": "0366dec6-afdd-45a9-922a-34422fc937c7"}, "294f9388-866e-4131-ab0c-6aa77ec13715": {"doc_hash": "dcc16b769ba1507a4e3940d3af00c3329b5e75958ab21ebc74fb5c1e05420401", "ref_doc_id": "d28071a5-dc63-4768-9fa2-42a47d76b49a"}, "509c1128-7fc7-4729-b016-f13b29faf07e": {"doc_hash": "05c57366cb9f8680e54b8d0e5b8f710f274d555066dcadcf8a15a130e23ba6a4", "ref_doc_id": "54691a2f-89cb-4fbc-8a2d-6d716e7c6e19"}, "cd02a707-db52-45c7-940d-67ff2b765fa8": {"doc_hash": "ff7968fdd250482bac31991b095708806c2a31db9d9a5cbbb1768271a9239549", "ref_doc_id": "d623e7b1-09ec-4870-9c38-647ac09d03f1"}, "f466afd8-29bb-4f73-afb3-385acc39be72": {"doc_hash": "9ca8d32c9d8bd6f1b61d331def999dcac6ae0e809c7e3a5c11722ce1903a1225", "ref_doc_id": "30969199-6085-4578-9221-f1f440c1ac40"}, "2876c355-820e-4e31-a68b-e3e831163e1b": {"doc_hash": "d2e0a87cc4c0e3d564402a62205d362102da0f7e2f6aa73cec7e8fc21c962fd6", "ref_doc_id": "97858353-81ad-429d-b869-b24eca3c982d"}, "d4e0783a-733b-4775-b0f9-b85c9ec0aaa5": {"doc_hash": "f19a05deee7ac2431f49365fa41bcebb377af41cd55e6e4208f09eb497b87294", "ref_doc_id": "28b424a1-5b01-4b13-81c1-c0cd1f6ae853"}, "eeda7dda-8902-430e-a000-596106f39ab9": {"doc_hash": "4d3a6a3337339bdc7d09580df4172dade398c71a319cb6dd19f9eafb1605b6e3", "ref_doc_id": "864e8da8-17d3-4d13-aa5a-b3a70fa7a20c"}, "cb341ae3-1545-4440-8aca-473cfc507e78": {"doc_hash": "96e470c6dd3275c68944e8aab7e7374fd79cdd3de2eeb5b50e4c98775510e3d4", "ref_doc_id": "f5a0d173-0573-4744-97b4-f39dd4091f5d"}, "8da91da0-7a85-4577-b293-3efd2f617353": {"doc_hash": "2e3cc52d5ccc7f0b64d0153018381ee7d2b93bfb0aba6d395a590ee0c1408971", "ref_doc_id": "9bb7a012-7163-443f-ab82-2ad62999d93a"}, "cf7bbc38-4df6-45d5-835b-2989e2d1c95f": {"doc_hash": "07e37db990f1a1ca89758d97a8f1a8cf1cfcc3def23e6f62a17ff82bbec2b1e2", "ref_doc_id": "b0790221-ca80-4f64-b6ea-69e125aa6c0c"}, "e9978a6f-0c1a-4705-8679-7207ffec10d9": {"doc_hash": "03f7d432ed69b0f432dd3d62cf69d70a5a33480d1512d652e793c159cbd572fe", "ref_doc_id": "a9cb6792-fd5e-4a72-8726-52d9d4173f2b"}, "bc6804f9-92c5-43d0-beb2-2fb8bca17404": {"doc_hash": "e4eb9532108080deb81cd2d363392ff8d3478f4a98f465c52db50338db821584", "ref_doc_id": "6ceb790f-eaa2-4f07-bd38-bb4386e9833b"}, "86665916-c47c-43a5-b17b-6b5987b7e8a6": {"doc_hash": "f74d86942e980f90baf6cff6bdabb6ecc00a942780aab69723271bc63a54d8c0", "ref_doc_id": "0e6ee11b-09c9-4283-beb3-dab424a3b7e5"}, "4b634dba-961f-4f7b-a842-4c67be2401c0": {"doc_hash": "e72b28cac7c88b159453227a4e17e7070996c23e571f913d32ee077f7d76e624", "ref_doc_id": "569b404b-52eb-45bc-8e55-42687ac31a94"}, "89c4e52b-dc97-4b33-990b-32116886e6e7": {"doc_hash": "f82645c247c2a56e41d3ef500bf4d55ecded316123fbec2478fe50152012691b", "ref_doc_id": "6be64de5-ba13-4db8-9654-11e5f88ef360"}, "bbf8f5f1-bb56-40fa-8ea7-5dc974aca21e": {"doc_hash": "1a7dbf1ef51d566ee6add7788c3f781cbd98b47e21ff4acf1d33b4585c0129f4", "ref_doc_id": "1bde34fa-52f4-4540-903a-12463f03952f"}, "a0aa3d46-2046-42af-85c6-94ca5e8e19f6": {"doc_hash": "94280836e0177c86b6468e70b25298c5349acef7765ffd6ead859e94e7a5ebec", "ref_doc_id": "a4c1b0ff-79d1-4310-953d-e4bf3b9c6b66"}, "eafde4f0-0216-4015-ad84-bd56047bc7d5": {"doc_hash": "4f1066da63c9d25beb7db6ffa5039320e9d5b1ac9875d410e63a578de98894e1", "ref_doc_id": "56dde7c9-f983-4dcf-86f0-c60ad35facb4"}, "1dfb7ead-0bb3-4f09-9088-e6dc45de257c": {"doc_hash": "c66c62e367edffcd92ebb9094722d511981c3eaf640a016a6ef86ffeb067a61a", "ref_doc_id": "971983bf-9542-4e21-8260-63bff691c029"}, "96d906e9-1754-4878-85b7-63c17c3ad108": {"doc_hash": "32398bb07d51015cce033576f5e33a104c5dd071c80d8ae5f5ef44f266ab6711", "ref_doc_id": "c5cdc269-d563-481b-8d71-1a0cfcfb3e0f"}, "319c6702-0378-4b39-a163-0b5f428d362f": {"doc_hash": "61248c59140b6c9454b5e1adaefa74976f4a355a32fa1ca411ebd04e0cf15132", "ref_doc_id": "d1e6b0a2-cdda-4bc3-86e0-e204b8181237"}, "42271eb6-dca6-4e99-acc4-d80224cadcb7": {"doc_hash": "718844ebf9d6c48fe8400b6bff6e6939e5599d8fdc330402761a32e578e86b50", "ref_doc_id": "84050f23-07dc-4fe5-a089-277cd68a87bc"}, "3add66e8-617f-4a99-b0e8-7c1d93cbecac": {"doc_hash": "25c617a74baa54fc4c8641685d701d68949db0706333db1cdfc583d9e22aad1e", "ref_doc_id": "99126150-4bad-4350-8ce9-920e7cbd7884"}, "f0cee30a-50d7-4f98-9e40-756259482c1f": {"doc_hash": "5f5b8f08a9d5c6437dcd75e4737a8994660da043bc64acec9c5cef3a67ca6bb3", "ref_doc_id": "75c67326-a331-4e8d-8832-f162f63b444e"}, "44351a43-2794-4c66-af29-ddb2480e68a9": {"doc_hash": "c0f36ae1a74357026f0e675fee47f23f8c6f017218df6dbb6cd757248b637297", "ref_doc_id": "1ca6cce4-e72a-444f-ac2a-43deff0205ad"}, "0f7b4159-17aa-4ce0-9224-598c462466e1": {"doc_hash": "ea7aa181edb54c12a8bb6bed459afe8d4bf442226e76722d2740241774c11538", "ref_doc_id": "82bc8d08-c1d9-4a3c-aa47-14524138935f"}, "036deeaa-89ce-4b84-87ae-4b6aec4c3906": {"doc_hash": "a0e9cd64ca849577e73c54a2e0b932d2dda9becc80b9510908d344d1a5ae9c78", "ref_doc_id": "d2d3d345-8fa6-4374-b0c3-288706de72bc"}, "2004dbb0-5b90-45bc-94cc-60e9002ad618": {"doc_hash": "72021d015fbbaa11ca40a78b8076e730f8d1b729c17f67e03bc5f9aad97366c8", "ref_doc_id": "faba9fee-404d-4c59-874f-f57f0d4703d8"}, "17be11f2-534d-4e92-891f-8a9549880701": {"doc_hash": "1dc416d0a1c3d4db54d0472fdbb363b70d99447c133eb47ddb1040413382ac69", "ref_doc_id": "804d66b5-09ea-46e4-9561-6ad0cca74770"}, "73591ad6-2183-4757-870d-aaf315d238f6": {"doc_hash": "2bf9d2716d6129dee6a4c77b62ceb1b13cdac1cf45101243aa956dc28573fab6", "ref_doc_id": "2fafc2ac-86cb-4ea6-bae7-ace0563ccf3a"}, "13f15cef-a770-4857-95e0-2f3119d4eb8b": {"doc_hash": "da30f5075eddd5a1e5ee316be2dce3c28b0bb67bd6a81b39949d53bb1b761153", "ref_doc_id": "fba1c0eb-4ac1-4887-8978-dcddbf45a5c1"}, "0d06b098-2efc-4e00-8176-e3317085491a": {"doc_hash": "5dd72531d63886c42c582dd97dbe4aa258198090423393b9e4f634e587b88c6d", "ref_doc_id": "dcc7dafd-c7b5-43a1-90d1-057288eaac32"}, "ba999abb-a9de-4d90-8330-975f4054dc3c": {"doc_hash": "a5ff9d345db74894b5a266e51a3667535faacd39b671ac2b30050a650aa406c6", "ref_doc_id": "be20631e-77f0-4d7b-9fbc-f9a796bdb73c"}, "62a8f625-ca96-45c3-9693-dd39c109f849": {"doc_hash": "b2107c83fa23a846330afaafd172298ab202e6c97602dca7af9f5d9be92be436", "ref_doc_id": "a983d791-6e5b-408b-9c31-46bfd637b3bb"}, "1b81eff3-a9e4-4974-9511-ecf9212c381b": {"doc_hash": "0f1e25472273e8b7fd64ffe64f3080b3dab5b40cf6c50dfc7543dbceb90642a1", "ref_doc_id": "6bd754d5-a894-4bfc-aa05-9debd265d1e0"}, "95f21d77-6be0-46c9-935a-8c42d92c0d94": {"doc_hash": "5176bb83e517665ca7744d7341b6d3de1d9f1e5565fc32de7db5958972c624d6", "ref_doc_id": "2493d4d1-7e7c-45b6-b8b0-2733e1314d1e"}, "c036cec1-d482-4a2c-8bdc-ca84eba44926": {"doc_hash": "8ef39840a5b268d0a6cb5d33cbf8dc44290251bf339416c2ce7f71a488f5e379", "ref_doc_id": "cf099bf1-4332-4afb-ad82-a6275a7eb773"}, "747483d0-42fb-4fba-885f-3d954df4f25a": {"doc_hash": "e0dc9173818bcd8c58f0827c362d964c1c9c1e810b9952726f6ace3624738ed1", "ref_doc_id": "b7274fe8-3501-49ce-8f1c-0bdb2aa36caf"}, "f48e5092-4abf-4d85-a15a-0d2726f7ac58": {"doc_hash": "fe9364649e2ce64c0563ad0eb75ea9c364b504b5b1b416800deb7e6460562244", "ref_doc_id": "b4fd42a2-63c0-436a-95f4-2a50666a1fcf"}, "2117805d-9a58-4a7e-b576-004b404a0bec": {"doc_hash": "9f064d8ecb7fe54ed10a25a6c24929c1c64c504a3a863291504f592c447f05fe", "ref_doc_id": "0e0dded5-3b0c-44a5-b22b-7fa94b52cbba"}, "7b794edc-9173-4211-bf66-fd3045ebbb01": {"doc_hash": "5988b9bf06e1cbec360e08e434b22d3561ef03b04ffb0bede6abe8fe9c764a00", "ref_doc_id": "2b0f7f56-b153-4d06-8ed2-6ea9deb16314"}, "35db387d-8474-4764-8f7a-523133d79742": {"doc_hash": "1fc38b55cf43928a8853762bf452b9d0f0a09dd9389bf2efc63dd4cdf87597fe", "ref_doc_id": "ae499a2b-cb67-4cf1-9be7-4e4a76a3e268"}, "e1b95781-f098-4e55-8cbf-054096b0df62": {"doc_hash": "1d39c4a54ffa43f74f02d4051306e6410018b718ad2fa9ac36b0f17b8e87f768", "ref_doc_id": "dafbb433-ef7d-40eb-a3d1-4ebb61517bc5"}, "fc6bc613-f563-4787-b42a-22d0e744062a": {"doc_hash": "aed95c484f0c2aa50747d2b45fbb91a18d839bd351131f3acac3a11286e86f2b", "ref_doc_id": "987ba5bb-fb6c-438d-80c0-d289953ccd14"}, "02a2fe03-451c-4a9d-870e-7c09af97640c": {"doc_hash": "bd37b4e865447a45d0e7ec6925969b333055cd4c014fb834d44b2aa45f97e6dc", "ref_doc_id": "a2441bad-0800-4101-8281-e9ad2a57ebfe"}, "7e0aa599-9a17-4f26-818d-a7656ced225a": {"doc_hash": "eb74ab8b8fedff8e9142ffd65012cb69c023336e2408c3d57fb471e6496dcc66", "ref_doc_id": "3cf8e927-ead8-4e2c-b8e2-936210b5079a"}, "46a4558e-5126-43d4-ac24-16dd16cf64d9": {"doc_hash": "754401126dda8e47602e78f41d6b6ad4e11fe73db0b1aa33905483c02cff28ab", "ref_doc_id": "c7616701-0aa2-4f09-b512-aa7d5aa87759"}, "5d492a7a-2898-446c-b4bb-58336d18d378": {"doc_hash": "62486071a9cb423e2138358c9330986c0114b404041193e7468fd28f4b492c82", "ref_doc_id": "1b0baee7-0595-43b6-ad90-0ea07d2c0955"}, "66869203-ddad-4fd7-af5b-44813d2a5e8b": {"doc_hash": "8d06ab6cfeeae2ddcbd4a1b2c91bb7cd59bffa2709517141b3fec08f1957c62a", "ref_doc_id": "31d46ea6-d267-4628-9924-c721964817ef"}, "b05988b5-4961-49f6-b31f-faf5e330ba32": {"doc_hash": "bade993eb23b174f18e24b88eb1819f4ba35b7fcae8a5f995146455f87b33c8f", "ref_doc_id": "282005e1-8b2e-400f-907e-6911914aebee"}, "207acf91-d020-47df-9576-e8cac033ee35": {"doc_hash": "6ec5af79dd74a59f0f41dd408562bf45b210c91270e90b08778adc863b5541fa", "ref_doc_id": "d6b007eb-8da9-464c-8176-6de8cd446cb6"}, "2c0cce0b-db98-49c1-bb13-3c0c3e230c70": {"doc_hash": "0648f7d3b9fd69e2ef9a589233c4dffb1c12facc4167830a352e94a6c47b13a4", "ref_doc_id": "ff910460-6fad-4b43-8c95-f24a6fe9f6b6"}, "6cf94ed3-8517-40ec-baad-09f03de870f1": {"doc_hash": "450cbe89bea1ac8f7255baf51f94b28cccf0e404d6dc81cbe45cf65651b02594", "ref_doc_id": "e9f842b8-ad30-4479-98c5-3845cc22a343"}, "867721e4-aa7a-4afc-8f84-87bd9670a49a": {"doc_hash": "41535aa3c802e65530499d199da9c00383a6b59e1871b6dadee4636ade5d7985", "ref_doc_id": "9097fd4f-f17d-43e5-9f3b-05d277ea6ff0"}, "998945fc-69b6-4897-8dda-8c3c098c41a9": {"doc_hash": "cbbf557da387ece132c43846a40dc48369716aeef5a492f9eb089e9b3ec5c099", "ref_doc_id": "7f889c2b-dbee-4f51-accc-8ba45db25e5a"}, "0289e801-99f3-47bf-b3e9-f0ca9829e04e": {"doc_hash": "a29c8b4e6b23affb8a403f38813cc556d930cb5c068fe2dcd345747442e70656", "ref_doc_id": "d176f800-d9c2-4502-8c4f-4a8ba4b98455"}, "8be326e3-1e82-4c2b-b72f-4d2c1893dc31": {"doc_hash": "a491cdc182559ca187ed84987c5c4da3df39d1291b4f66b735c8b08b976a3eb8", "ref_doc_id": "426b8838-6dbd-4ce1-9e62-4a7dd5d25754"}, "6c922f94-d980-4159-88ce-2671031cc20f": {"doc_hash": "98e812e5e00a1a49cdafde468a41085538968ec57b108958bbef94d47eacc3a7", "ref_doc_id": "51052632-3339-4f15-8a1b-d50e5d13eb34"}, "d0c07562-2367-4994-8642-ec514c193659": {"doc_hash": "33414b5598c06b718dd27f11674bb308790fd5c6e9263fcc875bb72e955a721a", "ref_doc_id": "ab13f966-da7f-4b06-b8b9-f67d9caa8262"}, "ccb78050-de0b-4ce3-9c79-8c07f7a4c288": {"doc_hash": "df4dc4c99fe3443bde0cf30d60f6fd7be5c768a4d3bd61652b111240b6d5b8a0", "ref_doc_id": "2060e13e-9074-422c-9308-ecbec79611c0"}, "68a061f8-ef31-48f9-8961-08613889e968": {"doc_hash": "54ae49ce944a4b2502cb85ae31989f01b78a450b49db1356e0cd4f10e410342c", "ref_doc_id": "4460bf1d-a326-4c70-927b-fd9e381a6eea"}, "80c04846-b36e-475c-8109-32ac11cacd27": {"doc_hash": "36253feb61319b2ee70b4040330e6e54546910a9ad1232a30d8789863b1fa114", "ref_doc_id": "d4296d53-305e-42c1-9629-0491912b9fe0"}, "a41a563f-b8a0-4304-a83c-24d02dbebca7": {"doc_hash": "2d58f2a362b6b6713746dc7c0d2664d731046632baa85a993ee40fde6ab95226", "ref_doc_id": "d5c0917c-aaa2-4077-bda8-bd5e838abc85"}, "1c2a6668-144c-4172-9d82-50e86c9ef006": {"doc_hash": "59620f14fb201d3b6aebb54a13a826a72fda8b162a71b38d9fec646202b8cf5a", "ref_doc_id": "3dc3e4e8-6507-42bd-ad99-1914b4e246c6"}, "4ef6463d-e027-4487-b5f9-00ec23d1bd01": {"doc_hash": "db5dc04ca534d86dd582bdafb9d67f9db0b515c31bd3cb1361885b2c5bcce580", "ref_doc_id": "71b68203-775b-420b-9a36-4a04646551ce"}, "6de3f18f-bb5a-496b-bc96-c1218574d6ab": {"doc_hash": "73d7ebe360cb0826fa4d8e28c464239357efa9681bde560fd8a50364918fd925", "ref_doc_id": "71b68203-775b-420b-9a36-4a04646551ce"}, "910ca6ab-ef48-4277-989f-7459461e69a6": {"doc_hash": "4f5c7058c15eb554804f512901713d14f2987908fb044b2b7bba7bc81e39179f", "ref_doc_id": "43b2b1d8-83c4-48cb-9866-0023673d0dd9"}, "f0673e0b-28c9-4291-937d-1d3db28782a4": {"doc_hash": "3acfda5a5d8910d72cfe3096e1d59c89b56f28c94f6b18bf5f5c1d5f3109ef0b", "ref_doc_id": "ca7fe14a-71f1-4eaf-92a6-9bec7f3e72b2"}, "508ce776-d7d8-4be4-9c09-13d13ad53257": {"doc_hash": "b7435681b59445726d58066fd5fa6a8d503e7e921531c3a3c609553382598fed", "ref_doc_id": "cddbd3f0-bfbd-4f3f-aef0-876e8aeac72d"}, "fcee35f1-d391-405c-990d-b33f8f84f4d0": {"doc_hash": "d42273a9c53c2f7fedf14c671ba5f8c861be99a77e1b96e86eb869b13be53200", "ref_doc_id": "020a85a8-2d42-4824-abfc-d2f43e8d9c90"}, "1787ac8b-fba3-40d4-9275-67a7cfc44975": {"doc_hash": "d912ae3d150143047b25db187e95b4c868af4669158fabcb5f1fc1c76bfe0370", "ref_doc_id": "68613df9-b843-4633-85b6-6322b0fdf762"}, "bbb70f70-ef7e-4984-b68f-82863556fb55": {"doc_hash": "9a3b038b4701c144fe10bd43473ef26f253d7c3f91ab99af231d8381a45f25f7", "ref_doc_id": "32d46f67-6973-434f-aa47-448cacbbfc7c"}, "8ace0f10-9738-4194-93bf-3185237d5adf": {"doc_hash": "4d3bc3eab52d2f764406fdec2144eb36c15c2165cd124b0abb536eb9191de4d1", "ref_doc_id": "a0270026-978b-4cd9-9469-10c9a2783bbc"}, "fb30c50e-4207-42f5-862e-b28ac7f9eeae": {"doc_hash": "9f124504bbe48f9c99cdb8fff0871c4418db08c1fb08c374a6736e23e208a146", "ref_doc_id": "130ca9bf-f780-4912-9725-094e49f23144"}, "cdfba9ed-f06e-4608-a36b-0e4811b68b05": {"doc_hash": "46b18eb434e9e8ae79bee8dd843f2a719a57ccfe9c31c93cb11496b76017d5cf", "ref_doc_id": "eefd56df-5adc-4e44-9a3b-cf43bb829d9c"}, "bdd044c0-1499-4158-95bb-b9ffa9649cbb": {"doc_hash": "04e4016a49213e6a94d860fe96c8ee70b268106d6e332e662a1267ae77096c73", "ref_doc_id": "1288119e-1daf-4d9f-bd44-30f7b889d380"}, "23260f41-50e1-49e2-a38a-9790e4193406": {"doc_hash": "cd7690c08b5846703afe7673b377f5473e6a9217d3d3cabb193c721f88c19fe0", "ref_doc_id": "109b9958-d9cc-4932-a62c-aa178e3a8102"}, "e01b0767-66e1-40e5-8feb-f605a505979d": {"doc_hash": "aeffded2bab3cbb35a9db64492b914ae85538221ccac838267592ee2c9957294", "ref_doc_id": "b3d735f8-b4b2-44a0-b67b-2acb36a81124"}, "188a4439-dbbe-4bee-a792-7200725bc32f": {"doc_hash": "c5becc275826e62710730a3ce03ce547915d8a922a18b5a3184fc388483f9d2f", "ref_doc_id": "ff465812-ef9b-474c-a26d-9b496e5a3f69"}, "1f8db9e9-0810-432e-8b2c-13fa9c2c4cfc": {"doc_hash": "6bffa68e22e1fdf14745284eb709619deabfe434f72f0ab68528c218ffacc724", "ref_doc_id": "64efa05b-1227-45cf-b1e0-f985f402d138"}, "8119c2d5-58cf-42e9-8db3-653f92dad061": {"doc_hash": "c5c1fc56b818b290bec43f84f25d312d2fec8fb2fb1bfb416ef19b5844910f58", "ref_doc_id": "74d213ba-51ce-43d9-9c4d-b33502bebad6"}, "26de3d5a-f4ff-4e08-b930-b73df3a5f8ae": {"doc_hash": "19f19a0b24916e2d7129470e0ccc535d6485576afab227d18f2437a98583bb75", "ref_doc_id": "bf5d0c5b-d9fd-497d-ae87-93c4bf9b7db2"}, "9707589f-18f7-4e3d-9a23-f4b0fae6279b": {"doc_hash": "2a74012ca966f4770605263029566055d5de9408634fffcf8b2e82928dcb00b3", "ref_doc_id": "160b30d5-c10b-4dbb-81f1-dbddc080128a"}, "a159fd72-3c5c-4108-a74c-9d82a16ea43e": {"doc_hash": "ea220c02397f4d8fafc92d8052dd8ad6226d7aa2b861d4a3816ecd9034e092eb", "ref_doc_id": "a684543c-2b4e-402c-8e26-134a0476151d"}, "efe732cf-652d-4340-b1a5-5a129c547507": {"doc_hash": "f7e1371dfe37872668c0443f531f869431636c28a2b728176d28f8f63b04b01d", "ref_doc_id": "f421a9b2-8b56-49da-9723-c20ca39e00d8"}, "4629030f-1eaf-4f74-a973-8c9ff7e6fee6": {"doc_hash": "2f57afa437cdc20c53c3e862330cf9a8088ab20c0d89c747072dd38665198b4d", "ref_doc_id": "4d0945b2-4bc9-4368-8a51-331d967ac990"}, "c4978ed3-e1f1-4cdd-afcb-3f626e5b84c8": {"doc_hash": "7158d24854e6805708726824a48b4903af5683fd9de8df53b4fa767dde232a60", "ref_doc_id": "e56e9c4c-7541-4cd0-9af1-0a4d62942f41"}, "f12838b1-4e79-482a-9104-1e25b6de5975": {"doc_hash": "f81002099d79d3ca91534bc5bc2e0ee7211177b2ee406aeccf2f9920b37839b9", "ref_doc_id": "8a07d81d-6212-4c73-a92c-c0d2c08bfdcc"}, "11b75de2-618a-49d3-bce0-1846f4c980d7": {"doc_hash": "07074c67a899da39e8fb227a0d847a6168f2e1181c4c0374c498dc7a9cc343d2", "ref_doc_id": "0c1b7c52-752b-4294-9cc2-7a5f135261e2"}, "aba8a10a-1cbb-4fa7-8c5a-d03b2388643d": {"doc_hash": "b0ccc6a8f81d5a2a78f1f2a511a0cb0fd1da0258d2e56784b6b3fe216d992d4d", "ref_doc_id": "8c4f8a63-5980-41d6-b3f0-979752b4e046"}, "f6715a9e-9698-478b-b4cf-ae7a02e07d1e": {"doc_hash": "21716a0a240155d51afe481593f0b2e1e1636efda9b6c06e6c928e0ef64498dd", "ref_doc_id": "ced0416e-6f27-44db-850c-26d825932625"}, "71841c1d-deb5-4d16-85da-f5057dbe5d65": {"doc_hash": "9e28dd364b586cb9a05b7e24305a31b76beb6c0103bd422a50ec7423ee7ac69d", "ref_doc_id": "9789ce45-2c2e-478f-b6aa-e448fd1f89d7"}, "0ecc5feb-aceb-42c4-9bef-40e8fc9a30f1": {"doc_hash": "0e1fae52a3c86a5fca6cb05482747e76783f9b14d12fcb39892058f1a7fab23b", "ref_doc_id": "55ee68cb-4244-49e7-b6d0-a9019bd95f3c"}, "b7588a18-1179-4054-9bc4-aee080d59feb": {"doc_hash": "7650ba475b4339fb3a9df26cb0daf5c6a0d30b0e98e711ca0e93c112576a2c2d", "ref_doc_id": "b55f1c7f-c0bb-43f5-890b-1b4d4c815996"}, "ecb9610f-ff79-456a-b464-af4afe3fc9fd": {"doc_hash": "adf6961557ba03ce41226cec063d49cc91bf11c3534ba6140ac247a6a1034f3e", "ref_doc_id": "7706171d-8a61-4005-8196-ff562817410a"}, "bbfa5a24-5570-40ab-9e88-1070ff3dea05": {"doc_hash": "49cf486d9ab603507082a165a1d2d7804a981fff94bd7dce122f56b121fb99fe", "ref_doc_id": "fadc733f-1d1a-49d1-9d24-b20379f32cbb"}, "0487c5ae-0bb0-4326-92c6-8b0c9de1aede": {"doc_hash": "90655e4e67fc6fe9832fe06535952d142b0f5bdf21c81c1d805c6d4deeed5948", "ref_doc_id": "e84242c7-f9ba-4b13-84d3-28d371e8cbe7"}, "4fc49fb2-2601-4038-ac8c-54869d048cbe": {"doc_hash": "00117ddf49fbacf553433b4debcfb038fb1d72659399cfb58e82b663dd30b30d", "ref_doc_id": "28e368dd-8028-423a-a280-5c48f639af16"}, "31b2022b-c507-4d4a-ba85-88aa5fa57d00": {"doc_hash": "298ecfe2d450dff08a3548b4090bbf34354d2d528ec2cbd7e4dae436852c580f", "ref_doc_id": "a699639e-bca0-4cd5-adb6-417d24d647b0"}, "17f799fc-0080-4c79-9532-021d4ed76562": {"doc_hash": "1fed88e353d5e154210392cbde5eeee7408131a037a51feb77bc948b3ca52560", "ref_doc_id": "6de7b98d-ff13-4fdd-ab73-f114d214d532"}, "b4192164-4d6b-410d-b76a-78558d81b6b7": {"doc_hash": "0c01de6c358476f4d28231290105e7b868c95f208bd9d1b471e2aff15222f1cd", "ref_doc_id": "d0942196-c0f1-4ea6-9fcc-8b32033e34f0"}, "02af3693-dec2-4ef2-a225-139e5688eb6d": {"doc_hash": "6f7cb40f60e3d954a2e3ad9ee63a65db58d2a460a56eb0c780f22964d5f0974e", "ref_doc_id": "cfeac3a4-8310-4bff-863f-1eab5428c119"}, "b4b472a7-99a1-493c-83d1-d9afbec08591": {"doc_hash": "d5a3ff44a830bea999cc6d39b2562c2ce1770c3b0af858770e4df767f4c8cac1", "ref_doc_id": "4679f4f6-fa1c-4b8b-a31c-3aef76b616fb"}, "5982e40a-f318-4eca-9e3a-e56bf8621ae9": {"doc_hash": "c54ebbc3d847e03cd2df80eb8c1a5f1a0e5e56b59bde6343ac0d4e1a0354597d", "ref_doc_id": "f3096a67-e85c-4426-9183-9bce6c436400"}, "4555d321-a74f-4b1c-9226-3c1f57aaaa83": {"doc_hash": "4c65084f62fe22ee1ccd5a084b4da9e52d599164bc7d4f1841862217a1c101c2", "ref_doc_id": "b01295f9-fbd5-4257-81f8-9f8e0e0d9761"}, "7820a1b0-3956-459c-b1e9-6ada6498d8f9": {"doc_hash": "54fb3bcf9907fd04ce029e8b3bd07c97e35a7676cba8d24d0f64431d67dc6afa", "ref_doc_id": "6a7cf54a-5c1b-47b2-a818-a8e69ad6c359"}, "42398a87-c411-4af3-be09-1d50e5a4822f": {"doc_hash": "826de5980d031a59fc2e5ed707719d0bceed3604c69714eb589d8e83d6f31263", "ref_doc_id": "f33f6653-5afd-4ff4-aef0-85f290e93a03"}, "7133b477-be42-46bc-a1cc-1c65119ff934": {"doc_hash": "620b7530e71f3ee7713442c45ca9612695fccf2bc65c76203e50d16e4906ffdb", "ref_doc_id": "d374bf29-cec2-436d-9cff-ea1d908c4bee"}, "b205836b-b2d8-4f6f-91f9-83795f6eed20": {"doc_hash": "de5ca051438a8c6cd1d1eb2ba0f070bc65f34645b9cff59047a63def4e072b02", "ref_doc_id": "b2c2a386-64fc-4042-bd59-0a7e627a16ab"}, "70bf1c15-5e06-418c-ba4c-568064428ffa": {"doc_hash": "c01f2f8a4bbcbf8a211c4f66aebf802e96229a66c316689cc02790921a0592a6", "ref_doc_id": "f4a210c9-ca68-4289-8ba6-61a4979a31a1"}, "b4b497d6-8070-4b37-bbbe-e642b7f867e0": {"doc_hash": "5f0ac042063cd908d07ac5084b71050930ced9c7c7e75ce9e8929fe5e07f6d9e", "ref_doc_id": "9b2696fa-239a-48e2-b1f3-36374ca3ab48"}, "ab53b641-576b-4bd8-b5ad-b1aab0a4e94a": {"doc_hash": "2072fac4a31fc7665ff31d10ff630cc51703d35a71e85f76987eff946a47e63f", "ref_doc_id": "73bc6c90-2e8b-4c4b-9735-fadfe29b721e"}, "5ee8f710-08e9-44f2-8e0d-78a4df39ab1e": {"doc_hash": "063426fef8ad3f460790d067e0201d418052c4a24dfacdc96cf1008e42789400", "ref_doc_id": "33646249-555b-4a63-88db-42b1698676d3"}, "41b2d0ba-bb59-43f9-be98-fbb171e67658": {"doc_hash": "6e383e5ef3872ed07e0931884ce68738dc08d54ab2db93a0dfe53eff70120eb2", "ref_doc_id": "1a519a03-f2c8-4903-9701-574a501fc006"}, "7a8611c5-04ed-4728-80cb-59b1adf3a17a": {"doc_hash": "b310af6c430a72c3d245979cbb0d9ff6cfebca5d7aada8c42272916bbca0706f", "ref_doc_id": "c6a1889b-6223-4696-bf0d-fb89b957e063"}, "b134b31d-0896-478e-9f03-182501dfd95b": {"doc_hash": "ca7ed030aca2717d8acf3134ea891b5c01eb339bca46ab3dcb69407b737d6648", "ref_doc_id": "cf477d16-c49a-414c-92b7-85d8f83e9791"}, "dfa37cee-8dfb-4bc4-bde6-07d3e2b5bf80": {"doc_hash": "3346a8d525f739dd5454375f245a17f4d87bfe6e2d15fe1e4f53b8f490ceb6be", "ref_doc_id": "6c882f15-c630-4b3f-a944-5637186abcac"}, "6ec048b2-c652-46e0-aa89-5632d8fca54b": {"doc_hash": "20ca4cb6914b5f408c7021c2f652d990c2bddb2b0363724c6cdbdeab77dac80a", "ref_doc_id": "e0934978-9309-40ff-96f4-5834ca9c6eef"}, "c118c3a9-01f7-4f93-b372-48f394797456": {"doc_hash": "59a5adc882ebbf18dc6ce2205772e3c4c234c5f48eeffb47304e9ed83af5d9a8", "ref_doc_id": "93cb5ba1-df53-493d-91bf-add9213e7d5c"}, "f05aee89-a1c5-43dd-a0d5-d362dcc094b7": {"doc_hash": "1a440c2f73d763abb17c5ce5dc58f59a591dd2047034faf62a71982d93c7c44a", "ref_doc_id": "d1ba9f54-2dd8-4428-b80d-9ec40e9e8ed6"}, "2dfd7b78-c397-4997-93c5-13915867be0e": {"doc_hash": "b48fc1f4b3f2ff2cb749028e9283bf38e460f7fd41aa862dec3c380aa19f280b", "ref_doc_id": "07904c0b-1395-4ece-9f39-e95cb1463312"}, "3f62d6b9-6613-4a32-bde9-1697596a054a": {"doc_hash": "495044425bf22919c69c1359b2fb92de53fb17f0b8f62c9ebbbfc0d4e61f8a70", "ref_doc_id": "61c47e37-f84a-49d8-9ca2-cce1a57b1e9b"}, "b1d91af9-b184-4a8e-8a55-3d41cb14e79c": {"doc_hash": "73dba350894726552f5eea5b371eb282583ff64338cf2c640e0a1451db6e9e4b", "ref_doc_id": "9521e2d2-600c-45f3-a51e-17d2e0a66468"}, "fce3b756-3e14-4188-8246-553d8fd0bfab": {"doc_hash": "023e16f135c4a8ffca80605d94a45c9a7f065c4c1373d372ade6c925acb22e6c", "ref_doc_id": "3a01a01d-316a-4056-abbe-5c923357eb64"}, "88f04a66-54bf-4987-9a85-aef746ce1545": {"doc_hash": "9bac179451dbb7d9c60c6ff0addfb3285ac2305aec95da08d64c38aebd3f8953", "ref_doc_id": "ec9af7d3-c0ff-4bad-89cd-db63a60b8a37"}, "a7d2e8d4-15a4-4a8a-97fd-83642ff5fcce": {"doc_hash": "3e939303721b9469fc6c1112779f25bc3112f3dac73eede29611b9320e46d96d", "ref_doc_id": "63251683-4b8a-408b-9bb8-2c2476718a2a"}, "1d7eabca-baf7-449a-b954-5ffbd17905f3": {"doc_hash": "a5d39921cf70cdf68368518f742b50b39ec7965c87bca131f4339432e0bfd363", "ref_doc_id": "91f81465-71ad-4397-8fe4-b85beac190d2"}, "bbeb9871-7723-4e10-84cb-da3dc14b2f2e": {"doc_hash": "0f9206cf0d1f199b9ead7e71a6c0f1151e30045d2c73677c21d32adf2fc63863", "ref_doc_id": "0e25104d-afc3-4ddb-b038-99e72e084c99"}, "91df546e-9d35-4166-b260-2f65c0063dca": {"doc_hash": "701ace2d3867f752ecbf30b95e19a8f617b85bf5e9c0d25e983bf92b4998604d", "ref_doc_id": "caef289a-0146-450e-8579-d593be74aa61"}, "40537af6-6198-4407-b8af-86f3dcc0c020": {"doc_hash": "e19952c8d59fc66a7b88f5005fd082c338c80441bf93ee91b7d9c54c3723fc3a", "ref_doc_id": "96c46d1c-f132-422c-ba42-ec1cc9d4a37d"}, "6b45b8b8-c0d3-4b55-a3cf-4ce4916c34be": {"doc_hash": "cc3f813d9603980095020937eae54414d0cecf1cd4efb0b13852a80827b5304b", "ref_doc_id": "254f834a-a45a-4c20-b76a-8dbd1bc5b0e3"}, "4c314ddf-0317-471e-b4fc-7e8b5bd48497": {"doc_hash": "52bc8b92d347070f9f77d8456d9f5274de601bc180ddf05080af67789b42e387", "ref_doc_id": "e0ec40fb-9a7d-4fd2-924a-7c6b40e7a484"}, "1594afa7-6d8d-4451-abb9-1ae1c0e200a8": {"doc_hash": "2992916c6036a0ede557b3c7a4435fb7937de28e7f1da05584048ada050c878b", "ref_doc_id": "5f38b593-f275-492b-9c6e-13c767e4a895"}, "b3a331c9-35bc-43bd-97e8-c20deb8c9623": {"doc_hash": "cd343975793d4ee8d1a091ad403213ef344a51d293b2bd2a26b0f6d57101974b", "ref_doc_id": "dcd94cf8-f137-4f97-8fd9-7477d7a91e15"}, "42735e49-ae12-4963-9434-79e9e24ece53": {"doc_hash": "2f1116be3d5707d46d76d029aa3f408782eba0513f4b3fada0aa19a8e635c2d8", "ref_doc_id": "68b9aa22-13f6-494e-b3a8-6335e09829eb"}, "968c399e-0c32-4318-a444-4df914f18a78": {"doc_hash": "e13517f692eab5fd57b9a312085fa020cbd0c145afd86a398b576c045c09681d", "ref_doc_id": "0c85331b-8854-4dab-bdf8-3280c407e252"}, "2d45e234-135b-4694-aaeb-b61782370124": {"doc_hash": "ce4a4f09489569c353541f493451ccdff3f826992776efcb36a23272080e09f6", "ref_doc_id": "ab439d72-3acb-4a1f-9886-3b8916fa44b3"}, "5cbc5313-5de9-42f9-b17a-59fa71e8f6ff": {"doc_hash": "1b48eb1a5f4974a39a267e1a0c1dbb044e003096db6a6f3d2eb56d00291dc469", "ref_doc_id": "f789e0a7-0e28-47ae-ab59-3d0d9f179465"}, "51c2d5f4-7088-440e-87d2-a4b4fc1c19e0": {"doc_hash": "b925996b4cc2810c719ff7ebe87b1227ef359a8b8814651c9425546b0f8f5db0", "ref_doc_id": "d1f46888-af6a-478a-9d73-a6bf77e9220b"}, "2e4b19dc-ba5b-4021-a3f8-b2746542b558": {"doc_hash": "a945d6cfb5184b7ebe2a7bd400885f96e2625ce9ecd0c35185c0a2ad91df9382", "ref_doc_id": "450ca569-6d9f-47c5-9a23-8f041807a11c"}, "a15aa4ae-db5d-43be-83e9-2a94eaa4346e": {"doc_hash": "4d2272d0d9bbf0a7ab0a3e7978e5742b4c4a3163b7d2ac39972ffb85bd58e50e", "ref_doc_id": "3b6a2885-1200-4887-83db-4313e1e406c1"}, "724fd726-c38f-4352-bc36-99456f123971": {"doc_hash": "2dfb3e53ddc3515ea241428aec761f9c26ff04189f33b0f4e50d1ce56c4328e7", "ref_doc_id": "fc955964-ce62-4dbc-b97c-8ffe8153c8ea"}, "83328087-dd27-4a99-9547-6095b094bcca": {"doc_hash": "f1684704a6151b5f0943ab359c242dc9cc1e9dbb48c8691d0af4ebfeac607159", "ref_doc_id": "4f924cb6-2415-4d94-89ca-55397bc16c6a"}, "39fc51ca-2f16-4bc3-a15e-06f1bba31104": {"doc_hash": "6f9a3e4f40609fbf54aab6d9eb66c2b3fa91505e19172cd7172edef26d01ccf1", "ref_doc_id": "94b14313-2a41-4826-adab-0e98b7b93609"}, "43fe4339-f785-4a70-9c9b-721a95717416": {"doc_hash": "229c330f6a177d43735bf7d8f426b820d5b4d67a7fce4cb76285c156836d5ea5", "ref_doc_id": "cba11a72-fbbc-4353-a8e7-5295c2e84400"}, "a4de1a22-e54e-433b-86cf-deb3f493fc59": {"doc_hash": "33a177f2f69fda97c92d59978341423c620ea1665517f19bc1fb7f72c8f64d27", "ref_doc_id": "5e83c011-99f1-4220-ba0a-f14481afd823"}, "6ea0b164-6607-4970-9c5a-18e0df76e625": {"doc_hash": "585d3e6d6877518f9aa01e00df210faaff828e2eb2937e63b3939a6218c9eebd", "ref_doc_id": "e657479d-ed7e-4588-9e02-315b87303f97"}, "c0cc9fbe-d9f1-4e0a-bf00-d0c9fdd87543": {"doc_hash": "cb1787ff8e61d404e9d9b1844b2635ca96b0fa137d0f78adc35b6faa25837f67", "ref_doc_id": "88cf75b4-4341-4b39-add8-71c8b8d85465"}, "c14ea4b0-5afd-488d-b81a-a748eb84949f": {"doc_hash": "9af4d0af3eef045cc039a3415d7745d314ffd871beaee6604d65517cd762206e", "ref_doc_id": "18da970b-7bec-43c3-ae49-20589d503edf"}, "d7227e4c-0a55-4c37-aa59-3592b87c2d75": {"doc_hash": "de376cf61e02ac46eebd9ca16e903a8c9242810c0b087241e008cc5e29f380ac", "ref_doc_id": "e2481236-722f-4a2f-986d-8f9df3fcf96c"}, "6bdb2bfc-9dc4-456f-b591-f588cf284bf2": {"doc_hash": "a92463e0533bcf933f55d938e026eee379f252066aaacd340e1a6c989ccc4254", "ref_doc_id": "1af1e19a-d55f-40a9-b23f-73e4f9fbff71"}, "40dd4549-8e59-4be3-b957-6d41371f357e": {"doc_hash": "68a27f03ae0fe62fdb686e3b84c8f14e57c53c3d574038ea018deb5a7cae0eed", "ref_doc_id": "ee2c0684-bb02-4c30-ae8b-20050ba6dd1d"}, "24a75ce5-5b5a-466c-a1e9-29c0c1483f4e": {"doc_hash": "2c8271a4720bc2db600a7973e142be7c31adf0e7aa8d39234de6a588b56f4dc1", "ref_doc_id": "1d9dee5a-cb80-437e-800c-51aeedbbee79"}, "0f12eff9-ff9f-4b16-94ea-fa40c478e339": {"doc_hash": "006cd9c6c9f3de4ad58add0f340fd075eb4c60683335b487c4936183deeaa8dc", "ref_doc_id": "1d9dee5a-cb80-437e-800c-51aeedbbee79"}, "1ade65fc-7423-4f68-bc86-5fa719f1bb42": {"doc_hash": "448d5db7ad24f572e57fe77e5205a8e35251c7b462d47dd365323388d2de4727", "ref_doc_id": "10274fe2-f588-4446-9cc7-85f785c0bb2c"}, "b4864571-2b3f-4cb6-97d9-f12a5f4eb5cb": {"doc_hash": "c4eec2dc41090c0e392362a25a0b8b925443a35b740b6d248fca8d8009711ea6", "ref_doc_id": "d4699e30-18f2-48c7-a579-769ec7cae559"}, "0d8224aa-0b3c-462b-9c66-8d7cd3342bfc": {"doc_hash": "48cd989e73e0e6009dd91393f854f7077c061894b4c78e5d3ef7e017dac1ca64", "ref_doc_id": "9e4faa9c-a637-4b60-af72-fc6e78cff3d9"}, "44fa7bf1-29f4-45bc-94d4-18ceea75c946": {"doc_hash": "0ab1f8018f19d136a6bb2764886c0e85a7e009d062e6d5746aec33d7876549e6", "ref_doc_id": "d56510d5-914f-41ac-a9b9-2801aef9772f"}, "483f6f18-68bc-4e9f-8959-dc8553ae8a4b": {"doc_hash": "06302bfafb3fcee27b7461d6fdb91a1f08fad03503683792e86d516f3eaa4abc", "ref_doc_id": "c8d32618-dd77-46de-88d9-0b268f7f5b89"}, "5a71e983-769c-44d2-bc0c-e2e46829a5c6": {"doc_hash": "6e308fd6aa4155779eb88aa700280ebe0eac5f76be0c1691ab0c1398da041961", "ref_doc_id": "5b07e079-d31d-4270-882e-316e48ee7913"}, "8638dc00-f234-413b-a28e-33c7fdbef708": {"doc_hash": "747d5f746c4b7f040bfff46b17ff7d82c92169db0e69e2acd38107edd1ad2a7f", "ref_doc_id": "9008f41e-992e-42e7-ac6d-7e25b65e5f43"}, "f2ed8b6b-f5f9-4f1e-8f24-ae3b2a5592c3": {"doc_hash": "a61aac690443733b1eb315bd3c8075c8c863c2e2e59855bdfd036d73244040b0", "ref_doc_id": "6bf7cb86-07a3-4796-840a-ce9ae4ef2cda"}, "ab7bc87b-2e7b-4b0d-af8f-e24f28bcc636": {"doc_hash": "ebece465b7ad97eea93b1b9afd55256f88babe8afe13897836c4de824831798e", "ref_doc_id": "be66d327-b470-4998-8cda-08a4400075a8"}, "f997f261-a64e-40fb-a857-8df56b419cb8": {"doc_hash": "f72d8ccf5714bdd35bf2adc97ab2502d2795aa86c6ced08a5fa7e957788a65bc", "ref_doc_id": "f5d2b179-533c-412a-9b70-ee3360290f75"}, "02cebaba-a770-4ab2-8a23-2914fa239df3": {"doc_hash": "3551ce9eb6023691f8d560a29ee3fee7061fae9dc79523c78dddeb47dde4c63c", "ref_doc_id": "6578f30c-79e5-4de7-a6fd-cfaeda163f10"}, "c44f7d26-c882-40dc-b0ce-bbbcfb24a4f1": {"doc_hash": "fa5a5fe45c4890b4c124035b99f9829afa10bab917cb30243c15fd265dd5d2f0", "ref_doc_id": "6e9595d2-b7e6-4fac-89ce-23ae16b31517"}, "4a0f5c50-c3de-46b1-a256-3c7556c48010": {"doc_hash": "dc14a57a6da7a62c0d9c47b1b1f3156ac0d596ff79cd65205d3d8930d86ed3cf", "ref_doc_id": "2fa136e7-1ad9-4a97-8732-fdba735eca2a"}, "3b4521ee-3459-4c07-91e1-a55bdc18eb18": {"doc_hash": "7e84b8b2ac30b8535a8fc4d613773020cbe265a0931c660c8471d0e33f23ebc0", "ref_doc_id": "219bc4cb-e111-4b42-90d4-bb088057caff"}, "3f46cbe0-1b79-4c62-a805-07b054d1f586": {"doc_hash": "68f3b436276758cc494d4a32fc1ec51199a2c9930293af29d0b05f1104b124fc", "ref_doc_id": "6e70b6cc-9731-41dd-a673-82eb52d6204a"}, "b1f4096a-2639-4b71-b403-6a3bab5e4960": {"doc_hash": "9003d0cd9c1f855bceec1013b0ca6acbbe379a9cb20688beda0c2ab989944998", "ref_doc_id": "9d115533-87a8-4a49-bcde-d0698f01f95a"}, "50d74fcd-e380-4383-82bf-7905581bd8e4": {"doc_hash": "e58993576cc5ebbefc988e977780929c29b03e45072df323b726bc99de288185", "ref_doc_id": "4742d777-1323-4c6c-8a39-d544997464ff"}, "a1b14bcc-7f65-4ba5-820b-de94d15bb10c": {"doc_hash": "d53d0d2aa2b8ca2cce09d3e42118b858af234ddde1f6817a89ea1ff75a99deb3", "ref_doc_id": "0eede7f1-dbdd-4084-a6ae-3edec85ca30c"}, "a3c7b034-046c-4505-bd28-358180dce4eb": {"doc_hash": "0989ca6e9fbb2fec97067f9af2bdd5ce11a94a5b8140d0cfa1a4b008a85a8fe4", "ref_doc_id": "7d29ebef-9e59-4f32-83e3-952018e16349"}, "43ca4f5b-6672-4e88-9a19-391df0ee778b": {"doc_hash": "ac98c175c6eeac291d3fe497bd9fe4ec454d75382052eed36669e7f29eaccca5", "ref_doc_id": "5d9e1f79-327b-4349-98ae-0909f1d20eaa"}, "dde949c0-78a0-40da-8ad9-2a3ecb602534": {"doc_hash": "0c45e8c7a8d8c780fa9165b14b92aad651e990e59d7f7a294f019ff6cd1bc14f", "ref_doc_id": "b1c1a9b5-7673-4e63-9261-279a897f5773"}, "35ce7004-3c39-4604-90c9-30fb268bd43f": {"doc_hash": "421affbfde9f3e39a4022a93db8c31e29ede8d6b3b8a63448a86d98b70a3ece8", "ref_doc_id": "f7132beb-df11-4b51-8a84-b39cc2c557e3"}, "24f0e8eb-c3d8-4f20-a168-f47585aa0345": {"doc_hash": "b7949bceaf9df0f1dc9b064d24da1d72bbf7d453295ad3e0b1faeffb62d5c816", "ref_doc_id": "fe8ebbc1-5be8-46af-b5c8-d9ba47012550"}, "998d537a-a824-425d-b275-0f3a77767c73": {"doc_hash": "f9498d614c927e77c63c2a3d5a5bb2149cd0e1734e2b26eeb6662e07cd8bdf38", "ref_doc_id": "20335c3f-0b91-447b-939e-af2e36e22355"}, "567cb8f3-cd58-4708-8595-471f3fef1d32": {"doc_hash": "598127a403b6813e440881eed00008a450dfd238a341dcee58537d6565ba5e73", "ref_doc_id": "fb3cca11-1b34-438b-a98d-8f55202472af"}, "cd62ba68-90c0-4918-a4fc-fd3dc2085175": {"doc_hash": "37d297d49a3e87ca9a09ccabaeb8eafab3e3e69ddf39e343fa40d34b3754bd15", "ref_doc_id": "066828ac-6f9d-4493-8586-5ab0ab80fca2"}, "7a68d825-32a7-48ff-9bb5-31b3fad2a607": {"doc_hash": "ee4b7cc2a1cc8447559f422abdc6b1ab2b9caddb81d7aa1dd766dc3b2178870f", "ref_doc_id": "a66eadd1-079f-4afc-8483-fbdd56f3312c"}, "851ac805-6411-42f1-86fb-8ccfc6d627e1": {"doc_hash": "e9fe242a05a0291107e080aa559da2752198526423cca4a6683d5760258e9594", "ref_doc_id": "551d0ed5-7629-4421-8a4f-41ce62de51fb"}, "480d0216-b617-44eb-94bf-09aee77bc2d3": {"doc_hash": "86e661d1b7c76751ee2d4fe00f4a3cfc69a963a6acc4fbb949de0a07d41f8a56", "ref_doc_id": "d3a9e826-5924-4523-b678-f5a8926c79ed"}, "8c372395-a0ef-46b8-afe1-185d2222344d": {"doc_hash": "cf3539c4eea6fdc62d2247ad902c402786b78d1545218d4283df1281e4c18cca", "ref_doc_id": "d16353fc-f3c9-4b37-9757-265c6993b23e"}, "0fd2e343-9acd-4b12-9d36-dd53e6670ffc": {"doc_hash": "fc0b3c1f362a6b4ee533b3a15d133a5a99ba445f814dc03cf37c8f87efe05dab", "ref_doc_id": "40271db9-e37b-4768-9a86-07238d1bd624"}, "74b0b8de-94f4-40e6-ab33-ecfd07fa510c": {"doc_hash": "ab3bcf008bcabf6b14b796bff127d0773b19303932ec85928e406d13ab811711", "ref_doc_id": "9fb68c1d-a2bd-47b8-be16-f8e32ffab8f2"}, "b5e21c7e-c5a9-4f7a-85a9-d08d7d90b941": {"doc_hash": "c12264155b28f6303a5e2af7add60ed85d1e219ba9e763c5feb50254abc71b76", "ref_doc_id": "afce9df1-82b9-439a-91c2-585166713ccd"}, "a36bd654-ffdb-4c54-ad1a-e9784d0f9f9e": {"doc_hash": "2153a56a123dc1107889b1c2022b64746f27a7d917491bef9b22fe21c89676de", "ref_doc_id": "385d68ac-6a22-410b-8b61-4f411d2142d2"}, "1f98fa6e-2be7-44ff-8b2f-0d90db9e27ef": {"doc_hash": "56ff89392c9e97caaa5b9ffa778f4a5385eb1a790258f8db37187cd8f79bfcab", "ref_doc_id": "bb998b89-ef7a-4209-8d93-d4330e4498d2"}, "482613fe-1d7d-4d6e-81f6-a549eea1a91d": {"doc_hash": "57b147d94c035118b54f1c0081cc51603c76dceeeaf823cf173c11b3796a7857", "ref_doc_id": "200df845-02fb-424d-80ab-40a2bffa97f3"}, "9eded505-7113-4d6f-aa02-42f709a5da66": {"doc_hash": "59d90cde8e46f8a75edfdf2d06015d009701050f4372763709f3b6424eeb9cc2", "ref_doc_id": "a9af46be-9a60-4040-ac59-7ed70a33a7b2"}, "fbe713bf-768c-4826-b541-800b2333a430": {"doc_hash": "b1da95bd8030e8b7eafc6fee6a05a3b268697e62ab3a66bc8ea1a8aad240e483", "ref_doc_id": "607f1e33-2605-491b-a23c-b8d81f574279"}, "9acd5be2-cb6c-4709-a496-14bbb8ffc34c": {"doc_hash": "969f17ce901dfa1da22cb5b00141c29348a019319f79f9cc0a72a184bfde2cfe", "ref_doc_id": "a744686d-1b9a-4f9d-aff9-2de55375798f"}, "89b6a3e9-902b-41f3-8af3-6e4dfc7677dc": {"doc_hash": "38cbaff4a8ded4052c0aab919e6af708317c6f69cffbec235f83e21448786444", "ref_doc_id": "1dcefb27-b1a2-4c71-a453-bfb30aafe29a"}, "45050730-de8d-4ee4-8af4-7ea0725d9577": {"doc_hash": "d889ceb71478f27f510e5007f2c7229f186d5c71ac76ae2cbcec8a35a513ad10", "ref_doc_id": "864edd93-b1d8-4b2b-b4e9-3213cad3ef49"}, "334f86ee-0a05-42d7-be55-bd48c6cef81c": {"doc_hash": "49ba3da76dff00fae7eebec4d3c4dfa2fa460ef148aa17410061b9f08465c26c", "ref_doc_id": "29aee9ab-d5b9-42e1-89b7-4c02e450513c"}, "a9eb34c8-683b-4bab-83b2-6d393c868de4": {"doc_hash": "d531ab8b4e8278dc48a22eb8b522ff52c4de2b18486fd2a227c15935405a0f76", "ref_doc_id": "016b6917-a164-4316-b09c-425ed49c2f2c"}, "cd2c3286-9b73-4433-82bf-1639d30a0533": {"doc_hash": "b762dd5ce2406d6099a65397b169cace25f6e1ca7c1357732df443b15bad78a7", "ref_doc_id": "f29676bf-e0a2-465a-9166-f13f8e78abba"}, "f597bd64-b5f4-4af4-9067-4c6ebf724b56": {"doc_hash": "997d16a63b317468166e7ae7b0e09b39de0bb722a2ac7fa990f20ca1334f5b93", "ref_doc_id": "a69dcd48-f575-4887-b361-345ad010b03c"}, "e593a962-ee8d-4263-9d0a-898ad50ad91d": {"doc_hash": "48eac30281b105341559a5b430435037169ab2d876bfb5f726622abac6bf3cf8", "ref_doc_id": "31db41e1-2fad-485c-9e9f-55c0cab8bd8e"}, "a282996b-2ec7-4b94-bec7-ae02483990e8": {"doc_hash": "7b210c3d76c0f78c9db14d25755dc425543064d25556998b66cebc08e643ad3a", "ref_doc_id": "1042402b-bbcc-407b-a10e-233a280892e3"}, "be6e06db-9ab2-4049-a181-5c00e3bef0fa": {"doc_hash": "1232d83eefb186de7167adcaaa462d52cf44d457b68a6be5bde993ea759ce225", "ref_doc_id": "6cc5711f-be69-4bd5-ad67-7e64f9cc7552"}, "06d3729f-e058-473b-b7b9-f1bcf1607c22": {"doc_hash": "9cf1ef06b3310ce78357890df19368ab2ca287a9db90ad938af9379a579b0dc1", "ref_doc_id": "defd9b5f-5f24-4de5-aabe-73c2fe626903"}, "4983989b-04e5-4f34-a9bc-a18aa3f381fb": {"doc_hash": "450f57eea0f6bfb184e71581ca0de4daf36b8a3deed863e6a75f11982aa5105b", "ref_doc_id": "e9c09b94-0484-43e5-86ea-a619d09dff35"}, "6f8ea305-5e32-484d-90e0-61a7d3da3311": {"doc_hash": "2020e8e82464804368de9ed6fcb99bf715c1d15c86fd08b5be8c62f9b94650d7", "ref_doc_id": "e2c03e3b-0410-4789-856b-2fd854d05840"}, "9a1e79c6-edbd-4508-b012-3ebdbcee3985": {"doc_hash": "760aac54e5dc23de254f491d70e41ca83524b2e0f49e220b7849a0308d71cf2b", "ref_doc_id": "1e45ce4f-4856-4da2-8c64-fbeb77e6bf20"}, "f1b0203d-0599-4314-b5df-31d85b348f6b": {"doc_hash": "116e44aee401c902e43ac75679bb7cc0793238bb490fc7badb6a055098bda993", "ref_doc_id": "73d64630-11d0-4470-8951-80633c25e02b"}, "ea07a59f-b7de-403f-9307-386f55fbd623": {"doc_hash": "f099aa2300e5646c6adac33e75ab2eb98d7272065c8f6fbfcda934ab6fb5765a", "ref_doc_id": "2224b39a-bf57-4926-9b66-eab46b49fcd7"}, "90fbd74a-c31f-4cf4-9967-3effcb9fe875": {"doc_hash": "b6be63fa1a57cdf1472797cf75f2ebfae518fc290d7273a4f811319460d94419", "ref_doc_id": "830f7bff-012f-4563-824e-ee7b19174305"}, "2f0df830-fa96-45cb-8308-9aa035c49b7c": {"doc_hash": "523c8a799342c6f96c53d10ebec3a94ccc64abe15d68dd3939699065eef9a8c1", "ref_doc_id": "0c9329fb-05a7-4a0a-b157-6b2e1a6301fa"}, "d35a654e-01d8-4ded-8751-7408b045e61c": {"doc_hash": "16ea10e181f6e61ddafece5feef1943b50da813ad427144504e210a17b8b1bb5", "ref_doc_id": "5d5e0954-8283-40bf-95f3-4151f71086c5"}, "2fa234ca-d799-4869-9cdc-0653cfd177b7": {"doc_hash": "130567dd9177c11baf593c3e662d092a95272ed9c49725b0155f2f765a1f6ea0", "ref_doc_id": "ab2c582c-4fcb-44f8-8b1d-b004a4f60230"}, "ed814763-1e6b-4d9d-8d74-9e10cf909801": {"doc_hash": "c96ea26ff9a9110cd523c7355f059f716f406d3291760f945e3fea48cef18447", "ref_doc_id": "be85e1f2-50de-45b6-abb1-53da29a52fa1"}, "c6e34e56-6fa3-45bc-8ed2-e89530c71574": {"doc_hash": "853dda841eca1d9a1321b9e976c87ce3dc7b9d06428497f316b460976a3db243", "ref_doc_id": "2e3ccea7-4ea6-4b80-85b1-0192b485113c"}, "7c8733b6-5f88-4c93-9d8d-66ded4599fe8": {"doc_hash": "26c46ce086ae4acfbd3c5fb78010fd07e8f2b4b2fc9b161d03b94b4aed1aca18", "ref_doc_id": "d66172d5-5c87-4774-b04b-9d75d2acd12b"}, "63b66362-2106-4ca8-a5fd-04b48c41ec1a": {"doc_hash": "f7515b7823e8f828d2191fb6145384e7be4c88838bc86547de9552cb9c37f210", "ref_doc_id": "9f4cf67f-16c1-4c00-8f21-148004e01c72"}, "cac38340-0975-40a9-9190-14bfcc9ae555": {"doc_hash": "f06a117e1295bc66ae6c2e80f1711ee1095db0d65cbb8410765b40414691c1b2", "ref_doc_id": "f54e4bc0-7397-43c5-9a9c-c2c90f82faa9"}, "c25687ff-5ab1-44c7-b027-7c29d813d957": {"doc_hash": "c150436e753eeb133a7e8f0629b42073e30607c3d597649e2c5cf32d428df94c", "ref_doc_id": "a82127f1-11be-4b79-ac97-22056bd3b524"}, "ce111cd6-6b11-4ebb-97e7-da8b353e0265": {"doc_hash": "97572c9f995c4036632661b7960a6795c533e8b40507af9d5b20f5ee607e03db", "ref_doc_id": "a398fe64-1b5b-4eb4-8e1d-a57663a2f83f"}, "6ed436ea-8c07-4280-bd4e-b37241178a0a": {"doc_hash": "82ff1c0bd4fea2b3d1f4c5bba023194d9c216610af01a6483f78780ad8036fec", "ref_doc_id": "7dbb6d46-82bb-4b56-aa33-d78c00d7ece2"}, "ce52a2ef-9b41-410a-b104-d0f6e6392843": {"doc_hash": "20fe5fd2cb3133530722a82491b0e262ea75ea16ffc7f6ec75c75b05da3f9c60", "ref_doc_id": "5651aed3-a2a3-4170-866c-3f61cdeb7a63"}, "1f755184-ed9b-40f0-9621-f70df9585ce5": {"doc_hash": "ee8ef931184f338b5028ac06189c32d3232406fc134abdf6bd97a2ec4e1eaf50", "ref_doc_id": "798cae7e-23d0-4662-816e-c08708bcdb07"}, "90972da9-91c5-4412-9f88-ce362199d172": {"doc_hash": "9de41ceabfb8fa65356260b49f87f552019df1272194746e6d91ea615ff20ea0", "ref_doc_id": "890f675c-9b03-48e6-8dfa-faaa19476c18"}, "bfe93013-57a3-445e-a4c8-5ba2d4680089": {"doc_hash": "7748b0f857c98567dd2b0ce3ab900eb7d5d91407b26d6d4ef3bac0d9818b9e22", "ref_doc_id": "cd796fd0-bd66-4d36-bad0-d6092c6791fb"}, "45465ea9-19e4-4d97-8ca6-791c46b65dd3": {"doc_hash": "2d508ccbeead53d2e54a133a5aae03ed41319baa81f0e61362c1a9aaf1070a70", "ref_doc_id": "6a044a89-72a3-4676-88e5-e6830a53ce30"}, "579c5922-2e00-40f1-847e-ab65500afeca": {"doc_hash": "e880cbbef6cdb048ea3eb54cb3ee849a5e318f71d9877f849c9391544e76d984", "ref_doc_id": "c043c329-0236-4773-b612-e98b3aacce30"}, "4e0b096f-8251-4a24-9ad2-5f38c2e6e3fa": {"doc_hash": "84cd6561a88a5193c549c109aff02469ca4d39c7e913dbf98f714ee9f5685817", "ref_doc_id": "081ac93b-bc28-46c8-ab99-3d21f40c5b25"}, "ada7371e-9776-4c83-bc3e-1377ec6a6699": {"doc_hash": "0a1275231078217e9829d44a53db4013a2e2f46556603385e87df12db03e93cf", "ref_doc_id": "bf47a3af-25e3-4f13-82b1-e119012a29e3"}, "840024f5-b890-459f-befa-db53fb8f8adc": {"doc_hash": "ea1b12a7b432786d35eab7c09f1e5c0eaa7bfc1ace49b24c9c33de50c87427a0", "ref_doc_id": "1ee68b7f-2c5b-4dc6-8311-460a38e5977c"}, "4138e68d-08e3-44c0-8e85-14215ddb9385": {"doc_hash": "340093cb69a9da782a9479603abfdd2132d7d47045376fbbc69317ab9ebd7420", "ref_doc_id": "79586157-f9ae-4ec8-8314-1e7bfe7d5c50"}, "ef14457b-712c-4319-a384-bca42703e430": {"doc_hash": "c044ac6bd664cd21428b5c463426d142cf19af1d57e7e90429630d5093917c6b", "ref_doc_id": "9ff34026-3b3b-4004-8dad-7c89304e7d83"}, "cc6f189e-2359-4374-a859-23f72580b43f": {"doc_hash": "d5d0bf632b94de4b92c8004947497b1cbb4ec1f7ae54c2b0f994f6f33dde2e93", "ref_doc_id": "8f55f959-7b4f-4e17-a63d-1675484569b3"}, "0789e879-56bd-4479-818e-5f5fb5da83e7": {"doc_hash": "3050609df59d58b9a78336016aee7271c344db1e70f4305bb06686bbaf54910d", "ref_doc_id": "bcbc1947-aee9-4644-860f-2aa81a9f826d"}, "9b940e7c-d5ad-44a3-b209-17d04005fd55": {"doc_hash": "4b395a72f9f6e92ec18ef165b696a9e6121a0c93b594d1374f8a9989de83c02a", "ref_doc_id": "9480d53f-c80b-42a3-8379-cc06459760cf"}, "4d3567e0-f139-448d-844b-6d38f9dcecac": {"doc_hash": "3a3f98c677fd2abab38e6f59c7f37e2511b8329ac00427c4685a53f1774bb421", "ref_doc_id": "a6767625-e211-40d1-af1c-bcb465387b0f"}, "c0581543-cc73-4c8f-a235-7b185e47d800": {"doc_hash": "cc4dd73b2a73c188aacabd8779de5aba61a13020441485ff661f28a62cd974ae", "ref_doc_id": "14ba5022-b35c-402e-b3d1-c3051c1c3597"}, "7a336483-247f-4991-87ef-fb975d854adf": {"doc_hash": "5290b59890e3da04e42ebfd50fa7149d9b4efd5e8e851042739ebc77a2efc684", "ref_doc_id": "d0ab9f07-aa64-4025-b27f-055690ac78de"}, "085718de-cb0d-4e20-9c6b-3bdd4312a8ed": {"doc_hash": "eac4532ac9d4d9ea83b8fd1a7d6d520f0afe06f236dbe85223dc54abf49077b8", "ref_doc_id": "b9f6313e-d215-4074-b111-872baac079ca"}, "73dff742-5608-4ff9-a83f-d71571c24847": {"doc_hash": "180378df904567ff6dd2e0540ead3c543b52521d91a95fdac9d5d944904e33cd", "ref_doc_id": "b3468217-d6e9-4c03-b2b4-589b7ef1b3db"}, "68cd523f-d086-4890-bbe7-d8928ef5f0af": {"doc_hash": "81712ef9705b7ef7d660f0678db6f807c8b642fb80be0d2974c2eab64ab2fea6", "ref_doc_id": "eb3f896c-a985-4cea-9e1f-a097a70bf163"}, "e60b7d92-49be-4e45-a9ba-677bd4426d5e": {"doc_hash": "a6b6ab8c7ed37999622840cbf9b45bd336f6e561e9398821f71b9f9b29b1889d", "ref_doc_id": "0b4b7c84-2096-4124-8fea-b55407a030f7"}, "ff05a485-e7a3-4baf-a65f-69e6adf0a2bc": {"doc_hash": "b2873e9432892721c4baa650d7f192795fcbfd7da1535df96720c4a1132b9a47", "ref_doc_id": "2cf87c05-9062-4a49-8c5a-f3b4c730098a"}, "bbdca2d6-7c6a-4c5a-b6e9-d3f931e05f19": {"doc_hash": "a251e1526d6f7d9008682da6a8cf7fca7b9f726be15552aeb94e16398fb5d167", "ref_doc_id": "3bb34d36-b517-472c-9f1b-553bd41084ac"}, "1aede15c-2422-4080-88b9-c8959e8f33e1": {"doc_hash": "5eb01b4592a4753c33462265e508c968d3e70b27e8e53e2ac88cf9340eff1884", "ref_doc_id": "1df2bddc-fcf9-47a8-95a1-abc273c6aaa7"}, "b554a9b8-59fb-43f8-bf37-655f523e8e09": {"doc_hash": "02721c783b78c3ce7ce6177ee32c8c9c3c46e3faf39de6a682979a22f36750d7", "ref_doc_id": "42a3a1f4-72f2-4ba0-a117-29b27c3bf0f6"}, "9ae0cc01-22dc-4ccd-953e-da54bec48fa4": {"doc_hash": "da4de8177306533092bc114f41b049fafae5b0b2c372bee5fbc40baa135a672d", "ref_doc_id": "476a6102-fafe-4a30-a441-61d1d688233a"}, "2a6af8f8-d1ae-4e70-a39a-ecc5cfe3a6c9": {"doc_hash": "c982d719a649378f1fe92db6399ae56d7f082929758f8b38fb140310a96d57c7", "ref_doc_id": "46cbdadf-bf54-4e41-a3d5-9b23f1ae87ce"}, "016891c5-3670-4b8d-88f1-584a87e69449": {"doc_hash": "7a4fdf00b7e4b08b150a2088265c1553577b6582dc3347f245fb6afa7d35985e", "ref_doc_id": "0737b0b9-8d92-4191-ba95-1486ef0ba9d2"}, "e04b4435-9a48-4a39-8e94-3ed0dc97449c": {"doc_hash": "392f83f7cc5c8b22942bc1f2ce1d4f8041fde3a0a7f97b34363cd55abfa765bb", "ref_doc_id": "c24ad952-0f84-445b-aac9-5d1a63fd9a4d"}, "009ed324-e378-4668-afc1-b54c4af139df": {"doc_hash": "ec1c00a44089a1fb90b0b2680225d8301b5162e608ae4a3a2010ff512a58e601", "ref_doc_id": "2236f0eb-2457-449a-a621-f767bac4099b"}, "fcf9d47a-3372-41c9-9387-96206f510658": {"doc_hash": "394f6294d42bcc84e3cbf4296b27403c6bbfc072942b12ea700858d9195cbfd9", "ref_doc_id": "4f8fee51-edb8-4586-abf9-6df2181f773b"}, "b97f0227-b1f4-4b1b-b642-119ed2af03cc": {"doc_hash": "4d5bc4b96eab9112ed0b86fede4cf8c06b1bd612e8655fe88b67ac5524241b11", "ref_doc_id": "fd63404f-c603-47d4-9230-c8b6f2009b14"}, "c9095183-c80e-45ab-a28a-69aa4593e2b3": {"doc_hash": "9907f14e4abe5c6270d89d7359f1ebc40036ebeeda53acbad05657bd6080e074", "ref_doc_id": "896a69bc-3255-4a14-9393-63518a7b9693"}, "16192993-cfc5-490b-827d-800e68238062": {"doc_hash": "3d92e8952e84f52f7566b761c65c9ca910ea18ecfbf7191fd30181bca3c4aef3", "ref_doc_id": "646b85ff-6473-4fbf-84f9-709631949fa5"}, "80ef2de0-66c0-4c67-8f43-3e902afbe6cb": {"doc_hash": "28ca52985e69c91acd78387645cddfd9e4afaadcbd0d5de395e053f9f637a44c", "ref_doc_id": "a29d51f8-8480-48b6-91f6-7840d05d0343"}, "06010588-37be-4d91-a2a2-172661c7eb90": {"doc_hash": "795ff3f868037dab48bf79e99801bba53a05425e8955b90d1da4e7023634161f", "ref_doc_id": "bd967916-493b-46fc-b39d-e9d4041bd6c7"}, "2ec0c04c-90cc-48e0-9d95-15f9604e38b5": {"doc_hash": "3531b39cfd0e56e31df0d4a28ef759391522c5f5f3573e3989efd0b74429596f", "ref_doc_id": "f1fc008e-65cf-4c40-b84d-509f5ca6d561"}, "29c89dbd-6326-4cbd-9a8a-2ded8148268d": {"doc_hash": "9f1f8387ec1ad992e3adf2f1c9ddeeb062aacca213208aba447dc3127521c772", "ref_doc_id": "633c1e8d-115b-4768-ade8-e94550ad85ce"}, "18213439-c4fe-4178-b0f3-2c378c4d675a": {"doc_hash": "256c4eefa62a4e810a793c0ce89b6f04e1ae51d683473efffdbef06ea41b0353", "ref_doc_id": "567e0dc5-5d02-4532-89fe-b97ba870ab92"}, "f53c4e68-3d6a-425e-825d-473ae0ccb697": {"doc_hash": "37865c4d0c524490d61f704fe625d483d6fecd9674159bce06206aa9e1ec48cf", "ref_doc_id": "58a8000e-a877-4b02-9119-46cc25b53a20"}, "430b26d6-616a-4488-926c-2ab730379230": {"doc_hash": "b5e7cfa73894b14ff2e3d8c86b4404a89647a3d637c0df2d0d0e6158d62b174c", "ref_doc_id": "c3769e97-8951-465f-a04f-14d1296e1c02"}, "38deae6a-6df0-4085-828c-fb5ef4d9b841": {"doc_hash": "9f8e5d142d95158080c9f9341a47b8b11d67381ec4ce0324043183e15a3dfa94", "ref_doc_id": "86f48024-32fe-41d4-992f-18d4fcfc9b94"}, "73253272-c53b-4b63-83cf-119ef30bd504": {"doc_hash": "710d7e697f76422302fa9acefa88be281db783e2c922425a47430e672ffa5e2e", "ref_doc_id": "c8227386-cadc-4d1d-a2d0-721ddcfb0fe0"}, "d825421e-7c53-4858-a460-4e2597330de4": {"doc_hash": "5b3a271921db8777ef810a24c3c259faac54869a1c955a7963813601ad2f6423", "ref_doc_id": "4cd4b3cd-2e73-487e-929e-abe9947605bb"}, "dd87714c-1909-4881-9083-d8b4255f975c": {"doc_hash": "3fb3033d9b51c433961beb0cdc4c9ed222618f3f3a921b59152402c90bbaf785", "ref_doc_id": "390b40e6-70d1-49a7-800e-d2330f7716c7"}, "5c1285ff-c1d9-4940-a2f3-94f1fdaabddd": {"doc_hash": "6d642c6bd8d4661e763d1e9b6e115cebc2e11e6a17db8d58b0f76e35943501c0", "ref_doc_id": "243aafdc-431b-47e3-bd00-1033a755c66d"}, "3972638c-89f1-4e3c-9793-becaa8ad912d": {"doc_hash": "466b85d44958b1419fefc9d3b82284b4d98b91ef727cc880d41b6f5f4ddd7ff3", "ref_doc_id": "765a9e98-524c-48ed-aae1-2548635bd96a"}, "dbd28fdb-8a33-44dc-a6a1-14d79327f14f": {"doc_hash": "06410292f70743add8f7e5e1a5a672d933654b49362c46f4767e33033e281532", "ref_doc_id": "c1c14f6f-9881-4305-8e47-fffafdf986a7"}, "a1c5711d-70d8-47fc-a118-21cd5572200a": {"doc_hash": "4178e5a562f76249d5a1e5464c30676f5a02fe18c6ad7d092da43eb1c9d2ab69", "ref_doc_id": "907c869d-6af4-400b-a7e2-8902a75233ef"}, "e3c439cf-7e0c-4eb4-aff4-e0c7f5f0a1a0": {"doc_hash": "a67f7146b6c22dbebd0d332d30e65fd79043cca08fb4c8a5677271c1ee21ff8b", "ref_doc_id": "76565ab8-6625-49bc-b791-b8217bd0dde6"}, "b46c5485-e571-4c1f-9831-720b5c478a20": {"doc_hash": "489f6ffc0983e4530be78081dc0b79905fd9b8a42a53256f2751f5e736e29c38", "ref_doc_id": "3e5ee014-1bc0-4736-a3bc-09a7810bc635"}, "dd47e34d-7af4-4db6-ad16-cc7a182f4179": {"doc_hash": "78252e63c3248017ce2978400764f152793cb4bf435743057bbed051559a525b", "ref_doc_id": "683f59d0-02c0-4b64-9db2-44bf0799aaf6"}, "1bb4ab3e-31fb-42f7-94d0-f6ecc748cbde": {"doc_hash": "3a9bf8d31faa8a5709b2d4569fff2b270a3ac5b4dd85d3eeec37f08a09f9109c", "ref_doc_id": "6f7a0035-bd79-4070-8cbe-58855794f9d7"}, "505c9f14-bf19-4050-b9bc-caf55feec1a8": {"doc_hash": "15f714b5c421463c1d97f2612c0fd8fabc5ad9d7611d973ddd96707a00a2d505", "ref_doc_id": "9aadcb44-f588-4a6f-b607-186acb4c376d"}, "5530e02b-449e-4a6e-a1e6-6f43ff70fcdb": {"doc_hash": "b7c2a181e5c47d8f890aeb184cd4677e7271b8bf8d227ecb23a1d8331292e0c1", "ref_doc_id": "cd890bb9-9be8-40fc-a6b8-f335e77bfe44"}, "cdc29e24-77ca-4049-ba5b-ea8d771a1e10": {"doc_hash": "7310f7f6e9c3ddbe0b327cc47322deaf140b3f1d82c06f23425176e7fe534f5b", "ref_doc_id": "9a353861-0601-4f06-b39c-57dabd68afa0"}, "311d43ce-e70a-4574-a47e-cd4828caac9f": {"doc_hash": "7eb06ab85d1a478346942b81e46366040401adb7105d301a25bd324e292f0673", "ref_doc_id": "44cbd1a7-7346-4ff7-a99d-920665f9c0c4"}, "a0724728-ca01-49b6-8f87-17154bb6a9aa": {"doc_hash": "9474bc3502d312baf82addca02327eedceca6e648277a3af3d629415c3f7903b", "ref_doc_id": "1dd2a4d5-7dfd-4099-bc2d-7415cc52d2ec"}, "99b9c3b0-666c-425f-9a85-60bb66f2b182": {"doc_hash": "0463a71f6bd6803e5a09a1b1c34a715fdc59cec21a2fd5e53fd1f0bf0975fd50", "ref_doc_id": "515e522d-cb4d-461b-8716-ebc1f88fc1fc"}, "9b2fc0d0-9a91-49f8-b668-f040b6869c91": {"doc_hash": "3b67aa1848c5ba202535e8da12cc5d40e82d616ff1ac4de069a0e140b16c1acf", "ref_doc_id": "c7d9801e-b41b-4bb0-b55c-3b6f41645198"}, "d1c82237-8bc1-4c64-ba2f-b52ec7c3c68c": {"doc_hash": "048335e0499ee64bacbff993d274c0db3bb3588d116a36fc2c9859c28e37af15", "ref_doc_id": "ad9de203-421b-4667-8507-5dbde850858c"}, "24f2c4bf-d5f7-4c72-817d-9ac5d97ac74b": {"doc_hash": "f4ed7d5c5792daa8990b8a359c760dc18fafaabc1f1eb3a9eb2b50382d98f1c4", "ref_doc_id": "ad9de203-421b-4667-8507-5dbde850858c"}, "a2c410c9-0a82-4fda-a12b-0ef0595bf471": {"doc_hash": "54066395b17736f57785e348c15e37dd3874d0c5ade5848bcbdf429a8b80f560", "ref_doc_id": "ed82fd56-ca7e-43e4-9ab3-9c44657e6ea2"}, "8284f43f-fb3d-453c-9bea-c7b78cb9064a": {"doc_hash": "2c00ac9824d3c73c100302f1889f1c1b0ae6a28214ad46c882546fded147e19b", "ref_doc_id": "ed82fd56-ca7e-43e4-9ab3-9c44657e6ea2"}, "54467f4d-d8be-46a1-b2be-a628f373b618": {"doc_hash": "be769dec39e5f8467245ba3bf2580f27cd03255f3fc2e782866425edd1301797", "ref_doc_id": "0491b36f-d847-4dd7-a9c3-58a8f2f61a89"}, "e11baa39-e7b3-48f9-a577-a30b95ff3bb6": {"doc_hash": "0ca08223fec45706eefcc607d600630952cfeb11c038424bfc6d7bf3097530b6", "ref_doc_id": "51d5fa42-8f2c-491d-8e22-1735a0e5455e"}, "c6b76ca3-b943-4945-bf82-02c63b3a4e45": {"doc_hash": "2a2b96111e41b50ba8dfbb465ae454bb41d02b3ca48216cee92b8940c1e0143e", "ref_doc_id": "eae95560-c74c-4697-9694-e3ff2e326153"}, "e227e0c7-af64-45f5-94bc-65031b194d35": {"doc_hash": "d0e9bb2c240c6cdee18089740af9984adf5fc47737dd3e11783634ca98529bc4", "ref_doc_id": "eae95560-c74c-4697-9694-e3ff2e326153"}, "c4663428-a7ad-41cc-be84-05daf9649fa5": {"doc_hash": "2e358fe2a2fa4f6d0156520b0082ade824612fb36a7a406dd7eb96b93e9c5f79", "ref_doc_id": "ea044575-f8a7-4b4b-829d-9477ae6e577f"}, "f41c9bf9-fa20-41c6-b2e2-add1d5852427": {"doc_hash": "ceb8eb9571fc5459bcaa1cf4941fc1a1418ffa4be54b5c6f2ae7835c7389da83", "ref_doc_id": "15f7c676-5147-48af-aec6-20fad312d9d1"}, "84332a99-4369-4945-9569-4b3c5a4d9af5": {"doc_hash": "2d97892ecd7e7ba0b26c289068589a080c00d0205435c977203be63c113b543a", "ref_doc_id": "84f5c419-ad8a-4b08-8346-075f0c1a199f"}, "7d506a5b-812c-41c1-a911-662568a3c373": {"doc_hash": "ad3a44ddf290d21219de8501c4e1d9b406a0aa155dbda9f2b16325d12fd32665", "ref_doc_id": "2cca7d5d-5e82-4eb3-99d4-b6e8293cb28e"}, "29f277f3-6e38-4630-b838-b4325e8c189a": {"doc_hash": "344325ba2401ef946f3b57b8163cbeb07becc295bb13b6eba222b775bd378129", "ref_doc_id": "b2b8b953-f39b-4755-bfd3-4368039bedc9"}, "17ecd3c8-3de2-4018-be42-5316f92aedf9": {"doc_hash": "02087869016e38ad98b8daa782d7b022234783be6e282b44d418acaf67781bdf", "ref_doc_id": "1ec923d1-3aae-400a-8605-4f388253f05c"}, "fd5ce960-0a2d-46ab-a3ad-94a279875110": {"doc_hash": "b281e93e86930d84cc86ff704062a9516cde58c18665a22f2c631b7ab9297c91", "ref_doc_id": "d09f6409-d387-4d39-a3d0-a5338ac8566e"}, "d7a7287a-7979-492c-8a11-7dc51f0ce9c4": {"doc_hash": "2b8452a68809bb6cc9924a94e801337501cc67df308053f09d679809b90e7aa8", "ref_doc_id": "119ca212-8a8e-478e-b555-3ac829c68b3d"}, "d9effaa8-b527-4685-b88a-48f7155fbe82": {"doc_hash": "c9a5a8d6b5ccd6cdbde5badca5d5a54e8cd60bc105ec39fb1caec3f1329fda54", "ref_doc_id": "430adb4b-bec2-4541-bfac-186e84f3dcc5"}, "92e8fddd-9c33-4897-b94c-36d2d2eb357b": {"doc_hash": "a5086d481162956d3351d524a8b3badbbc4bb3e7a2ad3299beb370ef14284b76", "ref_doc_id": "dc830c54-edde-4c6c-9f3a-cfeb0515e07b"}, "a8029e1f-e2b8-4eeb-95fc-49a88e87a1de": {"doc_hash": "73e89c72fb19c7d91bbd9d5a779a075de3da446efdf3c75117d884bdc0fe62ed", "ref_doc_id": "b6c05785-88af-499d-bba4-5818ebced598"}, "8ac38a36-8697-4c0c-bf90-e6a55e32edd8": {"doc_hash": "31a690b6ae2db765293ea2ebf8c803d19672b23918ab711cc75c068913e46a74", "ref_doc_id": "b1506c16-4cac-401a-8980-028809d6ebdd"}, "7b129ed1-9ec2-4684-93b5-a1152b40ff69": {"doc_hash": "0fbe3db782f19d78db73064425b61b6d7e2ce3d81ff6475a5129e2232948c868", "ref_doc_id": "c4159930-e9bc-4684-9f2d-df448e8b3142"}, "e7a5f071-1bdb-4504-8e69-ca8a5f1aba3d": {"doc_hash": "930330cf86177acde292f0c0975e0ebab295e88c829444307609d7137ecba4d1", "ref_doc_id": "744c407b-0a6c-467c-bbda-66f0f68ea438"}, "4229dfe0-21a6-4872-9514-e719255e5e1e": {"doc_hash": "8ced53a9bac64621f48d36f00c11cf61f4a93c55f199b3aefb7bfd665d69b747", "ref_doc_id": "15893b6e-9588-4d95-8e60-3c0ade826740"}, "a73dee91-9a96-46fc-8b00-f3a6e862ebe3": {"doc_hash": "77a1d3d1e98e53f17be1eb22c5006bbaff12d539a70cb00af376f34c149a7a27", "ref_doc_id": "45c18173-4b74-4433-abd3-d5342778adab"}, "c18eecdb-b125-425f-aa93-883c579ce57c": {"doc_hash": "c545a9a3d6ac09e259d789b36e9e82d1de04e6ed475d5ca596d31592d2c0b4e6", "ref_doc_id": "05bbdacd-b517-4cba-8e22-b263bfb39b04"}, "309ef138-456e-4867-814e-eb7aba692d0d": {"doc_hash": "1889c6494967c161696d4023d124904905a359481d5afddc4f5042f56e3f5651", "ref_doc_id": "9353f800-dccf-4804-81c6-4a7e0b728082"}, "ffa10416-ee53-4eda-81a3-d4df7c123079": {"doc_hash": "2559f8b41ea7c4c42addedc8b72e0e90f36e127011943a60e1a9ae1982da9069", "ref_doc_id": "3cf4579f-765a-48d9-b24f-84257403da46"}, "9632a7f2-9afd-480d-90ac-24c788d055f3": {"doc_hash": "219e4250a5a02d8e190012e214b90221502cd0526362242340ba989c6c6f0b98", "ref_doc_id": "890d97e6-77c5-4b56-93b5-8dc5c23be368"}, "b6e67135-e211-4cd5-a81d-fc57dbe3182d": {"doc_hash": "36448b7ff669af5d9178d6019c7cc2d30eeb7b4a9750ac6d29306156b654bc46", "ref_doc_id": "587c35bf-d49e-453b-bd1d-101813bc3803"}, "17c21e18-50cd-45da-9068-9d9e67c31ad5": {"doc_hash": "e9100f30748797b3bd434deefbcc9c8257362f1b6bdc544139e45269e5d4d9e3", "ref_doc_id": "65ba714e-d0b1-40a8-b862-70dada25177e"}, "27110594-a5b8-45ab-9ef4-655d0b145c33": {"doc_hash": "e9c93e89b933fbd7870e1c6624d5ffd8ef83122e292f947cbe02129dc310e59a", "ref_doc_id": "7780c01d-32d4-4b9e-b885-cde44be1fe18"}, "2dde1704-9ba0-40ae-ba21-54c27005be6d": {"doc_hash": "e8260b4b806f56eb064dd36d1b9a977dfabaf2d66c26deb33f282b0b88354e01", "ref_doc_id": "16e17059-437f-4dc2-86a6-e56c5bec37e6"}, "f39c8907-953f-48d6-ba36-2c7c6582b4c8": {"doc_hash": "b3211600978d55645c5f2562c21c8f0db047d385fed0ebcd5c1b5e0fc73174b8", "ref_doc_id": "364ea9bd-3a81-4c52-9a02-8d00ad590cb0"}, "b2e4feef-306b-49a7-a3c7-b3971169bb83": {"doc_hash": "0e0143ba6c0ca40599f73ce6aa6dc6487727bf44b31a6808abba279674c7c0a5", "ref_doc_id": "24e42174-6850-4ffa-880e-2838eeede7c7"}, "61425244-fbf8-4090-a9b6-57355549d5e3": {"doc_hash": "82e942ed212b7fbd7ada956d1f3e9d574a56db3dcec02d349a317497813f0d59", "ref_doc_id": "8850da52-324e-4616-9b75-40111487b6e4"}, "4fca6872-3f4d-4b2f-b7b2-457c5407f36a": {"doc_hash": "37b0b67987af508afad6832b3f58009cc91276ccaa59785540b8a89bbe060cd5", "ref_doc_id": "67a22e51-2b6d-442b-be97-c90f437a1f3f"}, "5bdada0f-7628-4949-aafe-e7957356e4f5": {"doc_hash": "9e6adc02c10b99eae78290ba9c595b674b3ac7f6c1b66ed7ea26817d9c14195b", "ref_doc_id": "620bc0bb-832c-483a-99ec-7851e6154b33"}, "4923efa5-3aa3-4cb8-9c22-b3a5ddc7ee96": {"doc_hash": "a2390b30a776989674d37c75dffcd2d5e885e628846ec04b91346203d47e8888", "ref_doc_id": "7f1341be-80db-4de4-bff7-636d6c7cd6fc"}, "d035c769-9d32-478c-a883-ce501cd61314": {"doc_hash": "6a3f53ee722267863b984905ad88ef2be9d163fd5907a318449eac748a988c27", "ref_doc_id": "f1a841b7-bd18-40ad-9be6-310d9c74470a"}, "aae1a597-6ae8-4090-9a1f-de46b6135c13": {"doc_hash": "29c532059e53dd4e02d42d66d449c3b904c78403d26687211643d80c04c1fa2a", "ref_doc_id": "d51c34ff-d274-434d-a6b6-109da4fff6ae"}, "21508e82-51ca-4f85-b375-af0aa6674294": {"doc_hash": "53ba486ef092b753f627c5aec960617b13eb94f0688c804f87f9f5f80240fc9e", "ref_doc_id": "2ef67385-f61d-47e6-87bd-29fc4c775555"}, "966e7a9a-f5bb-4755-93d7-96307ca32cb8": {"doc_hash": "b05578733aa2f8d6cff4a68975b6097a049985b54a4d4425402fa326347da4ec", "ref_doc_id": "27f77c74-98cd-4841-8e75-0482ac84a378"}, "ac52f110-579a-43f1-8d50-0f658cd0f73a": {"doc_hash": "df5d470a0e9510d407a9799f113bdb1407d66049ff1cd37014e581a2d0927b7c", "ref_doc_id": "26394e48-a8b2-469d-bd93-f350850ce6e4"}, "1783f64d-549a-48a8-aa95-dc3c228ff8a1": {"doc_hash": "d26b125b13734501be95db882989bd18d310c9ba4743be5ff8e4e7dba5f414a9", "ref_doc_id": "dc93ac9b-5e8f-40fb-8223-a1bfcc3478e7"}, "6b7c6cdf-3d00-44e0-bd62-21a922bb2a37": {"doc_hash": "a27d97b23b06eeb94c6685ed12c5024fc61fc0dacb0c75a813b6c317f6803b24", "ref_doc_id": "20522f53-d63a-488f-9e79-8b40bfc00b25"}, "f89070ba-8c07-420c-b843-0a1fa8ca5efb": {"doc_hash": "953336e4f1b623710b9ab0c8ec92ad1771cd56ff0143c265a16e3f4c1ad9d223", "ref_doc_id": "835f5ae4-f5d1-48f3-a9d5-fd1464fcfca9"}, "2cf00503-d809-43be-a4c6-0abca0d633ae": {"doc_hash": "cb54dfbaad0f868094dbda5a274014bcdfeb6b1fb0c5ed05f5b623b207347505", "ref_doc_id": "d6cabf4f-e2cf-4834-8c91-54fba7ddb801"}, "10efd3e2-9b49-4348-ad77-a7caee0e0d9a": {"doc_hash": "4e5c26aea978c0865a1398c4c0754a97382d44056abff23c9cfb26ea7b7d6b73", "ref_doc_id": "78c4bf7a-6919-42ae-ade7-f5478d491901"}, "995d4a4c-de56-440e-bb54-10e1f423ec88": {"doc_hash": "4a1a9813ca6fd88c92cd10f5042551f2b3ea87aa2c592f33e0f2001c38ef3b9d", "ref_doc_id": "38af870f-8f73-4f26-b2b3-a615dcac6c2b"}, "e379d22a-c04d-4adc-9af5-a3e2604cc116": {"doc_hash": "0ff10e25b11950ee8220c4ffd7e723f156e54c73165770db5a611378010cf854", "ref_doc_id": "26ba1eb2-5253-4e94-aace-aef0d559431b"}, "db63c922-ae5a-4c1f-b5e5-81db89bab57f": {"doc_hash": "f114c21a514d68f17b74dffe4fa8f8d5c0742b514b02d7dd0724214e2e8c7a89", "ref_doc_id": "2fd6f813-a6fa-4055-9eee-c05395734085"}, "3509d3de-ec8a-4ee4-b173-969e021decc3": {"doc_hash": "3a42bf95e727a3a8a6b4636a36556fde79c8010926c428ca76c590809e58b5bc", "ref_doc_id": "9eb559bb-ca03-4dc1-9501-f02b7c9c169d"}, "8b8fff3a-0596-4258-91b5-c07f19dd94f4": {"doc_hash": "f191277827ce79d895ff56a5268e59a8d6df6d12385a4895ee9b83e5de277559", "ref_doc_id": "1732a6f8-01c1-4218-b4f3-99d11cbe957e"}, "a52e5ef7-7393-4e6c-a358-3645ce95d1da": {"doc_hash": "7530dfceb7fdef2215125c723908b28d8ca3cf25df36cebc958d0fb100b04c1a", "ref_doc_id": "507ccc98-5fd9-47ca-ac07-25d22df75645"}, "78fbc8c8-c8e4-4ad4-9ac2-0919d814fd44": {"doc_hash": "fd7f8d6716aab2c6fe79e798b292435c2f6f940ee2e969fba716d0d4fe00eb2d", "ref_doc_id": "af1b8f7a-4bdf-4406-b185-4b44a6ccb98a"}, "b7c0f173-10c3-4700-bdac-279913bfbc8c": {"doc_hash": "8155a34c3b71da35e238b867a9507311f952777733d262fbe5d37543647ab40f", "ref_doc_id": "c9f3c7a6-ddd1-47dc-8859-a70e4a4b2be9"}, "1b96e79c-9da4-4909-ab88-d34788aede01": {"doc_hash": "b6185536542cb3501e59e944a305dae0631342008b7c7e1207c8cb8b0e25681d", "ref_doc_id": "4dda1b94-0c0d-4778-98a9-201045855444"}, "6bdc039b-61e1-4663-886d-ef40d5e482ba": {"doc_hash": "02a31395e8be123834d9b55cf0512aa39a92e31f9e0754cc032364ba7693ced7", "ref_doc_id": "2469be29-7572-4342-bb35-7cd3db8c8fbf"}, "47def66b-b3a9-4292-94a8-f1be0f473deb": {"doc_hash": "d62eb83c3c54c51669a612f1ab150c4c44f8b85a64fc2509196b8aa4143bbf2f", "ref_doc_id": "1d072a0b-ba58-4673-8370-1fed05b9976b"}, "fd4caa67-7854-493f-a245-c62e4816be1c": {"doc_hash": "5b419963e2cf42ea10ba7dd65976e654071a7db5cf100279a5dd827f264e64ad", "ref_doc_id": "0edd96fd-f414-486e-a804-a331afee648c"}, "40638989-acf5-4d36-bbf4-3bec6c732300": {"doc_hash": "b306db5ca0e28e77203b70b6ed5b744ee8a9bc6c0fbd553ef9850c9995646003", "ref_doc_id": "a56e6206-a570-4259-b7be-2abce519d5b0"}, "2087d7ba-b1fd-4b9b-a293-f1394a74db7d": {"doc_hash": "709499ac380a95190d6f9b431e183070b5bbcbd0727c2af9ea51c79e0e636b6b", "ref_doc_id": "3b063e1e-4251-447a-99b0-7879a71304ac"}, "acc0ead4-870d-413d-839a-277d7691c735": {"doc_hash": "6c6da734f670852292000e5e2dbcfc509fa4bb7d09b9800110fcabd4834c0341", "ref_doc_id": "a0832e99-ac1e-4648-a4ef-e3c891af5858"}, "a0189eb7-b573-4bdd-8020-c1343ea9deaa": {"doc_hash": "95bd616d25e84a1f736b735a6c34a1f9c68fa4c5cfd1a7cbc922a26ce489ebfa", "ref_doc_id": "19804c2d-938c-4f07-abdc-f027568795fb"}, "856adfe6-5ce1-47ef-9e74-6a4ddd3e4ae9": {"doc_hash": "dedd4c983d527801110d15911635467a1b07bd339f640a0d153102d8923b13ba", "ref_doc_id": "26880900-60fc-497e-ba27-1ea178ebade4"}, "2a05252e-9a9b-458e-a0d2-0d15b06de29e": {"doc_hash": "80eb87c2a89158353844e3381c7f3f422ffb356900c19c9975fad06ba6ce3f09", "ref_doc_id": "9df74982-31d5-4727-a3a2-d2727fc481e9"}, "f915f536-434b-42d4-b662-a46cbd1591f1": {"doc_hash": "04d6e253c0a7584fb5db38946bc42938d6c581876ee9f7705dc13e229b209e62", "ref_doc_id": "2f63fe20-0c8e-4263-902f-dfbe65a01215"}, "75f54cb3-443c-4d0b-9980-71258992fc3a": {"doc_hash": "d457641f58b764e81a40088b92422d41e299686def12ce1bf88452f1b8302a90", "ref_doc_id": "e579ddb6-8214-4a51-9332-5ef11f1d8a63"}, "8b184368-4eb3-40c4-996f-6d8cee2e63f0": {"doc_hash": "db93ae28129b2ad2fc004df57680dd4e07bd1c5fa99dae17000923f1c4b5d933", "ref_doc_id": "212411d6-20b1-464b-9afd-f7eca501b16d"}, "7961dc4d-f139-442f-922c-42bbc4321cf6": {"doc_hash": "65e087565dac6cff768bc8178fc8aeff08bc3e10b85c91b164b6985e1e29f663", "ref_doc_id": "24e9b62a-5dec-4e3d-8c08-de41e64ae5f5"}, "eed0536b-e2b4-4b6b-9d30-b868234ec025": {"doc_hash": "e3c94d1b55065368d4b88c47404a5cf73696a65080b4d65a3521af657e652a49", "ref_doc_id": "2c83ff71-abea-42b8-a0f1-746e89ce16a1"}, "0cdd205f-5ca5-498d-82d4-ab8e5120c7f3": {"doc_hash": "589f7b99a29ddc1b7e3358e690fbeff9824aa86fb983ff43fa3bd24f820bb1d6", "ref_doc_id": "87b341da-065a-483a-a356-e7f7c6a64394"}, "db9f8b9e-d41f-4f13-8040-9fb87cbbca33": {"doc_hash": "a74f3b46db2982241f6f389839d7271f8ae53e214bca7b71c270a220ed577074", "ref_doc_id": "544eee7c-ffc6-4d3b-8b7b-ad2d93fbd41b"}, "e5199167-e9ff-44e1-8a43-003dbb9c7545": {"doc_hash": "eab545a9e0657dff0ad7e0577e398a57e42c4b6db4f937d5e7ae53ba1f957142", "ref_doc_id": "2887db5d-bcf8-48e5-84c9-5a90aa2e0bad"}, "ec5ef2e9-4726-4121-a503-6248dc13bc80": {"doc_hash": "0c729256e51d101fb2ece7062d4a2d80ce79ccedd1c0acc5c46bb9c408cbe46a", "ref_doc_id": "5c85362d-aad5-4e70-8c34-bf5ad5a43e05"}, "834be78d-6808-4f9f-900c-355ab804dab6": {"doc_hash": "e4f1b13a7b50d7ea99a45da03d0808c5f454479fb57d6e0f342af7d432f027c4", "ref_doc_id": "9e3fceca-e435-43ec-93ed-9ae6e3f9f9f0"}, "666b28b6-6d43-4c28-8ef5-b8119b98f303": {"doc_hash": "3d940f428aa96630602eee810fdf0746d25fd429fa9da9de5f233288685a693d", "ref_doc_id": "859c6b72-a850-4e6a-82c1-88f7c26b1ea0"}, "982917d7-4f1f-4011-9870-fb119b56cd19": {"doc_hash": "c41ede649d9055a3139acb09e8980a686a1e6e36ddf2f80f6a4ffeac2d8f6fa4", "ref_doc_id": "307c75f8-8385-40d1-83c1-ca74f5093f70"}, "9451c91e-5cd2-4b07-8126-f86cde4ae9a0": {"doc_hash": "413dc2fdc0437a939515ad6fcafb19ddad43636cecfd357f3348218a2217433a", "ref_doc_id": "75e9e62b-1d76-48d5-9964-53d572d0924f"}, "f62182cc-4cbb-4f43-9149-617063909bf9": {"doc_hash": "703784217290c98de873511d8d0ce514d726d5c506149f2b7a00a5b3434fb6ef", "ref_doc_id": "c76e44ec-7081-45d2-bebd-c2e5317f9c34"}, "d01244cc-69c4-42b9-ba91-c9c69a0d0042": {"doc_hash": "80099b66151c9a93c961fd7fbb538f7d72dbd0457efe0f14e8a86181b2cc8c8b", "ref_doc_id": "8d57d29f-acc1-4b6d-b012-d95f788a42b7"}, "d0e7b778-9862-44a9-afc4-6a809e21a291": {"doc_hash": "9a92ae2ed628d2e3fac6a9ac52cad76487cc28661835f09d0159df6e74152a0b", "ref_doc_id": "e4bd2813-2a5e-476b-940d-e368a7c7a507"}, "b0e977a7-942e-4647-ae6c-5d6a43848932": {"doc_hash": "b48e2073667345b0199ec7a57ba5b6e99f0e61e98a52cca6faaf670ae6dddd1f", "ref_doc_id": "fd36819f-b150-4935-8771-0538b9592b9b"}, "1ed9b3d1-4698-4738-ac04-c1954ff3d573": {"doc_hash": "e2b8f6fa0ae11eca3ada18d5d3558f55133d940004b62d79a8379c47567afa22", "ref_doc_id": "1424a284-25c9-4e1e-9aca-046d0fa037e5"}, "2ffbdfb1-5a77-4338-80f1-a436b1a728b4": {"doc_hash": "bd92b291f7f2f37390d87222289531e3349b1cc3e4f121c23985c297769d7c5c", "ref_doc_id": "b450e992-5a88-4002-b156-6276ca33f482"}, "859757f8-6754-4869-8e80-db04e9dd71dc": {"doc_hash": "c699fa6db2958797f275953e1b78622d5885fec8d56f6f0155120c02df31d409", "ref_doc_id": "17e872b1-0bcd-42e6-9180-c671fb9402ab"}, "d12898bc-f373-4fef-ae23-ebbba1419232": {"doc_hash": "954db1bd563429c3d4cc7c32d84915d0471238dd236953bc38678094a802f87a", "ref_doc_id": "0f8cd6a3-733b-4aef-bf06-55d41d0ebb3d"}, "bdbb2bd8-0c04-4c0e-a0dc-238291db6b1c": {"doc_hash": "feacc7255de591afbfc1e8564f3cd2a8c58941e96deea6d54ae5b038be7f8725", "ref_doc_id": "3a86abe0-878d-4dab-b123-24b3e4e41de4"}, "cf32acb5-2a3b-4cd0-ad71-da4d36868d8c": {"doc_hash": "4b2e1567d73fe7b620ab93359c5e589e7e10634fba191de6a68858da94bb13e7", "ref_doc_id": "8789bf31-3874-4aff-9f1d-e93894d2f417"}, "1d6bdc77-291b-412a-82be-af198bfb31a3": {"doc_hash": "34826069f1c363b35c30855d9a879d05f640419e7a39ea2929521a12a0e0aea9", "ref_doc_id": "0f724f9a-261a-45dc-868f-4fb6ce8d4b12"}, "b06afe50-386d-4829-8a31-fa8075b1293b": {"doc_hash": "d5d45aa0ea8833c7c9b277f902b7ace8f5ec2ceb94fc60f673afc26279fe0e29", "ref_doc_id": "b6742c90-e8dd-4046-b535-899db847730e"}, "3700ca56-d9a8-46b5-b814-b14a18fef5c5": {"doc_hash": "e8afeb2dd4a2cd542f35469f2836f1d89f60dfc289a6af0292d085fdb309268e", "ref_doc_id": "a853a1be-9ead-4e6d-8faa-19baf84a27b4"}, "7a851ed6-52ee-4a10-8339-62860712c3f5": {"doc_hash": "5830f15703e2fad89c0b926cce5a545d92285b5102bcff6ba737518650cde315", "ref_doc_id": "14112a47-fb6f-4de7-b1ee-d396a86d0133"}, "7d960554-c8a0-45cd-8f05-dbe4ed527706": {"doc_hash": "4edeabfd16f68e76d9bc089d43e60daa2bb7a7bfdb25ddc7677edf2f3270ff45", "ref_doc_id": "66270d69-ecd3-4a9e-8ab0-b0be00805298"}, "703b8dd6-de68-4326-bb9a-b6350b57d03d": {"doc_hash": "075040ba3acea3dd36f4290f213cf7194533525c270f98cda24813439906d8ba", "ref_doc_id": "045d5eb6-8866-4894-9a1b-58923350de99"}, "0e7c6a1c-80c8-4f58-8dbb-ef96e4d09b1b": {"doc_hash": "7641c967001e2f25e6c0c31595cf641aa0035c31bfd58d861618b7b5d0a1d4c8", "ref_doc_id": "017e18ec-3413-4de4-b398-f372140c8c70"}, "22c72c54-55f9-4422-b0fb-ecc686336282": {"doc_hash": "b456560b558cebcf69d4340bb336e60f2695e9f9395783fd589764eaa48e64f3", "ref_doc_id": "70b0aac5-bae7-416a-a552-31ae317f7130"}, "1bdf9977-0fc6-43a7-aeef-8ca0f6fa08cc": {"doc_hash": "85acde8f5ff8757afee09e425765def839415ec65161bcb5a9080b4c724beb0a", "ref_doc_id": "66b6c0f4-7420-44e9-a216-4ee86ed8772a"}, "91f51b77-4c89-4056-9cd1-f44e96c4f8a6": {"doc_hash": "42a92bb82b80fe408e9226b8e709261e374dbd249bf16ce10c04502b761129de", "ref_doc_id": "5ed33ea0-a67f-4b43-ba36-9245588c1218"}, "71b0951f-fe18-4862-bb2e-806388d3b5e5": {"doc_hash": "7978978f98aefdfff6d432c3f3367859f38a6e9484e9b91a68ef8903c2820bb7", "ref_doc_id": "fa3ffb3e-e43d-4cf6-86da-90269b57abe2"}, "0f722215-431d-4bc4-bbdf-346c1150cadc": {"doc_hash": "82cab7fdf35d7599b5ef19b869f97bc4d963143a96f786007b14895223b82dcd", "ref_doc_id": "45f33713-b6aa-4935-a278-d1a8f94b7a86"}, "ca60b4d9-d605-4e42-a7b8-cd7f52694dd7": {"doc_hash": "33e2eab26a0b822e303705f119ddf2f5f4553275485a6440427cd9230ff79179", "ref_doc_id": "468dbc05-4ddf-4c75-a967-3fd1d3c4a3b7"}, "a9468516-50a6-4005-913a-2b8cf80b51c6": {"doc_hash": "9c6ba6f27fecab8807f65a8f0ce2c2cf6b5a96da14537904d731cb4f1d1315b8", "ref_doc_id": "17ed5d61-6115-4619-ad62-d83f972d4364"}, "bd92cd31-278a-49a1-a353-fd997a042d06": {"doc_hash": "470ea01bb19552c7d01c15fc1cac192979d99463ba0b3145647408f67ffd181c", "ref_doc_id": "1d478bcf-346a-4f4c-befd-66ccde15f541"}, "3bfc543f-3e0d-4d8c-aa01-9bcb84f54407": {"doc_hash": "39c7bf034c8b28abc87eea50ede258592e3b671a8a0d64791cf5deae4914da72", "ref_doc_id": "332edaba-6fc6-45f0-97cd-58fab188a06d"}, "6b00b6d6-4faf-4e86-ad4e-543a67605569": {"doc_hash": "eec2a187b64f12552c29f10c7b2926c5705349a608724b3095ca812d4bcb23de", "ref_doc_id": "38e8a702-13cb-4772-b4dc-c87cf9faed3f"}, "0d495a40-0072-475f-9582-46f13a42f52e": {"doc_hash": "5acf0ebd8ceabdb0e71f62106f0c8e75c71b309c40730016d9f0b86a65c2abeb", "ref_doc_id": "cc49a2ad-5616-4ea4-97de-2711e179ac7c"}, "05933c88-bea1-4e6b-aab3-da0a4b47651a": {"doc_hash": "363840265af16199dc228636ce0e631740094180f4d4cbc2bc7e42822777ea7a", "ref_doc_id": "e757524d-4f28-45b0-9838-4cca885d8c50"}, "2c14763a-b543-4dac-8aee-def227174b8b": {"doc_hash": "23687d03d6688b0630874cc94b3ce341b9521a02f5443cbf532cd2c11622312a", "ref_doc_id": "ceb9deb8-479f-476b-ab84-1b940b71e966"}, "c1c6ac30-05cd-47b0-af37-b4a5dbc9a137": {"doc_hash": "b290c0883ca99427a7bce5f1adaeb69a7bb09b205f896e7ec53bdbf1bafb8dc2", "ref_doc_id": "3b57089e-e78f-464a-8192-4bb3fcec53ab"}, "7171afe9-2651-4e5f-8670-ac1d6f7f1227": {"doc_hash": "8a7cdac76912eb0f5c91a70568967361b1adc31058b22313df82aa40f3b8ee51", "ref_doc_id": "0fa3631d-408b-4c3e-b940-357df38437ef"}, "6e356fba-b60d-421d-ac4b-e161495ad5f5": {"doc_hash": "576cbf5ee7ddb01f171640a37966839b33be94325b98371aa96c78bafd3d7134", "ref_doc_id": "ac183fc0-5bad-410d-9e2e-3a168f217883"}, "89270b80-2923-4580-ba20-afb22330f146": {"doc_hash": "083eb8f42f8f3dd423c51f51bcb842e5428a118958d0d63bba086858c1ecd74e", "ref_doc_id": "8e2ec2e5-1475-4ce7-af5c-cf243f8e56b3"}, "e8f6d3c1-772f-449d-ac29-edc3f2f8aaf9": {"doc_hash": "bca2237d968d778fd49363db06ab2714efa37ebfa96d28d364944229f25ee1cc", "ref_doc_id": "de96f9f4-c76c-49fd-bb9b-9889bf521be9"}, "c78a67d3-9479-4494-b5e6-a86abf599687": {"doc_hash": "6e7f8fc66eae2a9828acf50b098d6c31fc562250257ff08e54d038f0c86660fd", "ref_doc_id": "cd6e8b1f-a414-49bc-bf58-61f9ef93468b"}, "a314db8f-d50c-4326-8022-5d40d858c2c8": {"doc_hash": "c6d2517aa272174c58ae056d0fa49996c85d1293f9eb1037f75dcbd51e91d90d", "ref_doc_id": "139abc9b-b189-4da5-983b-4102e9712266"}, "ef4d319d-9aee-4da8-92d8-501068d9e8b1": {"doc_hash": "0ba8a091311ab0396d012596bc38de1ef3d8f10d618ebbe02ea19ca346079d44", "ref_doc_id": "e68ce08a-c3ae-41ab-bc5d-d0d2513130c8"}, "13d155d9-2084-4037-a1da-440d699d2760": {"doc_hash": "a455a60d61d6d351e642e8483402e00de1fdb267e7f52208ca14758f6fcc2d5d", "ref_doc_id": "7f4b7344-c284-4406-8be1-4a68d79537ab"}, "82b78fe3-fdf3-462d-8c0d-8a98719bb378": {"doc_hash": "73039c739eb5ab9407e286809db1971ac70cdb8dbf2d3c86aa1bb9b591c739c8", "ref_doc_id": "7c365f04-87d9-448b-8255-403363fd00ef"}, "115ee5cf-493c-49c8-97ec-e875b43b2502": {"doc_hash": "caf2cb4d5ee2f9a66dc7b889e1905f6d11c6650cda7e8ee9e24823a22169af3d", "ref_doc_id": "7c365f04-87d9-448b-8255-403363fd00ef"}, "2b6aa26e-9f4e-490a-bb9d-e28f4d9d08ba": {"doc_hash": "aaf93aa6b6e24812affe18ffff679a31717f13d55d8de83d84bf9b269295fccd", "ref_doc_id": "dd49bda7-f961-472d-bb42-8b127c3103bf"}, "dd740b46-4c1e-4f68-910b-ad0f1017e51d": {"doc_hash": "f5cbcb83d61bf816110704ab9b3fb91c410ed48390392766ecf5a4b6b8db4022", "ref_doc_id": "1953d877-8a1e-4b1b-b308-ab5674c29ff3"}, "b1cef72e-6c1e-462b-9b55-2cf473ffeca9": {"doc_hash": "3d7810bebfaa51db685d3c93a0611a78667756e86f0b7968b7e4e06008cccffc", "ref_doc_id": "1f2579c0-fe51-4401-8f2b-58cc64ff6c23"}, "4d24d3b4-ddf7-405d-bab6-d6c54c17adf6": {"doc_hash": "8b70af1852e852cd78d442d251dd5bc7c9f508579c19fdf4a1c22dc98800c798", "ref_doc_id": "cfab5cc6-7a03-4d26-968a-6d8c016655c7"}, "f55a9d5b-7a4b-42b8-9ee6-72568d5db22b": {"doc_hash": "89bd511d57f327baa1aa87fd2a7a95e680a81bf2265fa8375056009c77ac326a", "ref_doc_id": "c9af52b2-d04f-4895-83e1-21ac849d1688"}, "518b26b4-b22d-4799-8bea-8eb28a132857": {"doc_hash": "dde33cac57f1d43b3ae34edab677559547a0921ae62847d5e9da40d30ee4f5aa", "ref_doc_id": "426cde84-425e-4b61-99e9-023042bbdd19"}, "8aa970f8-fcec-440d-af97-16b81bdf624b": {"doc_hash": "4da1cdb4f2805ef2082399ced3cbf6fe04d2dec95090485847b80459ba3a3f1e", "ref_doc_id": "516e1e8f-6204-437f-991a-62c2cb08575d"}, "0312e532-d49a-4d7d-b8a6-f48b80cfada4": {"doc_hash": "2fb4ac5f118b627f27f74cd1afb24ebf8e1e34d8923197f44a5d0ac906a8f0f2", "ref_doc_id": "67e5da83-7725-4af6-a4c1-049aed2f8151"}, "0f0de495-1357-4845-ad7c-72f522f1c315": {"doc_hash": "32bc00bdf53bcd14414641eabab262ff4ce4985b77905df3837808d39708a337", "ref_doc_id": "e3cf8860-12a1-4ebe-8c91-5087a2184271"}, "668666ed-c419-423f-a62d-520a23760612": {"doc_hash": "74dd800366f9cb554cf9536adbea8dee69280557b06a732863dcbb0a88af5bdc", "ref_doc_id": "a7fd15b6-c60a-438c-a6c9-e72f78887474"}, "f3123d0a-d1c5-4c46-80d3-222ccc5d5ffd": {"doc_hash": "3724fe21391eebdf000c7f77ef12dc23217b42c15ce70b89827dc53aac532788", "ref_doc_id": "f41b4ef6-160b-43a3-832f-453466286a7b"}, "a4311929-6a9c-42e2-bf7b-801dec341307": {"doc_hash": "93923462b27dcde9ae2cf410efddd2f210506d13c8d87e4669fa8327f6d99ab7", "ref_doc_id": "5657d12b-0a6a-47c5-b788-f0e2307bdc50"}, "4cf0a829-c6e1-48e8-af19-51c5bbb5fb82": {"doc_hash": "6f815c09f0afd3cd827ecbf990d0ffdde84d4e9b57d733db82a579272c7f7e6f", "ref_doc_id": "f0047b98-d84e-4550-ac43-6a50499d5610"}, "de58378a-4a53-4bce-a1e4-c082d51b4774": {"doc_hash": "6b3f01ae12a042e5c127e17358dc82bdab393f01430ca79fe54a206e0fbe1a91", "ref_doc_id": "d9ba1f8c-c362-4d03-9377-4a31f2b61b7c"}, "47f6aa22-3cef-4e04-a3b5-24fb3e679a19": {"doc_hash": "e4dbce6891907bb349697578e59baa65e7a1bfd25e22e4b7b2b307295a5d3c9e", "ref_doc_id": "a9c76e60-c040-4486-9465-08123f8aa914"}, "1b804ccd-355c-42b7-b206-8e1c04d6b3f6": {"doc_hash": "efa764b9656a4cd126ceb2f54a1ca4957812fb0ee71fdfd24eafc45ca1dde389", "ref_doc_id": "ec7ede54-4847-4bae-9dc5-590886236728"}, "ebfd298e-fe6c-499f-9ccd-3faec0475720": {"doc_hash": "ad46841c76b337bce949e422748efe8444a70b0e893dbef0f8627159a0e1313d", "ref_doc_id": "0625b5ef-df5b-4459-a8d7-913314f6d6b8"}, "a99e3950-1ac9-4010-8275-4e3bd1640bd3": {"doc_hash": "912ad0575a6d86d98792dd2cebc18d674023d434d5728b68f68371b657c86879", "ref_doc_id": "e32cebb8-df3b-49fa-82a9-3c4700c241fc"}, "a295116b-a653-4f4d-b907-55618a397d5a": {"doc_hash": "a17f8beb735aa78ebb0023127a56cf4345ee8b3a6d2ace11e2bd5632203969a9", "ref_doc_id": "6a268334-8417-4230-95d0-3bd574353cca"}, "76670990-537d-4735-b4a5-fcde1418c43d": {"doc_hash": "ba48bfcac7c70bbda1c1326d5c923ad18a833966e15176f9b5c06942c72cc9a8", "ref_doc_id": "6a268334-8417-4230-95d0-3bd574353cca"}, "42c74a1c-18b9-4522-a0ae-a311c9b2ac60": {"doc_hash": "272765b52e4a4724caae7ad8c3c83ee363169da7b31a97776514332c664ce2c9", "ref_doc_id": "448358b8-c6d4-4253-9ec9-79e3f79da6df"}, "eb9ebbf7-681d-4ed5-9bae-ceb88f1d46fe": {"doc_hash": "3ed273cb63288146d9e326e15c441cbe51da411ae7dcb45762e403bc1c35202d", "ref_doc_id": "448358b8-c6d4-4253-9ec9-79e3f79da6df"}, "725a1768-01d5-49ad-a6f0-08747594c5ea": {"doc_hash": "b3f927119627ead562fa3b9fffbd7bd7a30f7de0fd98b56a074c29e66f96e1a7", "ref_doc_id": "db93a9b8-712c-4c28-947d-25cd3b704785"}, "0327a908-0e23-4773-bbcc-da0dbde9dba6": {"doc_hash": "4bfb08a437ee8368d25b40dd1c567900c1130b8700d0fbdde00d5890a03e188c", "ref_doc_id": "2244ed92-81c4-4379-a67e-377749487e92"}, "2864153e-8de1-45d1-b1e3-ed79efa1677a": {"doc_hash": "49de3197af80453f68df09384a47d813f279ca6be5218896f93bb22ea1d016b6", "ref_doc_id": "b7aa6b92-9474-47de-89ee-f40f48c31bb0"}, "ea50337c-dfe9-4320-bb0e-63a3b7e41c49": {"doc_hash": "1bfdfe032d32b3185cf076ed7f54819e7888d2dde46e86296c06bfb2346d14f0", "ref_doc_id": "50cf0158-ef10-401b-ba59-5729777cdfef"}, "d6f91792-7378-4dee-af67-2e5e9ef0809f": {"doc_hash": "6da59c999a273ff50c1f45eae752a5999a8c2c53a90ddcb0428e9b9b9463a6c4", "ref_doc_id": "186458d7-dbad-4a6e-a6a6-e6ac1949dd0a"}, "e1671e28-32c0-4dd4-b214-6ec152d851bb": {"doc_hash": "504663995cdef60be89ea4135591d4997a5065dd24c5bc5829df1da81164d94e", "ref_doc_id": "d9476070-4add-4a0c-9c6d-80b1e3f6ade4"}, "91648a46-44bb-4961-ac38-4c2a0af334ef": {"doc_hash": "07e7007fdfc94f592d3c207f634d3bff3b4e4c21d015065ac9f6cb8bad6b9e65", "ref_doc_id": "826de6dd-2bc5-4667-9e86-98f47b9db291"}, "bb0379aa-6e9e-47aa-8aeb-9b39488f12e7": {"doc_hash": "bede2f24331abe3cf2b6e6303b9014f3dfbec5312413a9ba4ca3cae509333de4", "ref_doc_id": "0fc3d9ed-ea21-4eb6-a4ce-13eeb347714f"}, "d57b330d-7b84-4fed-ab88-a39b05199ac3": {"doc_hash": "a5aedb9aadcda012bb2e3b0a5cf9a91581c9c593dd33aaf7f3d23f284320e8bc", "ref_doc_id": "ad54a376-98d7-4605-8568-c5baa0b63502"}, "b494b37a-c2e4-4aed-a483-daaebbff3339": {"doc_hash": "03cf50a8e44c348faf1c46296e041a65043beb642cb2d0b24d104dab44918ca0", "ref_doc_id": "e6a52ceb-fbdf-4c17-87ec-424a832afab4"}, "80b0c327-d722-465e-b108-3ce73792be36": {"doc_hash": "001e2fa1dd4f6ac9daa9d0d1ed97b18c2911438c4006d9983ab09c1f841098cf", "ref_doc_id": "8a4dba34-81d2-43f2-8785-c1faf3602d18"}, "6ed19751-2b57-4664-b81f-d5a182cb44c5": {"doc_hash": "326c8794d65923350c0809d9fc993b34d600eea6db56984931f9accfbd83c37d", "ref_doc_id": "48bb094f-fbf1-4362-85cd-3dc306d3905e"}, "774b5dd4-1626-4e5f-b00c-1ff810728790": {"doc_hash": "f2916e3f979826af8682ff68c9597535569942810f01d30576ab20eac04df362", "ref_doc_id": "37e60bff-86b3-4035-9306-6324a11ab1d4"}, "8b0c0a51-8db8-4906-9e79-cdf48ca53e1b": {"doc_hash": "b422aa094be38edee280d1bdf59b43a2c6dffcc77d765cb071faad857b9ed346", "ref_doc_id": "508e586f-4ed6-4ad7-bca8-6914181a85ee"}, "45cd4fa4-d72c-45f2-9790-7b8ba372fb94": {"doc_hash": "ec5d57134db330e2b003f7cfd61a5ad2dcc394f636282033c432ab106ad14667", "ref_doc_id": "4aaa9779-ea69-4ec3-b054-8befabc9664d"}, "ad281a15-c177-4155-a6ea-442c1ea0eaf3": {"doc_hash": "44aed356b57ec2710ef9350cca4e8a3c8d8c3e0626b0e4247020696cc17f2827", "ref_doc_id": "e8ccf932-3def-4315-983f-c35f7a8be0b1"}, "de27b6f4-2199-4a1a-90f3-3bf53c2a7ac8": {"doc_hash": "f3ad92c2f5cdebfdbdde8444d661061bced5ac166155cdf17c3ad11cb11e69bc", "ref_doc_id": "b17d234b-3de3-4989-9f6c-21e935cbaffd"}, "3a65fdfc-53bd-40fc-a833-88e8728e956c": {"doc_hash": "77bc97a0986fe7f1d7c2230e957bfd892e43941bc151dd0dbfba510d4b4640f1", "ref_doc_id": "5c92d7a3-073d-4feb-b557-efdafb37d046"}, "4f9d2610-5406-4f42-8ee3-1897f794e964": {"doc_hash": "99cb26c395be2c3cfa989d12aad77d83b48122162cbe092c66dc2b0c0e212c69", "ref_doc_id": "fe38761d-cdcd-4257-999c-78d7448f087c"}, "c46b4b6b-f01e-4899-9f39-b46f74a538d3": {"doc_hash": "113ef7c48334773cf20b8cb0e1ce7ca030c6c82c7811a87f04e7fac010421c74", "ref_doc_id": "ac932970-4b56-4655-8609-fcad93ff4662"}, "df557906-7bc9-4c37-b375-5e7923984e24": {"doc_hash": "e3c7d491b5ab0ff77f570f49cd56bf9af99988e97e125b64405054712efbfda1", "ref_doc_id": "b864ceb3-a0dd-4467-8acf-6f7b3e6d0f81"}, "8f9bc75d-9915-4f23-b4a9-01065081176b": {"doc_hash": "da0bf040d609e47b0accf07cb0fd311f47ebf9ca4f52eb01083395eb8639ec85", "ref_doc_id": "da6bdb74-491c-4e3d-9c6c-94a00ec7ca51"}, "93fa176e-9cf9-4354-b0bf-198b8059a8a4": {"doc_hash": "888f3338055afc18e1913901e1a7219972880b2118a75d79042d96317d952b2e", "ref_doc_id": "ee113b5d-88b8-4936-8585-e0ca52f133f5"}, "02f150a0-55e3-4a5e-b884-4b89565d3051": {"doc_hash": "ad2e3e19cc84a488dd327fd20b41b92eec4b1916addb796cc6bd233fc4e5ccca", "ref_doc_id": "8d5b8c89-77d8-46de-b6e8-4d36809e55bb"}, "b1a70cc4-70b8-4032-bf99-e98e3b512c4e": {"doc_hash": "ff5a358effb75b139dcfa47bb7051eec884ee4a00eba1ed979a54adbfa6af565", "ref_doc_id": "b794af8a-d3da-4f99-a825-497c8d7c8895"}, "eeb74e30-5716-42cf-9a33-a74d67854db1": {"doc_hash": "2c5ac3aff039d062c67ef8481abc4480aeb71caddb7f0cdedfc399fd02cb1c56", "ref_doc_id": "eddb19a5-9349-4aca-afeb-d4184a87cf76"}, "ab275705-6750-4f6f-9d66-3d7effd2c212": {"doc_hash": "1efb1e8020fa80365271c9357c226fb73a0d2cfbbcbbae7192cd5b7977720e24", "ref_doc_id": "c7d62a26-3881-4daa-a410-139ba68ebbe5"}, "270d9956-bcca-46f1-b1bf-11a44a42559b": {"doc_hash": "058f7dca99dcc20b399a4f76f92d1546871900563d7d674bc7e105364ecc4443", "ref_doc_id": "fc6b2b00-0c49-4eb6-8516-3ee5ef7950bd"}, "9e481b69-5509-4d1f-a550-6fd325450016": {"doc_hash": "ebd7ae0923deed352ec580b7a22af366b1b737c2f775273f76829dc0ccea51e2", "ref_doc_id": "b1c69d01-cee3-43fe-bac1-f6a736c9653b"}, "17e87fec-57f9-4b0c-9905-9d6211dcbb04": {"doc_hash": "0cdf165f12f8de6502a5ee62eaf31adc31e06428f32895bd9078713f87c8758e", "ref_doc_id": "9b724465-a69c-4c19-b8c7-d2b9c33329ea"}, "af846b12-9501-485a-94c3-4c84f6ed9bd5": {"doc_hash": "1d9384eff52c70cc089d65df30ac1eef9dadd10ddd0634cd47f5c3579524cf12", "ref_doc_id": "a1c726c9-27a8-46ab-8672-52c69a2b5b76"}, "5e5bc680-f409-4ef7-bfa5-6249d7565ca8": {"doc_hash": "c6ce819fd903ac5b0980c8567570a160f7cbc04ec904b61afa413d5670900b2d", "ref_doc_id": "7274af2d-02a5-4f48-a3a7-0d7b7050b6a3"}, "a0ce8097-c695-41ba-ad6d-900d72d7733a": {"doc_hash": "6242185c22e889a93d1c653533b45c5c7490e01a3889b98d99677476ca403bf4", "ref_doc_id": "78abfc42-d53c-4bec-bd06-ed1412506650"}, "38130fa0-c0df-432f-b804-70ecc89990e8": {"doc_hash": "0507b9b3168b5d99b2234ed74a096fb6a480596f48ec281d6034c537099d4ee0", "ref_doc_id": "5bc8225c-2bb1-4d0d-a78e-36abfda4fc71"}, "9f2bb612-e388-46aa-8194-8b6aaa56c818": {"doc_hash": "a66882ceb4f8b7924c5750e26e0a8048755fb330f28a49dba075630a500c1e3a", "ref_doc_id": "8e26418c-902e-4c72-aa2d-02d45af7e4c8"}, "55321e10-e786-4399-8c77-a8ff6977d2af": {"doc_hash": "65b99933f6878c0a57fc8d73025f00ae01d178a07f605fd5a76509ea35e9ef99", "ref_doc_id": "5679fb97-eb15-4f02-bb8d-7c4628433ecb"}, "ef0033be-f40c-423e-b394-18e66f8b86cd": {"doc_hash": "07ff0b95f33eba542394a7b13ef46af9ca169951f23417bff650ce9c1a89a54b", "ref_doc_id": "de9c11ea-b086-4ac9-9dce-68e826004ffe"}, "35187c62-7086-4005-9d3e-23dc6973f575": {"doc_hash": "8c64b77e9d144ccedfd586d40389988d139b2761868a608e3c105a3397a38cf3", "ref_doc_id": "35482ce9-2813-4334-aeb4-ed2a6cd74a16"}, "320f6ce3-69bb-4050-b83b-5671238190ae": {"doc_hash": "1eced8778df561fc3fd53bcef1a194c5ce71acde288d3fde0f7ce0cbfd4d4746", "ref_doc_id": "509661a8-4ef8-495d-bbd9-be671f4a0185"}, "e0a75e71-0781-46ae-84f8-fb55ed2ac311": {"doc_hash": "9d8a9f9a0e14bab80981e2cf88d4f8a3700426041434f3b530ad27b1ce7c43f9", "ref_doc_id": "f621cc47-5312-418c-b270-1151e347880d"}, "1be13b88-df6e-401f-ac76-ec6e1d03e2ad": {"doc_hash": "dab7dee573bdc2e664832e3db12dc55c557ccd5caec761c8ab67cee01ac8e17b", "ref_doc_id": "63e2ed12-14f8-480e-ae4a-2f100494ac8b"}, "5ce51d43-d267-4af5-ad44-2bbacca68d3b": {"doc_hash": "628d1bd2181b23291731a107b235433c02407ff8232e32582424ce35540ed583", "ref_doc_id": "68700776-4698-41dd-b88f-c3d7b77893be"}, "f77e1580-3202-4069-8396-b4d301c9eb63": {"doc_hash": "90c6f9c6c528601d0189c39aa2ad65bf68679b1ed57335fee5fd8f480a127b6a", "ref_doc_id": "04719050-31d9-4f6f-b79f-cdf83f9eb875"}, "f3a9d673-29be-4d30-9ce1-122a3ebc38bc": {"doc_hash": "4a11240852c05b75ff0e10551041809ccfc9146fda946f4ec50e548bbc15d8bb", "ref_doc_id": "45604538-8ec0-4b22-8efe-e581291900fd"}, "36308a0f-39e2-4f05-bcbd-418eec319b20": {"doc_hash": "bc398e83d48269f482513b3272d775e239966e711c5bb1dfceee143215476504", "ref_doc_id": "7706c506-7aab-4131-b8fd-7cc38fb6d9b9"}, "6d317841-d6b4-4d9a-9d91-aea659919974": {"doc_hash": "cc9766e702723f1caed402b89b647d8c4127e5775c99af72eea32fc4e6851b70", "ref_doc_id": "6b770969-928a-4557-91cc-3b75cf385d6c"}, "e37ee13f-6795-4075-b3ab-6daa57838177": {"doc_hash": "c4cc4c9610e822b4a76e0f9f0fbd5c0ce173b66994f33caa066d471e0b048da4", "ref_doc_id": "1615bf7e-867c-4039-9a78-65cbede6fa37"}, "66e85f99-0c67-41b0-be5d-5979d68087e3": {"doc_hash": "6352cdf8171f2e18a2d62e91c73fb374eac865dcd633584351a5dbc668bf10dd", "ref_doc_id": "e5bc341a-b7d8-4df2-b0bf-2b6f04d8114d"}, "9984db5b-b7c4-48ad-9de7-012bb6960189": {"doc_hash": "c2cd9c90302b4e2299b99ef56eea0244024451eb2f1b7dae05914c991141d52e", "ref_doc_id": "690ad7bd-6c9c-4e1d-a8ef-3911b671bf0a"}, "e5d4d179-a075-4fd0-96c2-389d6964f07d": {"doc_hash": "de63025aba993188dc8d5dc954a32776f7520817f0ac4e333f2a2be3fdb9c09d", "ref_doc_id": "fac7fc97-7b4d-4e5f-88ba-33534d065572"}, "d1934f06-3852-4b7d-a61c-356bc1cb2483": {"doc_hash": "bcc1e9236f05decc456f9c2563533609fe0d01b85c6a82f7c3f8c053068c23fd", "ref_doc_id": "c4bf5345-77f6-43e9-b79d-21a790aa1ebd"}, "783a9e90-e70b-472b-977a-bee0fdd8ef0f": {"doc_hash": "83045cbd2aa971ce5883155bac7ad9f1904fcc2db11aefd56ab2ea72590153ba", "ref_doc_id": "ce46e09e-73ba-438d-b8bf-95a85710187a"}, "4d0df072-f17d-4e20-95f6-4889ffc8832e": {"doc_hash": "63898d3283ce547158f443b1016f1fca3cf41fe7fb83af245f96bbc1c6b88e70", "ref_doc_id": "37c84395-2870-4773-8d1e-8e0bd03df537"}, "9e4669fb-533f-4142-93ba-d9cf72c3cda0": {"doc_hash": "a371b06d5522e7364e2c84a19b2440101b3a6f6dfd6c6dd28f52a8963147fae7", "ref_doc_id": "b706f7a6-6818-471b-8e59-a68955d784a0"}, "8f4d91e1-4e91-4182-aac8-26c46f018ffb": {"doc_hash": "34bcfcfbc62d3792c832d3ccebd3fc4d1cb23fde3e8842606f4dc7f4d2e3ba50", "ref_doc_id": "73bd739d-87cc-4c9a-a1db-05fdd5fedaf2"}, "1a9697de-950f-44de-81a2-b7c06bf8d755": {"doc_hash": "5a1703f6c29197e6465bef6bb8b999a02885affcaa6dc71a069c189fdfa007a1", "ref_doc_id": "edd683e8-36dd-437d-b3d1-210e8e63af37"}, "8ebd354d-e48e-4be1-acf7-54a730a1a065": {"doc_hash": "6a523074d23b5aa4699090ef41f618a8e007edb4cc26abb3afe98fe91c14d135", "ref_doc_id": "c2dba794-2510-4452-b49c-c8653cc43b56"}, "89db3279-9a74-4327-8723-d4e708eabda9": {"doc_hash": "c22073ce3e1932361cd703d48024055ba4440a17b63715868b2e3395562e83e7", "ref_doc_id": "d8d02010-ef0d-4d79-a340-710361d24a40"}, "c34a3bc8-fb03-4163-9904-b87f75e1a492": {"doc_hash": "29cb082ff5e4c2bb4cf01d3fbddd8f1c19dcb4e7821d696596923830a97d84a2", "ref_doc_id": "6f6cfdc5-d3c9-47f6-9435-b1066c34f7ce"}, "159bbe55-fb8f-43a7-a4a5-a5e4ec4bc2e3": {"doc_hash": "fdc0226da651949bc5b2138e0d0b9f623d769244c8d9fe07dde134d45bb5277b", "ref_doc_id": "98f68874-6f42-498c-9919-9a2cd3809441"}, "734fe262-1096-4705-a8a1-a1bdd0f6366b": {"doc_hash": "74ce644fda97fe25c370386d769b9fa3721fef5120eda66a9994d551e7870090", "ref_doc_id": "b8b14827-cf2c-4046-ac29-ad251910988d"}, "06c68378-00f0-40e1-93ba-9ba74a519054": {"doc_hash": "15a170525e61dac95658b5d0ef9f2d691bc9801ff94b163b98a752824889cfc4", "ref_doc_id": "5ac5b7a1-2d51-4b10-95df-633045b4f0e5"}, "f18da28c-51e1-4828-9eb2-29f1f2a0feff": {"doc_hash": "4fa7ed4197a60379cbb17e2a2315dce164214bfc3bc32904a21bba448be5be17", "ref_doc_id": "9c6af9c4-15a0-4449-9e92-5d81a4dad94e"}, "e2417396-627e-4ec8-b0db-0b873c97d11f": {"doc_hash": "949ee6ce814da9833f038ce2a4d16014d32b3353fdc89a04686866ef91de4010", "ref_doc_id": "fd2a8623-68d2-4d97-96cb-8a119015e1ae"}, "7f4765b9-948a-4531-b929-e23b4713ed4c": {"doc_hash": "36527b36ff46df7b75df9f82d0c78d3c5c33de60406e7032ba26a23c00d749a2", "ref_doc_id": "d5ca777d-e9f3-4fe7-9439-d1199975d918"}, "fa543bf2-77e4-47e0-88c6-f18ae73ac5c6": {"doc_hash": "750abd1ecc6f7a924dcca6d8ca49ed53043ef317fb004ab3183e52fa7649232b", "ref_doc_id": "7b022788-9596-4a0f-8a8c-a1b63a64704b"}, "235932b9-15ba-4d94-b75c-84e35134c9b0": {"doc_hash": "3145de1b06e903b5582a11b137945559625a30f880c54dcfd95f567e30ce39ef", "ref_doc_id": "68b29de0-926b-462c-b958-f686ee61e541"}, "705529df-d07e-4589-8098-3c91d6107682": {"doc_hash": "d553a98d41c8edbfc6894c8805424f37f5b9b8ef7f9a668dc1fc8efde2676a54", "ref_doc_id": "c82822c6-d9d4-4013-8ebd-a1bb97c792ca"}, "828cba8d-8dc6-441c-8d30-8673b94ec39f": {"doc_hash": "d9c75a7a9416c3671c9c2d455a98b095999f155c9834980ea090f46327fa62ac", "ref_doc_id": "7e7f80ed-82dc-4ca0-b969-b4240c770c45"}, "0ba10c19-4cd6-484c-9c63-2e336b42f5a2": {"doc_hash": "3f339a6de51fd1efb0fcc51b00a88ab9e7de79ae3c3634890a3cbddec091dbb5", "ref_doc_id": "f4485722-3856-4f6d-ae7f-46363560e2fb"}, "d2e8f69f-bd2b-4a1e-a835-42e28276d859": {"doc_hash": "0882b7ffbe3aadd1cbb6c08c086afe5461c9c761a4e83ab626800f86b898bb91", "ref_doc_id": "1f605cfc-0e3e-4581-aaee-17390f20445a"}, "03b27506-59c7-414b-9a62-3ac8ce856953": {"doc_hash": "54347e55841bf74339c03d9bf16e03d9986718a8bae17d40d4346335c2659b8e", "ref_doc_id": "27b568b7-1fc0-46f2-bf96-8650bda62a52"}, "699f744c-1efb-405e-ab46-ceed66236686": {"doc_hash": "14d2d0b7db6644e6e204249bcd856e59dc5ce322e398fb24b8c37b1a514fff99", "ref_doc_id": "f72d1ce6-1097-4635-9923-744aa6d44144"}, "bd4c3018-ee48-4766-872f-39649b85cb85": {"doc_hash": "c4f638baaa430f754a23444aa186af40dc617509c8dcad246207f7ba27f81a32", "ref_doc_id": "794f82f1-88db-47ab-b47c-42079e9292cd"}, "276f3dd7-abb2-4a06-9f63-f2a29f1afc7d": {"doc_hash": "86ef680b056f89ceb98d1093c974a486793d04df28d2938023728c5e6deb8c0b", "ref_doc_id": "81d8bfe7-6bc6-4eec-a784-5f6d073b01ae"}, "ca920b6a-bd01-4e88-9aae-bb57565bbbec": {"doc_hash": "e471ccc1d998cac56ccbe7c63e50b7224190436c38a843badd228729bb55f25d", "ref_doc_id": "a45869e4-360d-451d-8b34-6ead2327eb4f"}, "7098d7be-8914-4667-9c03-04622aa104f2": {"doc_hash": "5e71e7828f9d6ca2542a7441778d6a4540563488896860169b051bc46ba61970", "ref_doc_id": "7e1838c6-de90-41ec-bce8-046b2fade43d"}, "cb389878-18f1-4267-8852-fc3e013cf2c6": {"doc_hash": "21db92751d01feed0da2e300acb2eb2afa793e4bfb5ab7de2252166600e66ef7", "ref_doc_id": "02bdeffd-9cf4-4f2d-9bbb-641e7ba856f6"}, "2fc8f2db-8d87-4812-9933-348645e74b88": {"doc_hash": "0fb6f69c086790aa36be1af025dc5348e72622c903589f872a0e0f9d03cc847f", "ref_doc_id": "29efb7e4-d9ea-4eb8-bcfb-285aa7fa0115"}, "6c832131-72c3-49ea-aad5-8305fac61540": {"doc_hash": "75135b56feb6094b76ed53bd88d59a158ff58431a686f6b88467de96b2fe00cd", "ref_doc_id": "16eff6d4-b728-40cb-9d17-5492a4d3db27"}, "58872b50-a0f7-4fb6-ba8d-207e791c3a4f": {"doc_hash": "a4ba771e87e38fd216e66fc23a969b2fee02ef516b0cfb580de4010967a52cb3", "ref_doc_id": "2b197b28-e3f9-45ed-bced-90d2a1d64e87"}, "474e4628-d6fb-4f34-b0e2-f9a8b1274c44": {"doc_hash": "ee54dd1704432de150a84539b0e4d2f0a55731f04b8b7a6f02fb4be3d57bda89", "ref_doc_id": "568eab24-2c2a-4de1-af32-850ff3baba0b"}, "0e8c48c2-16a7-4be9-9f46-84898ecfb370": {"doc_hash": "ab62b1986ef05711395b7c80493290496803faba8203b335670f4d4d14e62d74", "ref_doc_id": "e6e6c0c9-4fd3-4aa7-9ddc-a997aa5a136a"}, "403573a8-ea4a-4b93-8ac4-df6370ddd886": {"doc_hash": "5614611257882709bad5fb00689b8f1e84480eeb2fb923a2182af6d961bf8a9b", "ref_doc_id": "7d3978ba-fbef-479f-8fb9-f735eb335f0f"}, "744885fd-4700-410a-bad7-c3f7bc3f018a": {"doc_hash": "6d9955092f0afd7429099428ce7fbd9458f17a5f1086ff71c3f7f84acd502365", "ref_doc_id": "7ee356cf-f18d-4658-be1d-3eecd39517a0"}, "be20fee5-c4ec-4130-96fb-bad5abb096a7": {"doc_hash": "a175f885f21f38f21e7d9cd27906213e24a8d96a1567cec54d65c3457dd7011c", "ref_doc_id": "62d9fc53-f44c-4af1-9459-75ac958c1f4a"}, "ea2e285e-6b32-41bb-96a9-6089b0137326": {"doc_hash": "62d4c2e0de32b51234f3ec354abb26c82ee1ae421c2bc7cafdd2075c780788e8", "ref_doc_id": "9d8bcc7d-99ce-4225-b038-e4d772436273"}, "3793f74e-f64e-4348-85a6-26c776d5d76a": {"doc_hash": "f0e67ff5835e869520a00937729a44a7f958085e1e6fd19e7aca67fe1ad0c552", "ref_doc_id": "f52bea60-2849-4f3c-bd0b-75b27307e96f"}, "2454e6ca-7c1b-44d0-acf1-3d17236067b2": {"doc_hash": "193a7c268d7e7868a5404f1502b564e91e0ace95632618d3417f78028659010b", "ref_doc_id": "f5385cf2-6400-4ad0-930e-d3759c81f28f"}, "5d2454f9-3542-4d87-817c-6c1cb452dd59": {"doc_hash": "ca05582ef66d7b4dbab83d2563063d944210eda9cc5592cec1fdbb54d8923de9", "ref_doc_id": "a1038074-afb3-4d44-86d9-0bd27efe70e2"}, "baf25430-f8b0-40bc-9fe0-02623a72acf1": {"doc_hash": "4f59d11b1c4be6086b72e04e9a78aabd7310fcadd21c018c0d3755b6440b871e", "ref_doc_id": "6542d896-6b73-473b-a9ec-7253daf085bf"}, "66642baa-b01e-409f-8652-a123639d3c04": {"doc_hash": "044ecdd8d8adc20f21afe28530319b2eb0e1a4db6e099998a94db81bffacf860", "ref_doc_id": "b85c8ed2-ac2f-493d-8710-e2bf7fe1c5db"}, "c58d7660-4fab-44bb-add9-27f8177afe25": {"doc_hash": "6b9f424faa689f472e3a43f7298a77f44632323da4be7bf815b0e4e9e5dc5190", "ref_doc_id": "afc6bf40-77f4-485f-b417-a6df5031ca7b"}, "bd0240fe-f10e-4f03-b1a2-ea94732890bc": {"doc_hash": "f96e5aae772373fbcb5ea07e76ec29369f3d93d47639b0b1ca67862d4fb09482", "ref_doc_id": "9a32da5a-e4b0-43f8-97e0-7c0a2cf4a8e6"}, "2a4126d3-22e1-4249-afb2-225a9fa318fe": {"doc_hash": "b8fb7b10921392fb10e01e3f2eefccc371344ebe4aeb20a0e63b97de579d0fec", "ref_doc_id": "c3285bfd-8ddb-4c86-af0e-d32bd0d6c98d"}, "b2ba7a9e-8818-4c58-9117-dff8b1eb6848": {"doc_hash": "3f1d8ccb52701b81b6cb29f5fb16164baaddfef8458e4f068cbe291b9797d03d", "ref_doc_id": "5201ede0-37a0-4b28-9a30-6a619a089421"}, "141f90ab-c2ff-4353-91f2-bb15abedc1dd": {"doc_hash": "e7d371c2e2273077f80021deab97e923b35edd1292907a16745b1a9935b96d49", "ref_doc_id": "02f2515f-45a9-49ea-8ccc-9abfa8f60b54"}, "f5a6670a-d8e2-4b86-91ba-a238000dc3ef": {"doc_hash": "77b44a08182da788593e6efd28b45058488a1d4d213168a83e823bf8108be159", "ref_doc_id": "cd571c8b-3d36-42b6-977d-a6763e091468"}, "fc3aa15d-caff-45d6-af58-701f219c5b97": {"doc_hash": "6e39d6945506e6ffd9d401d814684765321f90aea51bbce2de29832b2371a3c2", "ref_doc_id": "6d92b267-b467-4321-8bf1-7a35a3dd2511"}, "08cc121d-7520-4e8d-a75f-0f157dcbd0bc": {"doc_hash": "246a05f7dc6f4103f827badcaf33a4259ac25cf0d75ef1c745478272d2f47a95", "ref_doc_id": "4adbe36d-d504-4a00-bfc9-7edef0d7580b"}, "7f58b013-9f2b-48a9-a38c-4a0d10e9a9cf": {"doc_hash": "75c8546a215b67cec7e17009eb671af69ea4e890b0eabc14d8a4d0438d76c8c4", "ref_doc_id": "7079f221-2dd2-40e8-b574-f0ba821db952"}, "5d1503bd-979b-450a-b79b-cfd70c19c97c": {"doc_hash": "ba612094df0713f0a6951b1554e42282448a3fa8d28c9113da2ad0e6f8af6e26", "ref_doc_id": "91d3978d-23ed-4116-921c-a8045e65fd37"}, "b09ce588-62e9-41da-bf9a-a3e29c664f45": {"doc_hash": "948a4a86dec0262b358bd6254da5a98fe80ba7edcecea97290bca156092acda6", "ref_doc_id": "727a5a3b-4138-41ae-911b-743af3f007bd"}, "3562958f-4766-4dea-b564-00fcc221ddcc": {"doc_hash": "66a514b02bc91c2deec1196ac34b24b3020dd3a6140f7a31835dfbe4d74f1301", "ref_doc_id": "0dda1ba1-7dc8-4a51-bdd0-373731d9da18"}, "df019df8-06de-40b2-ad3b-5d55e799fe4f": {"doc_hash": "0eb0404456ff56d1011984aeecb0027b83be647cf61596b6a911e610624e60eb", "ref_doc_id": "46e6e8bb-f018-4f19-be2d-74344601ef18"}, "4e869199-d91f-4abd-9841-7cb53f3acfe0": {"doc_hash": "ce01ac741ba9b990c59b6397a49e9f42ad6d8fa893580940eb496789c6b3b1eb", "ref_doc_id": "dec8a953-54e9-4870-8a75-ca890ceae60c"}, "7f03b89b-7596-4b66-a30d-abdded90a372": {"doc_hash": "8d63e5927a09b165f4c19d3504d02a6a7921779b63e0c94c719105e456f94103", "ref_doc_id": "ae1446e9-16aa-4ed9-bcd8-3662ba9e36a8"}, "314e95de-31ee-473b-8b94-539b4a500bd4": {"doc_hash": "9eaa2f72bebd1b8411416fba8a589694489451daa0aa4c26a99e5591cf57a1e7", "ref_doc_id": "5ff1bc0f-9812-4b4f-9e0d-d1504c786d94"}, "a0796f4c-7754-4851-a0a9-da343529f29b": {"doc_hash": "3db66786bb80f33d6e6f4917e8603547e42810793c1285138e7c882c50f684c7", "ref_doc_id": "23ca34a3-6f46-4ffb-8aab-b38b43d770cc"}, "97403974-4a13-4e2f-bd52-9abdc013d60f": {"doc_hash": "5acce0ffa6d87998dbc3dbba2aafb3892373dc1304822638de7640fba1e2d43b", "ref_doc_id": "44292833-a1c8-4799-b97e-bed602ff59c5"}, "24204954-f247-4abf-b85a-2a17fc553d4b": {"doc_hash": "faf85df3e636d1ca8acdfb76d502b4162c61017337e2f3e62ed59c91d370c6f6", "ref_doc_id": "d1878d8d-8751-48aa-9f6d-4b42cf115000"}, "0afe720a-1989-418c-8728-c59f2604d4df": {"doc_hash": "31c9af26c6dc2c9a335773b31563076d6335456ba5b4b17fb6544104be1b4898", "ref_doc_id": "a2bb13cc-55a1-482b-a220-e4038c6857cf"}, "11e4fd97-41c4-44a1-a78a-3a6324b6e080": {"doc_hash": "d58c7aef7bb7906482aed3b4475710e6115c70185ca48be69f91fcd6c84dac44", "ref_doc_id": "4a4aea1f-f7ab-428a-8644-31751fda865e"}, "8359c892-d0fa-4c7a-82a6-8c7a1542897b": {"doc_hash": "c95ed356326711567e38db8b61ea1c101fe4ce2f10a3772f9b46c4a40703d62c", "ref_doc_id": "262c00e1-9ce0-455b-a193-6dbe6b572095"}, "9ccd6805-88ab-4279-b96c-86f020514ae4": {"doc_hash": "76029cab683774c95148716caaa69d35171a0f003b2d03b31a378e3f5ac6e3d9", "ref_doc_id": "359bd711-5c5d-4d8d-8daa-b337116a5316"}, "2b2927f5-45ce-4a31-ae8f-2f9c3587fd93": {"doc_hash": "728826b4ea3e301c092dd0241b7955ce07b950a8fc5827161e6d14f65cba34a5", "ref_doc_id": "d41e905d-f1c5-45a5-8149-92963e0f5e0b"}, "aee3e991-6615-4a6d-9f51-6d3e09bd144d": {"doc_hash": "79110be5aa410fda2ba248412101ed692e7fa046051235d929423f21efe11c15", "ref_doc_id": "edf96e66-a450-4cfe-8470-b59f90525f58"}, "9fee0525-b7d6-4464-a8f6-7a6c36a87773": {"doc_hash": "90aedc90c8ed17a9961508b42820a62aa9d6c84094b5f98d020d3bce096445c9", "ref_doc_id": "3b4a7024-577d-48f9-b074-3a56bfe3f453"}, "36853a7d-d51e-4425-96b2-04f3424f080a": {"doc_hash": "d663ad90706fecd983f50af73feac5d9704b80c8d6b752e34bdb73979d476a55", "ref_doc_id": "fb4b9232-e0fd-4ca7-bf8c-3b5550024f88"}, "857de485-41cc-4094-9320-20939a5cbad7": {"doc_hash": "a270fef6cd748d053b2e83acd42a24272bd6a8fa21d275feb1f49fe981915e0d", "ref_doc_id": "07556098-21e0-4d69-9ee8-34a69a059a42"}, "804b1b48-6006-4201-9caa-678c69b031d4": {"doc_hash": "24f91d75fb58824154ee9b6835599dafb58a60bf74dcf6662030f47caa984310", "ref_doc_id": "1aa3c3c3-b099-4721-9ede-af0e77295c21"}, "87275339-0bb9-48b1-ac2a-97cfb52a21bf": {"doc_hash": "19d431574614c70c66d4fbba1221674316e921ca2dbab5b8d4c175c6e3526578", "ref_doc_id": "933df950-0564-4583-98e1-16f9361c55f3"}, "9f3c18ba-5c68-45cc-8145-12b68dc2c4a7": {"doc_hash": "f5d3bacfd5985fe5d14b129b6ba375b93d8ef83b2a1399052948bbffe52cbc2a", "ref_doc_id": "be88b7d8-26c4-4c50-aca0-baae8e319692"}, "090eda9e-38a8-465a-8498-859013acd003": {"doc_hash": "2fffa77046580ecf36e48efcf8d4d93c75723dc87ce52444a39224c440b7b5e0", "ref_doc_id": "6167ee3f-81a2-4b5d-8fa1-486501fd4fc7"}, "1a55d121-2e0c-4174-9224-368c74c199c3": {"doc_hash": "bc8cf08ddfc0d1bd38e0c36bb4f8b5607ced42cd9860fb2a52780464311f4a44", "ref_doc_id": "bd59607a-61ff-4d76-be0e-acf057d46d33"}, "44f9b8c3-a6b9-445b-95d1-ed5c9c8ef8a4": {"doc_hash": "43526046997cf2dd79a3657aa23196763ae87d480e2e2a47e500eca29b5f081f", "ref_doc_id": "53bfea96-22e7-447a-a2a5-ea7f4db3fb44"}, "e94a7e1f-0324-4ca3-9438-3a3522b1bc63": {"doc_hash": "a17b0d567dd5b4a071e9669146474f34f5e8e065a3ce15284fb7b976f9c30f94", "ref_doc_id": "27479ff1-2c3e-4715-b37b-914fc1a1e01e"}, "9817471d-9a7e-4746-abc4-0151edc25779": {"doc_hash": "335083d9e96390971925ed825182d27ee9e264bb7b156dd3050de0abc98517b9", "ref_doc_id": "966f48ab-c8e6-45d3-9941-45aa91d8449f"}, "e5f01008-7f85-42a2-bdcd-84158742819a": {"doc_hash": "e77459679316af92b3ca2ce0d10a134c0675d9df32dd97c8fc6bff2aa1658175", "ref_doc_id": "264c24cc-2c56-4f38-a1dd-96b261b31ad6"}, "2815cdf3-a62d-43cc-9777-467928a74411": {"doc_hash": "b48cf6552491ea3ef732a66b44da0e6b09725e07fe930fc6f67395ce8eced02a", "ref_doc_id": "49e51f27-125c-41f4-ac7b-fc8997c40b9d"}, "d6b0351d-56cc-47c5-8e11-b6711ee6a7c4": {"doc_hash": "c6a75358f657082811c526763692c11b38087e6247b87db950b3e8514aa076e6", "ref_doc_id": "a980f5a8-1f58-4c64-b94d-4a51beb0c9cc"}, "1128b6a0-2285-499e-b14a-038e5091251b": {"doc_hash": "9642db9ac8497dc46e132a6d3f120cdca305c91ee8d68d105156f715bf34fb3f", "ref_doc_id": "ca7ee332-c272-494a-8d1a-a4fbb41d91c4"}, "e24853a3-614c-4452-9116-3302db507075": {"doc_hash": "2b682ff981b86771dbec261e8c1e7fbb66d12e301995c8e475c70d447b443c7c", "ref_doc_id": "55a84ef4-aeba-4c8b-a610-78cf28f6a425"}, "42e10955-9842-4acd-8f2b-5889104376e3": {"doc_hash": "58b9aa90a3f6ad100732dda0b215d0e6346776950d587a5164119bce9c2c62ff", "ref_doc_id": "d146a532-2f3c-49dc-85d3-b25e34d6a592"}, "5876cb2d-c94e-4921-9917-25941f9530c4": {"doc_hash": "bd43e7ab02bb58616363eefedad88c97dda04433ac5ec93a7209b9ecda6a7b51", "ref_doc_id": "1fe4e962-cd2d-4960-ab8e-f097c1d203c4"}, "2ceaea90-bf72-4740-95f9-92d79f3c778d": {"doc_hash": "d4d60760e1341d5e0d99fcdfe779468efacc11bed58142d40636d2b5ec35be72", "ref_doc_id": "31484385-bc46-421c-9c38-baeae26bd2b4"}, "2153a5a5-1056-43bf-9a7c-0eecc3551d8e": {"doc_hash": "10779f461aac69ff81fc81f7a77adc06b5830510861f2e544f2213927b1f7edd", "ref_doc_id": "31601c6f-72a7-41be-be94-382d9fc8b16f"}, "84710840-6eb9-4cd3-8442-7923e08d24e0": {"doc_hash": "0e72653f11d3e15c77c617cce27e3cc57c1485420d260153821de0ff9119b3c0", "ref_doc_id": "3d794c4e-8b37-4952-b371-8471e23c54cc"}, "3381bf6e-09d0-4bc7-b056-1614b131ed1e": {"doc_hash": "f9abcfa16535d7c72ab32c78fb807b7702aa030d2bcd4c63d908cc74f558b263", "ref_doc_id": "6cec1e5e-b4cb-4c62-aca7-10654095f57a"}, "8b29dfb1-6729-4bf3-954b-0f6b3fe879a4": {"doc_hash": "ecbbc7016f6120dbc3e44978c6c376968db8967f14da8f60e5dd3835835ba281", "ref_doc_id": "95884c02-e73a-45dd-a107-37f379deb437"}, "ee0f81b7-5b9a-44bf-97b7-e89f9fd49920": {"doc_hash": "a16b1de259b82572c5f08611151d7ba738d2218736a0b58abaf832d450a0e100", "ref_doc_id": "abfce39d-eb99-4e71-88a0-ceb5722cef89"}, "7c7f3259-2c7f-4f68-9823-235bda659090": {"doc_hash": "fead4917e9638b6f967726e44549cc1ce809741dab7e6d3d16bf747dc04ab81d", "ref_doc_id": "b8296b80-18fe-4a90-a4e6-0d05ab4481e6"}, "a3ceb45b-6da4-4b25-a6b6-b518c2fb52ac": {"doc_hash": "ef59ffc2e5c921ec25175cc6aca17efd102c7c1523b6ffa066f7f4e8247bc34a", "ref_doc_id": "de5c0c3b-b9b7-4b29-b428-67b61e26490a"}, "948d6ee0-2aad-45e7-a939-220a3aa4ee22": {"doc_hash": "4d6462cafe3f9ebcaa06d1ac1a552d04c48fb7027f58fc5d6ee57dc4b0298db4", "ref_doc_id": "2d4e2e04-ae67-4b1e-9fd0-6de65f487f74"}, "b4dbab65-0c5d-4328-9ffb-5163360f8500": {"doc_hash": "1bd0713c256b6dce77da9885ec8f5e671433c3754186533f6306a1bcad4a514a", "ref_doc_id": "f47de9d4-04d6-4492-8f9e-fada8643d096"}, "59ce14e2-9c6a-4e15-a9e4-c763084ddb9c": {"doc_hash": "5207b83108523771fa57dad5655afc69c4ab2b601bf0a59533059223d454a967", "ref_doc_id": "448cb869-e223-48a6-9466-f465b6af6234"}, "6edccddd-adc5-4b4f-8c73-7cb85e9f18c7": {"doc_hash": "6e68015906bd3e9136fe35e5a4db3294178b3dd5074efb372f699e655e988cd0", "ref_doc_id": "6c8b6da6-1589-449e-ad61-3103f05c0bc9"}, "74f0acb7-8822-41f1-b284-cae932aa3acd": {"doc_hash": "d251088e555db0ab7322cf89526fe5e4f9b0326780e0e08704551212cb47754a", "ref_doc_id": "ddf90a3c-417a-4f55-9e18-7c210bd23161"}, "d6ddd1b3-a9de-4253-a39b-4b90e3ad4446": {"doc_hash": "5a845cefc2aec2d8a42ebb4921a7d66a5154f2aead6c0a3c8bf72265e0545b9d", "ref_doc_id": "927e5ccc-9654-4efa-b67d-336e58e045a0"}, "8c0199ad-7f2f-402f-82ea-41f2f139dd68": {"doc_hash": "c2db4014d0c1927812e08c4f3110fe30a20596d5a57b245e8107ba9611a85dc5", "ref_doc_id": "adff3466-6057-4fc7-af37-89d64ecc8d78"}, "88f772ee-d8f6-42c0-b44c-8d67c53723af": {"doc_hash": "b53e7447d7f57000b8b2135568b41e900bc3fdd018feb85e7ee56cf43444f3d1", "ref_doc_id": "cf288c58-6c63-41c0-b8e9-5923b61e79e7"}, "a2a3f180-9fdf-4c08-bb2e-89ed1dcdc1b2": {"doc_hash": "23cba1fc556d191a63da13e23ac070f59f7a80715ee17850f070c9b3c19aa962", "ref_doc_id": "ad17cb25-0e69-4f5a-a218-697c597f70db"}, "6ccf0433-a8d8-4945-b0dd-b6da5f6376fc": {"doc_hash": "6e9e85b404f08493dfd6739ef2830c27038d5c857f44e56f03ec899ede2fca91", "ref_doc_id": "ee24f0f3-22e0-4115-ba51-42484dc95d81"}, "11b22650-43e3-4840-aea7-2345b75b4cc7": {"doc_hash": "f453ada22bc3d7c41ee8bc21c052f4f0a77372a7d8997cf39a7920a9c0f2cc39", "ref_doc_id": "ee8ce06d-e9fb-4f43-87c2-e25b8f16142a"}, "e5979a0e-a171-4043-8a90-254604770c94": {"doc_hash": "453a52ff8e52fe646a538259c27021a0bb8229a7a4ac05051d4e5ea995104f24", "ref_doc_id": "f6433652-fc14-40b7-ae20-cf5bd618af00"}, "9bc1cabc-b262-42f1-a20a-d1cafbfa15b2": {"doc_hash": "d0a6756446bbabfc61cb46e20da96a031561d20dddc8378d137bd50bb384609a", "ref_doc_id": "7b466cfa-d99a-46a1-83ee-6d3c084760f2"}, "7f995636-794e-411a-bbe0-d907fb5dfd88": {"doc_hash": "fd0834fdd1790b069359fb921008a62bd9b064187af69dc46e8e5486171caa86", "ref_doc_id": "2c05c809-20e5-4c2c-8f55-119d4bc65ddf"}, "e6fda905-21ad-4f80-b0c3-f8e3a61e7bac": {"doc_hash": "972f049eff383c776f6a2f42d175761b78bafdead608c751e035caef8b34ff37", "ref_doc_id": "cad1de93-88ca-4e08-8174-8fb3f4ac4477"}, "6162a03f-03d6-4b5f-ab67-504eea69f401": {"doc_hash": "de8eaa39ea60583f18448eeeb7bb4bfe232e85e58c7da2f357a3ef3a701ebbc9", "ref_doc_id": "75b5f002-9500-448f-9455-87f3442f42ed"}, "6d680c7f-75c2-4c0f-a2dd-9eddf2700330": {"doc_hash": "b368f0a0a02a64f177698913efb6544f81574afa2b1c124b9056680983836e05", "ref_doc_id": "5155a425-bbba-4e0d-b81c-bd73509a22e9"}, "22addc01-0df5-4ced-8aab-c2f59392b1db": {"doc_hash": "839304d5147c44c65e3ed0ba1755af0b87815314fe46dcfb4a6a1adb842d3f82", "ref_doc_id": "c9b3c287-44aa-4e4f-b1b2-e4708b5f00d5"}, "5e4f4147-16e1-4f8e-a89e-980befb4a411": {"doc_hash": "02c25eb40b55aca228d639d9e444c629856bba92b4ebd4e6539d39fe2fc70e6a", "ref_doc_id": "e2d42b5d-8155-42e0-a13b-da6a6d12d2e3"}, "17c90ec9-8504-479e-98cd-df51239686af": {"doc_hash": "c3c933a67387c97cdbbb5c7fa4dcd57debeab92845a38a4444830e288e518550", "ref_doc_id": "cb244484-4e89-46d9-843c-d9357ffe6105"}, "b752b76d-bef6-42e9-868f-2e710bde9440": {"doc_hash": "43ae52504862d55b7d3e77f5f7134da86ac401f4b65b460401257be61b02cbcd", "ref_doc_id": "a91fe56b-a7a9-48b3-9dda-24383d6d1ff2"}, "0f04cef2-36aa-4512-8bd9-c0e50fb6e61e": {"doc_hash": "9553833f18559f2abc0c10eab1233914d2e9246800427cce90884e4d63f70a5a", "ref_doc_id": "2ead6c59-9aad-4c3c-853b-5977a71b2f6a"}, "b885727c-93c2-4f33-8965-bcc04203a70b": {"doc_hash": "0a46739da1b5140e43dfb3ce0b44239d4c690b3202068d5117ca73869db4e4e1", "ref_doc_id": "16e4b73a-02cb-45de-b908-9428488e8b02"}, "4a1d3429-2942-4e2d-bf9f-2e9aa99466c8": {"doc_hash": "4f2ec66ae70b782299eccd15158c9f43a6df4d6b6c7f34cce21953dc0d69ec46", "ref_doc_id": "2b5e44a5-e136-438a-b707-13c9677b9d52"}, "ce5c7886-f7af-4317-9a1c-e721c3a0caed": {"doc_hash": "6b0e5a904da95e7d01bc1fc161fcab3689cfab499b654e1d5ea52f8dad79053c", "ref_doc_id": "3000855a-a40f-42dd-b782-0e2f718f848d"}, "52783869-7f39-449e-8fa2-c04c6872afe7": {"doc_hash": "80f701e555788677bc669fa8f4c61723ce938f23327f716b4152220490a7160f", "ref_doc_id": "38742736-69f8-4178-929a-21d9f6fcb7c7"}, "bb713ab9-c16e-4cd7-a3a8-6b17312384df": {"doc_hash": "919dd89eab7038b5a3a5f2329ab46c4d1d430b98193c40460f804c03bd11edf8", "ref_doc_id": "42e697c4-af99-42a5-be6f-8b715043aeaf"}, "2a9a17ed-518b-4da6-81ca-2f1b3d71b462": {"doc_hash": "3aab5143b17a9b288501f573ab2f46e011c010457374aac529a3c19d4bf01b85", "ref_doc_id": "2c34cf53-539e-4fe6-bc12-8744b8ab45b1"}, "1d2e1faf-6ed8-49eb-8db6-2d7d3cb5b03f": {"doc_hash": "0c891520d8fbed6288c7953c4708ba891dcbcc0fb52340bf8f0da7240d20d926", "ref_doc_id": "4ba56652-f7fa-436e-bac1-929f49062fb6"}, "08f62a96-ebae-4d52-b107-cb1d1c30dba8": {"doc_hash": "22292e08b62d693334de1ce910247bf2ff52b2d1baf4360d8b87cf4d53fa7c0c", "ref_doc_id": "8291bf50-0ba7-49a6-8c13-6c06e12e8af6"}, "0780a937-ec8c-4ed9-8486-7bb7155b732c": {"doc_hash": "5f22ac09d293e01ff5557ad26ca09413fc32318b7da30764a11fd651c19f3c4a", "ref_doc_id": "df2bc6c0-5550-4a18-98d1-ca0f8acd5c09"}, "ddf2788b-bea6-4ace-bcfa-ddeddff86082": {"doc_hash": "ecd9306c45d6b2913f0137d5f62bb7cabf28f76b1d7f270d3b41baefd388ca0a", "ref_doc_id": "47485790-4ffe-4c22-af58-6e528b760ead"}, "68321a47-9b7a-4ef3-a774-6b5ee5595047": {"doc_hash": "3352954e2928462e21782a396e3502b9db4c447aa0d94f1739bf1a01a7941bd4", "ref_doc_id": "11215cf3-b9d2-4731-b9fe-7f036853eb80"}, "d9d75b1a-2b3a-4167-8533-0b043c9d95b0": {"doc_hash": "59a0fc56d85812992712321fb1bd12ebbc7730254433a30e6dbc1455cb6bc529", "ref_doc_id": "c8e817ba-e86b-48d4-ac4e-a1aeffbd1d71"}, "5cb1281a-9a85-493f-bdd6-37fd180c1e9f": {"doc_hash": "907485a36c918b8da77c9cf5015ebbc4aa391bca0fef73d7d49d034939a21b12", "ref_doc_id": "b787ca16-3662-4fc8-be72-4eecca88405f"}, "dd163e78-b195-42d2-a50e-5bc7c06f99d8": {"doc_hash": "4d119c32f234b98d4f520a665facaab4007cb7e543f763a5cc006693a35d1216", "ref_doc_id": "7c28d313-2532-4c9f-8243-daf2347b7c11"}, "ab21cab0-2221-4684-9657-338b2519d8b6": {"doc_hash": "34dd1fcbcc457a3fe0b462e8539a1579a632871b02324b13649a8712112a8c4b", "ref_doc_id": "267bec77-a109-448a-9ce2-c9dab2a59cf0"}, "95390684-5560-4fc4-ad8c-8ec02dc575a1": {"doc_hash": "e8074d918756cb6dd157a8d581da35bec774890ec11720eb2bd119c7790f2bc7", "ref_doc_id": "a299c54e-0947-4772-a46f-d5d9a57a719a"}, "4bd82f63-bb4c-4e84-bf44-6a87939cf60b": {"doc_hash": "687ee93feb9ab64121223d24ae6425b3942aeb1c01e420a3b324ced90a94e69e", "ref_doc_id": "424a7c3f-4cbe-4b51-873f-cb880a91533f"}, "b78e44ce-8841-43b1-9356-95b875a27223": {"doc_hash": "cb7e9599a77db782e4b3c296d5ef55e0b89365373db6d3dc3bf7b2163f1fd8f4", "ref_doc_id": "a9644e0d-7967-4a02-aca2-837028f12e99"}, "9f41ad85-ff2f-4e67-b93d-383341400f98": {"doc_hash": "c632a3cfab4b948e861619d2e8c6a46ec1653fc81ae402125ab461890f6ff8b5", "ref_doc_id": "51214602-82da-427b-af0c-be8b40887e07"}, "e121fb3b-03ff-42f3-84c9-a5db85f241d8": {"doc_hash": "7c3e735d907bfcab4d13aeb74b5867021410e9485ddf8a9a8605083937543488", "ref_doc_id": "7c805a67-0a63-4054-9ca6-d9cef8ab4d91"}, "8f422671-7ff5-4473-acdf-c1a5300b06c4": {"doc_hash": "4823f4978780b0f69f3f0527eaa8d364273f591d726afaf3b4c8c9f4f6c7f19a", "ref_doc_id": "153f8743-c109-4227-8d0e-3cb67514c743"}, "2eae8a1f-7c00-4aff-8c35-1941ffa24651": {"doc_hash": "fd8b46b1674151bcd3095dd33fc3e9ac2bf00c294bd69c93830d7c271bf90d64", "ref_doc_id": "ce71b5d1-0b1b-40a9-a00a-26fba5fb8b4b"}, "ee6eeb25-c9b5-4a0a-ac6e-07b6973a45c4": {"doc_hash": "9eea84aab677304cb1fa646581dce8ca964fbd428b6f81d526f31e89ce15326b", "ref_doc_id": "4f88ba83-e254-4fe7-8620-46afd37b9966"}, "105adfb8-c69e-43ab-91ad-21a7dc27f40d": {"doc_hash": "9d396892b9dbd65fc7292bff850e94f7fcf3a426357bab043d09702f1ce201c1", "ref_doc_id": "85b8a2aa-3b2a-416f-8590-f7f3959d1cf8"}, "e5b35ed3-7cda-4f62-a004-03a375713c5f": {"doc_hash": "1077cfacf5e1682429ecb6b24dcbc49b0bab6e6b4eb144482676bed81c2430db", "ref_doc_id": "8176dbfb-831d-43b7-bddc-dd2a10529fd8"}, "520eb548-6559-433f-9948-6e73e1b43b30": {"doc_hash": "34b6e1d951e257d010e4cbea714021118b7aec3c683fa47729c9ff451b5a23fc", "ref_doc_id": "be5a3c9b-efc9-4b99-a413-3d2c72c0ce96"}, "00a0c6da-b58a-4d7e-ae2d-14e3ebdcd07e": {"doc_hash": "d3a3af1f3619a15502d5557ce5426a9d9c821fcddeb5d981745a21d76af842ed", "ref_doc_id": "9b69b97b-be75-40ee-a84d-807ffa98292b"}, "c33684d7-9377-4261-b531-3a085915a23e": {"doc_hash": "ebdd2aa49a3cdd0719ffbb73558c706e7be6f7a8ebe38f44b986dd38808fba5f", "ref_doc_id": "88c57ea9-bd30-4bef-8e83-b2b5a37ab8d8"}, "e63113c5-e2a9-498a-a0d6-d6bbb19f7875": {"doc_hash": "3f21562262110e8de4d7ae28a977d12835253bf1959658cdce59baeb6114abe3", "ref_doc_id": "be8fba07-60f5-4216-8222-bd8a3c56d0e9"}, "60087b53-0045-45fe-80a3-94536fc4e761": {"doc_hash": "3d5551b5cd75d33ee4d763fa525d27f2cc181e4447d703971271d898019421d7", "ref_doc_id": "e9697af5-4d69-4aff-a3f5-538429ef7550"}, "67d091fe-dc9b-46ae-b5ba-2df989b15fe4": {"doc_hash": "93b9bb06fdd9bd22c074875c841bbd60d1cb5cfceb28d3cc1b5537fb57444a4e", "ref_doc_id": "aafd4e9e-d5fa-43c2-9eca-b06a98e65060"}, "9c34ef9b-041e-4cb1-baa3-ce1ba0f59151": {"doc_hash": "b4d298338c0cc9b4d9a0c35e374d085249de4230d3267e18da48739337381dd7", "ref_doc_id": "4b87256a-30e7-44a9-91ff-f96bf683ae27"}, "20ab309f-ff31-4269-9fba-6d4909b01a86": {"doc_hash": "8365022676f8c001f89e44c0db0ff9cb88b97ab8b6c9640522bb087bcddadd1e", "ref_doc_id": "ed46322f-6583-4922-9e4a-01e75987ef46"}, "603bbae9-6235-40dd-84d0-f939c4110e04": {"doc_hash": "819fbbaec014e2ce7ca053f0fc62b1eb09abae9ac720535d0fd0a3455ababee6", "ref_doc_id": "dbde8267-f37d-4e57-a99b-5ef58957fcfd"}, "c0d5bbe9-09f1-4854-9c8d-7cf6ea109deb": {"doc_hash": "1dd0d79071606e002306cf18e5e98141a0c3eb7a2b74e4fd9a9d946667a96a11", "ref_doc_id": "dbde8267-f37d-4e57-a99b-5ef58957fcfd"}, "0b045def-dc24-4b65-86fb-555504f169f8": {"doc_hash": "fa1abbb201bbe557879503f557427bd23ea22633955ed4ce63ad4ee85105a0ae", "ref_doc_id": "cf3aec92-d71a-4240-afee-bcae965b15d0"}, "7a62ae3d-f907-4b3c-98a5-d8adea9cf3b1": {"doc_hash": "6dd41ea2d14a63bbabb2512d9623f697d7785db9b88b4788641d955fcfe4440b", "ref_doc_id": "764f70f0-b2b0-4ce2-9594-5cc7e4b35650"}, "4b7c982d-6e6f-4233-acf0-7a0e66280f28": {"doc_hash": "a35fb655af732752b4c2e575704bfda2717c9b85ddef8e0d05b5634c1b1abb16", "ref_doc_id": "c1e645a1-7aac-412a-b4c7-0d1e163e8716"}, "9c08bd1c-1a63-4776-a9f2-d5856aa28fcb": {"doc_hash": "809bce1c554a7eea989a9685fb70ed9e1154075967fd34cad4643581e01841f4", "ref_doc_id": "e29889e9-a394-44d3-b666-4cc65390941e"}, "e6c9566e-8751-4664-8782-d7b72ba64434": {"doc_hash": "cc3db62fef8ed762d34a42320df1cfcc099cfb2ca5adebda609d0d57dfb83379", "ref_doc_id": "d635aefd-96b4-49b6-8dfe-b7e606bb749f"}, "8c3a1e48-6d17-4190-b9ff-02cc831d3ed5": {"doc_hash": "b609a3d9b3ed52f3a97ad90d3d42c0b54a835c9f6289af740b5a43e6cee4d551", "ref_doc_id": "2cbd9874-e1d4-423b-992c-8b2db529cd7f"}, "5d101822-629a-4eb0-8767-d1a4449ce12f": {"doc_hash": "f9372bcd79152e549d8fa712faa997a17ec704f5ea7ed053ef92126b91eb6f82", "ref_doc_id": "188863b3-daac-4441-a607-fe109b0766dd"}, "aa31036c-f851-4ec4-ac18-ced0c21ded84": {"doc_hash": "116ab48de408c73e83b8fd61a88bda5301b62fd89923f1b0b2ae335e3d8dbe8a", "ref_doc_id": "bf955db7-1e7a-45a0-aef1-39cf22c1baea"}, "656ee8c3-a454-49f4-b21f-08c2f6aac575": {"doc_hash": "1cc637421f9257dd61cb9173800cd5653e250d6000da4df1ea2d7e163f45e8d3", "ref_doc_id": "3f8bbbc3-3ad3-42b7-bc4d-fd4f73f1e647"}, "b39e720b-d514-49c1-a485-eb73286fbb7c": {"doc_hash": "750ee11242e6d331fd704031631cd14f2f08c13755c6d9e0e0198f614c366813", "ref_doc_id": "a2b965a9-1a4f-4ee9-a1a5-21e15307ae2d"}, "b671dbb3-d1cb-4682-98b8-898df2326874": {"doc_hash": "2896b159e497e92496e4c0c785334293154081336ff573ea066702e52a094714", "ref_doc_id": "8b394317-0275-45f1-a6f5-ef641760a114"}, "1e5f1d46-3a11-4ff0-8151-b97cbf1ef3bf": {"doc_hash": "93d2127f3a5989415abc276bb772e2ee4da2d522515adc3667bfcef89c920344", "ref_doc_id": "6395cf6a-89c4-485e-8465-5c8f2253d8b7"}, "472660aa-0262-41e3-b4e3-4990581fa8d6": {"doc_hash": "8cffa1df1511939a786af8a06138acc6fb4f9f35f8edb9639b0aa9d8f26ede41", "ref_doc_id": "434751c2-f69d-4511-99ae-c9a2ffd49eb2"}, "e5811d56-5146-4e71-bbfd-decd52533389": {"doc_hash": "e0bc8b7da9e9d1270351ce7213fd6ba4e24e6eb057814c0a74f31c614ac3e4f3", "ref_doc_id": "da05a50d-7e02-4334-b061-2bdfe03432b9"}, "9b732d77-f709-42c0-a81b-3f2538c64755": {"doc_hash": "b00d91125b09056a5f6d98a153ebb1ee349c000abe758b7ff58a02c4640eeb74", "ref_doc_id": "bd801881-de1c-4f9f-8585-8503650f0e76"}, "64801aeb-2037-4cc7-b06d-b822589788d2": {"doc_hash": "b0eb3f79c47821d1544c882acacb763e5cb7b8b3b55a1d0afcbe06979286eef2", "ref_doc_id": "bd801881-de1c-4f9f-8585-8503650f0e76"}, "7db79abb-0693-4028-a608-3deae8cf3ed6": {"doc_hash": "354e87389a1f1a9107bf1bbe562f05387c467bdd8b112144ee30ed08b02888f7", "ref_doc_id": "6c5a2a59-6761-45d6-9e72-91a950acd176"}, "4d77d70d-92a4-41d5-9174-5172e5d3eee1": {"doc_hash": "e7bf0189fc60eda41fef6ec5dcbf2aef49e165caaeea39b5b6adb40344e28287", "ref_doc_id": "c85bc06f-c06a-4756-9e85-6ae6bd17e2a7"}, "c11f807e-f02c-4039-87b5-b7912935b536": {"doc_hash": "80fef9ad8f5d7deb92d1d272cbbd85739be1e404f424472d6de5045bf86340d7", "ref_doc_id": "c85bc06f-c06a-4756-9e85-6ae6bd17e2a7"}, "ac79371d-1e1d-4af1-842c-d0843ae4f7f5": {"doc_hash": "173da62f5abec82f7a1660c22efd580447e9a0999fd306f74f72ff26eb6df2b7", "ref_doc_id": "dee01888-6c66-454e-ac69-9613290d207f"}, "35b9af45-a378-4aaf-aa1e-c19782fb6315": {"doc_hash": "c3b389310493f58bce65da6344514f2f62cacbf5ed0584a2d87e2793494647dc", "ref_doc_id": "043e995c-ebc6-41aa-963e-b051cfb39023"}, "9fadcd69-b57d-49c1-aa39-8b52110a6b72": {"doc_hash": "d9b9b2c559f05b15d823a2103ed5c46dbe76ca9cf0b293e5adfdcca14be61675", "ref_doc_id": "9c88e89d-1572-498c-a162-4b2399be03e4"}, "72cf49ee-9743-46cd-afc1-11315b197351": {"doc_hash": "a712f4193f69a6567e12facddc9a47fc73759fa1c71dd9f38dd1a32cb47523c1", "ref_doc_id": "18e53e85-30c9-4f79-a6ac-cbb67c90bb20"}, "36b1e70d-2fec-476d-8ca0-aa589ecd16c5": {"doc_hash": "b6eef9bd818859b3affbaed5a4f7aa0663293fde7b9745b756c6669c2c5ea67a", "ref_doc_id": "bf3a0ad1-69a9-4a77-a836-004f10a7feb1"}, "7283e245-0259-4ec4-9fea-80cf06e8cdc3": {"doc_hash": "4adbcfc41a7e7f64d0b9290a52696444660a3103c6f693d527d1d10981a12b1d", "ref_doc_id": "f51033c6-0578-46b9-80c0-b8b9a7736f85"}, "95e79758-4a3b-47a1-abaf-a1033d83c3e9": {"doc_hash": "74aa0260ba29f98b2e241fe581d19141657cf21aac2720d63131e1eeda864df2", "ref_doc_id": "f7be9653-0593-489d-abc6-4164e0769f78"}, "aebfe93c-afb2-4d1f-bc1a-f0236e83d72a": {"doc_hash": "f819a38c2fcab070d347bb4e461116d176cfda6c38a9042cb4cc446f1fa710b9", "ref_doc_id": "2b49817c-3133-4373-a1c8-9d4f5f8d908f"}, "c6ed486b-ecb4-48db-9d74-50f0a167625b": {"doc_hash": "a501221594e64a0df68f9f80e85ce95ec7b0c1f90291ea389f153682c5e8223c", "ref_doc_id": "b56ba533-2859-4fc1-8842-0876c945c994"}, "8f51ee76-46d4-4678-af84-8ed3a4b10d74": {"doc_hash": "91b16744a6e4cd9d837aee1c11bbdcd88ee862244db06654ed1c2cd1d49632b8", "ref_doc_id": "c70ebe84-e40e-4547-b42f-69e1036482e8"}, "c30ea178-e97c-4de9-900a-f8b318d7b678": {"doc_hash": "f347b99d66c6792c4974d7c6909c8f1f6ccd8ef12af2349da1990e9172925853", "ref_doc_id": "c1921b58-16da-468f-afbd-15aa161c8103"}, "1f772bae-4358-47cd-9437-784c5f917b75": {"doc_hash": "63b066061120b52f232b50632ba7ea5e835c0ac139461ba1e0760653c0334787", "ref_doc_id": "711bd499-e9af-47bf-af3d-d868d7ae49bb"}, "2e026d2e-9a10-4d60-a3cd-e0873f78b389": {"doc_hash": "76f4b030f405295c125c7e820901edf1996d71d10497dff06d9fe8dbdb913653", "ref_doc_id": "a691f552-13c7-470e-91b4-835659309171"}, "4af72307-fad6-45ea-a56e-c9243718a6da": {"doc_hash": "187c0485612334a7d07d12c808eaf7835eb08d49352da6dbb86092c989fc110b", "ref_doc_id": "24ae88e3-bf25-43d3-868d-11372ed0bb7e"}, "7df5a440-856e-405e-99bd-e3943da3ce15": {"doc_hash": "aa488a83c92f57b2f40ca86346fd7a8ab1e1a1ce5f215ba7b8b337a0687a6d8e", "ref_doc_id": "5ea7bc45-6623-4d87-b6ae-d410bf58478f"}, "d0b93bdc-e656-4ea0-8ee8-abc701f10e68": {"doc_hash": "76a11f432a66c9deec7296fa1b020443126efa8cfa48d32b008bbf5b4eb1d95d", "ref_doc_id": "84dc5fb7-ecba-479b-9c9d-b186d8d29622"}, "e32bed8e-2fdd-481d-8e4f-16d5f79d0974": {"doc_hash": "4c912603d6663a1b1491fe737c26e441a01c4790503d2a0e222668fc03c533e6", "ref_doc_id": "9791e908-04ad-48b0-a9ea-7d90973d0f83"}, "b1bf9c5d-d6cc-4719-9d25-4f25b711a3ac": {"doc_hash": "2930f5fc75f50278452a50760ea57935ae66fece65d58a1e17dc7f5397427249", "ref_doc_id": "2d3f32ca-e39a-4bbb-b52c-b489c4653ef7"}, "21280591-bb69-497c-a6f2-34ae2d104fc1": {"doc_hash": "21027e6056488ec67dbc7b21d9e3ca642c268e0bc1fb0e644423a7fbe76f254e", "ref_doc_id": "3474da39-a95e-4fd0-a2ab-6b7569485ea6"}, "b615f598-7889-4626-a3eb-1e9d4a0375b1": {"doc_hash": "dab2cffeb60706d6d1fb6fa574bae05dc14d713dccacc47e1ab2ef45c084df6f", "ref_doc_id": "cf62ba83-137b-42a0-8ae8-6e29cae4d7d5"}, "db5f2b93-137c-44ce-ba27-931f3ac5a1b5": {"doc_hash": "dcd7e5086cbfb383a5ecb5200c90dcddd18a862f7a60a41bca6278adde45676c", "ref_doc_id": "5ca994dd-546a-4d4e-a130-976f64eaa4ee"}, "ffffc70c-5201-4fa4-a22d-ecd5005bb512": {"doc_hash": "c92f73f6caf6cbb584ddf749a99a84eccaf6907ccfc9145ae4ecf939d79a28e3", "ref_doc_id": "030089ec-2105-4886-ae34-1daa00b518b4"}, "0d472ffc-d582-42c7-9a0e-5d727d4b5807": {"doc_hash": "770abc58c1b1c4998b8e1c753c93f5668c82d71bf55a6595a7e99706a56dc421", "ref_doc_id": "fe686b58-d837-432e-b3cd-a6f918c6893f"}, "351c368f-bd61-4372-a13e-7b6fbc1f17b4": {"doc_hash": "55b5b9abd705f24f076196e40cb43ba7d3b7a402f4d00ea96e6f1e11a963323e", "ref_doc_id": "84472535-029a-4fe2-85b3-6aff6c5030d6"}, "9d686143-7b0a-4530-b6f9-856b4f638512": {"doc_hash": "8442a57c109682cbae8109203337f76200b0eabc162b109377a63c09bd134a0c", "ref_doc_id": "7eaf12b8-d3f1-4122-a030-afb3c65c122c"}, "f787b0c8-5ab5-44af-b1ab-63192b1b2591": {"doc_hash": "a4404f6655bffe25079718c7ad39068734cffa55359744466d00e36017f863dc", "ref_doc_id": "3bac9c96-5aa6-4031-b055-e7d034ca1ec2"}, "425309cf-d435-40f8-95dc-b6c98b585664": {"doc_hash": "3f02c7d6873a128335bd792433593654d400961fbdb08c1ee639ec541bd1421e", "ref_doc_id": "0bed854b-0d52-4e3d-a9cc-215a62cf62b8"}, "11ec3056-c3a7-4d3e-8b58-44d44e6d8f5b": {"doc_hash": "6f8c095b5670a28518ade2871a451b63308bc641cc302c96f9e642d56f19726a", "ref_doc_id": "f776f761-4dab-423a-9127-59af5ac54afb"}, "18bdf931-4694-4cf5-ac01-ed68f0e9d00f": {"doc_hash": "dc89deea8dd47e6d61c45aea838e37f9f75c0479ba0322d18aedf943ae2291e4", "ref_doc_id": "85ef6810-78f9-4ee3-803b-1832f959b972"}, "e0b0a6ae-4ac9-44d4-a9ad-49953cf0c4a8": {"doc_hash": "ef70f5e17449113441f7b8b286ae9911266bcabe9eac30de1c9818a057087546", "ref_doc_id": "cec06a64-d579-43f5-8eeb-7d566eefbb09"}, "dcb30c19-f285-40f4-b2ee-9bffa1ccd1ef": {"doc_hash": "da886a912f5ebb8586cbc1dd8672dd18a191f4db790281d2b1030aa1428eb7a1", "ref_doc_id": "0432f765-52ac-46b7-8a64-46993358215e"}, "0e36d0f2-76f4-4738-9a6f-b19365d0c4d1": {"doc_hash": "e55127825a53217afdb668ea870a9526179dfcff2d5c8316895fa620f24e3e88", "ref_doc_id": "0d3372c6-1931-4ee3-bad5-ef141ce3abd2"}, "065d99c3-808e-4a13-a2e5-1b237b08d3de": {"doc_hash": "00b724ecae40c65efd5afce777a30d46770bbde3f167e1e713073ca399d145ab", "ref_doc_id": "95426591-5709-442f-8cd2-8abf65a52984"}, "a30f9ca9-727e-46bd-a64c-1c524f941920": {"doc_hash": "c47d064788f40d346c14ef5ae171ebfb0eabefc7d41e417991cbc51570471aa9", "ref_doc_id": "6065b453-9eba-4f13-916f-941a767417b9"}, "51303d9a-7c64-4940-9d2e-fe8656f5aa01": {"doc_hash": "e712048932ec9754e82d434558d5cb32db1e348a784e57487631da5813515e61", "ref_doc_id": "b356567d-d8f2-4cfd-b2dd-9c9113b3039f"}, "b3527a61-ed6e-4de8-8cbd-9cd5b4f07700": {"doc_hash": "d295defebef381773e24283dc5a8fac5ef1417bff2e975dd8e9fcafab8995d0d", "ref_doc_id": "11703f22-745c-463d-b53e-eb05835c97bc"}, "0d3dadfa-6703-41e7-971b-57a5b2ef1aa5": {"doc_hash": "2346899a9ab4adbcfd2b5d6b2863662befcd5cbdee19f087fe6fd490220b8e29", "ref_doc_id": "39c69c99-8687-4d50-a2a8-b4a5d3b50782"}, "e8960da0-1db9-416d-9e30-77ca35ff721f": {"doc_hash": "498769fae0bccb292f62dceaa71180c6e2f090c332309bffc08118964d49becd", "ref_doc_id": "b109ff1f-c244-414c-b811-ceeec2cccaad"}, "fe30fa36-116a-46af-8217-5364e41a75d9": {"doc_hash": "1e4aafeec66cf2e7ebef3bbede47b1788d3ff20967d7d0355c24542a30bd860d", "ref_doc_id": "50b099ff-a2fb-4ee8-b292-12c29a73ef64"}, "443af01c-f4ea-47af-8c78-0faab5af3436": {"doc_hash": "5dd5e49b6d38ddac021c650519652b5258308ebbb60c8e73d4003ace7e1937d3", "ref_doc_id": "b5a44a25-85bf-411b-8bf2-5e06cac2476e"}, "89a6f371-31ee-46f2-92a5-7bb768ce96b2": {"doc_hash": "230ecdf2d2cbfa68313e2205cee44f686e76b9d98260a34620b34cbc8182107b", "ref_doc_id": "b86d7efe-4a9e-4ee8-a6fa-b700dd73f112"}, "0191d8bd-66a8-4914-af7d-8736dce04c78": {"doc_hash": "608c05552435c36963724aa84c548464b7e7acbe8f5a54773175f585563b4991", "ref_doc_id": "49cb956b-4ce2-4d7b-a872-8b8f03de9b63"}, "71b1f9ca-8959-46e8-8ebf-331f0102c9cb": {"doc_hash": "2c9a434ea8fc6b9cb707a6b7c277ab1fb70fd42372a44d38df0268fbc6c69bdd", "ref_doc_id": "0e6cc344-e889-44d2-8e98-166083dd04ab"}, "0c99654e-5608-4c86-b557-56483cc32623": {"doc_hash": "eadbc5acb38c6e4c8e39c87a6cbd00410e24b575d75d40e0f15fe01e126fb800", "ref_doc_id": "c61b74a1-7b19-490b-ab97-65784629777e"}, "f513bbed-6278-49a2-a71a-24b3505019ea": {"doc_hash": "fef3fee44136ed6deb083824a9b793cb70a3715ef75d96af388bfb88ee57442e", "ref_doc_id": "7839cb72-b817-44da-b6b0-15003fd7aead"}, "6b7ff5ff-9e1a-47bf-9e8a-9971c362adfa": {"doc_hash": "6d3ba003041d9ae8d346862ce0c1d43571c205f1785ba99ab1901131705947fb", "ref_doc_id": "5db7d381-2daa-4348-ab88-dd1911f16400"}, "412732c4-5b34-424a-9ded-76a11e37b52d": {"doc_hash": "abc5cf4ebe9dd8c4a569cb52eaf26f2558e31f9b905ab9332a7382566a569b1b", "ref_doc_id": "c031c70d-2e20-48ef-bbf2-03e0041c4461"}, "b71f7195-5add-455f-b868-adf835bbd3ad": {"doc_hash": "35540881d7ea99313b5f70894b4ebbfdc9eaf200933e8e886096b2001e7bf4ad", "ref_doc_id": "03ba42ed-82a1-4213-b845-2f336a9ec302"}, "ee64bb12-3e52-413b-a725-f2056f91b46b": {"doc_hash": "c37a7274b6068689c784674827d88fe6da1334e90c3b8ec5e860dee32f0f9934", "ref_doc_id": "51c66e7f-b309-4cfb-9c21-3335b3889c8c"}, "64a9b03a-71c8-410e-a854-4e2d32ab2337": {"doc_hash": "70c6d4b544f2b5765829e8480d7a34afbb1ca5de0ee9843e889130d0756b0ded", "ref_doc_id": "588f1e82-afec-4918-b42a-2ff948c3cc10"}, "e391b914-9ede-4c6d-84e5-bcf382a9c8d7": {"doc_hash": "89d307586269d866c626b07876fa4e62b157f401114872ef33b6dae2fac17fd7", "ref_doc_id": "8872df19-acec-436c-b456-b8d0c5d4b250"}, "d7445ab1-1718-4f75-b8ea-3ae091754f5c": {"doc_hash": "326bf693c8ad9d0f86c6d7fe5a5f5eaf1981d7147a26e9b05c914544395dfd50", "ref_doc_id": "27cd9a90-cb1d-4ee4-9bb4-ef3c74eacb17"}, "e509daa5-5acc-492d-a748-12d3957141f3": {"doc_hash": "dc651d04d586bac40135ef84a002ed802163e6d35371fd163dd923cacc5cbeb5", "ref_doc_id": "ce9258da-39c2-4b61-83f2-89cc5f8ca5e9"}, "b1128b08-8bd9-4477-a44c-a0d0c7e67572": {"doc_hash": "196707a05643236700bf188754858d333da6ee290d667d1fdeb8a4c786230fa5", "ref_doc_id": "e309113d-9eb5-4b40-8f10-c43fe9c54ef2"}, "54a5e2a2-ba94-47fb-9b3b-e9ce63feb942": {"doc_hash": "d80fbcb8bbe4ef4434bed6a999d73df9f677cba85db53646a07ce61e9adc91dd", "ref_doc_id": "602464f7-80eb-4e19-8327-f59b328d6d78"}, "9c9f43c1-9cd6-4e28-992a-8cbc3180f8d1": {"doc_hash": "15d063f1f424cc96d9da7b0ca07080e4de7865a9d519edf45297eefd4bc7c298", "ref_doc_id": "991697e3-7fe4-4267-9fa4-feab4a729ea9"}, "f7fad0b9-2844-4cb9-bd52-c5d9f147c21a": {"doc_hash": "53113eec8113ff2d1582c851b5d7579711705bb7c5304c8f8c7d329ea9c1c8f6", "ref_doc_id": "ecff123b-7552-4248-93e0-8c289547f96d"}, "14869d58-fc89-468b-ac00-95185073a0f9": {"doc_hash": "44e0b50ec4fb8c317d37c392d7baa115e48183ecc246341d70f57ec6aa98a7d7", "ref_doc_id": "aaa0a4d8-6001-4888-b687-1dd9816c32d6"}, "0dd6b08d-eb79-4b64-9372-652874309ea7": {"doc_hash": "2bb10dc1f5398749342d34a450e2eced0d005a8788f4c37dd2c62bcadbfe3dbe", "ref_doc_id": "90d22edc-2f3c-4d2d-8950-77d96e519f7a"}, "c190d23c-b1d0-451d-80b7-4d2573b2d88e": {"doc_hash": "fe31bcaf7670cdfd4ed7855b4b5010d09de662ffd069fedf13b9272133071636", "ref_doc_id": "3fbcd913-a890-4651-b833-e753bca1427b"}, "01bd7402-96a4-4164-870b-5c9dd0f225ef": {"doc_hash": "a16a43c2261b6c62858e01f9f6407640a7599a5ffef5f60a0db029e3a5d63940", "ref_doc_id": "249417f9-7762-49e9-aec1-a4a04edb652b"}, "29fd5fde-b463-4c33-b288-1189ec085985": {"doc_hash": "322cd297328a52a4f50d459e245bd0e3ba1143f7b013ac2c8a25b39998e2797f", "ref_doc_id": "2ee32b91-20d2-46d9-b8ae-688f57246466"}, "a2b4e785-9f94-403d-a3d8-01ec399ac9eb": {"doc_hash": "9b77fbde06a9ccb7121f39135947653a8f3c5a53630fba597e9cfbfc196c8190", "ref_doc_id": "d5557427-3952-46cd-9d61-f5600faad9a4"}, "0b15b222-8000-406a-9ce9-bca780217814": {"doc_hash": "196d0239d75c0214db4dab42d1f19cfffd43110f64503a7df06ad96d2a744d19", "ref_doc_id": "beec423b-6d55-4e96-bcb3-b6d24cd12f22"}, "dbc9e6e9-8b61-4b17-868d-55f91d1b332f": {"doc_hash": "5593952bc124c4d7a9c55dd5e9164f294be5b9310b542737b53605785a39dac9", "ref_doc_id": "b3ddb4ab-7699-4279-a54e-ef343a82216b"}, "5ec2393c-4a87-43ba-8734-e20e4faae9f2": {"doc_hash": "80aa9c69ba6ece3225a013722c57023a4f49da80f3c340cf0430b22ef9ecf735", "ref_doc_id": "371143be-00ad-41e1-9555-6c54db6b5d70"}, "f1411d93-4bd6-4ad2-a665-58eab8c08e52": {"doc_hash": "0ce47b8be5fe2d72aff175da6cae5af3e06a4b2252b9c3e7a96e5b29cfd90732", "ref_doc_id": "a1f5bd7d-8c87-40c0-a736-1cbb566acfcf"}, "e28091a5-55c2-4bf6-858b-5d9168151a8b": {"doc_hash": "325cc4d595bc3ea029945559dee9bba5149915c73fcd1e1e87e5210e663304b8", "ref_doc_id": "75f6d77b-b5fb-4a98-9181-63c2c8d6a0a0"}, "0f690db0-6e9f-4ac3-bf2a-66ba64e2745c": {"doc_hash": "7eee99941d2e635bad1d176ebd73e1cd93347aea91ca65f28c5cd132ebf68ec3", "ref_doc_id": "cdc66615-e8f7-454d-adde-c492ab98fec4"}, "020beea1-5122-43c3-8c61-b94a29d840ea": {"doc_hash": "6d6348a203698730e869a1a4e768b7941a4df4b90e49926f880c5d65cd502b51", "ref_doc_id": "70d270e3-7220-4e0f-94c9-a7e8051cfc2d"}, "1f15aa12-9adb-46c0-9097-d722f80adf07": {"doc_hash": "ff130dc4172cda8bea9c2f4ff70513f298554168c8f870ce0f0a8ae69bc2c9c2", "ref_doc_id": "9faf1f0c-f20e-45c4-a5fb-f00d2b379b39"}, "67cd1bd7-9233-4460-9e78-7ee08dc10107": {"doc_hash": "98b12ac0140b8bfca41936bffcca495ba17929712e5b54cef85dc587e368d42b", "ref_doc_id": "b8bbc4af-4a25-4e78-913b-68a40eb2da3f"}, "8432446f-5322-45ee-854c-36efaa264b3c": {"doc_hash": "df958ea9c39c01e1e8a4843eb07735de6c19df0a97ae2da763bc0909a17c019e", "ref_doc_id": "b8bbc4af-4a25-4e78-913b-68a40eb2da3f"}, "05b8bcb2-bb0e-42cf-8d1f-108223220ffb": {"doc_hash": "dd467cdac7fc625ccf33dd72288a91f1f640233e3a90b46bf866fe225521232c", "ref_doc_id": "fda93fcd-fd9b-4570-b8c8-4b013c4ffbf7"}, "1ec2911f-0c0a-488c-b41a-61c8310d01a8": {"doc_hash": "4725d77b8b515f6ee551f6bc9fd595e08bbf608d1865de80dc5ff676b64da693", "ref_doc_id": "e333f705-a029-434e-b76f-96a180a3ec34"}, "68a55943-a7b8-4f96-9cd4-97fe6b5d6f60": {"doc_hash": "ca37e2b893d6fb28dbacccc36990aca39279bae641b4a13836eca0bb943b05ae", "ref_doc_id": "e4bf679b-758e-4449-9934-9decdcd64c53"}, "2e345e3e-b3bd-40c6-97e0-9856bed35db2": {"doc_hash": "d6b9f30b7b7474315582af24d3264a057c7f82d42cd3fb332302b92738bcb550", "ref_doc_id": "b557cdc2-8574-43f1-a84b-ceae39550074"}, "984f68a1-5c59-4ab0-955e-1f4ce72d931f": {"doc_hash": "d1a786bef4a9f86a4c677c83fcb1963f7cf7facea4da072c3104acf360ef7775", "ref_doc_id": "475a5953-2fbc-485d-9a76-a275325c2dd3"}, "d12226bf-99f0-4c1b-a956-1f2e0a4e606c": {"doc_hash": "443b3b453aa660e7ad6bde477d804a438f20aa6d0d6728df36e6aac4f8dbfb60", "ref_doc_id": "eeabf41f-7fa0-4681-ba2a-3676bd7f9385"}, "4c861fe8-96e3-480c-bbba-85e31eb13593": {"doc_hash": "52b62d1eb1b72a3dd41c2f0152ce59c8aad4bb62c1d1713122089213c3dca4a9", "ref_doc_id": "eeabf41f-7fa0-4681-ba2a-3676bd7f9385"}, "5c3af813-c686-49f4-bea0-a67affde60f8": {"doc_hash": "6cd039d78a6a2e8fd67a7de50b33c47b7872e8fddd80c0d900bd0b6bfcb953b9", "ref_doc_id": "d2f2c5a3-3ef3-4208-9c97-8b6f48180a05"}, "cbcb7c17-f72b-408a-8688-5ddabe10e550": {"doc_hash": "de5fcc0f96aab71a651436374f4c585db9b7cd9d73ef838bf52df57489a016c1", "ref_doc_id": "7773128b-4279-4bd5-8144-65c100fd4481"}, "d4d958a7-9457-413a-9fd3-636f479b6646": {"doc_hash": "84cb7d4c97d8f5a95e39493b496f7ef0083776db84df80856ba094b17abee7de", "ref_doc_id": "0b53fdd7-cf28-4b6d-9a44-65a9f35ec628"}, "f7ce98de-95e4-4e73-8b1b-3feb8562c55c": {"doc_hash": "7a45d7bb8ea39d1e4a08af579a626ecbb6cbbe4413a92c32cf351410cf51d1c9", "ref_doc_id": "8a9e960d-a1d1-4c7e-b7af-2d9cd6a85549"}, "bb76bc25-e83e-4a8c-80ee-e1988629b130": {"doc_hash": "38314fff6703450545f6b16203fd7540c0ff092f7d6c85e630db4df68e13af8b", "ref_doc_id": "b9708d7b-2cbb-4ce0-aff5-31fe4187ccbd"}, "d0d0bbe7-4717-4156-876b-1b542b2a70ca": {"doc_hash": "1d5a55732af0efc221291e860cbb5c7f7e3cf66164a0ba8b336cc46651157f7a", "ref_doc_id": "406a13f3-24ae-4bef-b893-d053451e7b5a"}, "96a53220-c096-4b50-95fe-93dcae10c664": {"doc_hash": "4141047bc33e13ba94b16a5aa73f003fb059e15c8de8ddc5ef070b86c7e17d02", "ref_doc_id": "7cf12f6b-4a01-4d4c-a1ec-a4527afd1d5b"}, "5b2a49cf-65d9-42f6-8f7e-4ecb68364a03": {"doc_hash": "2cb0c39c36784af15de7974fea9ee7076fa84e311e5757480ced38533fb3a59a", "ref_doc_id": "4f03000c-d65f-41f7-96a0-619d946bedd3"}, "7706b2ab-2b76-4918-82f2-27a4bec76622": {"doc_hash": "687ce7776b5586a3b9e4d2ae0ea0b7e4a1cb8318b4c23225e845bf0c63e60291", "ref_doc_id": "784be029-23c4-47e2-bdf8-5090491c901e"}, "3895357c-aa77-4eca-a240-ebe16c6e2179": {"doc_hash": "a12f14b3eec9da55318b92ba54a332015404bacad94e54a69581d9b8ef5c6691", "ref_doc_id": "9a8c1c69-39cb-4151-882f-cbb0291d5719"}, "25ad30e8-57d2-4820-bdc3-fed766c18f44": {"doc_hash": "34cfa518d7d366884cae69aa82dd2c7f3ae6e12b8c875dfed50c43f7ca7a9d92", "ref_doc_id": "64175e74-07a9-49a0-8e56-3aa026a55bb2"}, "d75b9c67-0990-4fa8-8a7f-c6e95606572d": {"doc_hash": "1af34d267d062b6e65a720e7313f830e1ef13180ff824754533277b4e4e30352", "ref_doc_id": "beec1148-fd1f-46bb-a681-21bb55fea26c"}, "6bd2f278-aee8-4434-bd31-4fd92c885c72": {"doc_hash": "2154c8b0c1e956971c62cff82fc529295cd44a59b10455051213f73ad9d72446", "ref_doc_id": "48bae3a8-01e6-4887-abb1-ae54676b3cf6"}, "fb9456ab-509b-4178-b44b-7b0535a777f2": {"doc_hash": "6311374f6a7cf42d97fc5555222597a52d56e2ad51c0f49de8f6979dd3003f26", "ref_doc_id": "48bae3a8-01e6-4887-abb1-ae54676b3cf6"}, "c8008c2b-d093-4d35-99a6-58770f39a0aa": {"doc_hash": "a7715fb239d0519578d571dd9e8f536b601f8f2d25370719946b9ab0ce83e067", "ref_doc_id": "246062dc-ef95-400b-abac-45eb0bf6b60d"}, "a9088e13-8f94-4308-adb5-df414805c66f": {"doc_hash": "023971e0d5523038d75684dc09546b33ad9e43ba919161226f4dfd2b2e0589ac", "ref_doc_id": "a19c1116-e34d-4ae1-9ef6-d954e12bc1b3"}, "fdec6584-db17-495d-8060-e4a074ac7a32": {"doc_hash": "262cd9c694fbc8f725db7b8f91591c0d34bab6cdb268e251d07de8a1b4416e0e", "ref_doc_id": "6ce8bdbb-b31c-405b-a012-6c4c5db48cad"}, "6985f50c-b4b0-477a-aabc-18ef484e5059": {"doc_hash": "2f49e50546110dbcfe1ddfe0de44f805927e9b022e28926af5024da4b715984e", "ref_doc_id": "8e5eab7f-1012-4e31-a6c0-4f897ce5eee2"}, "e362f91a-0882-40d9-8cd8-ece14fed4064": {"doc_hash": "a054df4d136d7842a9fde534c69d80dd830c0cb80157cd76550443894a6898ee", "ref_doc_id": "2f5cbeb6-935c-4c7e-a439-edeef28d34c3"}, "cfd71b81-5f9d-48cf-86f1-99a2b519efb8": {"doc_hash": "fe1e4e43a8682f5ee6f6830a2f0cb035bec50ad8c4fc76bc97fb82e27bc3c879", "ref_doc_id": "9ee5bb24-c120-43a3-9528-a452917cf703"}, "66b9e61c-d156-4042-824b-83324d37f50e": {"doc_hash": "e78abeeacad7315897a5bd5a004047aeb07723ba33d076d994712defbc7f37b3", "ref_doc_id": "9ee5bb24-c120-43a3-9528-a452917cf703"}, "1e5966ae-30b1-4bb5-8e19-3413171d49e1": {"doc_hash": "5049e0e8ea1e113c7bdeea57bd329bb915f39d86f68906af7bc357b8de5f9863", "ref_doc_id": "51605275-0f9b-4bc1-b9af-171f8522911f"}, "69813ee1-f20c-4843-8c2a-def45907f5e9": {"doc_hash": "f2e38b800582279b18e4417241ba91be6dbde4aeed2b288815db1c16c6baf99b", "ref_doc_id": "124870af-5fb8-4ebe-b88c-db7bbd87dab2"}, "fd71d456-d9c7-4392-87e7-490b559d965b": {"doc_hash": "335c9847007a529108981a80387d11597a9ac2768aff29ee82bc630c44def549", "ref_doc_id": "359bcd3f-673d-4481-9332-631ed382ddae"}, "179b325d-5bab-459e-a0fa-d4515753018c": {"doc_hash": "1e50e048ded0a3b748054b71f88ddeccad5f487f94f9e8892d3a4183a9458197", "ref_doc_id": "6644cc7b-7f6c-49f8-a567-a9032fc20b74"}, "55892b6f-3b03-4d86-aa18-979028e09910": {"doc_hash": "61ba0fe5d4d7cd5c0f00a88c6515d342d0c5e1950b6e649e51cb226d3c72afb3", "ref_doc_id": "6644cc7b-7f6c-49f8-a567-a9032fc20b74"}, "3abd310c-0878-4a8f-896a-7da87c49c6e5": {"doc_hash": "836568ac48996398018d7abc3e1ce07d8b4074fa88241c666e7f85b4b8067bf6", "ref_doc_id": "1c8a088f-907f-4cd3-a9da-7279e4125309"}, "5c96a450-ef7d-4096-b67f-5bd8e80edc95": {"doc_hash": "677326068a03a9285af8351ce72440728615800b6ad4e0246533d36d96d58492", "ref_doc_id": "a8962609-5222-4c18-b050-d24755f4c61f"}, "163ba4dd-0169-4ef2-bfb8-c3a318b62152": {"doc_hash": "ea92a647f56c4ae81e5beea2bf07b07f837a4afa4174d222bca35f3653ee443e", "ref_doc_id": "bd8f1cba-b5d9-4b89-8fc0-bc7fbff84e63"}, "10c5b82a-6940-40fa-9423-30f346f62f9a": {"doc_hash": "b46212d9d5d0610ad87cb536f4b104a0024c6b1ed2510374aea6f0b537adc9f8", "ref_doc_id": "a078d418-b645-42b0-bc41-d0b160fbd004"}, "f23a7dac-1f1c-49df-91ba-bafb743f5333": {"doc_hash": "a27ea3375fbf379086724d4442f4f7628e3307e73075f0d4a9738bcf9cc89550", "ref_doc_id": "604fc341-6e0c-4e9e-9b25-38c0ca29f3d9"}, "5df50c6e-d10f-43cb-8791-ba26e34c1b23": {"doc_hash": "2e3d58e1c7ba7bd392d53428d4003fd75e27c5d4ad6a2e77b8b319a84c370403", "ref_doc_id": "2ec51096-01d3-4ed2-88a1-83e80259a523"}, "a410acb2-e91f-4ba1-aae6-36dca069cfd7": {"doc_hash": "e7e0eba99c4371e2ceab26dbb491056abae938e29fbfd348404466aaf1c3d001", "ref_doc_id": "0a41b72f-eb21-4a70-9f48-fa7efccc8866"}, "74605981-9cf7-4e6a-9992-0b05073d65d7": {"doc_hash": "7d441da4380b630caa32d3ef94189e5ef3ed325f8e5b18c8c67ca130c5396cfa", "ref_doc_id": "4ffe1006-dd87-4936-a453-7e81e59b4232"}, "d67530fe-05bb-492c-8137-8998b5ebbca6": {"doc_hash": "1659d14d05ab700db6d8d3257bb0a9c3a0453d200be99e2d8c6ae3d065126185", "ref_doc_id": "1b0a366f-eac6-4b24-bd1e-f195267bc162"}, "c307ee79-5254-4d9f-af82-9b67c0de71e1": {"doc_hash": "162d039a2c46ca4e621c8f8dafe25d9d6446e1ccfb20550161c23f6f4837fb95", "ref_doc_id": "32d20442-5c93-413d-adba-c9221154560f"}, "f116b160-1b0b-4177-b659-0c0790d1580e": {"doc_hash": "d857ce277f451ab26ccd8188b383af9f93e93107d111ec4b43451f39abff20cd", "ref_doc_id": "39da3daf-dac6-4def-bedb-9bcecdf5e61c"}, "49190b57-77dc-4635-89f9-ffa1a07ed402": {"doc_hash": "90ac6af1921a519cccbfdb9dde1bc37dac04bc63c9581b0fc9a54a5dc1d0f652", "ref_doc_id": "a36e428a-710a-47dc-8317-81cf415ab194"}, "8ea66508-37cc-4c0a-a121-3213ec875673": {"doc_hash": "c325891e0c4dcc545a296cd478a07502701b1d393cf2ad1546ccc513cf412395", "ref_doc_id": "6b1235b0-ce4f-4239-80ed-bf49f662714e"}, "c23c5bdf-a5b7-4681-8876-78abeb44ee50": {"doc_hash": "d0b93cdc97ad78f5dcb0b529721cffa325bd0fec1ce7c113b40e1fd85f904683", "ref_doc_id": "2cf657d8-c137-40af-b472-0306010a0b08"}, "4878d4d0-9dcf-4d59-a113-ce59de5a21a4": {"doc_hash": "95f60a16765942456aecdd540a71e3a3be1ca99a46c9a1cf999d3dfeae207fc9", "ref_doc_id": "f5e7b054-13fb-4818-8455-0aa0a204501b"}, "986cce1c-46ed-4ef7-b82f-de28706cb40a": {"doc_hash": "cbaf3f9b5de783c6e807a0f25ec453afe1836fceb161552a58da524205291880", "ref_doc_id": "cf1d45f7-7a98-4cfd-9fc5-cc9c550f4904"}, "77c0bd3d-6f57-465f-9270-bc7d1f1e00c0": {"doc_hash": "0ffa6964b4bd20fa9b38472a2c9a0bde8b18f59d963f6ca98bb785207689c229", "ref_doc_id": "94c21bec-196b-40a1-892d-f12d241167ea"}, "b721d2cd-c81b-4c99-9699-fa5d6786041a": {"doc_hash": "9f0c47730a5a5a269c8705bbf2057331932bb267bbc49b27a77aaa00eefd77b2", "ref_doc_id": "31fac131-303b-483f-a23b-4479b48169d4"}, "cb8fe440-ee71-46ef-ae48-7b27f9777a28": {"doc_hash": "f9b9fa111f2d068da69ee1b84d7c2bc38ec3f503bde5a3daa2c9b92d6d0dd122", "ref_doc_id": "a6b0afe5-5335-45b7-870b-94c517864f57"}, "5db0efcf-c8de-4053-8828-f4a23c6430a2": {"doc_hash": "0396785a104ac1dc31d853ac03e0a8970d0169e64c74a74e8ca2f842ebb30c7b", "ref_doc_id": "5e1d98cd-dda5-4e38-912e-9dfa1afbe47a"}, "fc32a593-37a6-4b89-a987-aa526e29baa5": {"doc_hash": "41f68b14086997ae049d62414477b3d9a8fd942775941436586de4b4893603a7", "ref_doc_id": "65db84e1-f92a-45fb-bc28-27bd55d563cd"}, "dd7138b1-f320-42a8-8554-925c09e16009": {"doc_hash": "c00312cd908dc1b1f27f116a2741038dbe212116c205ade8cf8725c8710caeaf", "ref_doc_id": "7338dfe4-7ca9-4526-bcc8-bfb199e92eb5"}, "1cd795f7-d7aa-42a9-b4b8-41bb723b1f91": {"doc_hash": "c28787a99e7946e63c4834d2458b717a88f5ecb06d47b5c8281f8436546aea8f", "ref_doc_id": "7f2e7113-15ed-4db4-981e-09bbc754cd89"}, "8ee354e4-9257-447b-ae48-666606e9a5af": {"doc_hash": "9d100472900dd5d09f132c2a593d53183ca493e1427c27d79079c902a4910508", "ref_doc_id": "f1ac8b69-9df5-4e79-9a21-4dae840f7a17"}, "59fcccaa-b9bf-4754-b54d-4690f97751be": {"doc_hash": "4cf7bc8f4b2e09a51e1fbcc3065701d7446c8de175774c29761070e2b910803e", "ref_doc_id": "110cc1fe-ac50-4d5c-bc9e-3e94486485a4"}, "b5114c0c-243d-46d2-9027-f3de53cbcf8d": {"doc_hash": "8e56841da3154572160321723ea03ee29cec5187de17e2f0c4155bb7899f17f2", "ref_doc_id": "ed270164-359f-487f-a02d-56f657461b19"}, "769fced0-12b8-43cd-97e8-fa27bc48a22e": {"doc_hash": "12f94990c481c5ec8cf2e8bc996f3d3f17e908b707e92463dd813cc3a34c2b77", "ref_doc_id": "56c0033d-29cc-40fe-9cbb-e7b585840e06"}, "3e5dc8f2-7def-4380-9433-ac7e1c908334": {"doc_hash": "8feb4960f1a83c12b0692dce4888c7dbda516ec375f21ab61a075a21c6c964ee", "ref_doc_id": "3725b24f-89e2-4b7c-ad3b-1bfffdb29df5"}, "63dca23a-ab70-4bd6-a36e-f3a0be20a20b": {"doc_hash": "0c100aa52e76b7f6655eef00a92310afddd0503f1eb5f9672289a8bd3143f2ec", "ref_doc_id": "6e927557-037e-4583-9ac7-66db4dc513ba"}, "7eb47cb7-6ae8-4c4a-b27a-2e9e8dec25d6": {"doc_hash": "6cf5b78215f8768e845182c8a20e6089498fb6f1d203975fed481ab34bbbea12", "ref_doc_id": "530608eb-cdb1-4fce-872b-6a58f7bb0466"}, "aaf6458c-af1c-4651-bc36-4c9e4d5ed576": {"doc_hash": "231c1f1bd61813882c1df451bffefb2e792545b6df524658f5f67bdc677cbddb", "ref_doc_id": "9d8a0107-2c78-4811-b35a-71a77dd713ef"}, "c8881196-1913-4fcd-bf15-3346ec1f87fd": {"doc_hash": "6c4a2bd5404302b75f7294af94b7b4ac4ca764bf3846a4b34d8d664538a67da0", "ref_doc_id": "7d3bb5f7-7610-443f-af36-1cc6c11d6b5f"}, "2bca6cd1-b02e-4386-8607-9b0734bb8362": {"doc_hash": "2bdddba991d203e6ee1844a20d9dd3d4c68c9eba9ec0a534e7fcb0bf99bbaee4", "ref_doc_id": "7d3bb5f7-7610-443f-af36-1cc6c11d6b5f"}, "7101c4a3-15af-479e-91b7-734c641d02de": {"doc_hash": "8ee78a640d3e26e444667f761b050c7455070e905497d1fad598cf10c12d2cdf", "ref_doc_id": "779ba5bd-e08f-4714-b0c5-43aa1106be38"}, "f8a58d56-c22e-44c0-b3aa-1c17950e20c2": {"doc_hash": "36837af0c605652ab281786b7c6a62ba5709536687def09cd3b539e856ff46ab", "ref_doc_id": "779ba5bd-e08f-4714-b0c5-43aa1106be38"}, "b96aff72-42bd-4642-967d-fbcaa8044a6e": {"doc_hash": "3048eac1b5f814f23d4f6e236ee278c613af1247926843956d2d4cdceef60eb7", "ref_doc_id": "e17272c6-7085-4124-be4d-f078d58a8ef6"}, "a1e3edb6-4574-4892-ba50-d1f4dd8ad966": {"doc_hash": "603cca81379bc01a3fe41120bab925118c52e213d9dc12a59d6909c9f4a11a63", "ref_doc_id": "e17272c6-7085-4124-be4d-f078d58a8ef6"}, "a0ff30db-152e-43a7-a7c2-af2889767a56": {"doc_hash": "15967da1416d73efb1db20d766c3c3b8cb2f0a0cb19f74578d367f8da4f36356", "ref_doc_id": "531f88b7-b6a2-4ccc-ac79-064f19d29c1b"}, "d8c0a719-fdfd-4e5a-98bd-ee52a8a2d2f0": {"doc_hash": "4099cb151921e887ea53eac76e63a6fa931a0c7a98e5db687fe14fa0f7b54340", "ref_doc_id": "531f88b7-b6a2-4ccc-ac79-064f19d29c1b"}, "e2659921-acd7-439e-8d69-f3badfd570a8": {"doc_hash": "aed87672488def860e4338a7814f10882d980dd360f9e86d730d5903d66473bc", "ref_doc_id": "1e037851-65ab-4083-b330-ba70f831a7ce"}, "136d7519-4d1e-4301-9825-7527f2f80092": {"doc_hash": "cf2116dda4a95a2a1b0d9b78610b6ca151b2eea32547832a14c97de8370c3ef9", "ref_doc_id": "9d288575-1054-4d06-a7e4-482c794c123b"}, "a24270a9-b8ae-4b89-8799-30a053344137": {"doc_hash": "553109d3db360685d55eefd1b80f222200e4b653880f398da4de0455cfe721a6", "ref_doc_id": "f035cf80-3eec-4d97-8d95-78b2eebc897b"}, "ad462851-148b-42d2-a010-23caba5d4321": {"doc_hash": "1e86ca7af8078c84740c48bbe7151fc56ad59c8509fdc8a9363d0aebb54344ce", "ref_doc_id": "e61ece3c-0d5c-4cd7-889c-24d278806d15"}, "b909f588-a3de-45df-8762-3c1426e2a3ef": {"doc_hash": "6e5b30337bc5758ec1aae13d8ae549b1af58f284504caaebf02a1bc07bd8f6ae", "ref_doc_id": "2d3ac2a7-90f8-40ad-8b83-c33860f25f7f"}, "7602450c-d56b-49ee-ab39-6c64047c491c": {"doc_hash": "8e5ba93a8b77a59f6b6dcfaca4abd9e90deb06ddd9e692f05ac8e9862a368597", "ref_doc_id": "b6d65c86-b497-4c6f-8d4b-8f8e3d78fb52"}, "e1ade370-fc46-43d6-8bf3-257686b1f547": {"doc_hash": "5d505e63134ef85023eb5e6d9c2a146fc4e16e9d0b9976f5d7e2eb9153c7ae07", "ref_doc_id": "588c75df-6df1-4243-bbcf-ec4da15cbaa2"}, "01ba77ba-fb95-46e8-be69-ff27d4090e4c": {"doc_hash": "8b5a7be1386366fa4536206cb428f8d7a049add59c1b2d895de60ec5b53eb13f", "ref_doc_id": "d151f6aa-271d-46c3-acdd-bd7172340711"}, "3278088a-a925-4b84-b8a3-89a238104c5d": {"doc_hash": "d4f5339d09b935e0f8b39f85b926387c2c154ac0532139c1f91740206c84d858", "ref_doc_id": "d3aebd53-d2df-4f54-9f60-d8e8ca347b15"}, "87b1f151-8445-40ba-85a8-a09fdf56bf7d": {"doc_hash": "a18f3ec3b1fa0c0be639fde9cd2ac50924bfd9d99ce46112387d72c4c90f440e", "ref_doc_id": "4cd67c24-fbe5-41bc-a187-15fc5f828cfc"}, "c07c8bb8-5ab0-4946-bce3-e6bbc55b1ac1": {"doc_hash": "11ffd4e2da23438b1d6ae30c6f5d6a7906bf97c59f60efad9e7c590cada64284", "ref_doc_id": "54e355fc-6c2d-4861-9d88-808af8e9725f"}, "c796b18f-18ac-4d2f-830a-c7324bcd2c55": {"doc_hash": "1e21edeff981c829f8c7faacf66c4454ec3a8dd13f1207911daa75d1ab1a9d1f", "ref_doc_id": "0768b5a9-5cb4-48d4-88ec-ed9a34ed203a"}, "cca303b5-728b-463f-b652-e7be1a4db934": {"doc_hash": "ac6aae32fb9be503204e23a19a7026a8f5de653a0ee6f5c6ea773aa5a14a4be8", "ref_doc_id": "9790a890-7e97-49eb-922b-537b4dc0c023"}, "7a74a2e4-82c5-4a12-89a1-d18eac3cdde6": {"doc_hash": "bdc3826eabe0529f1105217d38e62dff86e874398af0252880421ba232e48a58", "ref_doc_id": "f9a0f2bf-83b4-4fcb-bff8-bd785e32047e"}, "ac9043a9-033c-4fa6-9595-30f20ca0016d": {"doc_hash": "851eb1b5f7fd6e500b7b83d2c139ccbbfbc3335e48ce6ff7dc6477cb88563d84", "ref_doc_id": "31798e22-345d-431a-96cb-701d39cd51f1"}, "ed743a92-8157-4e05-885d-88fbbcdda53f": {"doc_hash": "8b0f4e89624a5185d37bc73c43d7bea730715ef5c99f6e7c2549d96171a04d22", "ref_doc_id": "698003fc-d2b5-49f0-a816-431af7f5337d"}, "1f562305-f063-46b8-99e4-a5190e1df54c": {"doc_hash": "ebf68b9d5044589046f0bc3733749b72c2cd534dbeb593cf6b541d38a2c59947", "ref_doc_id": "0effb730-ac3e-43fe-9627-820435850fa1"}, "85bb53d9-a52b-43a6-bb47-1a684aeb6b2f": {"doc_hash": "2bd7f60fc55da5a1df04a8de87f4acf3b19e5d301713faaefd5256e92dd687ee", "ref_doc_id": "e6e68ab4-27fd-45ae-ad8a-e064ed829306"}, "97024dc6-f113-4144-9201-04d5090f1167": {"doc_hash": "7d95bb90f2bbccecdb4e2730768c67488dcf5eb1d25571990550b2573c5806f6", "ref_doc_id": "a2c4ecac-c16e-459d-95e5-0ccd9444b343"}, "61549da7-5211-4da7-bc9b-3b26272a5681": {"doc_hash": "bb21964bb2734536b92a37cde4d8c428a16f657aae4101c90c822454e46ae9be", "ref_doc_id": "f2a23db9-93b8-4661-a0f1-37a4faafc8f4"}, "722f6dcd-5131-4050-94d5-a625d3f32955": {"doc_hash": "8c90b9a22a2ba8c90d4ec04812ebb74263874a58ebcf389befbacde39afe3dc6", "ref_doc_id": "cc824a43-68f6-49d9-93eb-6014ba1848b3"}, "60e4271f-b826-4e8d-ad41-8a2954dd5d71": {"doc_hash": "8ac98c81113bac237ac4ce3d68b8cea3271832d6fe2b9370c560f7a5d339cbb9", "ref_doc_id": "cc824a43-68f6-49d9-93eb-6014ba1848b3"}, "2fbcb017-27c6-4248-a54f-d983876c95df": {"doc_hash": "584798bd90d71ab367db7083898016c75feeb5c1a2e3837c243545b735d60f20", "ref_doc_id": "e36eff1a-6947-4f1b-b943-f7acecdc28e2"}, "fba9ba92-2d2d-4908-814f-9efe37937526": {"doc_hash": "b70f25a73749d0362181203f428267fd827ed9162d60447398405d2c053166b1", "ref_doc_id": "e36eff1a-6947-4f1b-b943-f7acecdc28e2"}, "2f1e704a-4097-4c0d-9b2c-d12c0d1aba46": {"doc_hash": "8d50048c24d93a6281af00b5950139aa92f8d3c7ddf155e43507a7fe8bcc797c", "ref_doc_id": "0a47d563-871b-40b8-bfe0-71af3042e0c7"}, "e8b5786c-e38c-4b64-9220-ebe75f58debb": {"doc_hash": "95a6de03bf40fe98db1ffbe7b463bab75f2c4da152567e115b4b0bac0f6f1745", "ref_doc_id": "0a47d563-871b-40b8-bfe0-71af3042e0c7"}, "a332080e-9ebd-4cd2-9b69-d2eb838c3362": {"doc_hash": "a013c7d3c8612581981f18015f5cad2de17babb37f051f06ec2044d4597c8145", "ref_doc_id": "dc43ca0c-bd26-4be6-a8a5-c8f2102a86ee"}, "ecd1c513-2e12-4122-bdd4-03e6d363e9e6": {"doc_hash": "ec9eb66f67228f9560f64c298fc28705244b223f1736f8b381ec5c9bf15d0ef2", "ref_doc_id": "dc43ca0c-bd26-4be6-a8a5-c8f2102a86ee"}, "a3ac1b31-0ede-450d-beda-33ece7bedca6": {"doc_hash": "5085fa84bdddd70ad7ab8b7b05473382b792aa36ba1ad628cf73e3354a49cec2", "ref_doc_id": "6bc13c2a-8305-4858-bb25-259f6f58e99d"}, "632d3bf8-166a-4d83-9dcb-cc29f0debe84": {"doc_hash": "f87426a8c5762e1e1d239bdc4902ae3f12fd03624f83ee7cab8f602bd2f9dd0b", "ref_doc_id": "6bc13c2a-8305-4858-bb25-259f6f58e99d"}, "251443ec-7dcb-4c95-afcb-4759f06f78ce": {"doc_hash": "93f30b8e55ecf0766071dca4643bdd12530c10417facea445fc0502112a840e0", "ref_doc_id": "8158715b-cef2-4015-9562-b06aaaa04497"}, "b01c3657-d3f7-4809-8f03-e30950548bbe": {"doc_hash": "d1fd1694eb2327969ed171e10aaebec1478f1a5d42997a15be1512fc06567fe7", "ref_doc_id": "8158715b-cef2-4015-9562-b06aaaa04497"}, "bad50e03-44d3-4963-9db0-3a340a81ecbe": {"doc_hash": "cb7ab1d8c48391e002ad9fe32ef6b32b6db7e2d97165b991a211e0007553ceb3", "ref_doc_id": "11672e81-c2ce-4630-90f3-30681ed5d73d"}, "b756e03f-a448-44f3-bd46-128035ca97df": {"doc_hash": "aee0c8827a07d2f094ea2a2f9abf450a3ea262f0a845ed52d4b4889fd4eb37b6", "ref_doc_id": "5204c427-8fd3-4b4d-9141-7e97fb38a56d"}, "93d10456-681b-4814-8c67-fdc2ab705ac9": {"doc_hash": "d317ba36266ddcd9ef53b373fceb87dcd9eaf662966e35470b5acaf3b4c8a7df", "ref_doc_id": "64fa15ac-c027-4f53-89c9-0c1b5583502f"}, "e7a7362e-7807-4628-a066-e150af48a4a1": {"doc_hash": "9de9426356fb7f6ca2c4adb53f34561f35b957491ac0e8b225283c3c77469f2e", "ref_doc_id": "27541c14-f639-42c6-bd2b-3e1bfe34bee3"}, "0ac8ffdc-d304-46c5-9acb-5060ce73d0f2": {"doc_hash": "ead6ec7bc30d5f9169a9e1ba6d5545ca9f821cc6bfc1126266c9ea2cbf81ad30", "ref_doc_id": "664e8370-87bf-4cbe-9936-e1a4a96f7d10"}, "15f3d42e-d3ac-42fd-8090-04302d307e3d": {"doc_hash": "97ece45bce89e795093e8c05fd230ae155cfde5e433da3d1bf432993944a0b51", "ref_doc_id": "db8c9948-9ca5-4ae9-b2ac-12da3acc3447"}, "cb266fcb-337c-4c10-9ca7-62bf70ff2fd3": {"doc_hash": "2d7a570fc89a5b852f8c0dcaefd9c4391fa4de0144fa6aa9502161e36dcce678", "ref_doc_id": "dc775fd5-8062-4090-98a0-b2e3300b9514"}, "6d79c9c1-069b-4797-8eaf-c01680bf7c5c": {"doc_hash": "9c8c7ab8fe33291d6630932c6c25f8cb5f55e8fa76bc548000702e1695bf0ab1", "ref_doc_id": "ace730b1-1f39-463d-b1eb-7fd401550204"}, "20910cd4-29e2-4d5e-935e-f389b86f469c": {"doc_hash": "64cef1bef8886aac40c516a61ce579c5726ce2d81b84de5fe77378ad7b18c2a1", "ref_doc_id": "ef979d4f-4980-46d6-8414-dfeaa0eb1be8"}, "809f3c92-df81-432f-aa34-97c9af2131ea": {"doc_hash": "e061e672d8b77245ea91fdee4d253b08756d8159c143f328f84f83c2199ca152", "ref_doc_id": "c3757882-d8e2-4e62-bcfc-a2f5f3ac06bd"}, "1f6130c1-a12a-4be6-9f0e-afa14a32757d": {"doc_hash": "cc66ce9b0d3d298c96c3d5cfee73c133df2ed5a323067ba47efd3e19285d3130", "ref_doc_id": "3a85ae34-00d2-4d2d-aca3-0d894760a221"}, "cda350f4-f66b-499c-a7ec-0c6088f77272": {"doc_hash": "1351b2e3b61f71f86857e2e80b494de10e2ef71572c2a35060d5732406a16507", "ref_doc_id": "8e5339f8-1724-4380-8282-3d6031fcd974"}, "2f47f31a-3ee9-43ad-86e7-fe98a4940e8e": {"doc_hash": "0c5a89ae6173532bba210ff413dbb449f81f0fe15f69769250c35b5aa51e6aa5", "ref_doc_id": "96edd470-d0a4-4e33-894b-30ca769bc094"}, "f2d97601-bd16-4f8b-90f6-104b41ccb935": {"doc_hash": "ef8719745ce2e30b0093e12c3a21c39be58aa06923a317f238de9ea3e4d0c25b", "ref_doc_id": "9d8e97e3-178f-42e9-a209-d5ddb5972831"}, "14e35c93-df0e-4104-9f27-bb087aa0ecf4": {"doc_hash": "568a83ddbaeea349f1f77061e08cf3950a46f5c8b8fc2083955126a00ac2713e", "ref_doc_id": "01db6d96-bc82-490d-8823-3e677c2905c1"}, "ccf19eb6-f8ec-40ab-9d22-e89afca3131b": {"doc_hash": "78e218f731a4682f3d50f8da5ec8569aa5a4c2836eafb216ef21a3676d0c95a1", "ref_doc_id": "a54b8dbe-4d11-4bc0-bd73-f1fdef0876ad"}, "1bc7ec52-c798-4997-801a-e49c9e988c28": {"doc_hash": "12afb85affb94a450868e2f0462da7afbcae52832b51705694f9dd5d7da30527", "ref_doc_id": "1067cad0-218b-4145-92a1-ef05d72d02f1"}, "a0f44d0e-799a-4497-a1df-c42dea6401be": {"doc_hash": "f77adea67ed4e13253f75cfc9f5d1a67e12913d9918d720ec11dadd6bb3ffb7a", "ref_doc_id": "0c8f2a83-1d10-4f97-9763-e6a7db2e948f"}, "e9f19840-b05b-41f3-bf86-ad080e9ebccc": {"doc_hash": "71ab450c396dcd435f61e22fa916152f097cbb2b5bf907bf8ba5ad1d8531779f", "ref_doc_id": "2e48ac36-3b63-4c9f-b356-0eba1c400395"}, "8c94ea6b-6624-441c-a44b-e85e5f4842e2": {"doc_hash": "8f0efcf5629f560ee9477e7d70f4830fdcf656c75873f58b41578da71faac608", "ref_doc_id": "4fbbfba3-46fc-46a7-9c7b-69f5140e7c79"}, "95d36ef0-7ab2-4476-b4ad-c07274d50388": {"doc_hash": "b7b417b2f6ef3d096a6f868d051e5e1374c0309afe894ecde83aec4119cfa280", "ref_doc_id": "7000771f-fa4f-4b34-a811-d9b8ded08d68"}, "fae7402a-d577-4023-84c4-c1a0ebe10ffd": {"doc_hash": "2dab82ec78cfeef38b79d579ef1a860e3e9ad7829d7725b5f186155af0f51f9a", "ref_doc_id": "bfd95a86-26e2-4148-b364-865713db1d51"}, "2328051a-1fa0-47ff-b173-5106686eb2a4": {"doc_hash": "bc0afc73eaa749c4421d1fce7af41f139f5869ba702aecba1227a4c40db8f266", "ref_doc_id": "25447448-a2e5-4b64-bfe2-63be4984a1af"}, "9e38bb4e-f293-4093-a863-ed4b94dca12e": {"doc_hash": "aa5505502623106e4c1aaacbb1a56e46cebf8fbfa58c7c269cc95e62bea1e787", "ref_doc_id": "17a136d1-14dd-4305-924f-c5a3579e5699"}, "826174a6-72d6-41ec-854b-654d4a3d4f03": {"doc_hash": "2ab7987ae099146ff90b470e65ed11a031440fd5f96009ba8554c1111b7d2ef2", "ref_doc_id": "88084533-c476-427c-8d28-250771cfefc1"}, "4ca65cd8-c9c5-4181-8520-ad31103ebce3": {"doc_hash": "cd13c5ac48017245582a15c6acdbcbaf0defe7672055dfef69314634bc5c3377", "ref_doc_id": "f19184bc-33fe-4325-a440-1dc077fc720c"}, "d369a3f7-414a-4e14-9aed-70c27582456d": {"doc_hash": "e92739c1f3ffae7f9d1455b281fc33903e164d638c1602043baac422cc56bc89", "ref_doc_id": "b0df48ad-3cb4-4ea4-a6d1-cc4211fab607"}, "c42f0acf-5c83-4942-989b-cc77266bfd9b": {"doc_hash": "1e86c0a6d74ed381fbc47d9326002b46cf3500bbd259b9500e4f1b288fb9517c", "ref_doc_id": "bb6cea9e-2448-4dfe-acaa-9532e1e68cf2"}, "6f88d2c5-d9df-428d-b946-fd7e7f215871": {"doc_hash": "f1f55a5f5bd435dd1d15232aba69d1234175f8f1d372018191e7d6a60c453905", "ref_doc_id": "655ebe1c-6034-4a99-9087-9f55318f1adc"}, "6b5cd206-2a5b-4ebe-9304-968d60a8d9a4": {"doc_hash": "9c3143b5dc9063218a8ecfa11c265dc76839dbda4b4744f3516e1aafe4dfeee0", "ref_doc_id": "cf6c459c-f5d8-48a3-bb57-9a1a481da80f"}, "d593b936-92cd-4459-9b35-ece89cd6177c": {"doc_hash": "c9dc328b6bf112d9f88337c60eff31018622590508fe5b3b815a01785a124663", "ref_doc_id": "1d42eace-8e1d-4d0a-af6c-e3a8da4b7f8c"}, "aa11f417-4fe2-4846-ab61-0f20bcab9492": {"doc_hash": "15ca51e90ec9bb38e0d3642f9eb2d6a0f25787ba4a5e234638c4de9a8bcf93be", "ref_doc_id": "bec61c2e-e6de-4e2b-833d-06eb5deae25b"}, "7b839114-8ef7-42b1-b4f1-351783de4038": {"doc_hash": "8c1a6202c07a6c0b9b9855712154acfc3bde33357544e67ef5f6e8edc3b2a6d5", "ref_doc_id": "dcf74ee0-abdb-45c0-9e06-35a3b586e2cc"}, "fd4ff1b2-b667-4c9f-863b-f7c781c461e4": {"doc_hash": "c58575831cb8ae8fde683c24d0aa2739a69d0f619fef5c323a47df082a846ef5", "ref_doc_id": "3bfa176a-1f95-4e51-9b3a-316ce84a22c7"}, "714c0367-e257-48e9-ab67-57cee81a2d05": {"doc_hash": "38083a0701724d1d73c7aa05ab836c005680d165710f8b53d063df6c3582b047", "ref_doc_id": "3f8c2413-171e-4f98-8d19-12346440980a"}, "67abb207-16af-4e1d-b5bc-210e4490f591": {"doc_hash": "8eb57986a71ed87037488c659fdef020c9cd011ea95519448f9d5194bb839fd7", "ref_doc_id": "c56c00d1-d371-46a7-b3ec-e569cbaa408d"}, "1a74591b-165a-45f1-ba40-8f92a5552061": {"doc_hash": "d5fb4e8b9a74a590fe18ff3b3d15de55c0be589f3262aa4c0c3a0878ae65c37c", "ref_doc_id": "da85d089-01fd-40c4-8a45-99c788db6dc7"}, "ed037b07-5946-47f9-b1b6-c6e585b7fad8": {"doc_hash": "8cbe35ea5880ef97278178c0ba1502f0f082a9878d5ef44a00b6c479ee1248db", "ref_doc_id": "58ddc913-545a-4626-88bb-0dab82093ead"}, "4994a402-6792-4c8b-a35b-d7c8d7582edf": {"doc_hash": "47c82363c03da7b4bec699a34464a828be37c17270517d6bcb6286978bc8c8df", "ref_doc_id": "8bf92c09-77c6-4be5-8579-06705122c6f1"}, "53f3f03f-c6c0-45d5-954e-329e7fe23f04": {"doc_hash": "9eefb1f4db37da3de4938d81dae60cc011bd60ebf525ec8ca0d146e1b3da14ad", "ref_doc_id": "a350b4db-e07a-41f3-b2f3-a47e9fa9d6b1"}, "ba197a59-a379-4f42-a9b0-bf2df0d229b7": {"doc_hash": "940996b0b57cc7615fcc1f31063f9336f155d71c2b979895b577fbe21cf733a2", "ref_doc_id": "c87b58e4-1df5-48a4-aa2b-4127c47028ac"}, "0cb4bd28-addc-47fd-b0ea-57a58e1472ca": {"doc_hash": "b9702b4270fe6e062daa0841ee98ebe1d7314ea92c544d54153eb2d53599a674", "ref_doc_id": "78753eb2-0c5c-4969-ac3a-542bf1993ec3"}, "67805cde-6ee4-497b-9d96-6f611ad78265": {"doc_hash": "b8e10794055cf1ac65ec57494f106277345607a341e882a08d4983dd37cfa591", "ref_doc_id": "fb00c216-81ba-4c6e-bc60-747457efe92f"}, "6e7373b5-b01b-4d75-906d-d82dc989740b": {"doc_hash": "80997b55431da1bb00308f32dcd73d93fa3fe6e111a6281112c082d8849abd9f", "ref_doc_id": "508d3bd4-596c-461f-9c79-52a21b0565e3"}, "24004148-918a-4956-8844-f18279dd1818": {"doc_hash": "241713b2f418a0c2ad7a09bb832bd22c6b8a35c2bf5611822f2eb14e1ff1ab9a", "ref_doc_id": "dacd34ff-0234-4b63-99dd-4e662e8506b5"}, "2ee663ba-ca7b-48d6-b22a-3996d2500b73": {"doc_hash": "69c1613a5723864e0a76da66fad999367989a61f4da48fc8705a2e23e7423c57", "ref_doc_id": "5dd81624-a8cb-4f8b-8721-a22720e67c6c"}, "1b09708e-dc0a-4045-ae22-67752f0e6a06": {"doc_hash": "74317011b87e0288d515f248d816af066c7655f79db61d71ff3280a887e14d98", "ref_doc_id": "bcc98809-e744-4dbe-9d47-643307221871"}, "084b72bb-9a1b-41b1-af62-f9e3c2072c02": {"doc_hash": "427c143a8dfa4bd0cf7c2f51e9842fce945127e5f2899b97669484e105397c45", "ref_doc_id": "a84966a9-6e4c-4a5a-84b5-57a368c79c36"}, "77df227b-fd7b-4820-9305-162fc0f7888f": {"doc_hash": "b9b43189cb65af9afcae5cd94bd5efb48ab714d31314c5488b2d13903a5f83b2", "ref_doc_id": "e6629f19-b56d-4506-8c48-e9ad2de85b52"}, "2f4f3e6c-7e05-47ab-88bd-ca2daa0c8e64": {"doc_hash": "7b8df688f16dfa0bfda637ef2e75c11dca32458d74a967ec029060af21696287", "ref_doc_id": "926f1fcf-b6ae-48e5-9a1c-1032cb961b1d"}, "b09cbb34-8cd8-4661-b852-7c0fc8f3122a": {"doc_hash": "9bceda43a9a10888ef2bf20784a5e1d7a756ef90e695bdf41ac0a98d46962756", "ref_doc_id": "81487b45-8413-4208-8041-c8aa1f1e063e"}, "366cd19e-63b4-4590-8615-829132669bb4": {"doc_hash": "4c571a66345a92aae208aff75adad29cd2264f2cd044017cf2ad3c90489dd0d0", "ref_doc_id": "51c128ec-97f0-48f8-a988-147c654b51d1"}, "658a896d-4e6c-4d84-babe-127997a58c4d": {"doc_hash": "6ecfe97cf345bbbf64f26efeb7b0beefe786583054f589855a72311dab667063", "ref_doc_id": "51c128ec-97f0-48f8-a988-147c654b51d1"}, "b073cf29-47ac-4500-8c23-8d93c9e7f736": {"doc_hash": "16fd53adb29e83a66895f00c36442d6d6c49b754e1673045a95b413dca780c9e", "ref_doc_id": "3731a957-0fe7-4560-994c-e56c8892a4aa"}, "92fef7a1-d40b-4c85-9f10-43706761fefe": {"doc_hash": "f74835aa53b2d4064d76c0f575af9bc20b7440e0a0eb5276b37bfb8594925aa6", "ref_doc_id": "a6886040-9fb9-4382-a229-a5d5cfcf8528"}, "50133199-caee-47d4-a702-34ff49ce8b49": {"doc_hash": "01d5ef2183763f304709d01b71e77bf96c500fbe7b502ca842d6bf5734fd023e", "ref_doc_id": "2bb89aa9-f4a4-4aee-b358-eb0db25c2f54"}, "e09db88b-e6b2-40a9-9eba-64ac79b4004e": {"doc_hash": "cd75374e4e64cc5924543586c591f86cca39b72974fa454553cd9192b0b3beda", "ref_doc_id": "834756d3-064a-45f1-a3e8-55f68788240a"}, "e2f51572-f016-4a1a-82e6-4e53b68435d6": {"doc_hash": "5b7cd5223129c2b6ff00379f9eb669663e7baa9d7e41b004479268c657505e42", "ref_doc_id": "a1aeb747-1ec0-4b4f-a55b-8e841ae01abc"}, "615bb5d2-444d-4b63-8b0d-eda0a988d0ec": {"doc_hash": "96a6c5c8a99d9dd9060a96169c8d4b1511bd0a3aab4f41d7d0c7e768449d1b95", "ref_doc_id": "7027798f-38c2-413d-bc7a-ebea06b4da2e"}, "3c80c011-772b-4d40-9040-950c099266b0": {"doc_hash": "4ad942abb307cfe882eabbf16cdcdca227cd7e7c722c85fc32ef3015bf3a7abd", "ref_doc_id": "98e00c25-ecb9-470d-bef3-bea7c8fafd91"}, "f1209d05-bdd2-48e8-a0ad-ad144b07f1c7": {"doc_hash": "780a634bc6e25eba3acd2e9e43a721959ea3e2564cfe04eeec77b015cae26d8d", "ref_doc_id": "a4463b90-911f-495e-afa2-52228065ed4d"}, "2e590f85-b193-4a1c-b5ab-98e5d1fe727d": {"doc_hash": "6a642aec4609ac927243c1f93d5c0630ab242b37ff6d1cb285638f537aca674a", "ref_doc_id": "438eab76-7f80-4a5d-9f52-a8cc82a88881"}, "93900cca-5ec3-4e1b-a8c5-f3ed7bc486f1": {"doc_hash": "839b68373776a64586a6d00f6c8c99331cbe5df74d8f2f21890f6ac9dd98e30b", "ref_doc_id": "618b35e2-f2b2-4622-87aa-6a58f48b1d3a"}, "24686375-678e-4794-99ea-f79390176955": {"doc_hash": "3f63b713068c5a37dd7eb58bad7c11f63b334bca0bc50ae18dd0656ac38e33a2", "ref_doc_id": "124a7aed-b02e-47bc-876b-63637c59811a"}, "ea7fb8a8-e8d3-4610-af9a-a0ea2dd76200": {"doc_hash": "65cdf1d77797aaeff08334a5acffde715b65c1f60bed6668859ac5ac6cab1dbb", "ref_doc_id": "e1669e80-5bf3-4b94-925c-9d6557068806"}, "d54636b7-e96a-4c8e-8948-0ddddd27d117": {"doc_hash": "4503e3f895733819fd83b317251c2347033a9f39b743d8805eed54982c21abcf", "ref_doc_id": "e1669e80-5bf3-4b94-925c-9d6557068806"}, "294056e6-c100-4864-8c30-afe02ef48173": {"doc_hash": "baaeba0e4d460390f4bfbe854bc80d6c391e2ecbe04dbb084ddefa3461522824", "ref_doc_id": "4a2e1732-fd51-4c0a-8340-1b06f8dc74e7"}, "be308fa9-4744-4784-9e41-2b055da0443c": {"doc_hash": "17a6468ccf5009ade5524288ae348fb55de735cca889d44d688e77a8ab1ecbcc", "ref_doc_id": "4a2e1732-fd51-4c0a-8340-1b06f8dc74e7"}, "b6cf1349-3614-4733-b85b-89eb48dd669f": {"doc_hash": "3bef34185d24d2b91e06192cb9abe8b866b4635a74253d2ff4374854b358b253", "ref_doc_id": "f342970e-0e28-4f0c-b773-4d31759ae4b3"}, "3f751bd9-1ee9-4fce-acc0-20df115a0f42": {"doc_hash": "2009258bbc99e9353e06fc0c506ac345a99dc6827c167f06bda63a3750268365", "ref_doc_id": "5d5f4a4e-86da-463d-9d7b-18e22090acf4"}, "538c2ad7-1046-402a-814e-467b18ff41de": {"doc_hash": "e04e2c16092d276689fbaaae675520ccae63efc5f57f3ec3ce58605110055157", "ref_doc_id": "3131d840-5cb3-4056-972b-9ed0f6588974"}, "3782df88-f77b-4264-893c-b8dabf6f7371": {"doc_hash": "fdd2f792de1e6a0031a5a345151ba444fbeac733adf2bf7c38ffbd69cd75032e", "ref_doc_id": "9c60d7e0-5ce9-4d9b-b874-642cd0715938"}, "6aedb46f-717f-4b11-b88a-091cbcc56a4d": {"doc_hash": "0b42772738b01f04f69e52bd043636db5ffe65157557b7b0cf9f33866021b04b", "ref_doc_id": "6593fb55-8dcc-41d1-af99-c1ff1d2685ee"}, "f58d150a-1790-482b-b36a-2c4480f9e3cf": {"doc_hash": "2265796f112673896decf436cdcdfc2b3c326ebdfcbbdcf1dc20372e7b558785", "ref_doc_id": "5fe9c554-fae6-43a5-b274-712b68c702b3"}, "1b3ad059-49bf-473d-a659-f70a03aff5bd": {"doc_hash": "a0cb47b19da50d8a3247dc47d18807393d32beaf87dc3e8cf488f3693f478cd3", "ref_doc_id": "aa20d37c-ddf9-4710-ae59-64d7253f0e77"}, "1e29e7ff-a941-4280-a840-a095a20f6444": {"doc_hash": "3abeded6f98535853a148eb8da5cbfbc15338b1defd093b4e55c9b0ae9b89db0", "ref_doc_id": "aa20d37c-ddf9-4710-ae59-64d7253f0e77"}, "90f27983-51e9-4dff-ae73-581a5bd0ee32": {"doc_hash": "98b3c8d02287914cf3b246689f667591847fb87659adeb9dd58ca96071c37f30", "ref_doc_id": "c410adb9-1403-47d3-859b-c4ad48a30e8f"}, "dd7d0ce9-ded7-4889-a786-3316e372f22a": {"doc_hash": "00efeec21b168714d4c40aae89b22678f383d848b3763e1b6dfe7422eb604d9b", "ref_doc_id": "16692cac-39ed-4da8-85cc-43d5493b1090"}, "dc60643f-f9fd-49e8-a5e6-5f289fddf027": {"doc_hash": "1543fd248b7c8c965ea72e0c02e2123edb720cdbdbdff2bec10618d341729e49", "ref_doc_id": "69ddaa41-fbd5-4111-8888-fde555a1cd99"}, "7db5ddab-0d7e-41cf-b98f-708becca1aad": {"doc_hash": "72ac52b147fe59b16d64c59db120b40131bc0c31156cab3815cccb9c5e00b434", "ref_doc_id": "c64dd00c-5a71-461b-a7aa-24a07610b74c"}, "d5a71e95-442a-4a83-942e-8b08b2dc9830": {"doc_hash": "2715870278225e82a82d7198708b61a855fc948e087817577b10d2b7e469fbea", "ref_doc_id": "43b0e7df-4912-4def-90b2-60604db225b6"}, "e239d1ba-cea9-4a53-b51e-5abacd7d6a8e": {"doc_hash": "66210e949e62e954a2331fe12a24409069e188cf94b6897efc883c4a41fe0659", "ref_doc_id": "ca28fa14-4437-48e2-ae23-9f732ed14586"}, "97b8d7a9-0656-4f20-b577-a1247826d991": {"doc_hash": "5d090d085223a1dc883bdec77a93a6f97c4a7a3f8697156e2e542fe8f46218a6", "ref_doc_id": "bacddf08-1901-4cbc-8828-930cbabc11fd"}, "07e28ca8-6908-4963-bb94-3c4153470361": {"doc_hash": "49662ccd6a43d4a7ccaf0b33cf748e8addd02bd6e2a60720d489f53dc42b0ff5", "ref_doc_id": "c97ea8d4-f522-4bed-a453-9eb1cb8cfa85"}, "3fadfbaf-bbe8-435e-bd03-b5ede69b4b64": {"doc_hash": "19b489abeae1ffe4c22c9d7a7f6c05110f8b3f113232a3351790881caad8b9ad", "ref_doc_id": "ec216030-44b3-4a56-8685-bb7fc93f4d2d"}, "a7423643-0738-46a7-8d01-4b63fa4b7bb2": {"doc_hash": "334a09935ed7b6c72ff6336bd3716ce924bb427f669e2eefa1eb6e887275da9e", "ref_doc_id": "c1d2415a-60aa-4f68-9d93-aa266d0da7be"}, "f38b6920-b358-482b-87d5-85c675588da0": {"doc_hash": "ed6870a6c8ab937d06e88160ec46055c2dd507d6d12714772c3ca86dda398153", "ref_doc_id": "c0669d3f-69d2-4384-af77-e2384c7f6750"}, "c0a62fd9-a516-466e-8c6d-f3a21952a78c": {"doc_hash": "5f7f6dd141b13dcb6cdc34123683733f7067d21fdbf7606a48acb4fa518f5c84", "ref_doc_id": "c0669d3f-69d2-4384-af77-e2384c7f6750"}, "e4b870d2-3473-4d64-af6d-4c7fce205f6f": {"doc_hash": "1951ecaec9484876443ba333cc594979aebd70e8c4ce6c1c2cd13bb986b71ed1", "ref_doc_id": "71ef1115-d0e5-44e1-a28c-a8a65590dc07"}, "632b18f6-6fce-457a-b513-a9a6ac1c8f7d": {"doc_hash": "f985bcb1090ae974b38aaa56c476f8b39efed59cebd6238458b1e1e3e9396c20", "ref_doc_id": "225e3686-c73a-4a0c-8918-a60289e2da04"}, "582f7517-d580-440f-a65e-459cfc99a4d1": {"doc_hash": "61008047f86df26c369e0e7198b912fbf760238f705407011347a049c8e7ca83", "ref_doc_id": "df079df9-43ca-4eb6-8a5f-ee9118bda107"}, "a6d6af9f-05d8-4977-9194-4dedc4078784": {"doc_hash": "b4a091700e8121139cc016eaff89d9521daa08d83218c0f13bdebb23411123af", "ref_doc_id": "c0ae27a9-c425-4183-ada0-09aea59be0ba"}, "d308549a-cf83-4b95-90a4-a8300017f518": {"doc_hash": "c2f0d3001363d1b09da2d7896b95563bad511c637f27140dfe237dac5921339d", "ref_doc_id": "a0651f0e-0784-437d-ae8c-c7bec54e1752"}, "8ef087df-59fc-48a4-bd99-73dfaed60dad": {"doc_hash": "5bb4ab51ee40afc9e4820d81432ffbc427d792ee53aebdad41dcf05e655617cc", "ref_doc_id": "b52f1475-6b60-455a-86f1-a3d49ec46177"}, "27ce6b65-4acb-4795-befb-74a42148a334": {"doc_hash": "5e505a5f18abfc99335f50b6f4cb56494b81fc87a5ae8a54c66451a3ce930760", "ref_doc_id": "3bba584d-9758-4f6e-a11c-5595973d85b3"}, "197c227d-d7a9-463f-96a6-87925b1079b7": {"doc_hash": "78748dd70cbc145530fc414e48a628d14d0dd4d76d2d5175658e6bbb31cfe300", "ref_doc_id": "7cf99aae-4b90-4823-a1af-72e4dd60279d"}, "dbd7f710-f45a-4cff-931e-b6e8407dd47a": {"doc_hash": "6da374008bcde4c3e0453a4af28f31fd687d58daaa5d9e55200dc3b47e073bb0", "ref_doc_id": "d67dcd29-2774-498e-95d5-b544c108feae"}, "7ff618b8-9e78-4eef-b760-9473611787f7": {"doc_hash": "6aa716109b290c41a8fbb53d3591f574231de9f2b66c4fb483b102309c7fb373", "ref_doc_id": "84273060-b30d-4298-8b78-dff3d59d10aa"}, "9971160d-74af-4728-aa34-0926c603b710": {"doc_hash": "ce4d52b1e86ddf75da0cd371abe40db93af3938146a85983ab53a91793c46a4b", "ref_doc_id": "b3a560e9-2cf1-47e8-9c38-b793af3d8272"}, "a6d5edac-2a60-46bf-a1c5-ad1c3fabb8e7": {"doc_hash": "d7f0a918410c39fc0201eec40ab7f983c328f6e236189bfa37389a60d5e66a30", "ref_doc_id": "df9d599a-bb9c-47ec-ba1f-07091b411110"}, "45e271a8-dfb3-434f-b696-4ed8a986bdcc": {"doc_hash": "f5d31977c3d016f9b60ab1fe20bb735fecbe0d310472acebceef7bfdb77bd4a0", "ref_doc_id": "b5492367-ff90-4338-a4bd-6e51c8408b41"}, "66d6f3d0-4ad1-445b-a986-e779601fafc1": {"doc_hash": "0d31b487ba96c4536a6cddb3ebe0c42073d829026740d78b3e5b409f97dd70ba", "ref_doc_id": "0c120afc-9622-4ac3-8db5-de67672b4743"}, "4ca407d9-efb0-489c-8165-ccf7402cf0b6": {"doc_hash": "7902e25ee8574803302568c190746e3a3a638b22c6aea489916d96aee1a770a4", "ref_doc_id": "b2ab75a7-5582-45bd-b8d6-8da6c499d32e"}, "9e3f3009-4b65-486f-83d9-540d0efc69da": {"doc_hash": "f4dcfbe811e6c5556e3248d61b877cd43df3432b43a6ba5308d7fa6f45631e9a", "ref_doc_id": "7b9a7816-9df1-4087-b9d3-fce837bbb5f0"}, "9e16bdc3-8b51-409f-be1d-7029b8cd680b": {"doc_hash": "117a07a9dabe2b78fc19a63c475b4cba25b80bdafe742ab621e3fecda8670761", "ref_doc_id": "3700e140-0b5b-49f5-84c0-5188fe804335"}, "55bba3ab-2576-44ac-bb3f-5cfa7d31e8c6": {"doc_hash": "55ab602c4c144d174a6e6b0e7fcb94af773b9fdf3c96924b4307f6a738cdfe44", "ref_doc_id": "0a4b4fa7-907d-42d2-9f07-aae78a62ca34"}, "6a8c631b-b9d2-4e97-ac9f-75f3bfa35893": {"doc_hash": "943d7a8653cffb69326bca16df9f8e15a4c202c171f3cf7050ac9874e8cdbeaf", "ref_doc_id": "b3d53c93-3816-4a3d-b988-89de6678d2a9"}, "63649d0c-a608-4e13-831f-9ad6be019cf1": {"doc_hash": "bc2ab1a853b33bec4071028f50e420b17781df7e80582e2cf89ccf5a75643c1d", "ref_doc_id": "43291d5d-62f5-499d-8c07-ffdeee836c43"}, "a3e56e09-2cfb-485c-aec3-15fe76030a26": {"doc_hash": "b3232eefb92bff68d7d76c13afbce4c4b1e9f71ef214f0786b393af5f12fb9ec", "ref_doc_id": "e468b855-cc21-4d80-b909-8200bf17080b"}, "0a2c1165-6a37-4719-9389-543b9c5d68e1": {"doc_hash": "5bdf0763965385c8e2d01a7013b1f527120bbea126439c9874cbafaaa32e716f", "ref_doc_id": "83f797ec-4ce8-44b5-89eb-850eb5133e1c"}, "e6cb8db1-9369-4c0b-bc7d-9056c48f1a70": {"doc_hash": "4b9995f7831bae9dbaf36769ed303488df381b0afedfd6e2686d00e0d448bf11", "ref_doc_id": "58b60538-3f44-4246-b039-bfa7e6c89fb6"}, "0547c61b-deae-493d-a650-abc29af755a1": {"doc_hash": "3902df11775474e652080cc6c95c1a06e0753d528c4d4792d75560271de480ab", "ref_doc_id": "5be7398b-a424-4fe8-b370-8b3f29de85f7"}, "fba56e42-afc4-44ce-9a92-8e8beed521c7": {"doc_hash": "8d8aff2e9d7ed8176c8b5dd329b0b7ce2ebb119857fba47fe886c50d0606b136", "ref_doc_id": "4a264752-044e-40fe-b85f-31e60aad4b47"}, "dccbc241-4231-4dc0-a06b-e964458bf1c2": {"doc_hash": "b7e18c6c34c6808a0dff52ba655a4bc671579600cd5e7947f6e65d1eb1e46419", "ref_doc_id": "4ef10fae-1db0-4059-a410-4269445e3742"}, "6f079f40-afe0-4b65-9866-27f79a152bc1": {"doc_hash": "75e9c15e6b1cc33b6d1cb602fd86a33fd6bcbb4ab5e6c69279de7800917821e9", "ref_doc_id": "8e8ba577-26c2-482b-8626-160edc2666b8"}, "baa2fbaf-78de-4a07-8d2c-41b9fd6d144b": {"doc_hash": "45e71524d3c371acf3402cba05b6ee5053b501b102ce026f5ea5752ed41bed0b", "ref_doc_id": "19737886-8029-4703-9ad1-9dbe0c77a189"}, "00f4552c-96ed-42f0-9c38-f753ce816a45": {"doc_hash": "8ce1ea7dd3a87a1ff6478a128a22b15a0c9e34aa73400ea4ab260a510b003651", "ref_doc_id": "fdd10dda-4495-45d7-b626-205c73b0e350"}, "413a6f6b-d2b6-4654-9171-78e0a3075635": {"doc_hash": "eeeeda2d0d53611ef4ba49b5df2915cfdb57f6c20be7f241301acc2127de09aa", "ref_doc_id": "3fe14965-f01f-4843-8c32-5de48b389f24"}, "43a6c154-d77c-4bba-a247-a6a1aa983001": {"doc_hash": "f8cccf234465e1f3439ba38967b061ba53d7b47c96b9fdeeaf123c72e4a07ce8", "ref_doc_id": "369c3d17-e714-4d75-96df-0da9758a69c0"}, "94cb6485-2b67-4601-bba9-7adb37ae7962": {"doc_hash": "974c1b52b59d512508258dae444377c7df9c4401c7eab0991993f912fd69b153", "ref_doc_id": "455ce185-23a1-4116-8a54-50d7c7a90082"}, "c5da40fb-97b7-4128-96ff-c568415710a3": {"doc_hash": "dab1c0f373eeca38be5ea29768fb650cb75f7e9f6ce5db544450342c16bddb42", "ref_doc_id": "2411fb60-fa01-4b22-afeb-bfb127e8e749"}, "c96a7c2e-19e3-4dbb-b068-68e7c45c2c44": {"doc_hash": "ff6894d3c14b0fdc1b1dd69e882221494b2f30f30b7b592b6dc414a1b2a1140a", "ref_doc_id": "6e7f2396-0cac-40bd-a590-95885dd01ad1"}, "9b90c7bf-fae1-47b2-bf1c-25c78c3999ee": {"doc_hash": "aad8be9a827925ffccba733c741a6b5c706d8a40da3f390403676deab9a98e7f", "ref_doc_id": "24aeb13b-8341-4cb5-86a3-2367cf7d611a"}, "7ca21c7b-703b-4f03-8532-5e5f75773b4f": {"doc_hash": "1e0411a36d2a447ca462fbeb35678d6c614b24caa1046f053334875ac5d07bc7", "ref_doc_id": "030e0f32-38fe-48b1-8cfc-88be95ab76fa"}, "2656542d-47c2-4360-a265-2c4a79b61074": {"doc_hash": "559ee142e0924181bcd0f61513fcef58a7d2e48a278c266b188999721f12b7f1", "ref_doc_id": "2de137e7-5a91-4e95-bc4a-fc3825196fe7"}, "312d428a-4940-4c60-917e-67ddb87cd1db": {"doc_hash": "750414a45f6cedb79766c3cdbd0dc149ee522105d5141b89e3f833f0e6801328", "ref_doc_id": "31ef0ef6-9099-4b04-9d31-09356ddf80c4"}, "1790231f-52d4-4bd3-a063-0cd2707dec6b": {"doc_hash": "38bffad8a367c358bc986dbdff9486ced628ebda75120b1afd9239b50f4654eb", "ref_doc_id": "9ca373e6-9500-4c0f-bc2c-c7c376545751"}, "65375221-f862-48ef-86da-ec0ef4d2f3c4": {"doc_hash": "ff3c6bcffba9e92befe699d21b4e27af99f0a868b014a5ef4004acd532f5fc73", "ref_doc_id": "1f14d2b8-2692-425e-a7af-117aa00dc560"}, "eb89a527-8106-4162-9e15-0999c469b114": {"doc_hash": "b93167433a0e43274a9622abe57243fd5ebe9720bbfb23a6b5ac642745eb6818", "ref_doc_id": "a9d95332-2e83-4b70-b3ef-13d46c8da8bc"}, "0ae78e98-da02-48b0-b925-7a69c9afc861": {"doc_hash": "bccc8690f2962d0d2ac5ccccda122d686c44328d278ef670f1ee1abd67920d5f", "ref_doc_id": "f78015cb-5822-4aba-9634-dc6813c5f4f2"}, "6d9059c9-f2a9-4b96-8c99-fb7ea6fa04bb": {"doc_hash": "4b75ac2db30c4e603728b91e6ae6755d59ad72b651ce61185f72881996234d41", "ref_doc_id": "88ace39e-f3cb-446f-9cf1-629e1172ce31"}, "23051ad3-4b2d-4ee6-9dca-488a3dad74f7": {"doc_hash": "ec2ef04db479bb9e80f3731d63eea8390a164837009e623072bacae549fb3c97", "ref_doc_id": "68b30d78-5beb-4b1b-b8c8-a3eaf7779648"}, "ba79490e-36bf-4af9-b070-a1072bfe6837": {"doc_hash": "3ba323df10d24e43c11ffe886ad2e334a1d0bacc2c3a1af557884a1628936883", "ref_doc_id": "68b30d78-5beb-4b1b-b8c8-a3eaf7779648"}, "fb056a27-3fe8-4236-a5e8-5dd2d7712844": {"doc_hash": "86dfaaf2dc6ff8af17ca937933aa2b7770d8f350b0ab29f9cd2e715ba5bcc07b", "ref_doc_id": "5d26d6f2-7afc-468b-b9c5-2bf5394a9a98"}, "d6080442-3796-4724-a4a8-5c1749a540ac": {"doc_hash": "4ac69e63b95b39227d3904f39616702eb055dbf7bbd3d57bfde65ee7fd537492", "ref_doc_id": "6e8c0b4b-529c-44de-8d1d-061d4eb83c4c"}, "6c731c5c-a58d-4383-8566-cb809b0e3a87": {"doc_hash": "216c93afa98e1f94ccad1bcf434f53cb4327a625ef94a39c47329070a4f164dd", "ref_doc_id": "9f3e69c2-0fbc-4c49-999a-c1f32ee6b5bb"}, "837717b5-85f6-4c5f-9406-030490cadde1": {"doc_hash": "7a002eec7220044779f9cdd112939defa047622e88a69db2897f028ddfb0acc2", "ref_doc_id": "ae9e144a-b356-4fd1-a5f2-3cfca09816e7"}, "2c01e35c-fc3d-426c-81c5-3daf593f820d": {"doc_hash": "94c04c0db3d933faa9e0f937b6af61b72ed15c254aaac8c23a082ef295fe9581", "ref_doc_id": "8f59441c-7148-4bb4-82e4-00d48a4b9bac"}, "b89a0101-81f4-42cc-9573-e9b1041461f3": {"doc_hash": "34e219f88d7258d6928f02c9ad006f2f29f1818a0e7007a5ff79bd97c49a6a92", "ref_doc_id": "4787e665-1654-4fba-9717-88d6e6213c24"}, "5d17c222-989c-40b5-97cf-326ddf0127af": {"doc_hash": "34e16248b2dab7ea31e5633e4cbb242fb89ebe513dde270506e0bec06b21820a", "ref_doc_id": "6098143c-9292-4f57-a499-bd881f4210f5"}, "71361730-f0d0-4ff9-b186-3854d8baf7fa": {"doc_hash": "6360967f76da8cdf1019b0e97db6906637b7454f38cc8ec12db469c816057ef7", "ref_doc_id": "174a8d1b-05e5-410d-b995-1b5f7f446daa"}, "d46e4021-3dc8-4bd8-bced-fc4fabda7c3f": {"doc_hash": "67282deda1230d30b4afaf5196282265843c646241a1508c8dc9934095441925", "ref_doc_id": "5aaef3ed-c7a0-4b55-b679-5fad8959b283"}, "4023bfd4-7339-4e80-9557-7544492e1bc5": {"doc_hash": "14d960fa3b2d1a54905892df98dab4a40fc7dffb502d7b6c46ab0c8481d48824", "ref_doc_id": "e0f20ab2-cf4e-43f6-bb0d-a998c38e63f6"}, "cc19044d-be9d-4f8c-a193-beebd29aeb75": {"doc_hash": "61612158bcdf4de2dfb2c80ceba981f2a30aa07b88f2f303d89a8b45cfa7ae81", "ref_doc_id": "3b4be816-716c-487b-8cb8-1fa3eaf70060"}, "5ce5ee54-71a2-4630-8537-9bee700c8cd1": {"doc_hash": "cd47079844ad944ea5ce3d03c1d96ee64c320cbd38ac1183c684d1db1035b1bd", "ref_doc_id": "eab24963-c5f1-4db8-b5e4-9a9dcc78bbeb"}, "0ba3b272-8108-4e27-ad1e-f299a4decb16": {"doc_hash": "e175d57bea188593921a5ef12a744fc44ca077f70d58b813ad06d98bb97949dc", "ref_doc_id": "fd558d3a-bbbd-4469-a5d9-37407de68fbf"}, "989507b8-d4f6-474b-a2b5-eb7115184462": {"doc_hash": "ba95756cb7345dd339577086c12d1abcb11c4f7f2b291fee83dd18783d20d2be", "ref_doc_id": "1e1692a1-aab8-46fd-870b-ca0de5950f7a"}, "6fa54a59-ede9-41ec-a26a-dcf5fde958b7": {"doc_hash": "bc461bf7c45f69ba3e2e16656f25737ff40d79a6a1be103e7ca751a7b1ea073c", "ref_doc_id": "492f26e7-34b3-45ba-801a-531824a6f307"}, "d442b39d-325b-4fcc-b611-1451523f279a": {"doc_hash": "8c5bff2473a32ebc39d71c9a7e617675133e67f244f215a648318c311e7bade4", "ref_doc_id": "717eefd6-75f1-44ff-8817-5aa70f56b554"}, "dc40fca0-96c3-4e69-a1d2-a09372e7aa11": {"doc_hash": "81ce9a22903e77e640843efb0b7d6bd86e37c85cfe74e7508ea00ed6a7d61777", "ref_doc_id": "e0cd431e-ad0c-45f0-a6e5-207c087fe494"}, "55c43a4d-85d5-4aac-a9cf-726606322bbe": {"doc_hash": "180cadfd33648924deb402eb5e45f06cb7ef2fef3af9444c92eabe01a78b607f", "ref_doc_id": "983ef592-fc28-44b2-a9a8-2ef46879f1ce"}, "d8d9475c-207f-409d-ad51-efb1dc40ac29": {"doc_hash": "e11e18e14a2bfa01ffd68ddbaadf32173f817a629b115f13fea2d5df9809bca9", "ref_doc_id": "86b3dac2-9179-40d3-b464-e76178b124bf"}, "d8f2f9b6-7855-430a-aa2b-16bcdc5a8527": {"doc_hash": "f980abf2308a039bc716e6c0dbb08bc650c53539b8e754135b0a6ad3a87c0baa", "ref_doc_id": "f08388fc-5680-40a0-9504-66e18baaaada"}, "6fa8f98d-d005-4856-9539-dbd1693c2a49": {"doc_hash": "f7bab55cd3bd034027e97cf05f778a3dbd80650890d019ab2fe6cbe5d9e06e50", "ref_doc_id": "126b16cd-8866-4519-979a-fbb79c99a932"}, "ada0f6ff-086d-41d1-a52d-0e458e57050f": {"doc_hash": "9631d36492aeaa2509e3ed5eaef6422f5e91b5f5f3c8806c5dd2a5db5ba21d54", "ref_doc_id": "61895bb2-23d8-47cc-b4b7-0c05bcfaeecd"}, "9b5bb047-bb25-4edd-90f9-ec19cf4a04de": {"doc_hash": "c2b2acdc808bd07c7735d5aea133cf5517e7b3312f5d5a24994a77f23503a158", "ref_doc_id": "c5da67f7-cb8a-4822-9aed-5886d8404d15"}, "6aaf7fa0-f7f4-45b8-9c86-fdd98956ff61": {"doc_hash": "39a23a1c3dfb45f32bf777154c2c3da047f7851196abc2fe9820fb1f67e8ca5a", "ref_doc_id": "bd41028f-590a-49f1-88df-bd062f17fd4a"}, "1917c322-83e2-4c3b-b51e-3c3b91d28e63": {"doc_hash": "e3fbec89f67fe1c6fe2e3efccf52b0f05205e85ac50a6b3cfdc6893475fbb067", "ref_doc_id": "f7dc9c37-f551-4a6e-a623-4c2e336a84a7"}, "da513750-5241-4602-be80-3b7edd0f3824": {"doc_hash": "85b2fea6f9d476762b737539ca3101fa234fe43ca3214ca2282cd22d88242ca1", "ref_doc_id": "5276800f-96e3-452e-aa87-d9c96a332856"}, "9e0a11ee-c4f7-41bd-8c2b-e0c2d39a5067": {"doc_hash": "e74c53c702e28ff8301cccff1df0cf054efd0569de31ce46bd9d12da916680a1", "ref_doc_id": "02b4f4f3-3929-4107-bcc9-dccc2f6fde59"}, "fa2eae93-a930-4735-98a0-f208e329207c": {"doc_hash": "d83f4b61e413f96c7dabb2eb3400a26464593285cc24916a107ead88ce2a34b3", "ref_doc_id": "304a7d0f-3c3e-4362-9359-c3d5b5777dd3"}, "ccff6b56-5230-4e30-a58f-203513ee8223": {"doc_hash": "b4b0a7a62cbe5b36c1395cd2d6fa4df001cd84d7f82dc9ea8c28a9a98fc6c5ca", "ref_doc_id": "8537139b-95f0-4aa1-82e6-e6c0686c7b93"}, "c9e3092c-942e-4b5b-9b34-178dc694e178": {"doc_hash": "ce3d85a3c612b2a25c46525ca2b61ff59cb4f18d7f9b65d4bec4e15fb59aa328", "ref_doc_id": "6d2e232e-c67b-4d67-818a-ca81529e7f92"}, "7137ccbf-b9f7-4a9c-ae4c-12fba3c78c38": {"doc_hash": "c3ac5092bb334e4d4911239a4ec5ef73b22f78f97ecdaa9e823f7be863f0d017", "ref_doc_id": "ad734571-2657-4907-8fc1-22e12393e69b"}, "b0a40b30-b1bf-4bde-ac31-56f5d404c874": {"doc_hash": "e94697bb69d35a7ac46a1be84dbb3460e22b6952351bb7c853cbff41393cb9b4", "ref_doc_id": "a9c17bef-cfe1-4da0-b79a-9043fa967d4f"}, "c60d27eb-6bb5-4363-a8a6-8a5fd3fa3bed": {"doc_hash": "c67a689a09b348a9f25d63ad45facccfe63cf102ec8e2a1b90f9501eb05f9ffc", "ref_doc_id": "5b72fad3-1ac5-426c-8a67-0b062ac2aa24"}, "699c7f56-58d2-4f9f-b095-5543ad619a8e": {"doc_hash": "5eb67d7548a92cf602506c39ae37d274dab8ad0a692074f32064d2f537703299", "ref_doc_id": "42d8241e-c9a5-4cae-9ebd-7c5168d2703f"}, "018101bc-3d92-4323-8f7c-793c858bba83": {"doc_hash": "8650e9f6a927333136c959061b12a4bb1e1e1b306d5a18bf0d243a67108fc47c", "ref_doc_id": "6de9d35f-0e37-457b-bf6e-b621a00458c9"}, "dc4d6639-6fb6-49a1-85bf-670b572da010": {"doc_hash": "c2e7ab14ed4235967c879365d0e032b8df99ead3d1eab8e0c6ca45c75900a55b", "ref_doc_id": "5fb4be06-fa68-46d2-bd2a-60c3500df870"}, "69778296-dfa3-44b1-b2ae-549b954a6a8f": {"doc_hash": "872e59bc6c09d785aecbe6c50d67fccadff01de7d9ccc241ea08e8d60157bc96", "ref_doc_id": "097a64d0-a747-4251-8a56-663b7c99f7b9"}, "b449a9b1-836b-4ffe-a656-fa1dcdcfc3c6": {"doc_hash": "cb6721817aa7be9fa612ab7c34dab15afda88949c4f2fb9bad3bfc11c6fc63b2", "ref_doc_id": "f1adb9f6-d60d-4cf8-9866-cbedc1ef30d1"}, "73b08989-3669-4118-9293-d457c2f58d02": {"doc_hash": "1962c4b51f887d5be754056fb67dd6653fd1e2d7d8957efd626fcddc4d72cc38", "ref_doc_id": "15f67db8-59ae-4e03-8fa8-a32d43fb4897"}, "df5138fc-f800-4612-b670-3d169b5b52fe": {"doc_hash": "a282aaac9317a4327bca58a3550123acf19a038810f51ef02d8be85a4d42599d", "ref_doc_id": "5f6a8165-b518-43f9-98f7-cbae8632931d"}, "175a053d-79e9-4cab-aecd-b33743b00019": {"doc_hash": "1a95685545f0a549970197b9d350a92027947e72f4bc1fdfc20249f8986419cd", "ref_doc_id": "48fc486b-3e45-4eb8-9cd3-7abf312194f1"}, "d47acd47-9612-4069-820d-c9f1a8a4305f": {"doc_hash": "95b39f3e98df56b4956433342f33f0c800a8956f3544e0008bb215de47ba9fed", "ref_doc_id": "ce3d2ef9-72fa-4372-bd47-e4c270b094c6"}, "ab8098c1-8ef6-4b5e-9929-1411d322665b": {"doc_hash": "5aff8f791de302963dbc49b1da74e45d2ee9c358bbea9e5d4f540e13287f7cb6", "ref_doc_id": "80fd7197-223c-41be-92ac-82cff791983c"}, "0ae79d6b-9679-4e8a-89f3-0d81a200131c": {"doc_hash": "20f91eb075bec1d916d2b1f3174813a602b1a48a5b260178e93aea00e7642e2a", "ref_doc_id": "94629437-77de-469e-b198-a264957172ba"}, "23b15c56-56d3-4322-8136-070545946120": {"doc_hash": "f185038db90a3503f0340ea4de55e04b5c175902b8efafeff990f62ecef2c49a", "ref_doc_id": "9710fcf5-be7b-4307-995a-a044827c7eae"}, "ebfc489f-02b1-49aa-ae19-ef351bd59a80": {"doc_hash": "daebeed5c3604438408fe556ff04951875d8362a2ed6fd8e0281e4770fb9756c", "ref_doc_id": "9a26dbff-c0b5-4f07-8e19-057c9c37ab10"}, "d817af21-a84b-4bd8-bc0e-530fa27921bd": {"doc_hash": "c8ca12db44f0f1c4479c4be6828af6a225c3e8e8e2e96cccaf4ec3cfec579eb8", "ref_doc_id": "effc38b6-67cc-4a2d-8120-0c0df34121d4"}, "ca3122e4-a77e-4c27-9702-50700e9c16c6": {"doc_hash": "8a1aa155d20660bc9b67ca0ca6aff68f500f003be6806470323fb49ba7f1c517", "ref_doc_id": "05a6157c-cb63-4311-8c4b-5842260982e4"}, "009141f8-67f7-452a-a37d-3b98f4d36440": {"doc_hash": "96022139b37b12f33683474f10a15578a7b01496081533295dfd449f2f7c5557", "ref_doc_id": "560d8476-0eb4-4264-b22a-3fa5bfb876a5"}, "3b786500-41b7-4103-abf4-16fe61447225": {"doc_hash": "4d827a267ba9e8a0cbab0eef03962ccd10463f74197a93cc9d36787ec5ce29f7", "ref_doc_id": "b5277ad7-283f-41e5-9995-d49586d4cb92"}, "d0433cd4-9744-4ff2-a556-48e2aaf450e7": {"doc_hash": "4eb3164083e8348f5a61e1c12398ff98d94d719f923e1ece4061c38739b2623f", "ref_doc_id": "77c7181f-a596-4142-a757-d270eaccf56b"}, "ab6d3e51-1b31-4bd6-a5d0-ffeb262805ad": {"doc_hash": "2acf72419621e91385d95791ee2daf08f9f19fdb8f16f4af8046a15810b1a39f", "ref_doc_id": "c767897f-b55a-4b94-bd50-cd45f5603ca8"}, "eb76921b-bb2d-47ea-83f0-b2d39ff1a5f2": {"doc_hash": "c88207180bb11b546d38049ba650aa028d69a8b7dad6659671803b3e8fc1d1e3", "ref_doc_id": "bfad893a-dda3-472a-bfd9-5e154c9dc397"}, "4c3e8cfa-02f3-4276-b531-df4013d1734e": {"doc_hash": "ed0126187ba01314a5c14c39c0471bd5a1fdbfc015e20950c56f175702b6bd9a", "ref_doc_id": "5cce5e56-9480-46e8-9baf-d6d0e1f634f3"}, "738cc6bf-32c8-4d6c-826b-86c31e6f9e6d": {"doc_hash": "a33dfaa21de4590aa47cb799383987570640ab3b138cfcd68822af2b22900d60", "ref_doc_id": "6836bd8c-15dc-4239-864a-2d93e1461e3b"}, "78c3b75b-7a60-4e63-8645-5404d79cd004": {"doc_hash": "8bc6b27998b0df400aaf460262c0730e226ee837005717c70e934cf0eaf0a584", "ref_doc_id": "3d432e47-783f-4545-b1bd-5139c3c15059"}, "6a43bb76-f705-4e83-8df6-77fe05367747": {"doc_hash": "a36b0f08c79d6986577a79155b7d92fdcae4521b3c40b5533749237d4176e373", "ref_doc_id": "853908f1-595d-4280-882e-879d1228ca47"}, "a940d782-a78b-4fd3-9270-c198e3281230": {"doc_hash": "bf6674b93570289947cb73bfdbab40bb7f585ff5015db477469d9afd51c7eaaa", "ref_doc_id": "c5877935-558a-4b3d-90f8-5066af04aed0"}, "04a6eded-c1ba-4f6b-9353-d37268b84945": {"doc_hash": "1b347e867c646890b66609b33f82d48a8fd509f1ccc875ed31e78dfe735c20a5", "ref_doc_id": "c06221d7-0038-4909-a32a-10beb9601cc5"}, "40e0c668-5f4b-40c4-b3ff-090f74aee5f9": {"doc_hash": "19862c81aeb6f0c9b5b41791ec40b64445a6e3eed167aadff0bda35c1c628628", "ref_doc_id": "ee500273-a3d5-49d8-9acc-091f39ada3a5"}, "2837d0ff-25fc-44a3-b8a7-a313f811c646": {"doc_hash": "caac5c01a759b6357daaf2f2261ac982af5bcf9237066927956a0f6e8edb7605", "ref_doc_id": "ac812169-2dbe-4ae8-8f5e-4db944318afa"}, "f120881f-f240-4591-b8ed-812bd92e1ce4": {"doc_hash": "d8a0a71a4ad58f7f1126161c385a7690a827e9e46f84a1dd35a4c945c3a7faf0", "ref_doc_id": "928cc9ae-35f5-4bf8-a84b-6147914d4e8a"}, "2d143041-3a1f-49c1-8b12-9e5982acfcba": {"doc_hash": "7506fe549f0a8c1220b979102738ef2cb1e6c494e6d89266f7608e18cc2e1b70", "ref_doc_id": "8d16451d-7b2d-4127-a41f-d1430e17378a"}, "13b12716-f2b1-46d3-923b-7857a3ebde7b": {"doc_hash": "15762110ace6a9e3ffe68b215b1b6efc87c77ba9a728ea882d61818ed9c6301d", "ref_doc_id": "e2a745d1-c965-4707-b4ab-55713cec3b46"}, "39f38b70-3d51-4914-8f57-cb0073e3948e": {"doc_hash": "67fc01042763abdc3c1c8c8165f6575b319c4a248abca6158c6d54a66ceb5c76", "ref_doc_id": "e822d38c-1118-46b7-89ed-6fa36726c5d6"}, "d4e39efd-ef3d-4f5b-b895-8234df8e531a": {"doc_hash": "f607480237c010b4a9ada644895eb6f8d7a34dac8bf18ef7d7922b7cf8d52eb9", "ref_doc_id": "0f92aa44-848f-46db-ace6-2f909b451715"}, "e16f0be8-ee70-42b7-9c87-b08f5ccffbee": {"doc_hash": "79a591a4b256e2c8d4132ff90bfaed2609ff8c25b2e3352d405f55f08d489f34", "ref_doc_id": "33c3afa9-7d09-49dc-b043-fd7549e02697"}, "43d85eab-56aa-404d-8a6c-5f569f0be93f": {"doc_hash": "81aed450a867b6484a370f5913ef7fb016c449e45fdd1d5950b41edf300853c3", "ref_doc_id": "ffc8974e-cefa-4a8b-8532-60a43124722f"}, "e1b37d06-9524-4968-bff0-c018b96c690f": {"doc_hash": "cf6a35c9a0681c6037bcd530df0ea919064abb8c374897c2eef2de383bfa667b", "ref_doc_id": "5fbf7d35-020c-4161-bbd9-282f30ff0783"}, "81f0d7cc-e154-458c-8fe0-1ab14de17ab3": {"doc_hash": "e957b5d33dae9f834455330b6e0af3716fbef9413a7956b8789f39df3676b7e7", "ref_doc_id": "b3d05ef5-22b1-4476-9f6b-29d23d41a99c"}, "a653b7b8-1908-4ae7-8c5b-fadb23b96c1d": {"doc_hash": "00466577c290820f6bbb1af8c1eebe643405be9eaaed9133a853c05d1c64426a", "ref_doc_id": "c385ae98-d182-49d1-af2d-babe1a090f6f"}, "509576d7-b2f8-440b-9dd2-23fe8a5fa0c1": {"doc_hash": "83291c231d6b65b58857b885caaddf4979941aefa879dfd05b64b1faeb840be7", "ref_doc_id": "32211d49-2073-48b4-8ff7-1f734f10a6b3"}, "4de06e6a-06ef-4d12-83ba-b9c7253cb552": {"doc_hash": "802dcd4ec88df1aaf5c290ed328a1ea4a49b95a68a80c89cb0729ab49328eaff", "ref_doc_id": "3e62bcc8-9676-48c7-b5e7-23c67f8ca9cb"}, "a507e63b-8fe6-4935-b765-a29166829d47": {"doc_hash": "0ac65e6d249bd77ffa3dd5c5833656c2798b30f15bc1cc7626df852dcb8b4ea1", "ref_doc_id": "07fd9c5c-57e0-4e10-bed5-2745789d0e70"}, "062c43cc-3bf7-4a67-a318-11330492a136": {"doc_hash": "af4aca06489239cf57cf3221fe591db7937a9870cdbc54f0c6b830b56adf28e5", "ref_doc_id": "2c9f70ba-b429-4425-a7b0-faf45f8f87be"}, "f3b5f6a5-5ab2-4eeb-9995-51dbc62e6026": {"doc_hash": "b4b4441ba9ce1035ede5d907ab5d69bf14bdd63a0bc5d33b5f527593f158aff1", "ref_doc_id": "18ef0941-93ff-42dc-a734-660d9b930f7c"}, "5372f5d7-2582-47ef-8b71-f1e410b9c3c4": {"doc_hash": "161c9d9a211d95175333225e02f1b25e6defd2b510ea7f8f7880716e07650755", "ref_doc_id": "656daad1-1fe0-4de6-84cf-c83fc5bb6f24"}, "84319c2f-37eb-49ac-8024-ef05b77288db": {"doc_hash": "ec04ab60eec8f7256056ba573f6b7c36e28352571a8f7f9fd8eeea7a51f87bcd", "ref_doc_id": "8955b9b8-b58a-41a4-bcfc-55eb3bfd1b93"}, "e29898ed-9490-41ad-96da-6895c9f657be": {"doc_hash": "389b21b2401786972d63add03cef083c33e5fc97eb05ceffca3eeaf60f9a5ce6", "ref_doc_id": "397457e8-308a-4713-94ab-883b715a2a67"}, "a002ac36-61a7-44ca-9dde-c6db7b6d6097": {"doc_hash": "eb0f636261a00d9d245c3386963f3ae294f9379fcf428efeae322361874057bb", "ref_doc_id": "ab540cf0-82ba-494d-ab73-f5c4e3a97bd9"}, "40c0cba8-4942-4aa4-b683-db27a40dd5a9": {"doc_hash": "4d9fd8526e0399352c0e2307687a80f18a8e7f7b30c940f9c4b13d2985e450d9", "ref_doc_id": "b9b9a2f5-a75e-492a-90dd-6617e54b0b80"}, "e897a7b9-9bcc-4806-b322-a71bde74d230": {"doc_hash": "8b50045fba70743300f1d53f8fb91e17bfb78b79ca4d13c9cd27ac85a9fc3286", "ref_doc_id": "a51a6fa6-02c9-4e5d-859f-1021e93cabbe"}, "8020bdcd-949e-479c-a4cb-4feb516a0d9f": {"doc_hash": "424e3fe94a2906db67de6b6464d1440c271cd2f29fa9072c1691db4cdc82e966", "ref_doc_id": "f62047b8-29c1-4507-aff8-886f2f84387b"}, "f00dc602-1d44-45d0-938d-f91f95dc3f09": {"doc_hash": "3aace01c6714f6c702ff8bb0f5cdd46d16e2409dfb39ac55842e3ff620dfe8f5", "ref_doc_id": "7c9250d3-ade2-46d3-b651-687b499fad8f"}, "b856c401-a290-43ab-bf1f-1509000bb591": {"doc_hash": "21d1cfae5e660bc56fc759bb48918a6282f069bb06d693910b8d4b3382c05c32", "ref_doc_id": "613e48f1-6a4e-45a9-bd81-6a4cf41dbcb7"}, "3539a9a9-7781-4e12-a8a8-d4941c031bef": {"doc_hash": "22a45228bb85255af912584389f369868288b642913d0ed6dadf339f860d2426", "ref_doc_id": "d10b4bba-644b-4f1e-8905-26ff299c52c4"}, "1984ab8a-5c18-423e-9d40-a1a3d9793bee": {"doc_hash": "6cc3b9f0beb7c0e25844d02fa710da40b0d3c99ade196a8a129afa0fff7575ed", "ref_doc_id": "e4e1e00a-dc67-4746-9699-3add70bf247c"}, "6545d33f-26c0-43f0-ad39-5b044e97d1c6": {"doc_hash": "fe8799f3b7e4832a4852b198db1f905db424a68044115a55fbdc348d093eda8c", "ref_doc_id": "68d3b583-4b4b-4cf9-9661-d40e639c7d84"}, "de1c3eb5-6100-4c2e-88b5-54d9553743d7": {"doc_hash": "e1245990c16b5cbe687b001dca1f5ebf25418afa45429f5cfc7e14463dc40e26", "ref_doc_id": "68d3b583-4b4b-4cf9-9661-d40e639c7d84"}, "889a3660-0620-4518-a83c-b8f692658d97": {"doc_hash": "452f8708d463e462cf2e6389c6847aa33d132ba52ee925a23980ec5906d1a33e", "ref_doc_id": "fefecc41-9720-411d-9e18-8b2087abd80c"}, "77f0f40c-d5f4-4334-ad1c-4c269c4a5382": {"doc_hash": "5d58554dc590b29c36a7eab63ddf3742dfa00d5ef1734621060e9872a3e0444f", "ref_doc_id": "2e2552ba-c063-48e5-9d9d-03148eff3553"}, "df25089f-6287-4466-9eee-01a4ac93b621": {"doc_hash": "a910154b4623e0968274e33ae83675a4c7322aecbc9cfe9524733454263b9792", "ref_doc_id": "3b4af78f-dfbb-4a17-ba98-4c9db7e74835"}, "4d7bca40-c464-4262-a036-8b5b0fe2bfa6": {"doc_hash": "60ecf0bd845b695d04ca75db3e22ddca88d26b4575b207765d464da7afb05534", "ref_doc_id": "5b66a0c3-0d2b-4040-b753-4ca068f46d45"}, "ce74b24f-9b5c-4a0f-8800-df3ee01374fa": {"doc_hash": "e89d25ad03ef86e5cfd27e1046d9dec6fcf0439cc69d1ab71157f4f051cfd963", "ref_doc_id": "b955caf2-228a-4001-ace2-365da4d78617"}, "fd33374a-a801-4cdb-9718-bca9893063ad": {"doc_hash": "b12ce2dcf7cad2508fa35ce79d5ca23f6196d8de951ef5e88282801b9acc88cf", "ref_doc_id": "063e8cf1-f44a-4f0a-a7d6-da2b1b7fdca8"}, "d2578d28-073c-4da0-80ae-37f1e17077a4": {"doc_hash": "336217e241d253b8cd4df3b26a40ec7fcc7a79d952be095e867d060418887d48", "ref_doc_id": "4aaf6edd-5198-4cd2-964a-7b0955556f5d"}, "606325b1-1772-4deb-95a6-6da8f9fd3438": {"doc_hash": "687e71a4841a81dd208882a0125d8bf50128e5ce3aa45b66d6d9eeab23144973", "ref_doc_id": "64608d2b-9958-42f8-b7d1-d31f57b2a7f6"}, "ae9c5ebe-ccf6-40ef-b4bc-7b1d3979081d": {"doc_hash": "b08505d4cffb9acf5a39f8cce903f04a9cb6c3b637722d63e84734f96a70369e", "ref_doc_id": "55645ccd-a764-4c82-8a5b-d5c1bdb4977f"}, "bc24e162-302d-4645-a4e5-2c5a38efda88": {"doc_hash": "2e19fbdf93a8b6e2745164186d2e50cfad5241cd6675aea89f9e426aac417f18", "ref_doc_id": "6c9c85b2-4210-453f-ae49-4804109196de"}, "a6a4194a-90d4-4c1c-bc96-ff6dfd17ee81": {"doc_hash": "f55984444e55025b32e3ba3c6979d19cbba11553729ba922190f9d69731747f1", "ref_doc_id": "e8f083ac-d9bc-4dff-8791-3514d3f1221e"}, "cf445afd-d4cc-49e4-973b-2e95bb6d81f3": {"doc_hash": "b88be3a22d9f22fa8addc2d2bb9ec6284c8ad242137f24a437608ffd19976469", "ref_doc_id": "e8f083ac-d9bc-4dff-8791-3514d3f1221e"}, "9d83e7d1-c2a4-4c7b-9656-b5760bfe2787": {"doc_hash": "8efe150eefeeba082bd9ab056ae5086d46fa29c3f4fbb736c50ca945dfa67129", "ref_doc_id": "57a0300a-28b0-4de2-bf3d-482f16b49a6d"}, "fd851b36-0457-457b-8c7e-47a2b4f3e81d": {"doc_hash": "1730dedf34b2ec2bb422501f4404a236a512732c2ed794f971482db39723e667", "ref_doc_id": "fec5a5df-e972-44a8-9721-4a6a5bd71f4c"}, "a3866778-7d08-48de-9558-b255ef53cde5": {"doc_hash": "15669350a121c93ff0dfbb8eaa666c4c922fa78410d718fec145daf14acb8f79", "ref_doc_id": "512837ee-867d-4121-8de8-ba4ac5dd4ddb"}, "8bf41e91-3dba-4515-888d-6edaabfbbe14": {"doc_hash": "826d8ca86009ce75fe962b403a0072bd681f9db4590e752f82294c19324e328b", "ref_doc_id": "afd9f82c-3eca-4281-b9aa-3d0696d81217"}, "81d02376-5daa-4180-99b4-310e9610b631": {"doc_hash": "d5de90418bb4fce2240a6f25dddd8f4db85773905a7df8af81dd19287c98f606", "ref_doc_id": "7536d56a-0f50-4193-99b2-5211a824a7af"}, "a8f5e2fd-0bff-42eb-8742-7c43413a3ee8": {"doc_hash": "2fcf2bc1c83aebc4513059ffc1453109dfe98e1a1a730b38531720feb8a58d73", "ref_doc_id": "7536d56a-0f50-4193-99b2-5211a824a7af"}, "9245306f-beb5-4254-802c-8963c01f79a7": {"doc_hash": "4b8b62961f05fafc5557946c950b85f95a80c3b013d13c13fb905e2b23599161", "ref_doc_id": "f514bc10-07f7-4289-b367-16ad3e66bea9"}, "7c565a9e-fc17-4e4f-b29a-e4434618c3ea": {"doc_hash": "96370a815c948fab5fb0604f64b50dc24a07c638fb2d5f790d3f72248ea20806", "ref_doc_id": "f514bc10-07f7-4289-b367-16ad3e66bea9"}, "5a693e7e-99df-4ede-81b6-8dc141f90ef2": {"doc_hash": "2ff9389e4e0e7a163179cf4de15982d6052119193a3cad72f68c48bf94bc3861", "ref_doc_id": "641b07f9-8b08-4fc6-bb19-5fb76f470409"}, "f20d8c72-0584-4d23-ac18-eb554985e933": {"doc_hash": "cab4521cc8ed0b6a5a198ecef801a3f54eec41d68c86abc8fbceca644ca3aa1d", "ref_doc_id": "2277f387-c738-46eb-8fd3-5261f944ae81"}, "e9038e64-d2b0-4162-aaef-1b3dd5c6a8f2": {"doc_hash": "a54f43cba9aaa5a8016a7de10c9aaac6254a83e1625f8c9b59a31a527df46eab", "ref_doc_id": "cc9b6c1e-7161-463f-86cd-c63038729c4c"}}, "docstore/data": {"d824f03e-7afe-4bae-bc25-fccfbb343a7e": {"__data__": {"id_": "d824f03e-7afe-4bae-bc25-fccfbb343a7e", "embedding": null, "metadata": {"page_label": "i", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "718eff5e-0d3f-43ac-8226-5232381f4af0", "node_type": "4", "metadata": {"page_label": "i", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f2af3fe376f5b62fb5f2eb636867457b6c77d5e24e7457afd63dd8853e682c41", "class_name": "RelatedNodeInfo"}}, "text": "Dive into Deep Learning\nASTON ZHANG, ZACHARY C. LIPTON, MU LI, AND ALEXANDER J.\nSMOLA", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 85, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "733d022d-092e-45d9-95a6-aacded131edf": {"__data__": {"id_": "733d022d-092e-45d9-95a6-aacded131edf", "embedding": null, "metadata": {"page_label": "ii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68413b82-f021-4b71-91fe-e2bb6ca0eba6", "node_type": "4", "metadata": {"page_label": "ii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5078d9da3d977f63e97a759e44688c64192e2dd217feb532f1ad57f801b64f3e", "class_name": "RelatedNodeInfo"}}, "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ac3d829-791e-493b-8b5e-6a90dfdc8c82": {"__data__": {"id_": "4ac3d829-791e-493b-8b5e-6a90dfdc8c82", "embedding": null, "metadata": {"page_label": "iii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84480dbe-b03c-42bf-9b51-3e044e5d48a4", "node_type": "4", "metadata": {"page_label": "iii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a39e157b68b0eac36d25a445661f4fcadee7d8c77134f2b4d3a6c4ecdbacea31", "class_name": "RelatedNodeInfo"}}, "text": "Contents\nPreface pagexxv\nInstallation xxxiv\nNotation xxxvii\n1 Introduction 1\n1.1 A Motivating Example 2\n1.2 Key Components 4\n1.3 Kinds of Machine Learning Problems 7\n1.4 Roots 20\n1.5 The Road to Deep Learning 22\n1.6 Success Stories 25\n1.7 The Essence of Deep Learning 27\n1.8 Summary 29\n1.9 Exercises 29\n2 Preliminaries 30\n2.1 Data Manipulation 30\n2.1.1 Getting Started 30\n2.1.2 Indexing and Slicing 33\n2.1.3 Operations 34\n2.1.4 Broadcasting 35\n2.1.5 Saving Memory 36\n2.1.6 Conversion to Other Python Objects 37\n2.1.7 Summary 37\n2.1.8 Exercises 38\n2.2 Data Preprocessing 38\n2.2.1 Reading the Dataset 38\n2.2.2 Data Preparation 39\n2.2.3 Conversion to the Tensor Format 40\n2.2.4 Discussion 40\n2.2.5 Exercises 40\n2.3 Linear Algebra 41\n2.3.1 Scalars 41\niii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9071df74-4cbd-46d6-af21-f7fd4b3fc58a": {"__data__": {"id_": "9071df74-4cbd-46d6-af21-f7fd4b3fc58a", "embedding": null, "metadata": {"page_label": "iv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9829986e-2155-4196-a42f-1c6e40ac130f", "node_type": "4", "metadata": {"page_label": "iv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2f18301b716ab11ee3a2a63c654beace0569538952d6ea742f3f9052cd85de40", "class_name": "RelatedNodeInfo"}}, "text": "2.3.2 Vectors 42\n2.3.3 Matrices 43\n2.3.4 Tensors 44\n2.3.5 Basic Properties of Tensor Arithmetic 45\n2.3.6 Reduction 46\n2.3.7 Non-Reduction Sum 47\n2.3.8 Dot Products 48\n2.3.9 Matrix\u2013Vector Products 48\n2.3.10 Matrix\u2013Matrix Multiplication 49\n2.3.11 Norms 50\n2.3.12 Discussion 52\n2.3.13 Exercises 53\n2.4 Calculus 54\n2.4.1 Derivatives and Differentiation 54\n2.4.2 Visualization Utilities 56\n2.4.3 Partial Derivatives and Gradients 58\n2.4.4 Chain Rule 58\n2.4.5 Discussion 59\n2.4.6 Exercises 59\n2.5 Automatic Differentiation 60\n2.5.1 A Simple Function 60\n2.5.2 Backward for Non-Scalar Variables 61\n2.5.3 Detaching Computation 62\n2.5.4 Gradients and Python Control Flow 63\n2.5.5 Discussion 64\n2.5.6 Exercises 64\n2.6 Probability and Statistics 65\n2.6.1 A Simple Example: Tossing Coins 66\n2.6.2 A More Formal Treatment 68\n2.6.3 Random Variables 69\n2.6.4 Multiple Random Variables 70\n2.6.5 An Example 73\n2.6.6 Expectations 74\n2.6.7 Discussion 76\n2.6.8 Exercises 77\n2.7 Documentation 78\n2.7.1 Functions and Classes in a Module 78\n2.7.2 Specific Functions and Classes 79\n3 Linear Neural Networks for Regression 82\n3.1 Linear Regression 82\n3.1.1 Basics 83\n3.1.2 Vectorization for Speed 88\n3.1.3 The Normal Distribution and Squared Loss 88\n3.1.4 Linear Regression as a Neural Network 90\niv", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e88c2583-26da-4339-9d3a-0ab76027e03b": {"__data__": {"id_": "e88c2583-26da-4339-9d3a-0ab76027e03b", "embedding": null, "metadata": {"page_label": "v", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79806eb3-8406-4f54-bdbd-95a12dcf08ed", "node_type": "4", "metadata": {"page_label": "v", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "98ebab5d8e342b5765eb38b149a57216a83303f20a3312e896774205f44bbc7c", "class_name": "RelatedNodeInfo"}}, "text": "3.1.5 Summary 91\n3.1.6 Exercises 92\n3.2 Object-Oriented Design for Implementation 93\n3.2.1 Utilities 94\n3.2.2 Models 96\n3.2.3 Data 97\n3.2.4 Training 97\n3.2.5 Summary 98\n3.2.6 Exercises 98\n3.3 Synthetic Regression Data 99\n3.3.1 Generating the Dataset 99\n3.3.2 Reading the Dataset 100\n3.3.3 Concise Implementation of the Data Loader 101\n3.3.4 Summary 102\n3.3.5 Exercises 102\n3.4 Linear Regression Implementation from Scratch 103\n3.4.1 Defining the Model 103\n3.4.2 Defining the Loss Function 104\n3.4.3 Defining the Optimization Algorithm 104\n3.4.4 Training 105\n3.4.5 Summary 107\n3.4.6 Exercises 107\n3.5 Concise Implementation of Linear Regression 108\n3.5.1 Defining the Model 109\n3.5.2 Defining the Loss Function 109\n3.5.3 Defining the Optimization Algorithm 110\n3.5.4 Training 110\n3.5.5 Summary 111\n3.5.6 Exercises 111\n3.6 Generalization 112\n3.6.1 Training Error and Generalization Error 113\n3.6.2 Underfitting or Overfitting? 115\n3.6.3 Model Selection 116\n3.6.4 Summary 117\n3.6.5 Exercises 117\n3.7 Weight Decay 118\n3.7.1 Norms and Weight Decay 119\n3.7.2 High-Dimensional Linear Regression 120\n3.7.3 Implementation from Scratch 121\n3.7.4 Concise Implementation 122\n3.7.5 Summary 124\n3.7.6 Exercises 124\n4 Linear Neural Networks for Classi\ufb01cation 125\n4.1 Softmax Regression 125\nv", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51b54416-e865-40dd-b796-b3ae2fa4bd0d": {"__data__": {"id_": "51b54416-e865-40dd-b796-b3ae2fa4bd0d", "embedding": null, "metadata": {"page_label": "vi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4e48a309-2246-45e8-802d-53cf0501fb06", "node_type": "4", "metadata": {"page_label": "vi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e053f3006acc572062bc177daa3afd491c7f759d4cb66017c67c6bd2f2775e5b", "class_name": "RelatedNodeInfo"}}, "text": "4.1.1 Classification 126\n4.1.2 Loss Function 129\n4.1.3 Information Theory Basics 130\n4.1.4 Summary and Discussion 131\n4.1.5 Exercises 132\n4.2 The Image Classification Dataset 134\n4.2.1 Loading the Dataset 134\n4.2.2 Reading a Minibatch 135\n4.2.3 Visualization 136\n4.2.4 Summary 137\n4.2.5 Exercises 137\n4.3 The Base Classification Model 138\n4.3.1 TheClassifier Class 138\n4.3.2 Accuracy 138\n4.3.3 Summary 139\n4.3.4 Exercises 139\n4.4 Softmax Regression Implementation from Scratch 140\n4.4.1 The Softmax 140\n4.4.2 The Model 141\n4.4.3 The Cross-Entropy Loss 141\n4.4.4 Training 142\n4.4.5 Prediction 143\n4.4.6 Summary 143\n4.4.7 Exercises 144\n4.5 Concise Implementation of Softmax Regression 144\n4.5.1 Defining the Model 145\n4.5.2 Softmax Revisited 145\n4.5.3 Training 146\n4.5.4 Summary 146\n4.5.5 Exercises 147\n4.6 Generalization in Classification 147\n4.6.1 The Test Set 148\n4.6.2 Test Set Reuse 150\n4.6.3 Statistical Learning Theory 151\n4.6.4 Summary 153\n4.6.5 Exercises 154\n4.7 Environment and Distribution Shift 154\n4.7.1 Types of Distribution Shift 155\n4.7.2 Examples of Distribution Shift 157\n4.7.3 Correction of Distribution Shift 159\n4.7.4 A Taxonomy of Learning Problems 163\n4.7.5 Fairness, Accountability, and Transparency in Machine\nLearning 164\n4.7.6 Summary 165\n4.7.7 Exercises 166\nvi", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fbce58b-8128-4ce4-b322-170528063ddc": {"__data__": {"id_": "3fbce58b-8128-4ce4-b322-170528063ddc", "embedding": null, "metadata": {"page_label": "vii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9478ab6-7eaf-4ce6-bbbc-d5258f31b157", "node_type": "4", "metadata": {"page_label": "vii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7946e42c1fcf99918efd14a87628b0b935fe79561c05ecf3168009d43481cf04", "class_name": "RelatedNodeInfo"}}, "text": "5 Multilayer Perceptrons 167\n5.1 Multilayer Perceptrons 167\n5.1.1 Hidden Layers 167\n5.1.2 Activation Functions 171\n5.1.3 Summary and Discussion 174\n5.1.4 Exercises 175\n5.2 Implementation of Multilayer Perceptrons 176\n5.2.1 Implementation from Scratch 176\n5.2.2 Concise Implementation 177\n5.2.3 Summary 178\n5.2.4 Exercises 179\n5.3 Forward Propagation, Backward Propagation, and Computational Graphs 180\n5.3.1 Forward Propagation 180\n5.3.2 Computational Graph of Forward Propagation 181\n5.3.3 Backpropagation 181\n5.3.4 Training Neural Networks 183\n5.3.5 Summary 183\n5.3.6 Exercises 183\n5.4 Numerical Stability and Initialization 184\n5.4.1 Vanishing and Exploding Gradients 184\n5.4.2 Parameter Initialization 187\n5.4.3 Summary 188\n5.4.4 Exercises 189\n5.5 Generalization in Deep Learning 189\n5.5.1 Revisiting Overfitting and Regularization 190\n5.5.2 Inspiration from Nonparametrics 191\n5.5.3 Early Stopping 192\n5.5.4 Classical Regularization Methods for Deep Networks 193\n5.5.5 Summary 193\n5.5.6 Exercises 194\n5.6 Dropout 194\n5.6.1 Dropout in Practice 195\n5.6.2 Implementation from Scratch 196\n5.6.3 Concise Implementation 197\n5.6.4 Summary 198\n5.6.5 Exercises 198\n5.7 Predicting House Prices on Kaggle 199\n5.7.1 Downloading Data 199\n5.7.2 Kaggle 200\n5.7.3 Accessing and Reading the Dataset 201\n5.7.4 Data Preprocessing 201\n5.7.5 Error Measure 203\n5.7.6 \ud835\udc3e-Fold Cross-Validation 204\n5.7.7 Model Selection 204\n5.7.8 Submitting Predictions on Kaggle 205\nvii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eadb352f-c537-4734-81b2-392af6f7014a": {"__data__": {"id_": "eadb352f-c537-4734-81b2-392af6f7014a", "embedding": null, "metadata": {"page_label": "viii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bc9a497f-1214-4008-a29e-fad59826e416", "node_type": "4", "metadata": {"page_label": "viii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c90d1c3344fa8c700815edf6e02e69af037548f25ad0ac57d283f87b3e669154", "class_name": "RelatedNodeInfo"}}, "text": "5.7.9 Summary and Discussion 206\n5.7.10 Exercises 206\n6 Builders\u2019 Guide 207\n6.1 Layers and Modules 207\n6.1.1 A Custom Module 209\n6.1.2 The Sequential Module 211\n6.1.3 Executing Code in the Forward Propagation Method 211\n6.1.4 Summary 213\n6.1.5 Exercises 213\n6.2 Parameter Management 213\n6.2.1 Parameter Access 214\n6.2.2 Tied Parameters 215\n6.2.3 Summary 216\n6.2.4 Exercises 216\n6.3 Parameter Initialization 216\n6.3.1 Built-in Initialization 217\n6.3.2 Summary 219\n6.3.3 Exercises 219\n6.4 Lazy Initialization 219\n6.4.1 Summary 220\n6.4.2 Exercises 221\n6.5 Custom Layers 221\n6.5.1 Layers without Parameters 221\n6.5.2 Layers with Parameters 222\n6.5.3 Summary 223\n6.5.4 Exercises 223\n6.6 File I/O 223\n6.6.1 Loading and Saving Tensors 224\n6.6.2 Loading and Saving Model Parameters 225\n6.6.3 Summary 226\n6.6.4 Exercises 226\n6.7 GPUs 226\n6.7.1 Computing Devices 227\n6.7.2 Tensors and GPUs 228\n6.7.3 Neural Networks and GPUs 230\n6.7.4 Summary 231\n6.7.5 Exercises 231\n7 Convolutional Neural Networks 233\n7.1 From Fully Connected Layers to Convolutions 234\n7.1.1 Invariance 234\n7.1.2 Constraining the MLP 235\n7.1.3 Convolutions 237\n7.1.4 Channels 238\nviii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bd3cd9f-ea0a-4096-aa21-0f6881fa2b2a": {"__data__": {"id_": "3bd3cd9f-ea0a-4096-aa21-0f6881fa2b2a", "embedding": null, "metadata": {"page_label": "ix", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fdae7f2-4ed0-41ad-9971-a8272a8fd4b8", "node_type": "4", "metadata": {"page_label": "ix", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c777671c11cc83215ac5a373a036287779c722ee66dd7ab5d3736476e4e2c4bd", "class_name": "RelatedNodeInfo"}}, "text": "7.1.5 Summary and Discussion 239\n7.1.6 Exercises 239\n7.2 Convolutions for Images 240\n7.2.1 The Cross-Correlation Operation 240\n7.2.2 Convolutional Layers 242\n7.2.3 Object Edge Detection in Images 242\n7.2.4 Learning a Kernel 244\n7.2.5 Cross-Correlation and Convolution 245\n7.2.6 Feature Map and Receptive Field 245\n7.2.7 Summary 246\n7.2.8 Exercises 247\n7.3 Padding and Stride 247\n7.3.1 Padding 248\n7.3.2 Stride 250\n7.3.3 Summary and Discussion 251\n7.3.4 Exercises 251\n7.4 Multiple Input and Multiple Output Channels 252\n7.4.1 Multiple Input Channels 252\n7.4.2 Multiple Output Channels 253\n7.4.3 1\u00021Convolutional Layer 255\n7.4.4 Discussion 256\n7.4.5 Exercises 256\n7.5 Pooling 257\n7.5.1 Maximum Pooling and Average Pooling 258\n7.5.2 Padding and Stride 260\n7.5.3 Multiple Channels 261\n7.5.4 Summary 261\n7.5.5 Exercises 262\n7.6 Convolutional Neural Networks (LeNet) 262\n7.6.1 LeNet 263\n7.6.2 Training 265\n7.6.3 Summary 266\n7.6.4 Exercises 266\n8 Modern Convolutional Neural Networks 268\n8.1 Deep Convolutional Neural Networks (AlexNet) 269\n8.1.1 Representation Learning 270\n8.1.2 AlexNet 273\n8.1.3 Training 276\n8.1.4 Discussion 276\n8.1.5 Exercises 277\n8.2 Networks Using Blocks (VGG) 278\n8.2.1 VGG Blocks 279\n8.2.2 VGG Network 279\n8.2.3 Training 281\nix", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f5db6d4-b49a-49e3-a339-d4253926c36d": {"__data__": {"id_": "1f5db6d4-b49a-49e3-a339-d4253926c36d", "embedding": null, "metadata": {"page_label": "x", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "020cc405-8012-470b-8ca8-192e424019a4", "node_type": "4", "metadata": {"page_label": "x", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "59e8200309dc7c91e942d18995d4097c003a7cbe08b9c98935e62164ed05c1f4", "class_name": "RelatedNodeInfo"}}, "text": "8.2.4 Summary 282\n8.2.5 Exercises 282\n8.3 Network in Network (NiN) 283\n8.3.1 NiN Blocks 283\n8.3.2 NiN Model 284\n8.3.3 Training 285\n8.3.4 Summary 286\n8.3.5 Exercises 286\n8.4 Multi-Branch Networks (GoogLeNet) 287\n8.4.1 Inception Blocks 287\n8.4.2 GoogLeNet Model 288\n8.4.3 Training 291\n8.4.4 Discussion 291\n8.4.5 Exercises 292\n8.5 Batch Normalization 292\n8.5.1 Training Deep Networks 293\n8.5.2 Batch Normalization Layers 295\n8.5.3 Implementation from Scratch 297\n8.5.4 LeNet with Batch Normalization 298\n8.5.5 Concise Implementation 299\n8.5.6 Discussion 300\n8.5.7 Exercises 301\n8.6 Residual Networks (ResNet) and ResNeXt 302\n8.6.1 Function Classes 302\n8.6.2 Residual Blocks 304\n8.6.3 ResNet Model 306\n8.6.4 Training 308\n8.6.5 ResNeXt 308\n8.6.6 Summary and Discussion 310\n8.6.7 Exercises 311\n8.7 Densely Connected Networks (DenseNet) 312\n8.7.1 From ResNet to DenseNet 312\n8.7.2 Dense Blocks 313\n8.7.3 Transition Layers 314\n8.7.4 DenseNet Model 315\n8.7.5 Training 315\n8.7.6 Summary and Discussion 316\n8.7.7 Exercises 316\n8.8 Designing Convolution Network Architectures 317\n8.8.1 The AnyNet Design Space 318\n8.8.2 Distributions and Parameters of Design Spaces 320\n8.8.3 RegNet 322\n8.8.4 Training 323\n8.8.5 Discussion 323\n8.8.6 Exercises 324\nx", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f2b435c-1f9f-4244-8865-f0ecbcf51166": {"__data__": {"id_": "4f2b435c-1f9f-4244-8865-f0ecbcf51166", "embedding": null, "metadata": {"page_label": "xi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e12f008-0c37-4675-9d4c-25703a56040a", "node_type": "4", "metadata": {"page_label": "xi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4d1626c64d76473ae4f51ca8b7bf94f21ea43d2e7e28ef0878999ae43f8322ed", "class_name": "RelatedNodeInfo"}}, "text": "9 Recurrent Neural Networks 325\n9.1 Working with Sequences 327\n9.1.1 Autoregressive Models 328\n9.1.2 Sequence Models 330\n9.1.3 Training 331\n9.1.4 Prediction 333\n9.1.5 Summary 335\n9.1.6 Exercises 335\n9.2 Converting Raw Text into Sequence Data 336\n9.2.1 Reading the Dataset 336\n9.2.2 Tokenization 337\n9.2.3 Vocabulary 337\n9.2.4 Putting It All Together 338\n9.2.5 Exploratory Language Statistics 339\n9.2.6 Summary 341\n9.2.7 Exercises 342\n9.3 Language Models 342\n9.3.1 Learning Language Models 343\n9.3.2 Perplexity 345\n9.3.3 Partitioning Sequences 346\n9.3.4 Summary and Discussion 347\n9.3.5 Exercises 348\n9.4 Recurrent Neural Networks 348\n9.4.1 Neural Networks without Hidden States 349\n9.4.2 Recurrent Neural Networks with Hidden States 349\n9.4.3 RNN-Based Character-Level Language Models 351\n9.4.4 Summary 352\n9.4.5 Exercises 352\n9.5 Recurrent Neural Network Implementation from Scratch 352\n9.5.1 RNN Model 353\n9.5.2 RNN-Based Language Model 354\n9.5.3 Gradient Clipping 356\n9.5.4 Training 357\n9.5.5 Decoding 358\n9.5.6 Summary 359\n9.5.7 Exercises 359\n9.6 Concise Implementation of Recurrent Neural Networks 360\n9.6.1 Defining the Model 360\n9.6.2 Training and Predicting 361\n9.6.3 Summary 362\n9.6.4 Exercises 362\n9.7 Backpropagation Through Time 362\n9.7.1 Analysis of Gradients in RNNs 362\n9.7.2 Backpropagation Through Time in Detail 365\n9.7.3 Summary 368\nxi", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a659beeb-9513-49c8-b514-9eccae8ad32b": {"__data__": {"id_": "a659beeb-9513-49c8-b514-9eccae8ad32b", "embedding": null, "metadata": {"page_label": "xii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "625db00f-e7c2-4132-8c75-cc7160e87160", "node_type": "4", "metadata": {"page_label": "xii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f9da88544b4e6759c75e3a84d4d259e4fe09b5fdc8dbf21ed7ecb1f30642b3bb", "class_name": "RelatedNodeInfo"}}, "text": "9.7.4 Exercises 368\n10 Modern Recurrent Neural Networks 369\n10.1 Long Short-Term Memory (LSTM) 370\n10.1.1 Gated Memory Cell 370\n10.1.2 Implementation from Scratch 373\n10.1.3 Concise Implementation 375\n10.1.4 Summary 376\n10.1.5 Exercises 376\n10.2 Gated Recurrent Units (GRU) 376\n10.2.1 Reset Gate and Update Gate 377\n10.2.2 Candidate Hidden State 378\n10.2.3 Hidden State 378\n10.2.4 Implementation from Scratch 379\n10.2.5 Concise Implementation 380\n10.2.6 Summary 381\n10.2.7 Exercises 381\n10.3 Deep Recurrent Neural Networks 382\n10.3.1 Implementation from Scratch 383\n10.3.2 Concise Implementation 384\n10.3.3 Summary 385\n10.3.4 Exercises 385\n10.4 Bidirectional Recurrent Neural Networks 385\n10.4.1 Implementation from Scratch 387\n10.4.2 Concise Implementation 387\n10.4.3 Summary 388\n10.4.4 Exercises 388\n10.5 Machine Translation and the Dataset 388\n10.5.1 Downloading and Preprocessing the Dataset 389\n10.5.2 Tokenization 390\n10.5.3 Loading Sequences of Fixed Length 391\n10.5.4 Reading the Dataset 392\n10.5.5 Summary 393\n10.5.6 Exercises 394\n10.6 The Encoder\u0000Decoder Architecture 394\n10.6.1 Encoder 394\n10.6.2 Decoder 395\n10.6.3 Putting the Encoder and Decoder Together 395\n10.6.4 Summary 396\n10.6.5 Exercises 396\n10.7 Sequence-to-Sequence Learning for Machine Translation 396\n10.7.1 Teacher Forcing 397\n10.7.2 Encoder 397\n10.7.3 Decoder 399\n10.7.4 Encoder\u2013Decoder for Sequence-to-Sequence Learning 400\nxii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10279548-9563-4846-9cfe-9b14b61999d9": {"__data__": {"id_": "10279548-9563-4846-9cfe-9b14b61999d9", "embedding": null, "metadata": {"page_label": "xiii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1eb5e137-c72c-4e63-b7c2-b7153801d13a", "node_type": "4", "metadata": {"page_label": "xiii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9d81c29b9d56da32881634b81709ed6bb2d4e2753e1959e6daf94fe71b019891", "class_name": "RelatedNodeInfo"}}, "text": "10.7.5 Loss Function with Masking 401\n10.7.6 Training 401\n10.7.7 Prediction 402\n10.7.8 Evaluation of Predicted Sequences 403\n10.7.9 Summary 404\n10.7.10 Exercises 404\n10.8 Beam Search 405\n10.8.1 Greedy Search 405\n10.8.2 Exhaustive Search 407\n10.8.3 Beam Search 407\n10.8.4 Summary 408\n10.8.5 Exercises 408\n11 Attention Mechanisms and Transformers 409\n11.1 Queries, Keys, and Values 411\n11.1.1 Visualization 413\n11.1.2 Summary 414\n11.1.3 Exercises 414\n11.2 Attention Pooling by Similarity 415\n11.2.1 Kernels and Data 415\n11.2.2 Attention Pooling via Nadaraya\u2013Watson Regression 417\n11.2.3 Adapting Attention Pooling 418\n11.2.4 Summary 419\n11.2.5 Exercises 420\n11.3 Attention Scoring Functions 420\n11.3.1 Dot Product Attention 421\n11.3.2 Convenience Functions 421\n11.3.3 Scaled Dot Product Attention 423\n11.3.4 Additive Attention 424\n11.3.5 Summary 426\n11.3.6 Exercises 426\n11.4 The Bahdanau Attention Mechanism 427\n11.4.1 Model 428\n11.4.2 Defining the Decoder with Attention 428\n11.4.3 Training 430\n11.4.4 Summary 431\n11.4.5 Exercises 432\n11.5 Multi-Head Attention 432\n11.5.1 Model 433\n11.5.2 Implementation 433\n11.5.3 Summary 435\n11.5.4 Exercises 435\n11.6 Self-Attention and Positional Encoding 435\n11.6.1 Self-Attention 436\n11.6.2 Comparing CNNs, RNNs, and Self-Attention 436\nxiii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd5b61ac-21c6-4a55-a278-1974e0e29e92": {"__data__": {"id_": "cd5b61ac-21c6-4a55-a278-1974e0e29e92", "embedding": null, "metadata": {"page_label": "xiv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e50abfaf-1789-47b9-8d95-1af68955910b", "node_type": "4", "metadata": {"page_label": "xiv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "155cff4aed96f4242ca043d81e97adcdcd36bd36dd7465c5bf52c9700f968aff", "class_name": "RelatedNodeInfo"}}, "text": "11.6.3 Positional Encoding 437\n11.6.4 Summary 440\n11.6.5 Exercises 440\n11.7 The Transformer Architecture 440\n11.7.1 Model 441\n11.7.2 Positionwise Feed-Forward Networks 442\n11.7.3 Residual Connection and Layer Normalization 443\n11.7.4 Encoder 444\n11.7.5 Decoder 445\n11.7.6 Training 447\n11.7.7 Summary 451\n11.7.8 Exercises 451\n11.8 Transformers for Vision 451\n11.8.1 Model 452\n11.8.2 Patch Embedding 453\n11.8.3 Vision Transformer Encoder 453\n11.8.4 Putting It All Together 454\n11.8.5 Training 455\n11.8.6 Summary and Discussion 455\n11.8.7 Exercises 456\n11.9 Large-Scale Pretraining with Transformers 456\n11.9.1 Encoder-Only 457\n11.9.2 Encoder\u2013Decoder 459\n11.9.3 Decoder-Only 461\n11.9.4 Scalability 463\n11.9.5 Large Language Models 465\n11.9.6 Summary and Discussion 466\n11.9.7 Exercises 467\n12 Optimization Algorithms 468\n12.1 Optimization and Deep Learning 468\n12.1.1 Goal of Optimization 469\n12.1.2 Optimization Challenges in Deep Learning 469\n12.1.3 Summary 473\n12.1.4 Exercises 473\n12.2 Convexity 474\n12.2.1 Definitions 474\n12.2.2 Properties 476\n12.2.3 Constraints 479\n12.2.4 Summary 481\n12.2.5 Exercises 482\n12.3 Gradient Descent 482\n12.3.1 One-Dimensional Gradient Descent 482\n12.3.2 Multivariate Gradient Descent 486\n12.3.3 Adaptive Methods 488\nxiv", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb04e546-3cfa-40e9-8be5-2a80074711c5": {"__data__": {"id_": "bb04e546-3cfa-40e9-8be5-2a80074711c5", "embedding": null, "metadata": {"page_label": "xv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7956e872-9719-427d-b09e-502e92e1afb8", "node_type": "4", "metadata": {"page_label": "xv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7526e1fc679e96f972a57d3a94ba320e5fe4d09b9f918589e8efd8fd4d332a70", "class_name": "RelatedNodeInfo"}}, "text": "12.3.4 Summary 492\n12.3.5 Exercises 492\n12.4 Stochastic Gradient Descent 493\n12.4.1 Stochastic Gradient Updates 493\n12.4.2 Dynamic Learning Rate 495\n12.4.3 Convergence Analysis for Convex Objectives 496\n12.4.4 Stochastic Gradients and Finite Samples 498\n12.4.5 Summary 499\n12.4.6 Exercises 499\n12.5 Minibatch Stochastic Gradient Descent 500\n12.5.1 Vectorization and Caches 500\n12.5.2 Minibatches 503\n12.5.3 Reading the Dataset 504\n12.5.4 Implementation from Scratch 504\n12.5.5 Concise Implementation 507\n12.5.6 Summary 509\n12.5.7 Exercises 509\n12.6 Momentum 510\n12.6.1 Basics 510\n12.6.2 Practical Experiments 514\n12.6.3 Theoretical Analysis 516\n12.6.4 Summary 518\n12.6.5 Exercises 519\n12.7 Adagrad 519\n12.7.1 Sparse Features and Learning Rates 519\n12.7.2 Preconditioning 520\n12.7.3 The Algorithm 521\n12.7.4 Implementation from Scratch 523\n12.7.5 Concise Implementation 524\n12.7.6 Summary 524\n12.7.7 Exercises 525\n12.8 RMSProp 525\n12.8.1 The Algorithm 526\n12.8.2 Implementation from Scratch 526\n12.8.3 Concise Implementation 528\n12.8.4 Summary 528\n12.8.5 Exercises 529\n12.9 Adadelta 529\n12.9.1 The Algorithm 529\n12.9.2 Implementation 530\n12.9.3 Summary 531\n12.9.4 Exercises 532\n12.10 Adam 532\n12.10.1 The Algorithm 532\n12.10.2 Implementation 533\nxv", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1247, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3c30157-ab36-4772-bfd4-7366d3b329b6": {"__data__": {"id_": "d3c30157-ab36-4772-bfd4-7366d3b329b6", "embedding": null, "metadata": {"page_label": "xvi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca9742ea-0801-4e54-93a2-2aa9863274ad", "node_type": "4", "metadata": {"page_label": "xvi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7c02ce6a5802ad26cbf7ae7235086cb7ed66e4ad1f4f398990f4eb5926654d28", "class_name": "RelatedNodeInfo"}}, "text": "12.10.3 Yogi 534\n12.10.4 Summary 535\n12.10.5 Exercises 536\n12.11 Learning Rate Scheduling 536\n12.11.1 Toy Problem 537\n12.11.2 Schedulers 539\n12.11.3 Policies 540\n12.11.4 Summary 545\n12.11.5 Exercises 545\n13 Computational Performance 547\n13.1 Compilers and Interpreters 547\n13.1.1 Symbolic Programming 548\n13.1.2 Hybrid Programming 549\n13.1.3 Hybridizing the Sequential Class 550\n13.1.4 Summary 552\n13.1.5 Exercises 552\n13.2 Asynchronous Computation 552\n13.2.1 Asynchrony via Backend 553\n13.2.2 Barriers and Blockers 554\n13.2.3 Improving Computation 555\n13.2.4 Summary 555\n13.2.5 Exercises 555\n13.3 Automatic Parallelism 555\n13.3.1 Parallel Computation on GPUs 556\n13.3.2 Parallel Computation and Communication 557\n13.3.3 Summary 558\n13.3.4 Exercises 559\n13.4 Hardware 559\n13.4.1 Computers 560\n13.4.2 Memory 561\n13.4.3 Storage 562\n13.4.4 CPUs 563\n13.4.5 GPUs and other Accelerators 566\n13.4.6 Networks and Buses 569\n13.4.7 More Latency Numbers 570\n13.4.8 Summary 571\n13.4.9 Exercises 571\n13.5 Training on Multiple GPUs 572\n13.5.1 Splitting the Problem 573\n13.5.2 Data Parallelism 574\n13.5.3 A Toy Network 575\n13.5.4 Data Synchronization 576\n13.5.5 Distributing Data 577\n13.5.6 Training 578\nxvi", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f0ee467-7666-47f5-8fb4-cd7c3d1d2c66": {"__data__": {"id_": "9f0ee467-7666-47f5-8fb4-cd7c3d1d2c66", "embedding": null, "metadata": {"page_label": "xvii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5937be6b-7e42-440c-a403-e75af91ccc9c", "node_type": "4", "metadata": {"page_label": "xvii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b074b294f1509adc613585960be93f7c5699feeb11791799af939fd3cb4ab0fc", "class_name": "RelatedNodeInfo"}}, "text": "13.5.7 Summary 580\n13.5.8 Exercises 580\n13.6 Concise Implementation for Multiple GPUs 581\n13.6.1 A Toy Network 581\n13.6.2 Network Initialization 582\n13.6.3 Training 582\n13.6.4 Summary 583\n13.6.5 Exercises 584\n13.7 Parameter Servers 584\n13.7.1 Data-Parallel Training 584\n13.7.2 Ring Synchronization 586\n13.7.3 Multi-Machine Training 588\n13.7.4 Key\u2013Value Stores 589\n13.7.5 Summary 591\n13.7.6 Exercises 591\n14 Computer Vision 592\n14.1 Image Augmentation 592\n14.1.1 Common Image Augmentation Methods 593\n14.1.2 Training with Image Augmentation 596\n14.1.3 Summary 599\n14.1.4 Exercises 599\n14.2 Fine-Tuning 600\n14.2.1 Steps 600\n14.2.2 Hot Dog Recognition 601\n14.2.3 Summary 605\n14.2.4 Exercises 606\n14.3 Object Detection and Bounding Boxes 606\n14.3.1 Bounding Boxes 607\n14.3.2 Summary 609\n14.3.3 Exercises 609\n14.4 Anchor Boxes 609\n14.4.1 Generating Multiple Anchor Boxes 610\n14.4.2 Intersection over Union (IoU) 612\n14.4.3 Labeling Anchor Boxes in Training Data 613\n14.4.4 PredictingBoundingBoxeswithNon-MaximumSuppression 619\n14.4.5 Summary 622\n14.4.6 Exercises 623\n14.5 Multiscale Object Detection 623\n14.5.1 Multiscale Anchor Boxes 623\n14.5.2 Multiscale Detection 625\n14.5.3 Summary 626\n14.5.4 Exercises 626\n14.6 The Object Detection Dataset 627\n14.6.1 Downloading the Dataset 627\nxvii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1283, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50240af0-e371-4bd2-bd53-7293a9cad051": {"__data__": {"id_": "50240af0-e371-4bd2-bd53-7293a9cad051", "embedding": null, "metadata": {"page_label": "xviii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f131e00b-603d-4242-a728-fc6ddcd552aa", "node_type": "4", "metadata": {"page_label": "xviii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6dfd7d6d87e025cb9074979f31eb4e416ba59a2a569f61efd99952140e9a74cc", "class_name": "RelatedNodeInfo"}}, "text": "14.6.2 Reading the Dataset 627\n14.6.3 Demonstration 629\n14.6.4 Summary 629\n14.6.5 Exercises 630\n14.7 Single Shot Multibox Detection 630\n14.7.1 Model 630\n14.7.2 Training 636\n14.7.3 Prediction 638\n14.7.4 Summary 639\n14.7.5 Exercises 640\n14.8 Region-based CNNs (R-CNNs) 642\n14.8.1 R-CNNs 642\n14.8.2 Fast R-CNN 643\n14.8.3 Faster R-CNN 645\n14.8.4 Mask R-CNN 646\n14.8.5 Summary 647\n14.8.6 Exercises 647\n14.9 Semantic Segmentation and the Dataset 648\n14.9.1 Image Segmentation and Instance Segmentation 648\n14.9.2 The Pascal VOC2012 Semantic Segmentation Dataset 648\n14.9.3 Summary 654\n14.9.4 Exercises 654\n14.10 Transposed Convolution 654\n14.10.1 Basic Operation 654\n14.10.2 Padding, Strides, and Multiple Channels 656\n14.10.3 Connection to Matrix Transposition 657\n14.10.4 Summary 659\n14.10.5 Exercises 659\n14.11 Fully Convolutional Networks 659\n14.11.1 The Model 660\n14.11.2 Initializing Transposed Convolutional Layers 662\n14.11.3 Reading the Dataset 663\n14.11.4 Training 664\n14.11.5 Prediction 664\n14.11.6 Summary 666\n14.11.7 Exercises 666\n14.12 Neural Style Transfer 666\n14.12.1 Method 666\n14.12.2 Reading the Content and Style Images 668\n14.12.3 Preprocessing and Postprocessing 668\n14.12.4 Extracting Features 669\n14.12.5 Defining the Loss Function 670\n14.12.6 Initializing the Synthesized Image 672\n14.12.7 Training 673\n14.12.8 Summary 674\nxviii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1347, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "590d9183-af52-409b-99e2-28a524a7582b": {"__data__": {"id_": "590d9183-af52-409b-99e2-28a524a7582b", "embedding": null, "metadata": {"page_label": "xix", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c3379b8-6495-43da-81f6-82621a32bb47", "node_type": "4", "metadata": {"page_label": "xix", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "85ddefbcfc39ae8007ff23299f259a7a56c481d02c93aa8de539dc3d1306a9f5", "class_name": "RelatedNodeInfo"}}, "text": "14.12.9 Exercises 674\n14.13 Image Classification (CIFAR-10) on Kaggle 674\n14.13.1 Obtaining and Organizing the Dataset 675\n14.13.2 Image Augmentation 678\n14.13.3 Reading the Dataset 678\n14.13.4 Defining the Model 679\n14.13.5 Defining the Training Function 679\n14.13.6 Training and Validating the Model 680\n14.13.7 Classifying the Testing Set and Submitting Results on Kaggle 680\n14.13.8 Summary 681\n14.13.9 Exercises 682\n14.14 Dog Breed Identification (ImageNet Dogs) on Kaggle 682\n14.14.1 Obtaining and Organizing the Dataset 682\n14.14.2 Image Augmentation 684\n14.14.3 Reading the Dataset 685\n14.14.4 Fine-Tuning a Pretrained Model 685\n14.14.5 Defining the Training Function 686\n14.14.6 Training and Validating the Model 687\n14.14.7 Classifying the Testing Set and Submitting Results on Kaggle 688\n14.14.8 Summary 688\n14.14.9 Exercises 689\n15 Natural Language Processing: Pretraining 690\n15.1 Word Embedding (word2vec) 691\n15.1.1 One-Hot Vectors Are a Bad Choice 691\n15.1.2 Self-Supervised word2vec 691\n15.1.3 The Skip-Gram Model 692\n15.1.4 The Continuous Bag of Words (CBOW) Model 694\n15.1.5 Summary 695\n15.1.6 Exercises 695\n15.2 Approximate Training 696\n15.2.1 Negative Sampling 696\n15.2.2 Hierarchical Softmax 698\n15.2.3 Summary 699\n15.2.4 Exercises 699\n15.3 The Dataset for Pretraining Word Embeddings 699\n15.3.1 Reading the Dataset 699\n15.3.2 Subsampling 700\n15.3.3 Extracting Center Words and Context Words 702\n15.3.4 Negative Sampling 703\n15.3.5 Loading Training Examples in Minibatches 704\n15.3.6 Putting It All Together 705\n15.3.7 Summary 706\n15.3.8 Exercises 706\n15.4 Pretraining word2vec 707\nxix", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7931a6d6-a775-4d34-bbe4-5f6d0822ba06": {"__data__": {"id_": "7931a6d6-a775-4d34-bbe4-5f6d0822ba06", "embedding": null, "metadata": {"page_label": "xx", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23adfc25-5664-4baf-ab8a-1beac1691f15", "node_type": "4", "metadata": {"page_label": "xx", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d7ae9fd6809b2abde79258cd5c8db89e29753643179f6a4effe1ea6d78006784", "class_name": "RelatedNodeInfo"}}, "text": "15.4.1 The Skip-Gram Model 707\n15.4.2 Training 708\n15.4.3 Applying Word Embeddings 711\n15.4.4 Summary 711\n15.4.5 Exercises 711\n15.5 Word Embedding with Global Vectors (GloVe) 711\n15.5.1 Skip-Gram with Global Corpus Statistics 712\n15.5.2 The GloVe Model 713\n15.5.3 Interpreting GloVe from the Ratio of Co-occurrence\nProbabilities 713\n15.5.4 Summary 715\n15.5.5 Exercises 715\n15.6 Subword Embedding 715\n15.6.1 The fastText Model 715\n15.6.2 Byte Pair Encoding 716\n15.6.3 Summary 719\n15.6.4 Exercises 719\n15.7 Word Similarity and Analogy 720\n15.7.1 Loading Pretrained Word Vectors 720\n15.7.2 Applying Pretrained Word Vectors 722\n15.7.3 Summary 724\n15.7.4 Exercises 724\n15.8 Bidirectional Encoder Representations from Transformers (BERT) 724\n15.8.1 From Context-Independent to Context-Sensitive 724\n15.8.2 From Task-Specific to Task-Agnostic 725\n15.8.3 BERT: Combining the Best of Both Worlds 725\n15.8.4 Input Representation 726\n15.8.5 Pretraining Tasks 728\n15.8.6 Putting It All Together 731\n15.8.7 Summary 732\n15.8.8 Exercises 733\n15.9 The Dataset for Pretraining BERT 733\n15.9.1 Defining Helper Functions for Pretraining Tasks 734\n15.9.2 Transforming Text into the Pretraining Dataset 736\n15.9.3 Summary 738\n15.9.4 Exercises 739\n15.10 Pretraining BERT 739\n15.10.1 Pretraining BERT 739\n15.10.2 Representing Text with BERT 741\n15.10.3 Summary 742\n15.10.4 Exercises 743\n16 Natural Language Processing: Applications 744\n16.1 Sentiment Analysis and the Dataset 745\n16.1.1 Reading the Dataset 745\nxx", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1490, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d6e4149-8f6a-471e-a4e2-58b010c887b7": {"__data__": {"id_": "8d6e4149-8f6a-471e-a4e2-58b010c887b7", "embedding": null, "metadata": {"page_label": "xxi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a86e926-5a74-4ed2-a224-6ffc6784f2ba", "node_type": "4", "metadata": {"page_label": "xxi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7a630ab7568b450a7e4fcaf4cfe12be37aedd210711ba679d7235f554488a91b", "class_name": "RelatedNodeInfo"}}, "text": "16.1.2 Preprocessing the Dataset 746\n16.1.3 Creating Data Iterators 747\n16.1.4 Putting It All Together 747\n16.1.5 Summary 748\n16.1.6 Exercises 748\n16.2 Sentiment Analysis: Using Recurrent Neural Networks 748\n16.2.1 Representing Single Text with RNNs 749\n16.2.2 Loading Pretrained Word Vectors 750\n16.2.3 Training and Evaluating the Model 751\n16.2.4 Summary 751\n16.2.5 Exercises 752\n16.3 Sentiment Analysis: Using Convolutional Neural Networks 752\n16.3.1 One-Dimensional Convolutions 753\n16.3.2 Max-Over-Time Pooling 754\n16.3.3 The textCNN Model 755\n16.3.4 Summary 758\n16.3.5 Exercises 758\n16.4 Natural Language Inference and the Dataset 759\n16.4.1 Natural Language Inference 759\n16.4.2 The Stanford Natural Language Inference (SNLI) Dataset 760\n16.4.3 Summary 763\n16.4.4 Exercises 763\n16.5 Natural Language Inference: Using Attention 763\n16.5.1 The Model 764\n16.5.2 Training and Evaluating the Model 768\n16.5.3 Summary 770\n16.5.4 Exercises 770\n16.6 Fine-Tuning BERT for Sequence-Level and Token-Level Applications 771\n16.6.1 Single Text Classification 771\n16.6.2 Text Pair Classification or Regression 772\n16.6.3 Text Tagging 773\n16.6.4 Question Answering 773\n16.6.5 Summary 774\n16.6.6 Exercises 774\n16.7 Natural Language Inference: Fine-Tuning BERT 775\n16.7.1 Loading Pretrained BERT 775\n16.7.2 The Dataset for Fine-Tuning BERT 776\n16.7.3 Fine-Tuning BERT 778\n16.7.4 Summary 779\n16.7.5 Exercises 779\n17 Reinforcement Learning 781\n17.1 Markov Decision Process (MDP) 782\n17.1.1 Definition of an MDP 782\n17.1.2 Return and Discount Factor 783\nxxi", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ddb7105-bfe1-41b8-a8fd-a999a33c51f4": {"__data__": {"id_": "4ddb7105-bfe1-41b8-a8fd-a999a33c51f4", "embedding": null, "metadata": {"page_label": "xxii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "755050d1-ae98-4329-a2ac-a91dadf0a1a2", "node_type": "4", "metadata": {"page_label": "xxii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "edfb1df63eda93fd907ed65b6a41f954a95676d1eb8f991306974122adf8d458", "class_name": "RelatedNodeInfo"}}, "text": "17.1.3 Discussion of the Markov Assumption 784\n17.1.4 Summary 785\n17.1.5 Exercises 785\n17.2 Value Iteration 785\n17.2.1 Stochastic Policy 785\n17.2.2 Value Function 786\n17.2.3 Action-Value Function 786\n17.2.4 Optimal Stochastic Policy 787\n17.2.5 Principle of Dynamic Programming 787\n17.2.6 Value Iteration 788\n17.2.7 Policy Evaluation 788\n17.2.8 Implementation of Value Iteration 789\n17.2.9 Summary 790\n17.2.10 Exercises 791\n17.3 Q-Learning 791\n17.3.1 The Q-Learning Algorithm 791\n17.3.2 An Optimization Problem Underlying Q-Learning 791\n17.3.3 Exploration in Q-Learning 793\n17.3.4 The \u201cSelf-correcting\u201d Property of Q-Learning 793\n17.3.5 Implementation of Q-Learning 794\n17.3.6 Summary 795\n17.3.7 Exercises 796\n18 Gaussian Processes 797\n18.1 Introduction to Gaussian Processes 798\n18.1.1 Summary 807\n18.1.2 Exercises 808\n18.2 Gaussian Process Priors 809\n18.2.1 Definition 809\n18.2.2 A Simple Gaussian Process 810\n18.2.3 From Weight Space to Function Space 811\n18.2.4 The Radial Basis Function (RBF) Kernel 811\n18.2.5 The Neural Network Kernel 813\n18.2.6 Summary 814\n18.2.7 Exercises 814\n18.3 Gaussian Process Inference 815\n18.3.1 Posterior Inference for Regression 815\n18.3.2 Equations for Making Predictions and Learning Kernel\nHyperparameters in GP Regression 817\n18.3.3 Interpreting Equations for Learning and Predictions 817\n18.3.4 Worked Example from Scratch 818\n18.3.5 Making Life Easy with GPyTorch 822\n18.3.6 Summary 825\n18.3.7 Exercises 826\nxxii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54e8887f-8f0d-47ed-8caf-11d09abfe87e": {"__data__": {"id_": "54e8887f-8f0d-47ed-8caf-11d09abfe87e", "embedding": null, "metadata": {"page_label": "xxiii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8cc005f2-60fa-4609-acec-094babedb29b", "node_type": "4", "metadata": {"page_label": "xxiii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b0251315c325fce33ee1317e2e47bb27efb7c2e90bab0b9a85d50e533a73ac97", "class_name": "RelatedNodeInfo"}}, "text": "19 Hyperparameter Optimization 828\n19.1 What Is Hyperparameter Optimization? 828\n19.1.1 The Optimization Problem 829\n19.1.2 Random Search 832\n19.1.3 Summary 834\n19.1.4 Exercises 835\n19.2 Hyperparameter Optimization API 836\n19.2.1 Searcher 836\n19.2.2 Scheduler 837\n19.2.3 Tuner 837\n19.2.4 Bookkeeping the Performance of HPO Algorithms 838\n19.2.5 Example: Optimizing the Hyperparameters of a Convolu-\ntional Neural Network 839\n19.2.6 Comparing HPO Algorithms 841\n19.2.7 Summary 842\n19.2.8 Exercises 842\n19.3 Asynchronous Random Search 843\n19.3.1 Objective Function 844\n19.3.2 Asynchronous Scheduler 845\n19.3.3 Visualize the Asynchronous Optimization Process 851\n19.3.4 Summary 852\n19.3.5 Exercises 853\n19.4 Multi-Fidelity Hyperparameter Optimization 853\n19.4.1 Successive Halving 855\n19.4.2 Summary 866\n19.5 Asynchronous Successive Halving 867\n19.5.1 Objective Function 869\n19.5.2 Asynchronous Scheduler 870\n19.5.3 Visualize the Optimization Process 879\n19.5.4 Summary 879\n20 Generative Adversarial Networks 880\n20.1 Generative Adversarial Networks 880\n20.1.1 Generate Some \u201cReal\u201d Data 882\n20.1.2 Generator 883\n20.1.3 Discriminator 883\n20.1.4 Training 883\n20.1.5 Summary 885\n20.1.6 Exercises 885\n20.2 Deep Convolutional Generative Adversarial Networks 886\n20.2.1 The Pokemon Dataset 886\n20.2.2 The Generator 887\n20.2.3 Discriminator 889\n20.2.4 Training 891\n20.2.5 Summary 892\nxxiii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1496c6c7-7bef-4855-a34a-225211f5e342": {"__data__": {"id_": "1496c6c7-7bef-4855-a34a-225211f5e342", "embedding": null, "metadata": {"page_label": "xxiv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3e6c603-b7d7-464c-87f7-68992ff22852", "node_type": "4", "metadata": {"page_label": "xxiv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "caced76ebce96314ab03a82db9f97fb66f88d098bc1e810a8bd05b7e67a867d4", "class_name": "RelatedNodeInfo"}}, "text": "xxiv Contents\n20.2.6 Exercises 892\n21 Recommender Systems 893\n21.1 Overview of Recommender Systems 893\n21.1.1 Collaborative Filtering 894\n21.1.2 Explicit Feedback and Implicit Feedback 895\n21.1.3 Recommendation Tasks 895\n21.1.4 Summary 895\n21.1.5 Exercises 895\nAppendix A Mathematics for Deep Learning 897\nAppendix B Tools for Deep Learning 1035\nReferences 1089", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 361, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3eab7099-37aa-4f51-a768-aba28481320f": {"__data__": {"id_": "3eab7099-37aa-4f51-a768-aba28481320f", "embedding": null, "metadata": {"page_label": "xxv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b39b720a-aba3-401e-9e2c-2097e339cc6f", "node_type": "4", "metadata": {"page_label": "xxv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a198b87397acb6716787234770e42d2dc14744c1c29b8f4dc20b25b03107578d", "class_name": "RelatedNodeInfo"}}, "text": "Preface\nJust a few years ago, there were no legions of deep learning scientists developing intelli-\ngent products and services at major companies and startups. When we entered the field,\nmachinelearningdidnotcommandheadlinesindailynewspapers. Ourparentshadnoidea\nwhatmachinelearningwas, letalonewhywemightpreferittoacareerinmedicineorlaw.\nMachine learning was a blue skies academic discipline whose industrial significance was\nlimited to a narrow set of real-world applications, including speech recognition and com-\nputer vision. Moreover, many of these applications required so much domain knowledge\nthat they were often regarded as entirely separate areas for which machine learning was\none small component. At that time, neural networks\u2014the predecessors of the deep learn-\ning methods that we focus on in this book\u2014were generally regarded as outmoded.\nYet in just few years, deep learning has taken the world by surprise, driving rapid progress\nin such diverse fields as computer vision, natural language processing, automatic speech\nrecognition, reinforcement learning, and biomedical informatics. Moreover, the success\nof deep learning in so many tasks of practical interest has even catalyzed developments in\ntheoreticalmachinelearningandstatistics. Withtheseadvancesinhand, wecannowbuild\ncars that drive themselves with more autonomy than ever before (though less autonomy\nthansomecompaniesmighthaveyoubelieve),dialoguesystemsthatdebugcodebyasking\nclarifying questions, and software agents beating the best human players in the world at\nboardgamessuchas Go, a featonce thought tobe decades away. Already, thesetools exert\never-wider influence on industry and society, changing the way movies are made, diseases\nare diagnosed, and playing a growing role in basic sciences\u2014from astrophysics, to climate\nmodeling, to weather prediction, to biomedicine.\nAboutThis Book\nThis book represents our attempt to make deep learning approachable, teaching you the\nconcepts , thecontext, and thecode.\nOne Medium Combining Code, Math, and HTML\nFor any computing technology to reach its full impact, it must be well understood, well\ndocumented, and supported by mature, well-maintained tools. The key ideas should be\nclearly distilled, minimizing the onboarding time needed to bring new practitioners up to\nxxv", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2302, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c524c6d5-3fdd-4a1f-a53c-85daa3af1204": {"__data__": {"id_": "c524c6d5-3fdd-4a1f-a53c-85daa3af1204", "embedding": null, "metadata": {"page_label": "xxvi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16bc8577-167b-416d-b8de-db9bd3ebc3aa", "node_type": "4", "metadata": {"page_label": "xxvi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ddd90571006a76786c903cad98317c23ccd398b9203b757c7d5e9e1545c32efd", "class_name": "RelatedNodeInfo"}}, "text": "xxvi Preface\n1\n2date. Mature libraries should automate common tasks, and exemplar code should make\nit easy for practitioners to modify, apply, and extend common applications to suit their\nneeds.\nAs an example, take dynamic web applications. Despite a large number of companies,\nsuch as Amazon, developing successful database-driven web applications in the 1990s, the\npotentialofthistechnologytoaidcreativeentrepreneurswasrealizedtoafargreaterdegree\nonly in the past ten years, owing in part to the development of powerful, well-documented\nframeworks.\nTesting the potential of deep learning presents unique challenges because any single appli-\ncationbringstogethervariousdisciplines. Applyingdeeplearningrequiressimultaneously\nunderstanding (i) the motivations for casting a problem in a particular way; (ii) the math-\nematical form of a given model; (iii) the optimization algorithms for fitting the models to\ndata;(iv)thestatisticalprinciplesthattelluswhenweshouldexpectourmodelstogeneral-\nize to unseen data and practical methods for certifying that they have, in fact, generalized;\nand (v) the engineering techniques required to train models efficiently, navigating the pit-\nfalls of numerical computing and getting the most out of available hardware. Teaching the\ncritical thinking skills required to formulate problems, the mathematics to solve them, and\nthe software tools to implement those solutions all in one place presents formidable chal-\nlenges. Ourgoalinthisbookistopresentaunifiedresourcetobringwould-bepractitioners\nup to speed.\nWhenwestartedthisbookproject,therewerenoresourcesthatsimultaneously(i)remained\nup to date; (ii) covered the breadth of modern machine learning practices with sufficient\ntechnical depth; and (iii) interleaved exposition of the quality one expects of a textbook\nwith the clean runnable code that one expects of a hands-on tutorial. We found plenty of\ncode examples illustrating how to use a given deep learning framework (e.g., how to do\nbasic numerical computing with matrices in TensorFlow) or for implementing particular\ntechniques (e.g., code snippets for LeNet, AlexNet, ResNet, etc.) scattered across various\nblog posts and GitHub repositories. However, these examples typically focused on howto\nimplement a given approach, but left out the discussion of whycertain algorithmic deci-\nsionsaremade. Whilesomeinteractiveresourceshavepoppedupsporadicallytoaddressa\nparticulartopic,e.g.,theengagingblogpostspublishedonthewebsite Distill1,orpersonal\nblogs,theyonlycoveredselectedtopicsindeeplearning, andoftenlackedassociatedcode.\nOn the other hand, while several deep learning textbooks have emerged\u2014e.g., Goodfellow\net al.(2016), which offers a comprehensive survey on the basics of deep learning\u2014these\nresources do not marry the descriptions to realizations of the concepts in code, sometimes\nleaving readers clueless as to how to implement them. Moreover, too many resources are\nhidden behind the paywalls of commercial course providers.\nWesetouttocreatearesourcethatcould(i)befreelyavailableforeveryone; (ii)offersuffi-\ncienttechnicaldepthtoprovideastartingpointonthepathtoactuallybecominganapplied\nmachine learning scientist; (iii) include runnable code, showing readers howto solve prob-\nlemsinpractice;(iv)allowforrapidupdates,bothbyusandalsobythecommunityatlarge;\nand (v) be complemented by a forum2for interactive discussion of technical details and to\nanswer questions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e435bd9-f4b8-4863-bb5d-666584c289dd": {"__data__": {"id_": "3e435bd9-f4b8-4863-bb5d-666584c289dd", "embedding": null, "metadata": {"page_label": "xxvii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a036a6c9-2a57-4805-92bb-a42b6ebc9dc4", "node_type": "4", "metadata": {"page_label": "xxvii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9c3a3d56e84e4dfa271c40d741df0408aa290829a9978356ac7f92b73e7d27c4", "class_name": "RelatedNodeInfo"}}, "text": "xxvii Preface\nThesegoalswereofteninconflict. Equations,theorems,andcitationsarebestmanagedand\nlaid out in LaTeX. Code is best described in Python. And webpages are native in HTML\nandJavaScript. Furthermore,wewantthecontenttobeaccessiblebothasexecutablecode,\nasaphysicalbook,asadownloadablePDF,andontheInternetasawebsite. Noworkflows\nseemed suited to these demands, so we decided to assemble our own ( Section B.6 ). We\nsettled on GitHub to share the source and to facilitate community contributions; Jupyter\nnotebooksformixingcode,equationsandtext;Sphinxasarenderingengine;andDiscourse\nasadiscussionplatform. Whileoursystemisnotperfect,thesechoicesstrikeacompromise\namongthecompetingconcerns. Webelievethat DiveintoDeepLearning mightbethefirst\nbook published using such an integrated workflow.\nLearningbyDoing\nMany textbooks present concepts in succession, covering each in exhaustive detail. For\nexample, the excellent textbook of Bishop ( 2006), teaches each topic so thoroughly that\ngetting to the chapter on linear regression requires a nontrivial amount of work. While\nexpertslovethisbookpreciselyforitsthoroughness,fortruebeginners,thispropertylimits\nits usefulness as an introductory text.\nIn this book, we teach most concepts just in time . In other words, you will learn concepts\nat the very moment that they are needed to accomplish some practical end. While we\ntake some time at the outset to teach fundamental preliminaries, like linear algebra and\nprobability,wewantyoutotastethesatisfactionoftrainingyourfirstmodelbeforeworrying\nabout more esoteric concepts.\nAside from a few preliminary notebooks that provide a crash course in the basic mathe-\nmatical background, each subsequent chapter both introduces a reasonable number of new\nconcepts and provides several self-contained working examples, using real datasets. This\npresented an organizational challenge. Some models might logically be grouped together\nin a single notebook. And some ideas might be best taught by executing several models\nin succession. By contrast, there is a big advantage to adhering to a policy of one working\nexample,onenotebook : Thismakesitaseasyaspossibleforyoutostartyourownresearch\nprojects by leveraging our code. Just copy a notebook and start modifying it.\nThroughout, we interleave the runnable code with background material as needed. In gen-\neral, we err on the side of making tools available before explaining them fully (often filling\nin the background later). For instance, we might use stochastic gradient descent before\nexplaining why it is useful or offering some intuition for why it works. This helps to give\npractitionersthenecessaryammunitiontosolveproblemsquickly, attheexpenseofrequir-\ning the reader to trust us with some curatorial decisions.\nThis book teaches deep learning concepts from scratch. Sometimes, we delve into fine\ndetails about models that would typically be hidden from users by modern deep learning\nframeworks. This comes up especially in the basic tutorials, where we want you to un-\nderstand everything that happens in a given layer or optimizer. In these cases, we often\npresent two versions of the example: one where we implement everything from scratch,\nrelying only on NumPy-like functionality and automatic differentiation, and a more prac-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6abc4124-71a4-4bde-b930-e176e8118d05": {"__data__": {"id_": "6abc4124-71a4-4bde-b930-e176e8118d05", "embedding": null, "metadata": {"page_label": "xxviii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50d12424-e819-4a2f-8b84-b71f94cc56ef", "node_type": "4", "metadata": {"page_label": "xxviii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bf30a669f76684622b1294feeca415c4c875c90370f06f77a75c8d2d7240aa5f", "class_name": "RelatedNodeInfo"}}, "text": "xxviii Preface\ntical example, where we write succinct code using the high-level APIs of deep learning\nframeworks. After explaining how some component works, we rely on the high-level API\nin subsequent tutorials.\nContent and Structure\nThebookcanbedividedintoroughlythreeparts,dealingwithpreliminaries,deeplearning\ntechniques, and advanced topics focused on real systems and applications ( Fig. 1).\ntFig. 1 Book structure.\n\u000fPart1: BasicsandPreliminaries .Chapter 1 is an introduction to deep learning. Then,\ninChapter2 ,wequicklybringyouuptospeedontheprerequisitesrequiredforhands-\non deep learning, such as how to store and manipulate data, and how to apply vari-\nousnumericaloperationsbasedonelementaryconceptsfromlinearalgebra, calculus,\nand probability. Chapter 3 andChapter 5 cover the most fundamental concepts and\ntechniques in deep learning, including regression and classification; linear models;\nmultilayer perceptrons; and overfitting and regularization.\n\u000fPart 2: Modern Deep Learning Techniques .Chapter 6 describes the key computa-\ntional components of deep learning systems and lays the groundwork for our sub-\nsequent implementations of more complex models. Next, Chapter 7 andChapter 8\npresent convolutional neural networks (CNNs), powerful tools that form the back-\nbone of most modern computer vision systems. Similarly, Chapter 9 andChapter 10\nintroducerecurrentneuralnetworks(RNNs),modelsthatexploitsequential(e.g.,tem-\nporal) structure in data and are commonly used for natural language processing and\ntime series prediction. In Chapter 11 , we describe a relatively new class of models,\nbased on so-called attention mechanisms , that has displaced RNNs as the dominant\narchitecture for most natural language processing tasks. These sections will bring\nyou up to speed on the most powerful and general tools that are widely used by deep\nlearning practitioners.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "416fb828-8a58-461a-837c-24e2a6d72663": {"__data__": {"id_": "416fb828-8a58-461a-837c-24e2a6d72663", "embedding": null, "metadata": {"page_label": "xxix", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f30241e7-7fbc-40a1-be80-fd5120d627ce", "node_type": "4", "metadata": {"page_label": "xxix", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "71188e290e56ded64e98836f960eb76216eeb5646e1700e65d071f2202b23fa2", "class_name": "RelatedNodeInfo"}}, "text": "xxix Preface\n3\u000fPart3: Scalability,E\ufb00iciency,andApplications (available online3). InChapter12,we\ndiscuss several common optimization algorithms used to train deep learning models.\nNext, in Chapter 13, we examine several key factors that influence the computational\nperformance of deep learning code. Then, in Chapter 14, we illustrate major applica-\ntions of deep learning in computer vision. Finally, in Chapter 15 and Chapter 16, we\ndemonstratehowtopretrainlanguagerepresentationmodelsandapplythemtonatural\nlanguage processing tasks.\nCode\nMostsectionsofthisbookfeatureexecutablecode. Webelievethatsomeintuitionsarebest\ndeveloped via trial and error, tweaking the code in small ways and observing the results.\nIdeally, an elegant mathematical theory might tell us precisely how to tweak our code to\nachieveadesiredresult. However,deeplearningpractitionerstodaymustoftentreadwhere\nno solid theory provides guidance. Despite our best attempts, formal explanations for the\nefficacy of various techniques are still lacking, for a variety of reasons: the mathematics to\ncharacterize these models can be so difficult; the explanation likely depends on properties\nof the data that currently lack clear definitions; and serious inquiry on these topics has\nonly recently kicked into high gear. We are hopeful that as the theory of deep learning\nprogresses,eachfutureeditionofthisbookwillprovideinsightsthateclipsethosepresently\navailable.\nToavoidunnecessaryrepetition,wecapturesomeofourmostfrequentlyimportedandused\nfunctions and classes in the d2lpackage. Throughout, we mark blocks of code (such as\nfunctions,classes,orcollectionofimportstatements)with #@savetoindicatethattheywill\nbe accessed later via the d2lpackage. We offer a detailed overview of these classes and\nfunctions in Section B.8 . The d2lpackage is lightweight and only requires the following\ndependencies:\n#@save\nimport collections\nimport hashlib\nimport inspect\nimport math\nimport os\nimport random\nimport re\nimport shutil\nimport sys\nimport tarfile\nimport time\nimport zipfile\nfrom collections import defaultdict\nimport pandas aspd\nimport requests\nfrom IPython import display\nfrom matplotlib import pyplot asplt\nfrom matplotlib_inline import backend_inline\nd2l =sys.modules[ __name__ ]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2243, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35f3d01c-a53c-44f9-ae06-c5ff6aa73251": {"__data__": {"id_": "35f3d01c-a53c-44f9-ae06-c5ff6aa73251", "embedding": null, "metadata": {"page_label": "xxx", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad5b7a7e-e541-4670-b0cf-ceb0f3fc9c15", "node_type": "4", "metadata": {"page_label": "xxx", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bd282e50e04a4d5a75146b862c135ac515f1c463e440e58f83dbc8e4520e7d26", "class_name": "RelatedNodeInfo"}}, "text": "xxx Preface\n4\n5\n6\n7\n8\n9\n10Most of the code in this book is based on PyTorch, a popular open-source framework that\nhas been enthusiastically embraced by the deep learning research community. All of the\ncode in this book has passed tests under the latest stable version of PyTorch. However, due\nto the rapid development of deep learning, some code in the print edition may not work\nproperly in future versions of PyTorch. We plan to keep the online version up to date.\nIn case you encounter any problems, please consult Installation (page xxxiv) to update\nyour code and runtime environment. Below lists dependencies in our PyTorch implemen-\ntation.\n#@save\nimport numpy asnp\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom scipy .spatial import distance_matrix\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom torchvision import transforms\nTargetAudience\nThisbookisforstudents(undergraduateorgraduate),engineers,andresearchers,whoseek\na solid grasp of the practical techniques of deep learning. Because we explain every con-\nceptfromscratch,nopreviousbackgroundindeeplearningormachinelearningisrequired.\nFully explaining the methods of deep learning requires some mathematics and program-\nming, but we will only assume that you enter with some basics, including modest amounts\nof linear algebra, calculus, probability, and Python programming. Just in case you have\nforgotten anything, the online Appendix4provides a refresher on most of the mathematics\nyouwillfindinthisbook. Usually, wewillprioritizeintuitionandideasovermathematical\nrigor. If you would like to extend these foundations beyond the prerequisites to understand\nour book, we happily recommend some other terrific resources: Linear Analysis by Bol-\nlob\u00e1s (1999) covers linear algebra and functional analysis in great depth. All of Statistics\n(Wasserman, 2013 ) provides a marvelous introduction to statistics. Joe Blitzstein\u2019s books5\nandcourses6on probability and inference are pedagogical gems. And if you have not used\nPython before, you may want to peruse this Python tutorial7.\nNotebooks,Website,GitHub, and Forum\nAll of our notebooks are available for download on the D2L.ai website8and onGitHub9.\nAssociated with this book, we have launched a discussion forum, located at discuss.d2l.ai\n10. Whenever you have questions on any section of the book, you can find a link to the\nassociated discussion page at the end of each notebook.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40b49747-41b5-4b32-bf96-12bddc89bf79": {"__data__": {"id_": "40b49747-41b5-4b32-bf96-12bddc89bf79", "embedding": null, "metadata": {"page_label": "xxxi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f2fadff-692c-4d4e-b1ed-8c90d1920f2a", "node_type": "4", "metadata": {"page_label": "xxxi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0abd985f2083f6a1028bac48023e0b88b201f2e158cdf95771ec8e5e2fe6dcd6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bd548fe-e6a6-4f90-a147-50f50e795611", "node_type": "1", "metadata": {}, "hash": "064da3a2e6771409aaa8f4ff2326f6d3b148f9a7571dbbf835c12392faf3f407", "class_name": "RelatedNodeInfo"}}, "text": "xxxi Preface\nAcknowledgments\nWeareindebtedtothehundredsofcontributorsforboththeEnglishandtheChinesedrafts.\nThey helped improve the content and offered valuable feedback. This book was originally\nimplemented with MXNet as the primary framework. We thank Anirudh Dagar and Yuan\nTang for adapting a majority part of earlier MXNet code into PyTorch and TensorFlow im-\nplementations, respectively. Since July 2021, we have redesigned and reimplemented this\nbook in PyTorch, MXNet, and TensorFlow, choosing PyTorch as the primary framework.\nWe thank Anirudh Dagar for adapting a majority part of more recent PyTorch code into\nJAX implementations. We thank Gaosheng Wu, Liujun Hu, Ge Zhang, and Jiehang Xie\nfrom Baidu for adapting a majority part of more recent PyTorch code into PaddlePaddle\nimplementations in the Chinese draft. We thank Shuai Zhang for integrating the LaTeX\nstyle from the press into the PDF building.\nOn GitHub, we thank every contributor of this English draft for making it better for ev-\neryone.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bd548fe-e6a6-4f90-a147-50f50e795611": {"__data__": {"id_": "9bd548fe-e6a6-4f90-a147-50f50e795611", "embedding": null, "metadata": {"page_label": "xxxi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f2fadff-692c-4d4e-b1ed-8c90d1920f2a", "node_type": "4", "metadata": {"page_label": "xxxi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0abd985f2083f6a1028bac48023e0b88b201f2e158cdf95771ec8e5e2fe6dcd6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "40b49747-41b5-4b32-bf96-12bddc89bf79", "node_type": "1", "metadata": {"page_label": "xxxi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e6665d3cea809160e452cf676dc0f4f6d50920ed0f5f5f40c9058ea5d47274da", "class_name": "RelatedNodeInfo"}}, "text": "We thank Anirudh Dagar and Yuan\nTang for adapting a majority part of earlier MXNet code into PyTorch and TensorFlow im-\nplementations, respectively. Since July 2021, we have redesigned and reimplemented this\nbook in PyTorch, MXNet, and TensorFlow, choosing PyTorch as the primary framework.\nWe thank Anirudh Dagar for adapting a majority part of more recent PyTorch code into\nJAX implementations. We thank Gaosheng Wu, Liujun Hu, Ge Zhang, and Jiehang Xie\nfrom Baidu for adapting a majority part of more recent PyTorch code into PaddlePaddle\nimplementations in the Chinese draft. We thank Shuai Zhang for integrating the LaTeX\nstyle from the press into the PDF building.\nOn GitHub, we thank every contributor of this English draft for making it better for ev-\neryone. Their GitHub IDs or names are (in no particular order): alxnorden, avinashingit,\nbowen0701, brettkoonce, Chaitanya Prakash Bapat, cryptonaut, Davide Fiocco, edgarro-\nman, gkutiel, JohnMitro, LiangPu, RahulAgarwal, MohamedAliJamaoui, Michael(Stu)\nStewart, Mike M\u00fcller, NRauschmayr, Prakhar Srivastav, sad-, sfermigier, Sheng Zha, sun-\ndeepteki, topecongiro, tpdi, vermicelli, Vishaal Kapoor, Vishwesh Ravi Shrimali, YaYaB,\nYuhong Chen, Evgeniy Smirnov, lgov, Simon Corston-Oliver, Igor Dzreyev, Ha Nguyen,\npmuens, Andrei Lukovenko, senorcinco, vfdev-5, dsweet, Mohammad Mahdi Rahimi, Ab-\nhishek Gupta, uwsd, DomKM, Lisa Oakley, Bowen Li, Aarush Ahuja, Prasanth Bud-\ndareddygari, brianhendee, mani2106, mtn, lkevinzc, caojilin, Lakshya, Fiete L\u00fcer, Surbhi\nVijayvargeeya, Muhyun Kim, dennismalmgren, adursun, Anirudh Dagar, liqingnz, Pe-\ndro Larroy, lgov, ati-ozgur, Jun Wu, Matthias Blume, Lin Yuan, geogunow, Josh Gard-\nner, Maximilian B\u00f6ther, Rakib Islam, Leonard Lausen, Abhinav Upadhyay, rongruosong,\nSteve Sedlmeyer, Ruslan Baratov, Rafael Schlatter, liusy182, Giannis Pappas, ati-ozgur,\nqbaza, dchoi77, Adam Gerson, Phuc Le, Mark Atwood, christabella, vn09, Haibin Lin,\njjangga0214, RichyChen, noelo, hansent, Giel Dops, dvincent1337, WhiteD3vil, Peter\nKulits, codypenta, joseppinilla, ahmaurya, karolszk, heytitle, Peter Goetz, rigtorp, Tiep\nVu,sfilip,mlxd,Kale-abTessera,SanjarAdilov,MatteoFerrara,hsneto,KatarzynaBiesial-\nska, Gregory Bruss, Duy\u2013Thanh Doan, paulaurel, graytowne, Duc Pham, sl7423, Jaedong\nHwang, Yida Wang, cys4, clhm, Jean Kaddour, austinmw, trebeljahr, tbaums, Cuong V.\nNguyen, pavelkomarov, vzlamal, NotAnotherSystem, J-Arun-Mani, jancio, eldarkurtic,\nthe-great-shazbot, doctorcolossus, gducharme, cclauss, Daniel-Mietchen, hoonose, bia-\ngiom, abhinavsp0730, jonathanhrandall, ysraell, Nodar Okroshiashvili, UgurKap, Jiyang\nKang, StevenJokes, Tomer Kaftan, liweiwp, netyster, ypandya, NishantTharani, heiligerl,\nSportsTHU,HoaNguyen,manuel-arno-korfmann-webentwicklung,aterzis-personal,nxby,\nXiaoting He, Josiah Yoder, mathresearch, mzz2017, jroberayalas, iluu, ghejc, BSharmi,\nvkramdev,simonwardjones,LakshKD,TalNeoran,djliden,Nikhil95,OrenBarkan,guoweis,\nhaozhu233, pratikhack, YueYing, tayfununal, steinsag, charleybeller, AndrewLumsdaine,\nJiekui Zhang, Deepak Pathak, Florian Donhauser, Tim Gates, Adriaan Tijsseling, Ron", "mimetype": "text/plain", "start_char_idx": 244, "end_char_idx": 3363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b2a8efa-0b87-436a-b681-bddfafed6913": {"__data__": {"id_": "0b2a8efa-0b87-436a-b681-bddfafed6913", "embedding": null, "metadata": {"page_label": "xxxii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f270e715-0e54-40e6-8bdb-4fc61767b8ab", "node_type": "4", "metadata": {"page_label": "xxxii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f3a955e1ab62b5551cefba3d1a289f1815358d1d0dd7ac388462a59962913e2e", "class_name": "RelatedNodeInfo"}}, "text": "xxxii Preface\n11Medina, Gaurav Saha, Murat Semerci, Lei Mao, Levi McClenny, Joshua Broyde, jake221,\njonbally, zyhazwraith, Brian Pulfer, Nick Tomasino, Lefan Zhang, Hongshen Yang, Vin-\nney Cavallo, yuntai, Yuanxiang Zhu, amarazov, pasricha, Ben Greenawald, Shivam Upad-\nhyay, Quanshangze Du, Biswajit Sahoo, Parthe Pandit, Ishan Kumar, HomunculusK, Lane\nSchwartz,varadgunjal,JasonWiener,ArminGholampoor,Shreshtha13,eigen-arnav,Hyeong-\ngyu Kim, EmilyOng, B\u00e1lint Mucs\u00e1nyi, Chase DuBois, Juntian Tao, Wenxiang Xu, Lifu\nHuang, filevich, quake2005, nils-werner, Yiming Li, Marsel Khisamutdinov, Francesco\n\u201cFuma\u201dFumagalli,PeilinSun,VincentGurgul,qingfengtommy,JanmeyShukla,MoShan,\nKaanSancak,regob,AlexSauer,GopalakrishnaRamachandra,TobiasUelwer,ChaoWang,\nTianCao,NicolasCorthorn,akash5474,kxxt,zxydi1992,JacobBritton,ShuangchiHe,zh-\nmou, krahets, Jie-Han Chen, Atishay Garg, Marcel Flygare, adtygan, Nik Vaessen, bolded,\nLouisSchlessinger,BalajiVaratharajan,atgctg,KaixinLi,VictorBarbaros,RiccardoMusto,\nElizabeth Ho, azimjonn, Guilherme Miotto, Alessandro Finamore, Joji Joseph, Anthony\nBiel,ZemingZhao,shjustinbaek,gab-chen,nantekoto,YutaroNishiyama,OrenAmsalem,\nTian-MaoMao, Amin Allahyar, Gijs van Tulder, Mikhail Berkov, iamorphen, Matthew\nCaseres, Andrew Walsh, pggPL, RohanKarthikeyan, Ryan Choi, and Likun Lei.\nWe thank Amazon Web Services, especially Wen-Ming Ye, George Karypis, Swami Siva-\nsubramanian,PeterDeSantis,AdamSelipsky,andAndrewJassyfortheirgeneroussupport\nin writing this book. Without the available time, resources, discussions with colleagues,\nand continuous encouragement, this book would not have happened. During the prepara-\ntion of the book for publication, Cambridge University Press has offered excellent support.\nWethankourcommissioningeditorDavidTranahforhishelpandprofessionalism.\nSummary\nDeep learning has revolutionized pattern recognition, introducing technology that now\npowers a wide range of technologies, in such diverse fields as computer vision, natural\nlanguage processing, and automatic speech recognition. To successfully apply deep learn-\ning, you must understand how to cast a problem, the basic mathematics of modeling, the\nalgorithms for fitting your models to data, and the engineering techniques to implement it\nall. This book presents a comprehensive resource, including prose, figures, mathematics,\nand code, all in one place.\nExercises\n1.Register an account on the discussion forum of this book discuss.d2l.ai11.\n2.Install Python on your computer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dacbf4f4-304e-4819-95fb-2f135c59e6de": {"__data__": {"id_": "dacbf4f4-304e-4819-95fb-2f135c59e6de", "embedding": null, "metadata": {"page_label": "xxxiii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1893d1a-6cd6-4d84-9764-db71eeaf4acb", "node_type": "4", "metadata": {"page_label": "xxxiii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c51b0903064e1b053ed805b5fa5a6cbbad61dc3084b4bf2190bf1718c94fbcd0", "class_name": "RelatedNodeInfo"}}, "text": "xxxiii Preface\n123.Follow the links at the bottom of the section to the forum, where you will be able to\nseek out help and discuss the book and find answers to your questions by engaging the\nauthors and broader community.\nDiscussions12.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "641f9f46-a463-457f-93b5-2dd0c1394831": {"__data__": {"id_": "641f9f46-a463-457f-93b5-2dd0c1394831", "embedding": null, "metadata": {"page_label": "xxxiv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18327ec2-e894-42cc-9a56-5eadcd04dc1c", "node_type": "4", "metadata": {"page_label": "xxxiv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6d18a427353a4eb106b5745ecaf5d2bac50f695e361be985fbfe48e9b3a0a810", "class_name": "RelatedNodeInfo"}}, "text": "13\n14\nInstallation\nInordertogetupandrunning,wewillneedanenvironmentforrunningPython,theJupyter\nNotebook, the relevant libraries, and the code needed to run the book itself.\nInstalling Miniconda\nYoursimplestoptionistoinstall Miniconda13. NotethatthePython3.xversionisrequired.\nYou can skip the following steps if your machine already has conda installed.\nVisit the Miniconda website and determine the appropriate version for your system based\non your Python 3.x version and machine architecture. Suppose that your Python version is\n3.9(ourtestedversion). IfyouareusingmacOS,youwoulddownloadthebashscriptwhose\nname contains the strings \u201cMacOSX\u201d, navigate to the download location, and execute the\ninstallation as follows (taking Intel Macs as an example):\n# The file name is subject to changes\nshMiniconda3-py39_4.12.0-MacOSX-x86_64.sh -b\nALinuxuserwoulddownloadthefilewhosenamecontainsthestrings\u201cLinux\u201dandexecute\nthe following at the download location:\n# The file name is subject to changes\nshMiniconda3-py39_4.12.0-Linux-x86_64.sh -b\nAWindowsuserwoulddownloadandinstallMinicondabyfollowingits onlineinstructions\n14. On Windows, you may search for cmdto open the Command Prompt (command-line\ninterpreter) for running commands.\nNext, initialize the shell so we can run condadirectly.\n~/miniconda3/bin/conda init\nThen close and reopen your current shell. You should be able to create a new environment\nas follows:\nxxxiv", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9ec3348-8024-4dcb-9e06-2bce7509ad71": {"__data__": {"id_": "a9ec3348-8024-4dcb-9e06-2bce7509ad71", "embedding": null, "metadata": {"page_label": "xxxv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33c1f82e-ff9a-413a-b81f-487249871b1d", "node_type": "4", "metadata": {"page_label": "xxxv", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7778101e92d7d20b12a86db7b9c836e8b9c1ade604465f153916469f55483a51", "class_name": "RelatedNodeInfo"}}, "text": "xxxv Installation\n15\n16conda create --name d2l python =3.9-y\nNow we can activate the d2lenvironment:\nconda activate d2l\nInstalling the Deep Learning Frameworkand the\nd2lPackage\nBefore installing any deep learning framework, please first check whether or not you have\nproper GPUs on your machine (the GPUs that power the display on a standard laptop are\nnot relevant for our purposes). For example, if your computer has NVIDIA GPUs and has\ninstalled CUDA15, then you are all set. If your machine does not house any GPU, there\nis no need to worry just yet. Your CPU provides more than enough horsepower to get you\nthrough the first few chapters. Just remember that you will want to access GPUs before\nrunning larger models.\nYoucan installPyTorch(the specified versionsare testedat the time ofwriting) with either\nCPU or GPU support as follows:\npip install torch ==2.0.0 torchvision ==0.15.1\nOur next step is to install the d2lpackage that we developed in order to encapsulate fre-\nquently used functions and classes found throughout this book:\npip install d2l==1.0.3\nDownloadingand Runningthe Code\nNext, you will want to download the notebooks so that you can run each of the book\u2019s\ncode blocks. Simply click on the \u201cNotebooks\u201d tab at the top of any HTML page on the\nD2L.ai website16to download the code and then unzip it. Alternatively, you can fetch the\nnotebooks from the command line as follows:\nmkdir d2l-en &&cdd2l-en\ncurl https://d2l.ai/d2l-en-1.0.3.zip -od2l-en.zip\nunzip d2l-en.zip &&rmd2l-en.zip\ncdpytorch", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d19c769a-2083-46cb-af7d-b21ae6794916": {"__data__": {"id_": "d19c769a-2083-46cb-af7d-b21ae6794916", "embedding": null, "metadata": {"page_label": "xxxvi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "725b4510-4255-4142-95f1-94093d6ec3cb", "node_type": "4", "metadata": {"page_label": "xxxvi", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7be90c932fcdc75d114d7c13e60ae5c78fc1637066db49e41a601893cebdc18f", "class_name": "RelatedNodeInfo"}}, "text": "xxxvi Installation\n17If you do not already have unzipinstalled, first run sudo apt-get install unzip . Now\nwe can start the Jupyter Notebook server by running:\njupyter notebook\nAtthispoint,youcanopen http://localhost:8888 (itmayhavealreadyopenedautomatically)\nin your web browser. Then we can run the code for each section of the book. Whenever\nyou open a new command line window, you will need to execute conda activate d2l\nto activate the runtime environment before running the D2L notebooks, or updating your\npackages(eitherthedeeplearningframeworkorthe d2lpackage). Toexittheenvironment,\nrunconda deactivate .\nDiscussions17.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f379a3e0-3b49-4107-ab83-75744809f415": {"__data__": {"id_": "f379a3e0-3b49-4107-ab83-75744809f415", "embedding": null, "metadata": {"page_label": "xxxvii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6afe5b7b-8993-4a70-9982-e59a7f594cbf", "node_type": "4", "metadata": {"page_label": "xxxvii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "18dddadeb0109af1039bc4ffdeec8411c4cc0f18e7e6ffb811f2849e88cfa745", "class_name": "RelatedNodeInfo"}}, "text": "Notation\nThroughout this book, we adhere to the following notational conventions. Note that some\nof these symbols are placeholders, while others refer to specific objects. As a general rule\nof thumb, the indefinite article \u201ca\u201d often indicates that the symbol is a placeholder and that\nsimilarly formatted symbols can denote other objects of the same type. For example, \u201c \ud835\udc65: a\nscalar\u201d means that lowercased letters generally represent scalar values, but \u201c Z: the set of\nintegers\u201d refers specifically to the symbol Z.\nNumericalObjects\n\u000f\ud835\udc65: a scalar\n\u000fx: a vector\n\u000fX: a matrix\n\u000fX: a general tensor\n\u000fI: the identity matrix (of some given dimension), i.e., a square matrix with 1on all\ndiagonal entries and 0on all off-diagonals\n\u000f\ud835\udc65\ud835\udc56,\u00bbx\u00bc\ud835\udc56: the\ud835\udc56thelement of vector x\n\u000f\ud835\udc65\ud835\udc56\ud835\udc57,\ud835\udc65\ud835\udc56,\ud835\udc57,\u00bbX\u00bc\ud835\udc56\ud835\udc57,\u00bbX\u00bc\ud835\udc56,\ud835\udc57: the element of matrix Xat row\ud835\udc56and column \ud835\udc57.\nSetTheory\n\u000fX: a set\n\u000fZ: the set of integers\n\u000fZ\u00b8: the set of positive integers\n\u000fR: the set of real numbers\n\u000fR\ud835\udc5b: the set of\ud835\udc5b-dimensional vectors of real numbers\nxxxvii", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5033a77-aeca-4b6e-ac2a-c4892ae75f89": {"__data__": {"id_": "a5033a77-aeca-4b6e-ac2a-c4892ae75f89", "embedding": null, "metadata": {"page_label": "xxxviii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "faeec7e9-a99b-43a5-9bc7-90765ce8fa36", "node_type": "4", "metadata": {"page_label": "xxxviii", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "84b7deb121d04bf52781293623118bf18d01483c97a05a21aabbfceae52488cc", "class_name": "RelatedNodeInfo"}}, "text": "xxxviii Notation\n\u000fR\ud835\udc4e\u0002\ud835\udc4f: The set of matrices of real numbers with \ud835\udc4erows and\ud835\udc4fcolumns\n\u000fjXj: cardinality (number of elements) of set X\n\u000fA[B: union of setsAandB\n\u000fA\\B: intersection of sets AandB\n\u000fAnB: set subtraction of BfromA(contains only those elements of Athat do not\nbelong toB)\nFunctions and Operators\n\u000f\ud835\udc53\u00b9\u0001\u00ba: a function\n\u000flog\u00b9\u0001\u00ba: the natural logarithm (base \ud835\udc52)\n\u000flog2\u00b9\u0001\u00ba: logarithm to base 2\n\u000fexp\u00b9\u0001\u00ba: the exponential function\n\u000f1\u00b9\u0001\u00ba: the indicator function; evaluates to 1if the boolean argument is true, and 0other-\nwise\n\u000f1X\u00b9\ud835\udc67\u00ba: the set-membership indicator function; evaluates to 1if the element \ud835\udc67belongs to\nthe setXand0otherwise\n\u000f\u00b9\u0001\u00ba>: transpose of a vector or a matrix\n\u000fX\u00001: inverse of matrix X\n\u000f\f: Hadamard (elementwise) product\n\u000f\u00bb\u0001,\u0001\u00bc: concatenation\n\u000fk\u0001k\ud835\udc5d:\u2113\ud835\udc5dnorm\n\u000fk\u0001k:\u21132norm\n\u000fhx,yi: inner (dot) product of vectors xandy\n\u000f\u00cd: summation over a collection of elements\n\u000f\u00ce: product over a collection of elements\n\u000fdef=: an equality asserted as a definition of the symbol on the left-hand side", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d7fcf72-05fc-4066-8d12-7d7ac24c04a8": {"__data__": {"id_": "6d7fcf72-05fc-4066-8d12-7d7ac24c04a8", "embedding": null, "metadata": {"page_label": "xxxix", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b1341be-2b39-48fe-81c3-6c4688f41ba4", "node_type": "4", "metadata": {"page_label": "xxxix", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d74308c478e3e8593c003ea9944e0a34a88b9b88b262b05284bf9d0b6b9956a1", "class_name": "RelatedNodeInfo"}}, "text": "xxxix Notation\n18Calculus\n\u000f\ud835\udc51\ud835\udc66\n\ud835\udc51\ud835\udc65: derivative of \ud835\udc66with respect to \ud835\udc65\n\u000f\ud835\udf15\ud835\udc66\n\ud835\udf15\ud835\udc65: partial derivative of \ud835\udc66with respect to \ud835\udc65\n\u000frx\ud835\udc66: gradient of \ud835\udc66with respect to x\n\u000f\u00af\ud835\udc4f\n\ud835\udc4e\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65: definite integral of \ud835\udc53from\ud835\udc4eto\ud835\udc4fwith respect to \ud835\udc65\n\u000f\u00af\n\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65: indefinite integral of \ud835\udc53with respect to \ud835\udc65\nProbabilityand InformationTheory\n\u000f\ud835\udc4b: a random variable\n\u000f\ud835\udc43: a probability distribution\n\u000f\ud835\udc4b\u0018\ud835\udc43: the random variable \ud835\udc4bfollows distribution \ud835\udc43\n\u000f\ud835\udc43\u00b9\ud835\udc4b=\ud835\udc65\u00ba: the probability assigned to the event where random variable \ud835\udc4btakes value\ud835\udc65\n\u000f\ud835\udc43\u00b9\ud835\udc4bj\ud835\udc4c\u00ba: the conditional probability distribution of \ud835\udc4bgiven\ud835\udc4c\n\u000f\ud835\udc5d\u00b9\u0001\u00ba: a probability density function (PDF) associated with distribution \ud835\udc43\n\u000f\ud835\udc38\u00bb\ud835\udc4b\u00bc: expectation of a random variable \ud835\udc4b\n\u000f\ud835\udc4b?\ud835\udc4c: random variables \ud835\udc4band\ud835\udc4care independent\n\u000f\ud835\udc4b?\ud835\udc4cj\ud835\udc4d: random variables \ud835\udc4band\ud835\udc4care conditionally independent given \ud835\udc4d\n\u000f\ud835\udf0e\ud835\udc4b: standard deviation of random variable \ud835\udc4b\n\u000fVar\u00b9\ud835\udc4b\u00ba: variance of random variable \ud835\udc4b, equal to\ud835\udf0e2\n\ud835\udc4b\n\u000fCov\u00b9\ud835\udc4b,\ud835\udc4c\u00ba: covariance of random variables \ud835\udc4band\ud835\udc4c\n\u000f\ud835\udf0c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba: the Pearson correlation coefficient between \ud835\udc4band\ud835\udc4c, equalsCov\u00b9\ud835\udc4b,\ud835\udc4c\u00ba\n\ud835\udf0e\ud835\udc4b\ud835\udf0e\ud835\udc4c\n\u000f\ud835\udc3b\u00b9\ud835\udc4b\u00ba: entropy of random variable \ud835\udc4b\n\u000f\ud835\udc37KL\u00b9\ud835\udc43k\ud835\udc44\u00ba: the KL-divergence (or relative entropy) from distribution \ud835\udc44to distribution\n\ud835\udc43\nDiscussions18.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db467600-1227-47c6-9134-4b3e30cc1a4b": {"__data__": {"id_": "db467600-1227-47c6-9134-4b3e30cc1a4b", "embedding": null, "metadata": {"page_label": "xl", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "87af99ee-1cc2-4516-bed4-556292d32e37", "node_type": "4", "metadata": {"page_label": "xl", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f09dfe863fe94e1b2aa734bd0ef8213c043999cbebcf8cdafe8d9542084cbe06", "class_name": "RelatedNodeInfo"}}, "text": "", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 0, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb8e34ab-e7a6-4b6b-b1c5-9a3fb1acefc6": {"__data__": {"id_": "bb8e34ab-e7a6-4b6b-b1c5-9a3fb1acefc6", "embedding": null, "metadata": {"page_label": "1", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d707ff2b-acdc-47e9-b94e-baf71a1ec631", "node_type": "4", "metadata": {"page_label": "1", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2080e75b80031701df6531060639713fa8fdcdd520feda628dcdd004846d9e38", "class_name": "RelatedNodeInfo"}}, "text": "1 Introduction\nUntilrecently,nearlyeverycomputerprogramthatyoumighthaveinteractedwithduringan\nordinarydaywascodedupasarigidsetofrulesspecifyingpreciselyhowitshouldbehave.\nSay that we wanted to write an application to manage an e-commerce platform. After\nhuddling around a whiteboard for a few hours to ponder the problem, we might settle on\nthe broad strokes of a working solution, for example: (i) users interact with the application\nthrough an interface running in a web browser or mobile application; (ii) our application\ninteracts with a commercial-grade database engine to keep track of each user\u2019s state and\nmaintain records of historical transactions; and (iii) at the heart of our application, the\nbusiness logic (you might say, the brains) of our application spells out a set of rules that\nmap every conceivable circumstance to the corresponding action that our program should\ntake.\nTo build the brains of our application, we might enumerate all the common events that our\nprogram should handle. For example, whenever a customer clicks to add an item to their\nshoppingcart,ourprogramshouldaddanentrytotheshoppingcartdatabasetable,associ-\nating that user\u2019s ID with the requested product\u2019s ID. We might then attempt to step through\nevery possible corner case, testing the appropriateness of our rules and making any neces-\nsary modifications. What happens if a user initiates a purchase with an empty cart? While\nfew developers ever get it completely right the first time (it might take some test runs to\nwork out the kinks), for the most part we can write such programs and confidently launch\nthembeforeever seeing a real customer. Our ability to manually design automated sys-\ntems that drive functioning products and systems, often in novel situations, is a remarkable\ncognitive feat. And when you are able to devise solutions that work 100%of the time, you\ntypically should not be worrying about machine learning.\nFortunately for the growing community of machine learning scientists, many tasks that we\nwouldliketoautomatedonotbendsoeasilytohumaningenuity. Imaginehuddlingaround\nthe whiteboard with the smartest minds you know, but this time you are tackling one of the\nfollowing problems:\n\u000fWriteaprogramthatpredictstomorrow\u2019sweathergivengeographicinformation,satellite\nimages, and a trailing window of past weather.\n\u000fWriteaprogramthattakesinafactoidquestion,expressedinfree-formtext,andanswers\nit correctly.\n\u000fWrite a program that, given an image, identifies every person depicted in it and draws\noutlines around each.\n1", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b647dc54-6834-49b6-83b1-cb969bb077b3": {"__data__": {"id_": "b647dc54-6834-49b6-83b1-cb969bb077b3", "embedding": null, "metadata": {"page_label": "2", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ada9682-d106-4b7e-9d4f-0e8990038a59", "node_type": "4", "metadata": {"page_label": "2", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "eaa1298194f7af79069d48dee019f97606e7033c11ea8412fd1fc81978303789", "class_name": "RelatedNodeInfo"}}, "text": "2 Introduction\n\u000fWrite a program that presents users with products that they are likely to enjoy but un-\nlikely, in the natural course of browsing, to encounter.\nFor these problems, even elite programmers would struggle to code up solutions from\nscratch. The reasons can vary. Sometimes the program that we are looking for follows\na pattern that changes over time, so there is no fixed right answer! In such cases, any\nsuccessful solution must adapt gracefully to a changing world. At other times, the rela-\ntionship (say between pixels, and abstract categories) may be too complicated, requiring\nthousands or millions of computations and following unknown principles. In the case of\nimage recognition, the precise steps required to perform the task lie beyond our conscious\nunderstanding, even though our subconscious cognitive processes execute the task effort-\nlessly.\nMachinelearning is the study of algorithms that can learn from experience. As a machine\nlearning algorithm accumulates more experience, typically in the form of observational\ndata or interactions with an environment, its performance improves. Contrast this with\nour deterministic e-commerce platform, which follows the same business logic, no matter\nhow much experience accrues, until the developers themselves learn and decide that it is\ntime to update the software. In this book, we will teach you the fundamentals of machine\nlearning, focusing in particular on deep learning , a powerful set of techniques driving in-\nnovations in areas as diverse as computer vision, natural language processing, healthcare,\nand genomics.\n1.1AMotivatingExample\nBefore beginning writing, the authors of this book, like much of the work force, had to\nbecomecaffeinated. Wehoppedinthecarandstarteddriving. UsinganiPhone,Alexcalled\nout \u201cHey Siri\u201d, awakening the phone\u2019s voice recognition system. Then Mu commanded\n\u201cdirections to Blue Bottle coffee shop\u201d. The phone quickly displayed the transcription of\nhis command. It also recognized that we wereasking for directions and launched the Maps\napplication (app) to fulfill our request. Once launched, the Maps app identified a number\nof routes. Next to each route, the phone displayed a predicted transit time. While this story\nwas fabricated for pedagogical convenience, it demonstrates that in the span of just a few\nseconds,oureverydayinteractionswithasmartphonecanengageseveralmachinelearning\nmodels.\nImagine just writing a program to respond to a wakeword such as \u201cAlexa\u201d, \u201cOK Google\u201d,\nand \u201cHey Siri\u201d. Try coding it up in a room by yourself with nothing but a computer and\na code editor, as illustrated in Fig. 1.1.1 . How would you write such a program from first\nprinciples? Think about it\u2026 the problem is hard. Every second, the microphone will col-\nlect roughly 44,000 samples. Each sample is a measurement of the amplitude of the sound\nwave. What rule could map reliably from a snippet of raw audio to confident predictions\nfyes,nogaboutwhetherthesnippetcontainsthewakeword? Ifyouarestuck,donotworry.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3012, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a2b2869-b579-4b93-a420-73182e14a780": {"__data__": {"id_": "0a2b2869-b579-4b93-a420-73182e14a780", "embedding": null, "metadata": {"page_label": "3", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab7fa45c-4770-4bbd-99c6-0160ce218d38", "node_type": "4", "metadata": {"page_label": "3", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4a4e0a9ec6f356263768e81c050e32a1215d7d476472f6ad75ad2df522064b59", "class_name": "RelatedNodeInfo"}}, "text": "3 A Motivating Example\nWe do not know how to write such a program from scratch either. That is why we use ma-\nchine learning.\ntFig. 1.1.1 Identify a wake word.\nHere is the trick. Often, even when we do not know how to tell a computer explicitly how\nto map from inputs to outputs, we are nonetheless capable of performing the cognitive feat\nourselves. In other words, even if you do not know how to program a computer to rec-\nognize the word \u201cAlexa\u201d, you yourself are able to recognize it. Armed with this ability,\nwe can collect a huge datasetcontaining examples of audio snippets and associated labels,\nindicating which snippets contain the wake word. In the currently dominant approach to\nmachinelearning,wedonotattempttodesignasystem explicitly torecognizewakewords.\nInstead, we define a flexible program whose behavior is determined by a number of pa-\nrameters . Then we use the dataset to determine the best possible parameter values, i.e.,\nthose that improve the performance of our program with respect to a chosen performance\nmeasure.\nYou can think of the parameters as knobs that we can turn, manipulating the behavior of\nthe program. Once the parameters are fixed, we call the program a model. The set of all\ndistinct programs (input\u2013output mappings) that we can produce just by manipulating the\nparameters is called a familyof models. And the \u201cmeta-program\u201d that uses our dataset to\nchoose the parameters is called a learning algorithm .\nBefore we can go ahead and engage the learning algorithm, we have to define the problem\nprecisely, pinning down the exact nature of the inputs and outputs, and choosing an ap-\npropriate model family. In this case, our model receives a snippet of audio as input, and\nthe model generates a selection among fyes,nogasoutput. If all goes according to plan\nthe model\u2019s guesses will typically be correct as to whether the snippet contains the wake\nword.\nIf we choose the right family of models, there should exist one setting of the knobs such\nthatthemodelfires\u201cyes\u201deverytimeithearstheword\u201cAlexa\u201d. Becausetheexactchoiceof\nthe wake word is arbitrary, we will probably need a model family sufficiently rich that, via\nanother setting of the knobs, it could fire \u201cyes\u201d only upon hearing the word \u201cApricot\u201d. We\nexpectthatthesamemodelfamilyshouldbesuitablefor\u201cAlexa\u201drecognitionand\u201cApricot\u201d\nrecognition because they seem, intuitively, to be similar tasks. However, we might need a\ndifferent family of models entirely if we want to deal with fundamentally different inputs\nor outputs, say if we wanted to map from images to captions, or from English sentences to\nChinese sentences.\nAs you might guess, if we just set all of the knobs randomly, it is unlikely that our model\nwill recognize \u201cAlexa\u201d, \u201cApricot\u201d, or any other English word. In machine learning, the\nlearning is the process by which we discover the right setting of the knobs for coercing the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bf459a42-d58d-4a62-9379-7e579bba9c57": {"__data__": {"id_": "bf459a42-d58d-4a62-9379-7e579bba9c57", "embedding": null, "metadata": {"page_label": "4", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e1e3990-cfa5-4bbc-980f-a8e3a7a2b9e9", "node_type": "4", "metadata": {"page_label": "4", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "102c647f8d075859720dd48e290f6e95055e00f4ae893da18d6ff1ba6ce5967d", "class_name": "RelatedNodeInfo"}}, "text": "4 Introduction\ndesired behavior from our model. In other words, we trainour model with data. As shown\ninFig. 1.1.2 , the training process usually looks like the following:\n1.Start off with a randomly initialized model that cannot do anything useful.\n2.Grab some of your data (e.g., audio snippets and corresponding fyes,noglabels).\n3.Tweak the knobs to make the model perform better as assessed on those examples.\n4.Repeat Steps 2 and 3 until the model is awesome.\ntFig. 1.1.2 A typical training process.\nTo summarize, rather than code up a wakeword recognizer, wecode up a program that can\nlearnto recognize wake words, if presented with a large labeled dataset. You can think of\nthisactofdeterminingaprogram\u2019sbehaviorbypresentingitwithadatasetas programming\nwithdata . Thatistosay,wecan\u201cprogram\u201dacatdetectorbyprovidingourmachinelearning\nsystem with many examples of cats and dogs. This way the detector will eventually learn\nto emit a very large positive number if it is a cat, a very large negative number if it is a\ndog, andsomethingclosertozeroifitisnotsure. Thisbarelyscratchesthesurfaceofwhat\nmachine learning can do. Deep learning, which we will explain in greater detail later, is\njust one among many popular methods for solving machine learning problems.\n1.2KeyComponents\nIn our wake word example, we described a dataset consisting of audio snippets and binary\nlabels, and we gave a hand-wavy sense of how we might train a model to approximate a\nmapping from snippets to classifications. This sort of problem, where we try to predict a\ndesignated unknown label based on known inputs given a dataset consisting of examples\nforwhichthelabelsareknown, iscalled supervisedlearning . Thisisjustoneamongmany\nkinds of machine learning problems. Before we explore other varieties, we would like to\nshed more light on some core components that will follow us around, no matter what kind\nof machine learning problem we tackle:\n1.Thedatathat we can learn from.\n2.Amodelof how to transform the data.\n3.Anobjectivefunction that quantifies how well (or badly) the model is doing.\n4.Analgorithm to adjust the model\u2019s parameters to optimize the objective function.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fea18bae-690f-454d-9427-bd3039f7c5be": {"__data__": {"id_": "fea18bae-690f-454d-9427-bd3039f7c5be", "embedding": null, "metadata": {"page_label": "5", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "268eabe5-f4e3-4f49-9e16-049d211c0a4f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "53f5049e32174e452b6c028d12b65cb625207509055479134122d7e20f59f236", "class_name": "RelatedNodeInfo"}}, "text": "5 Key Components\n1.2.1Data\nIt might go without saying that you cannot do data science without data. We could lose\nhundreds of pages pondering what precisely data is, but for now, we will focus on the key\npropertiesofthedatasetsthatwewillbeconcernedwith. Generally,weareconcernedwith\na collection of examples. In order to work with data usefully, we typically need to come\nup with a suitable numerical representation. Each example (ordata point ,data instance ,\nsample)typicallyconsistsofasetofattributescalled features (sometimescalled covariates\norinputs), based on which the model must make its predictions. In supervised learning\nproblems, our goal is to predict the value of a special attribute, called the label(ortarget),\nthat is not part of the model\u2019s input.\nIf we were working with image data, each example might consist of an individual photo-\ngraph(thefeatures)andanumberindicatingthecategorytowhichthephotographbelongs\n(the label). The photograph would be represented numerically as three grids of numerical\nvalues representing the brightness of red, green, and blue light at each pixel location. For\nexample, a 200\u0002200pixel color photograph would consist of 200\u0002200\u00023=120000\nnumerical values.\nAlternatively, we might work with electronic health record data and tackle the task of pre-\ndicting the likelihood that a given patient will survive the next 30 days. Here, our features\nmight consist of a collection of readily available attributes and frequently recorded mea-\nsurements, including age, vital signs, comorbidities, current medications, and recent pro-\ncedures. The label available for training would be a binary value indicating whether each\npatient in the historical data survived within the 30-day window.\nIn such cases, when every example is characterized by the same number of numerical fea-\ntures, we say that the inputs are fixed-length vectors and we call the (constant) length of\nthe vectors the dimensionality of the data. As you might imagine, fixed-length inputs can\nbe convenient, giving us one less complication to worry about. However, not all data can\neasilyberepresentedas fixed-length vectors. Whilewemightexpectmicroscopeimagesto\ncomefromstandardequipment,wecannotexpectimagesminedfromtheInternetalltohave\nthe same resolution or shape. For images, we might consider cropping them to a standard\nsize, but that strategy only gets us so far. We risk losing information in the cropped-out\nportions. Moreover, text data resists fixed-length representations even more stubbornly.\nConsider the customer reviews left on e-commerce sites such as Amazon, IMDb, and Tri-\npAdvisor. Some are short: \u201cit stinks!\u201d. Others ramble for pages. One major advantage of\ndeeplearningovertraditionalmethodsisthecomparativegracewithwhichmodernmodels\ncan handle varying-length data.\nGenerally,themoredatawehave,theeasierourjobbecomes. Whenwehavemoredata,we\ncan train more powerful models and rely less heavily on preconceived assumptions. The\nregime change from (comparatively) small to big data is a major contributor to the success\nof modern deep learning. To drive the point home, many of the most exciting models in\ndeep learning do not work without large datasets. Some others might work in the small\ndata regime, but are no better than traditional approaches.\nFinally, it is not enough to have lots of data and to process it cleverly. We need the right", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f6a5995-e723-4227-a57a-89c267b10c9a": {"__data__": {"id_": "1f6a5995-e723-4227-a57a-89c267b10c9a", "embedding": null, "metadata": {"page_label": "6", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7a6ceddd-0d6a-4d15-89d0-3ea9d8c02b9a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "78d221c01b0de52b1c013e6491f9b96ec4768363ff504dccc1f9f1f04102f10e", "class_name": "RelatedNodeInfo"}}, "text": "6 Introduction\ndata. If the data is full of mistakes, or if the chosen features are not predictive of the target\nquantity of interest, learning is going to fail. The situation is captured well by the clich\u00e9:\ngarbage in, garbage out . Moreover, poor predictive performance is not the only poten-\ntial consequence. In sensitive applications of machine learning, like predictive policing,\nresumescreening, andriskmodelsusedforlending, wemustbeespeciallyalerttothecon-\nsequencesofgarbagedata. Onecommonlyoccurringfailuremodeconcernsdatasetswhere\nsomegroupsofpeopleareunrepresentedinthetrainingdata. Imagineapplyingaskincan-\ncer recognition system that had never seen black skin before. Failure can also occur when\nthedatadoesnotonlyunder-representsomegroupsbutreflectssocietalprejudices. Forex-\nample,ifpasthiringdecisionsareusedtotrainapredictivemodelthatwillbeusedtoscreen\nresumesthenmachinelearningmodelscouldinadvertentlycaptureandautomatehistorical\ninjustices. Note that this can all happen without the data scientist actively conspiring, or\neven being aware.\n1.2.2Models\nMost machine learning involves transforming the data in some sense. We might want to\nbuildasystemthatingestsphotosandpredictssmiley-ness. Alternatively,wemightwantto\ningest a set of sensor readings and predict how normal vs. anomalous the readings are. By\nmodel, we denote the computational machinery for ingesting data of one type, and spitting\nout predictions of a possibly different type. In particular, we are interested in statistical\nmodelsthat can be estimated from data. While simple models are perfectly capable of ad-\ndressing appropriately simple problems, the problems that we focus on in this book stretch\nthe limits of classical methods. Deep learning is differentiated from classical approaches\nprincipally by the set of powerful models that it focuses on. These models consist of many\nsuccessive transformations of the data that are chained together top to bottom, thus the\nnamedeep learning . On our way to discussing deep models, we will also discuss some\nmore traditional methods.\n1.2.3ObjectiveFunctions\nEarlier,weintroducedmachinelearningaslearningfromexperience. By learning here,we\nmeanimprovingatsometaskovertime. Butwhoistosaywhatconstitutesanimprovement?\nYou might imagine that we could propose updating our model, and some people might\ndisagree on whether our proposal constituted an improvement or not.\nIn order to develop a formal mathematical system of learning machines, we need to have\nformal measures of how good (or bad) our models are. In machine learning, and optimiza-\ntion more generally, we call these objective functions . By convention, we usually define\nobjective functions so that lower is better. This is merely a convention. You can take any\nfunctionforwhichhigheris better, and turn itinto a newfunction that is qualitativelyiden-\ntical but for which lower is better by flipping the sign. Because we choose lower to be\nbetter, these functions are sometimes called loss functions .\nWhen trying to predict numerical values, the most common loss function is squarederror ,\ni.e., the square of the difference between the prediction and the ground truth target. For\nclassification, the most common objective is to minimize error rate, i.e., the fraction of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa6d42ac-4c18-4dde-b74c-06217b84e8b4": {"__data__": {"id_": "aa6d42ac-4c18-4dde-b74c-06217b84e8b4", "embedding": null, "metadata": {"page_label": "7", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "90f29465-ad78-4161-bdd4-4ae10781de61", "node_type": "4", "metadata": {"page_label": "7", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "337e815769710cdc0d0e3d51cb555bdc9617850948aa3048a66268191caa68b2", "class_name": "RelatedNodeInfo"}}, "text": "7 Kinds of Machine Learning Problems\nexamples on which our predictions disagree with the ground truth. Some objectives (e.g.,\nsquared error) are easy to optimize, while others (e.g., error rate) are difficult to optimize\ndirectly,owingtonon-differentiabilityorothercomplications. Inthesecases,itiscommon\ninstead to optimize a surrogateobjective .\nDuringoptimization, wethinkofthelossasafunctionofthemodel\u2019sparameters, andtreat\nthe training dataset as a constant. We learn the best values of our model\u2019s parameters by\nminimizing the loss incurred on a set consisting of some number of examples collected for\ntraining. However, doing well on the training data does not guarantee that we will do well\non unseen data. So we will typically want to split the available data into two partitions:\nthetraining dataset (ortraining set ), for learning model parameters; and the test dataset\n(ortest set), which is held out for evaluation. At the end of the day, we typically report\nhow our models perform on both partitions. You could think of training performance as\nanalogous to the scores that a student achieves on the practice exams used to prepare for\nsome real final exam. Even if the results are encouraging, that does not guarantee success\non the final exam. Over the course of studying, the student might begin to memorize the\npractice questions, appearing to master the topic but faltering when faced with previously\nunseen questions on the actual final exam. When a model performs well on the training set\nbutfailstogeneralizetounseendata,wesaythatitis overfitting tothetrainingdata.\n1.2.4OptimizationAlgorithms\nOnce we have got some data source and representation, a model, and a well-defined objec-\ntive function, we need an algorithm capable of searching for the best possible parameters\nfor minimizing the loss function. Popular optimization algorithms for deep learning are\nbased on an approach called gradient descent . In brief, at each step, this method checks\nto see, for each parameter, how that training set loss would change if you perturbed that\nparameter by just a small amount. It would then update the parameter in the direction that\nlowers the loss.\n1.3Kinds of MachineLearning Problems\nThe wake word problem in our motivating example is just one among many that machine\nlearning can tackle. To motivate the reader further and provide us with some common\nlanguage that will follow us throughout the book, we now provide a broad overview of the\nlandscape of machine learning problems.\n1.3.1SupervisedLearning\nSupervised learning describes tasks where we are given a dataset containing both features\nand labels and asked to produce a model that predicts the labels when given input features.\nEach feature\u2013label pair is called an example. Sometimes, when the context is clear, we\nmay use the term examples to refer to a collection of inputs, even when the corresponding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c36801fb-e233-40ea-b740-562992b9cff5": {"__data__": {"id_": "c36801fb-e233-40ea-b740-562992b9cff5", "embedding": null, "metadata": {"page_label": "8", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b4aba06-fc20-41e8-b5d2-f09aa3342cc2", "node_type": "4", "metadata": {"page_label": "8", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "09bee5fd9653dbe96e4f0282ddffa012ed8864750d72c18eff1cc9222c0d8548", "class_name": "RelatedNodeInfo"}}, "text": "8 Introduction\nlabels are unknown. The supervision comes into play because, for choosing the parame-\nters, we (the supervisors) provide the model with a dataset consisting of labeled examples.\nIn probabilistic terms, we typically are interested in estimating the conditional probability\nof a label given input features. While it is just one among several paradigms, supervised\nlearning accounts for the majority of successful applications of machine learning in indus-\ntry. Partly that is because many important tasks can be described crisply as estimating the\nprobability of something unknown given a particular set of available data:\n\u000fPredict cancer vs. not cancer, given a computer tomography image.\n\u000fPredict the correct translation in French, given a sentence in English.\n\u000fPredict the price of a stock next month based on this month\u2019s financial reporting data.\nWhile all supervised learning problems are captured by the simple description \u201cpredicting\nthelabelsgiveninputfeatures\u201d,supervisedlearningitselfcantakediverseformsandrequire\ntons of modeling decisions, depending on (among other considerations) the type, size, and\nquantity of the inputs and outputs. For example, we use different models for processing\nsequences of arbitrary lengths and fixed-length vector representations. We will visit many\nof these problems in depth throughout this book.\nInformally, the learning process looks something like the following. First, grab a big col-\nlectionofexamplesforwhichthefeaturesareknownandselectfromthemarandomsubset,\nacquiring the ground truth labels for each. Sometimes these labels might be available data\nthat have already been collected (e.g., did a patient die within the following year?) and\nother times we might need to employ human annotators to label the data, (e.g., assigning\nimages to categories). Together, these inputs and corresponding labels comprise the train-\ning set. We feed the training dataset into a supervised learning algorithm, a function that\ntakes as input a dataset and outputs another function: the learned model. Finally, we can\nfeed previously unseen inputs to the learned model, using its outputs as predictions of the\ncorresponding label. The full process is drawn in Fig. 1.3.1 .\ntFig. 1.3.1 Supervised learning.\nRegression\nPerhapsthesimplestsupervisedlearningtasktowrapyourheadaroundis regression . Con-\nsider, for example, a set of data harvested from a database of home sales. We might con-\nstruct a table, in which each row corresponds to a different house, and each column cor-\nresponds to some relevant attribute, such as the square footage of a house, the number of\nbedrooms, the number of bathrooms, and the number of minutes (walking) to the center\nof town. In this dataset, each example would be a specific house, and the corresponding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2787, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f110e45-85d8-4ee6-8215-31ef759867a5": {"__data__": {"id_": "3f110e45-85d8-4ee6-8215-31ef759867a5", "embedding": null, "metadata": {"page_label": "9", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f3ac8ae-3b5c-4aad-aea4-d16630952fae", "node_type": "4", "metadata": {"page_label": "9", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0866519932f70c8e415e3990f6f6abe8de86a0a9668b807592ee63b04a733ea3", "class_name": "RelatedNodeInfo"}}, "text": "9 Kinds of Machine Learning Problems\n19featurevectorwouldbeonerowinthetable. IfyouliveinNewYorkorSanFrancisco, and\nyou are not the CEO of Amazon, Google, Microsoft, or Facebook, the (sq. footage, no. of\nbedrooms, no. of bathrooms, walking distance) feature vector for your home might look\nsomething like:\u00bb600,1,1,60\u00bc. However, if you live in Pittsburgh, it might look more like\n\u00bb3000,4,3,10\u00bc. Fixed-lengthfeaturevectorslikethisareessentialformostclassicmachine\nlearning algorithms.\nWhatmakesaproblemaregressionisactuallytheformofthetarget. Saythatyouareinthe\nmarket for a new home. You might want to estimate the fair market value of a house, given\nsomefeaturessuchasabove. Thedataheremightconsistofhistoricalhomelistingsandthe\nlabels might be the observed sales prices. When labels take on arbitrary numerical values\n(even within some interval), we call this a regression problem. The goal is to produce a\nmodel whose predictions closely approximate the actual label values.\nLotsofpracticalproblemsareeasilydescribedasregressionproblems. Predictingtherating\nthat a user will assign to a movie can be thought of as a regression problem and if you\ndesigned a great algorithm to accomplish this feat in 2009, you might have won the 1-\nmillion-dollar Netflix prize19. Predicting the length of stay for patients in the hospital is\nalso a regression problem. A good rule of thumb is that any how much? orhow many?\nproblem is likely to be regression. For example:\n\u000fHow many hours will this surgery take?\n\u000fHow much rainfall will this town have in the next six hours?\nEven if you have never worked with machine learning before, you have probably worked\nthrougharegressionprobleminformally. Imagine,forexample,thatyouhadyourdrainsre-\npairedandthatyourcontractorspent3hoursremovinggunkfromyoursewagepipes. Then\nthey sent you a bill of 350 dollars. Now imagine that your friend hired the same contractor\nfor 2 hours and received a bill of 250 dollars. If someone then asked you how much to\nexpect on their upcoming gunk-removal invoice you might make some reasonable assump-\ntions, such as more hours worked costs more dollars. You might also assume that there is\nsome base charge and that the contractor then charges per hour. If these assumptions held\ntrue,thengiventhesetwodataexamples,youcouldalreadyidentifythecontractor\u2019spricing\nstructure: 100 dollars per hour plus 50 dollars to show up at your house. If you followed\nthatmuch,thenyoualreadyunderstandthehigh-levelideabehind linearregression.\nIn this case, we could produce the parameters that exactly matched the contractor\u2019s prices.\nSometimes this is not possible, e.g., if some of the variation arises from factors beyond\nyour two features. In these cases, we will try to learn models that minimize the distance\nbetween our predictions and the observed values. In most of our chapters, we will focus on\nminimizing the squared error loss function. As we will see later, this loss corresponds to\nthe assumption that our data were corrupted by Gaussian noise.\nClassification\nWhileregressionmodelsaregreatforaddressing howmany? questions,lotsofproblemsdo\nnot fit comfortably in this template. Consider, for example, a bank that wants to develop a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5881026a-a66b-4449-93df-a2eb9e9fd38f": {"__data__": {"id_": "5881026a-a66b-4449-93df-a2eb9e9fd38f", "embedding": null, "metadata": {"page_label": "10", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f851b04-1eb8-4853-8cb2-6522a9935683", "node_type": "4", "metadata": {"page_label": "10", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c88e4e1a9f30d0fea618c9486f3f331d0409455ceeb85d03ed9b677a6397a100", "class_name": "RelatedNodeInfo"}}, "text": "10 Introduction\ncheck scanning feature for its mobile app. Ideally, the customer would simply snap a photo\nof a check and the app would automatically recognize the text from the image. Assuming\nthat we had some ability to segment out image patches corresponding to each handwritten\ncharacter, then the primary remaining task would be to determine which character among\nsome known set is depicted in each image patch. These kinds of which one? problems\nare called classification and require a different set of tools from those used for regression,\nalthough many techniques will carry over.\nInclassification , we want our model to look at features, e.g., the pixel values in an image,\nand then predict to which category (sometimes called a class) among some discrete set\nof options, an example belongs. For handwritten digits, we might have ten classes, corre-\nsponding to the digits 0 through 9. The simplest form of classification is when there are\nonly two classes, a problem which we call binary classification . For example, our dataset\ncouldconsistofimagesofanimalsandourlabelsmightbetheclasses{cat, dog}. Whereas\nin regression we sought a regressor to output a numerical value, in classification we seek a\nclassifier, whose output is the predicted class assignment.\nFor reasons that we will get into as the book gets more technical, it can be difficult to opti-\nmizeamodelthatcanonlyoutputa firmcategoricalassignment,e.g.,either\u201ccat\u201dor\u201cdog\u201d.\nIn these cases, it is usually much easier to express our model in the language of probabili-\nties. Given features of an example, our model assigns a probability to each possible class.\nReturning to our animal classification example where the classes are {cat, dog}, a classi-\nfier might see an image and output the probability that the image is a cat as 0.9. We can\ninterpret this number by saying that the classifier is 90% sure that the image depicts a cat.\nThe magnitude of the probability for the predicted class conveys a notion of uncertainty.\nIt is not the only one available and we will discuss others in chapters dealing with more\nadvanced topics.\nWhenwehavemorethantwopossibleclasses,wecalltheproblem multiclassclassification .\nCommonexamplesincludehandwrittencharacterrecognition{0, 1, 2, ... 9, a, b, c, ...}. While\nwe attacked regression problems by trying to minimize the squared error loss function, the\ncommonlossfunctionforclassificationproblemsiscalled cross-entropy , whosenamewill\nbe demystified when we introduce information theory in later chapters.\nNote that the most likely class is not necessarily the one that you are going to use for your\ndecision. Assume that you find a beautiful mushroom in your backyard as shown in Fig.\n1.3.2.\nNow, assumethatyoubuiltaclassifierandtrainedittopredictwhetheramushroomispoi-\nsonous based on a photograph. Say our poison-detection classifier outputs that the proba-\nbility that Fig. 1.3.2 shows a death cap is 0.2. In other words, the classifier is 80% sure that\nourmushroomisnotadeathcap. Still,youwouldhavetobeafooltoeatit. Thatisbecause\nthe certain benefit of a delicious dinner is not worth a 20% risk of dying from it. In other\nwords, the effect of the uncertain risk outweighs the benefit by far. Thus, in order to make\na decision about whether to eat the mushroom, we need to compute the expected detriment\nassociated with each action which depends both on the likely outcomes and the benefits or\nharms associated with each. In this case, the detriment incurred by eating the mushroom", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3499, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01a51abe-c613-4a2f-8b18-eaa5607a0c0f": {"__data__": {"id_": "01a51abe-c613-4a2f-8b18-eaa5607a0c0f", "embedding": null, "metadata": {"page_label": "11", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d9331a7-06bc-4156-a1cf-c028d1b8f952", "node_type": "4", "metadata": {"page_label": "11", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c4100b1e7cafd8901049c2084f374eb9f12a81d4c215052b02bc3fcd0643ac55", "class_name": "RelatedNodeInfo"}}, "text": "11 Kinds of Machine Learning Problems\ntFig. 1.3.2 Death cap - do not eat!\n20might be 0.2\u00021\u00b8 0.8\u00020=1, whereas the loss of discarding it is 0.2\u00020\u00b80.8\u00021=0.8.\nOur caution was justified: as any mycologist would tell us, the mushroom in Fig. 1.3.2 is\nactually a death cap.\nClassification can get much more complicated than just binary or multiclass classification.\nFor instance, there are some variants of classification addressing hierarchically structured\nclasses. Insuchcasesnotallerrorsareequal\u2014ifwemusterr,wemightprefertomisclassify\nto a related class rather than a distant class. Usually, this is referred to as hierarchical\nclassification . For inspiration, you might think of Linnaeus20, who organized fauna in a\nhierarchy.\nInthecaseofanimalclassification,itmightnotbesobadtomistakeapoodleforaschnauzer,\nbut our model would pay a huge penalty if it confused a poodle with a dinosaur. Which\nhierarchy is relevant might depend on how you plan to use the model. For example, rat-\ntlesnakes and garter snakes might be close on the phylogenetic tree, but mistaking a rattler\nfor a garter could have fatal consequences.\nTagging\nSome classification problems fit neatly into the binary or multiclass classification setups.\nForexample,wecouldtrainanormalbinaryclassifiertodistinguishcatsfromdogs. Given\nthecurrentstateofcomputervision,wecandothiseasily,withoff-the-shelftools. Nonethe-\nless, no matter how accurate our model gets, we might find ourselves in trouble when the\nclassifier encounters an image of the Town Musicians of Bremen , a popular German fairy\ntale featuring four animals ( Fig. 1.3.3 ).\nAs you can see, the photo features a cat, a rooster, a dog, and a donkey, with some trees in\nthebackground. Ifweanticipateencounteringsuchimages, multiclassclassificationmight\nnot be the right problem formulation. Instead, we might want to give the model the option\nof saying the image depicts a cat, a dog, a donkey, anda rooster.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93f3b5b6-01ac-4cd1-8473-ac16d244aa27": {"__data__": {"id_": "93f3b5b6-01ac-4cd1-8473-ac16d244aa27", "embedding": null, "metadata": {"page_label": "12", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "993ecd16-3dbb-4ea7-8382-601df0df9a73", "node_type": "4", "metadata": {"page_label": "12", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4c6ee53f4aa6a6e79c24c351f95e78ec456526ee3baf4ea3ddda9db5612c1e90", "class_name": "RelatedNodeInfo"}}, "text": "12 Introduction\ntFig. 1.3.3 A donkey, a dog, a cat, and a rooster.\n21The problem of learning to predict classes that are not mutually exclusive is called multi-\nlabel classification . Auto-tagging problems are typically best described in terms of multi-\nlabel classification. Think of the tags people might apply to posts on a technical blog, e.g.,\n\u201cmachine learning\u201d, \u201ctechnology\u201d, \u201cgadgets\u201d, \u201cprogramming languages\u201d, \u201cLinux\u201d, \u201ccloud\ncomputing\u201d, \u201cAWS\u201d. A typical article might have 5\u201310 tags applied. Typically, tags will\nexhibit some correlation structure. Posts about \u201ccloud computing\u201d are likely to mention\n\u201cAWS\u201d and posts about \u201cmachine learning\u201d are likely to mention \u201cGPUs\u201d.\nSometimes such tagging problems draw on enormous label sets. The National Library of\nMedicineemploysmanyprofessionalannotatorswhoassociateeacharticletobeindexedin\nPubMed with a set of tags drawn from the Medical Subject Headings (MeSH) ontology, a\ncollection of roughly 28,000 tags. Correctly tagging articles is important because it allows\nresearchers to conduct exhaustive reviews of the literature. This is a time-consuming pro-\ncessand typicallythere isa one-yearlagbetweenarchivingand tagging. Machinelearning\ncan provide provisional tags until each article has a proper manual review. Indeed, for\nseveral years, the BioASQ organization has hosted competitions21for this task.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1363, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13975d75-bf86-4362-8056-afadc898ab21": {"__data__": {"id_": "13975d75-bf86-4362-8056-afadc898ab21", "embedding": null, "metadata": {"page_label": "13", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3353383-62e7-460c-9725-5c306b9e0579", "node_type": "4", "metadata": {"page_label": "13", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8fa555a58d0247396d0b7455ecbb3539681ed7589d5316778f1cdebe365b88e0", "class_name": "RelatedNodeInfo"}}, "text": "13 Kinds of Machine Learning Problems\n22Search\nIn the field of information retrieval, we often impose ranks on sets of items. Take web\nsearchforexample. Thegoalislesstodetermine whether aparticularpageisrelevantfora\nquery,butratherwhich,amongasetofrelevantresults,shouldbeshownmostprominently\nto a particular user. One way of doing this might be to first assign a score to every element\nin the set and then to retrieve the top-rated elements. PageRank22, the original secret\nsauce behind the Google search engine, was an early example of such a scoring system.\nWeirdly, the scoring provided by PageRank did not depend on the actual query. Instead,\nthey relied on a simple relevance filter to identify the set of relevant candidates and then\nused PageRank to prioritize the more authoritative pages. Nowadays, search engines use\nmachinelearningandbehavioralmodelstoobtainquery-dependentrelevancescores. There\nare entire academic conferences devoted to this subject.\nRecommender Systems\nRecommender systems are another problem setting that is related to search and ranking.\nThe problems are similar insofar as the goal is to display a set of items relevant to the user.\nThe main difference is the emphasis on personalization to specific users in the context of\nrecommender systems. For instance, for movie recommendations, the results page for a\nscience fiction fan and the results page for a connoisseur of Peter Sellers comedies might\ndiffer significantly. Similar problems pop up in other recommendation settings, e.g., for\nretail products, music, and news recommendation.\nInsomecases,customersprovideexplicitfeedback,communicatinghowmuchtheylikeda\nparticularproduct(e.g.,theproductratingsandreviewsonAmazon,IMDb,orGoodreads).\nIn other cases, they provide implicit feedback, e.g., by skipping titles on a playlist, which\nmight indicate dissatisfaction or maybe just indicate that the song was inappropriate in\ncontext. In the simplest formulations, these systems are trained to estimate some score,\nsuchasanexpectedstarratingortheprobabilitythatagivenuserwillpurchaseaparticular\nitem.\nGiven such a model, for any given user, we could retrieve the set of objects with the largest\nscores, which could then be recommended to the user. Production systems are consider-\nably more advanced and take detailed user activity and item characteristics into account\nwhencomputingsuchscores. Fig.1.3.4 displaysthedeeplearningbooksrecommendedby\nAmazon based on personalization algorithms tuned to capture Aston\u2019s preferences.\nDespite their tremendous economic value, recommender systems naively built on top of\npredictivemodelssuffersomeseriousconceptualflaws. Tostart,weonlyobserve censored\nfeedback : users preferentially rate movies that they feel strongly about. For example, on\na five-point scale, you might notice that items receive many one- and five-star ratings but\nthat there are conspicuously few three-star ratings. Moreover, current purchase habits are\noften a result of the recommendation algorithm currently in place, but learning algorithms\ndo not always take this detail into account. Thus it is possible for feedback loops to form\nwhere a recommender system preferentially pushes an item that is then taken to be better\n(due to greater purchases) and in turn is recommended even more frequently. Many of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3137bc4c-ddf3-466c-8443-480352a6fc6f": {"__data__": {"id_": "3137bc4c-ddf3-466c-8443-480352a6fc6f", "embedding": null, "metadata": {"page_label": "14", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e26e1194-279a-474f-bf68-a760cc5f5fe2", "node_type": "4", "metadata": {"page_label": "14", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7762bda0ade3dc820abdc887038014479a903c2d7a5ee858e62ee5e4a5574566", "class_name": "RelatedNodeInfo"}}, "text": "14 Introduction\ntFig. 1.3.4 Deep learning books recommended by Amazon.\nthese problems\u2014about how to deal with censoring, incentives, and feedback loops\u2014are\nimportant open research questions.\nSequenceLearning\nSofar,wehavelookedatproblemswherewehavesomefixednumberofinputsandproduce\na fixed number of outputs. For example, we considered predicting house prices given a\nfixed set of features: square footage, number of bedrooms, number of bathrooms, and the\ntransit time to downtown. We also discussed mapping from an image (of fixed dimension)\nto the predicted probabilities that it belongs to each among a fixed number of classes and\npredictingstarratingsassociatedwithpurchasesbasedontheuserIDandproductIDalone.\nIn these cases, once our model is trained, after each test example is fed into our model, it\nis immediately forgotten. We assumed that successive observations were independent and\nthus there was no need to hold on to this context.\nBut how should we deal with video snippets? In this case, each snippet might consist of\na different number of frames. And our guess of what is going on in each frame might be\nmuchstrongerifwetakeintoaccountthepreviousorsucceedingframes. Thesamegoesfor\nlanguage. Forexample,onepopulardeeplearningproblemismachinetranslation: thetask\nof ingesting sentences in some source language and predicting their translations in another\nlanguage.\nSuch problems also occur in medicine. We might want a model to monitor patients in the\nintensive care unit and to fire off alerts whenever their risk of dying in the next 24 hours\nexceeds some threshold. Here, we would not throw away everything that we know about", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7df582c1-602c-4ae5-9472-b8cdd17100be": {"__data__": {"id_": "7df582c1-602c-4ae5-9472-b8cdd17100be", "embedding": null, "metadata": {"page_label": "15", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30aa02eb-ec59-4612-b2fe-c41afbf31f05", "node_type": "4", "metadata": {"page_label": "15", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6b7f3484c9d503b715b52509559468ab48199a21a4db1b87d1cd269826aa1075", "class_name": "RelatedNodeInfo"}}, "text": "15 Kinds of Machine Learning Problems\nthe patient history every hour, because we might not want to make predictions based only\non the most recent measurements.\nQuestions like these are among the most exciting applications of machine learning and\nthey are instances of sequence learning . They require a model either to ingest sequences\nof inputs or to emit sequences of outputs (or both). Specifically, sequence-to-sequence\nlearning considers problems where both inputs and outputs consist of variable-length se-\nquences. Examples include machine translation and speech-to-text transcription. While it\nis impossible to consider all types of sequence transformations, the following special cases\nare worth mentioning.\nTagging and Parsing . This involves annotating a text sequence with attributes. Here,\nthe inputs and outputs are aligned, i.e., they are of the same number and occur in a corre-\nsponding order. For instance, in part-of-speech (PoS) tagging , we annotate every word in\na sentence with the corresponding part of speech, i.e., \u201cnoun\u201d or \u201cdirect object\u201d. Alterna-\ntively, we might want to know which groups of contiguous words refer to named entities,\nlikepeople,places, ororganizations . In the cartoonishly simple example below, we might\njust want to indicate whether or not any word in the sentence is part of a named entity\n(tagged as \u201cEnt\u201d).\nTom has dinner in Washington with Sally\nEnt - - - Ent - Ent\nAutomatic Speech Recognition . With speech recognition, the input sequence is an audio\nrecording of a speaker ( Fig. 1.3.5 ), and the output is a transcript of what the speaker said.\nThe challenge is that there are many more audio frames (sound is typically sampled at\n8kHz or 16kHz) than text, i.e., there is no 1:1 correspondence between audio and text,\nsince thousands of samples may correspond to a single spoken word. These are sequence-\nto-sequence learning problems, where the output is much shorter than the input. While\nhumans are remarkably good at recognizing speech, even from low-quality audio, getting\ncomputers to perform the same feat is a formidable challenge.\ntFig. 1.3.5 -D-e-e-p- L-ea-r-ni-ng- in an audio recording.\nTexttoSpeech . This is the inverse of automatic speech recognition. Here, the input is text\nandtheoutputisanaudiofile. Inthiscase,theoutputismuchlongerthantheinput.\nMachineTranslation . Unlike the case of speech recognition, where corresponding inputs\nand outputs occur in the same order, in machine translation, unaligned data poses a new\nchallenge. Here the input and output sequences can have different lengths, and the corre-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2586, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f892c4cf-8c0e-4278-bd9c-95a8c6225667": {"__data__": {"id_": "f892c4cf-8c0e-4278-bd9c-95a8c6225667", "embedding": null, "metadata": {"page_label": "16", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b3e409d-a989-46f2-a0e8-e68eeb5bed44", "node_type": "4", "metadata": {"page_label": "16", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2fec4b4ac47827ed5e37284ce596aa3b0481805f6d87665f61492c25c06f19c9", "class_name": "RelatedNodeInfo"}}, "text": "16 Introduction\nsponding regions of the respective sequences may appear in a different order. Consider the\nfollowingillustrativeexampleofthepeculiartendencyofGermanstoplacetheverbsatthe\nend of sentences:\nGerman: Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?\nEnglish: Have you already looked at this excellent textbook?\nWrong alignment: Have you yourself already this excellent textbook looked at?\nMany related problems pop up in other learning tasks. For instance, determining the order\nin which a user reads a webpage is a two-dimensional layout analysis problem. Dialogue\nproblemsexhibitallkindsofadditionalcomplications,wheredeterminingwhattosaynext\nrequires taking into account real-world knowledge and the prior state of the conversation\nacross long temporal distances. Such topics are active areas of research.\n1.3.2Unsupervisedand Self-Supervised Learning\nThe previous examples focused on supervised learning, where we feed the model a giant\ndataset containing both the features and corresponding label values. You could think of\nthe supervised learner as having an extremely specialized job and an extremely dictatorial\nboss. Thebossstandsoverthelearner\u2019sshoulderandtellsthemexactlywhattodoinevery\nsituationuntiltheylearntomapfromsituationstoactions. Workingforsuchabosssounds\npretty lame. On the other hand, pleasing such a boss is pretty easy. You just recognize the\npattern as quickly as possible and imitate the boss\u2019s actions.\nConsidering the opposite situation, it could be frustrating to work for a boss who has no\nidea what they want you to do. However, if you plan to be a data scientist, you had better\nget used to it. The boss might just hand you a giant dump of data and tell you to do some\ndatasciencewithit! This sounds vague because it is vague. We call this class of problems\nunsupervisedlearning , and the type and number of questions we can ask is limited only by\nourcreativity. Wewilladdressunsupervisedlearningtechniquesinlaterchapters. Towhet\nyour appetite for now, we describe a few of the following questions you might ask.\n\u000fCan we find a small number of prototypes that accurately summarize the data? Given a\nsetofphotos,canwegroupthemintolandscapephotos,picturesofdogs,babies,cats,\nand mountain peaks? Likewise, given a collection of users\u2019 browsing activities, can\nwe group them into users with similar behavior? This problem is typically known as\nclustering .\n\u000fCanwefindasmallnumberofparametersthataccuratelycapturetherelevantproperties\nof the data? The trajectories of a ball are well described by velocity, diameter, and\nmass of the ball. Tailors have developed a small number of parameters that describe\nhuman body shape fairlyaccuratelyforthe purpose of fitting clothes. These problems\narereferredtoas subspaceestimation . Ifthedependenceislinear,itiscalled principal\ncomponentanalysis .\n\u000fIs there a representation of (arbitrarily structured) objects in Euclidean space such that\nsymbolic properties can be well matched? This can be used to describe entities and\ntheir relations, such as \u201cRome\u201d \u0000\u201cItaly\u201d\u00b8\u201cFrance\u201d =\u201cParis\u201d.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a586481f-c25e-4ad9-bc02-65108fb45c8c": {"__data__": {"id_": "a586481f-c25e-4ad9-bc02-65108fb45c8c", "embedding": null, "metadata": {"page_label": "17", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f893caf-6c66-4b59-bbd6-636fb8686bea", "node_type": "4", "metadata": {"page_label": "17", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3d1e6674c9d4ce58d6eb7af9e275670d2307daeb026145afc848f44280fe3e08", "class_name": "RelatedNodeInfo"}}, "text": "17 Kinds of Machine Learning Problems\n\u000fIsthereadescriptionoftherootcausesofmuchofthedatathatweobserve? Forinstance,\nifwehavedemographicdataabouthouseprices,pollution,crime,location,education,\nand salaries, can we discover how they are related simply based on empirical data?\nThe fields concerned with causality andprobabilistic graphical models tackle such\nquestions.\n\u000fAnother important and exciting recent development in unsupervised learning is the ad-\nvent ofdeep generativemodels . These models estimate the density of the data, either\nexplicitly or implicitly . Once trained, we can use a generative model either to score\nexamples according to how likely they are, or to sample synthetic examples from the\nlearned distribution. Early deep learning breakthroughs in generative modeling came\nwith the invention of variational autoencoders (Kingma and Welling, 2014 ,Rezende\net al., 2014) and continued with the development of generative adversarial networks\n(Goodfellow et al., 2014). More recent advances include normalizing flows ( Dinhet\nal., 2014,Dinhet al., 2017) and diffusion models ( Hoet al., 2020,Sohl-Dickstein et\nal., 2015,Song and Ermon, 2019 ,Songetal., 2021).\nA further development in unsupervised learning has been the rise of self-supervisedlearn-\ning, techniques that leverage some aspect of the unlabeled data to provide supervision. For\ntext, we can train models to \u201cfill in the blanks\u201d by predicting randomly masked words us-\ning their surrounding words (contexts) in big corpora without any labeling effort ( Devlin\net al., 2018)! For images, we may train models to tell the relative position between two\ncroppedregionsofthesameimage( Doerschetal., 2015), topredictanoccludedpartofan\nimage based on the remaining portions of the image, or to predict whether two examples\nare perturbed versions of the same underlying image. Self-supervised models often learn\nrepresentationsthataresubsequentlyleveragedbyfine-tuningtheresultingmodelsonsome\ndownstream task of interest.\n1.3.3Interactingwith an Environment\nSo far, we have not discussed where data actually comes from, or what actually happens\nwhen a machine learning model generates an output. That is because supervised learning\nand unsupervised learning do not address these issues in a very sophisticated way. In each\ncase,wegrababigpileofdataupfront,thensetourpatternrecognitionmachinesinmotion\nwithout ever interacting with the environment again. Because all the learning takes place\nafter the algorithm is disconnected from the environment, this is sometimes called offline\nlearning . Forexample,supervisedlearningassumesthesimpleinteractionpatterndepicted\ninFig. 1.3.6 .\nThis simplicity of offline learning has its charms. The upside is that we can worry about\npattern recognition in isolation, with no concern about complications arising from interac-\ntions with a dynamic environment. But this problem formulation is limiting. If you grew\nup reading Asimov\u2019s Robot novels, then you probably picture artificially intelligent agents\ncapable not only of making predictions, but also of taking actions in the world. We want\nto think about intelligent agents, not just predictive models. This means that we need to\nthink about choosing actions, not just making predictions. In contrast to mere predictions,\nactions actually impact the environment. If we want to train an intelligent agent, we must", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78b2ab0b-58ff-4e42-b704-d156302e243e": {"__data__": {"id_": "78b2ab0b-58ff-4e42-b704-d156302e243e", "embedding": null, "metadata": {"page_label": "18", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b059111-7d3f-4f17-a0d4-e5c40ca3df67", "node_type": "4", "metadata": {"page_label": "18", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b1e1a7046b882addc1aa3e6a3274e0476227baae5abadbe9907ea9df998ac470", "class_name": "RelatedNodeInfo"}}, "text": "18 Introduction\ntFig. 1.3.6 Collecting data for supervised learning from an environment.\naccount for the way its actions might impact the future observations of the agent, and so\noffline learning is inappropriate.\nConsidering the interaction with an environment opens a whole set of new modeling ques-\ntions. The following are just a few examples.\n\u000fDoes the environment remember what we did previously?\n\u000fDoes the environment want to help us, e.g., a user reading text into a speech recognizer?\n\u000fDoes the environment want to beat us, e.g., spammers adapting their emails to evade\nspam filters?\n\u000fDoes the environment have shifting dynamics? For example, would future data always\nresemble the past or would the patterns change over time, either naturally or in re-\nsponse to our automated tools?\nThese questions raise the problem of distribution shift , where training and test data are\ndifferent. An example of this, that many of us may have met, is when taking exams written\nby a lecturer, while the homework was composed by their teaching assistants. Next, we\nbriefly describe reinforcement learning, a rich framework for posing learning problems in\nwhich an agent interacts with an environment.\n1.3.4ReinforcementLearning\nIf you are interested in using machine learning to develop an agent that interacts with an\nenvironment and takes actions, then you are probably going to wind up focusing on re-\ninforcement learning . This might include applications to robotics, to dialogue systems,\nand even to developing artificial intelligence (AI) for video games. Deep reinforcement\nlearning , which applies deep learning to reinforcement learning problems, has surged in\npopularity. The breakthroughdeepQ-network, thatbeathumans atAtari gamesusingonly\nthe visual input ( Mnihetal., 2015), and the AlphaGo program, which dethroned the world\nchampion at the board game Go ( Silveretal., 2016), are two prominent examples.\nReinforcementlearninggivesaverygeneralstatementofaprobleminwhichanagentinter-\nacts with an environment over a series of time steps. At each time step, the agent receives\nsomeobservation from the environment and must choose an actionthat is subsequently\ntransmitted back to the environment via some mechanism (sometimes called an actuator ),\nwhen, after each loop, the agent receives a reward from the environment. This process is", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "599dbd15-a868-4dfa-81ce-1175022507a9": {"__data__": {"id_": "599dbd15-a868-4dfa-81ce-1175022507a9", "embedding": null, "metadata": {"page_label": "19", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71d356a3-d694-40ad-9d24-75e98adb1162", "node_type": "4", "metadata": {"page_label": "19", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f73a2859013fc6e3c96d88e9114e18381e4870e017ddd3eb7ce36ea401a66736", "class_name": "RelatedNodeInfo"}}, "text": "19 Kinds of Machine Learning Problems\nillustrated in Fig. 1.3.7 . The agent then receives a subsequent observation, and chooses a\nsubsequent action, and so on. The behavior of a reinforcement learning agent is governed\nby apolicy. In brief, a policyis just a function that maps from observations of the environ-\nment to actions. The goal of reinforcement learning is to produce good policies.\ntFig. 1.3.7 The interaction between reinforcement learning and an environment.\nItishardtooverstatethegeneralityofthereinforcementlearningframework. Forexample,\nsupervised learning can be recast as reinforcement learning. Say we had a classification\nproblem. We could create a reinforcement learning agent with one action corresponding\nto each class. We could then create an environment which gave a reward that was exactly\nequal to the loss function from the original supervised learning problem.\nFurther, reinforcement learning can also address many problems that supervised learning\ncannot. Forexample,insupervisedlearning,wealwaysexpectthatthetraininginputcomes\nassociated with the correct label. But in reinforcement learning, we do not assume that,\nfor each observation the environment tells us the optimal action. In general, we just get\nsome reward. Moreover, the environment may not even tell us which actions led to the\nreward.\nConsiderthegameofchess. Theonlyrealrewardsignalcomesattheendofthegamewhen\nwe either win, earning a reward of, say, 1, or when we lose, receiving a reward of, say,\n\u00001. So reinforcement learners must deal with the creditassignment problem: determining\nwhich actions to credit or blame for an outcome. The same goes for an employee who gets\napromotiononOctober11. Thatpromotionlikelyreflectsanumberofwell-chosenactions\nover the previous year. Getting promoted in the future requires figuring out which actions\nalong the way led to the earlier promotions.\nReinforcement learners may also have to deal with the problem of partial observability.\nThat is, the current observation might not tell you everything about your current state. Say\nyour cleaning robot found itself trapped in one of many identical closets in your house.\nRescuing the robot involves inferring its precise location which might require considering\nearlier observations prior to it entering the closet.\nFinally, at any given point, reinforcement learners might know of one good policy, but\nthere might be many other better policies that the agent has never tried. The reinforcement\nlearner must constantly choose whether to exploitthe best (currently) known strategy as a\npolicy, or to explorethe space of strategies, potentially giving up some short-term reward\nin exchange for knowledge.\nThe general reinforcement learning problem has a very general setting. Actions affect sub-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2774, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bfac92d-e15a-47b8-bbad-cef729e947e9": {"__data__": {"id_": "2bfac92d-e15a-47b8-bbad-cef729e947e9", "embedding": null, "metadata": {"page_label": "20", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9dcc3c3-a559-4304-9591-3304f007e38e", "node_type": "4", "metadata": {"page_label": "20", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7ec41309c977af2255428246816d81ec46c91dc5d1994a8b43b2979e7019d365", "class_name": "RelatedNodeInfo"}}, "text": "20 Introduction\n23\n24\n25\n26sequent observations. Rewards are only observed when they correspond to the chosen ac-\ntions. The environment may be either fully or partially observed. Accounting for all this\ncomplexity at once may be asking too much. Moreover, not every practical problem ex-\nhibits all this complexity. As a result, researchers have studied a number of special cases\nof reinforcement learning problems.\nWhen the environment is fully observed, we call the reinforcement learning problem a\nMarkovdecisionprocess . When the state does not depend on the previous actions, we call\nit acontextual bandit problem . When there is no state, just a set of available actions with\ninitially unknown rewards, we have the classic multi-armed bandit problem .\n1.4Roots\nWe have just reviewed a small subset of problems that machine learning can address. For\na diverse set of machine learning problems, deep learning provides powerful tools for their\nsolution. Although many deep learning methods are recent inventions, the core ideas be-\nhind learning from data have been studied for centuries. In fact, humans have held the\ndesire to analyze data and to predict future outcomes for ages, and it is this desire that is\nat the root of much of natural science and mathematics. Two examples are the Bernoulli\ndistribution, named after Jacob Bernoulli (1655\u20131705)23, and the Gaussian distribution\ndiscovered by Carl Friedrich Gauss (1777\u20131855)24. Gauss invented, for instance, the least\nmean squares algorithm, which is still used today for a multitude of problems from insur-\nance calculations to medical diagnostics. Such tools enhanced the experimental approach\nin the natural sciences\u2014for instance, Ohm\u2019s law relating current and voltage in a resistor\nis perfectly described by a linear model.\nEven in the middle ages, mathematicians had a keen intuition of estimates. For instance,\nthe geometry book of Jacob K\u00f6bel (1460\u20131533)25illustrates averaging the length of 16\nadult men\u2019s feet to estimate the typical foot length in the population ( Fig. 1.4.1 ).\nAs a group of individuals exited a church, 16 adult men were asked to line up in a row\nand have their feet measured. The sum of these measurements was then divided by 16 to\nobtain an estimate for what now is called one foot. This \u201calgorithm\u201d was later improved to\ndeal with misshapen feet; The two men with the shortest and longest feet were sent away,\naveraging only over the remainder. This is among the earliest examples of a trimmed mean\nestimate.\nStatistics really took off with the availability and collection of data. One of its pioneers,\nRonald Fisher (1890\u20131962)26, contributed significantly to its theory and also its applica-\ntions in genetics. Many of his algorithms (such as linear discriminant analysis) and con-\ncepts (such as the Fisher information matrix) still hold a prominent place in the founda-\ntions of modern statistics. Even his data resources had a lasting impact. The Iris dataset\nthat Fisher released in 1936 is still sometimes used to demonstrate machine learning algo-\nrithms. Fisher was also a proponent of eugenics, which should remind us that the morally", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a66fb34c-e373-4216-90d6-cef2b90fcbed": {"__data__": {"id_": "a66fb34c-e373-4216-90d6-cef2b90fcbed", "embedding": null, "metadata": {"page_label": "21", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80fde06b-e558-4d1c-8b95-96f704a0c8ed", "node_type": "4", "metadata": {"page_label": "21", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "836f758eb0bbcec20507ef14e4530496ec7ae83513533d40ac2d2f98cbec8407", "class_name": "RelatedNodeInfo"}}, "text": "21 Roots\ntFig. 1.4.1 Estimating the length of a foot.\n27\n28\n29dubious use of data science has as long and enduring a history as its productive use in\nindustry and the natural sciences.\nOther influences for machine learning came from the information theory of Claude Shan-\nnon (1916\u20132001)27and the theory of computation proposed by Alan Turing (1912\u20131954)\n28. Turing posed the question \u201ccan machines think?\u201d in his famous paper Computing Ma-\nchineryandIntelligence (Turing, 1950 ). Describing what is now known as the Turing test,\nhe proposed that a machine can be considered intelligent if it is difficult for a human evalu-\nator to distinguish between the replies from a machine and those of a human, based purely\non textual interactions.\nFurther influences came from neuroscience and psychology. After all, humans clearly ex-\nhibit intelligent behavior. Many scholars have asked whether one could explain and pos-\nsibly reverse engineer this capacity. One of the first biologically inspired algorithms was\nformulated by Donald Hebb (1904\u20131985)29. In his groundbreaking book The Organiza-\ntion of Behavior (Hebb, 1949 ), he posited that neurons learn by positive reinforcement.\nThis became known as the Hebbian learning rule. These ideas inspired later work, such\nas Rosenblatt\u2019s perceptron learning algorithm, and laid the foundations of many stochastic\ngradient descent algorithms that underpin deep learning today: reinforce desirable behav-\nior and diminish undesirable behavior to obtain good settings of the parameters in a neural\nnetwork.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8ad36c6-143f-4a88-b578-a7a3ceaf3953": {"__data__": {"id_": "c8ad36c6-143f-4a88-b578-a7a3ceaf3953", "embedding": null, "metadata": {"page_label": "22", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "faa5f3df-ccc3-4ce9-9ab7-76660c24116b", "node_type": "4", "metadata": {"page_label": "22", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0520f42c2fada15749a0347c14348e69a7c052a569503a47e656cf56ba929c5b", "class_name": "RelatedNodeInfo"}}, "text": "22 Introduction\nBiological inspiration is what gave neuralnetworks their name. For over a century (dating\nback to the models of Alexander Bain, 1873, and James Sherrington, 1890), researchers\nhavetriedtoassemblecomputationalcircuitsthatresemblenetworksofinteractingneurons.\nOver time, the interpretation of biology has become less literal, but the name stuck. At its\nheart lie a few key principles that can be found in most networks today:\n\u000fThe alternation of linear and nonlinear processing units, often referred to as layers.\n\u000fThe use of the chain rule (also known as backpropagation ) for adjusting parameters in\nthe entire network at once.\nAfterinitialrapidprogress, researchinneuralnetworkslanguishedfromaround1995until\n2005. This was mainly due to two reasons. First, training a network is computationally\nvery expensive. While random-access memory was plentiful at the end of the past century,\ncomputational power was scarce. Second, datasets were relatively small. In fact, Fisher\u2019s\nIris dataset from 1936 was still a popular tool for testing the efficacy of algorithms. The\nMNIST dataset with its 60,000 handwritten digits was considered huge.\nGiven the scarcity of data and computation, strong statistical tools such as kernel methods,\ndecision trees, and graphical models proved empirically superior in many applications.\nMoreover, unlike neural networks, they did not require weeks to train and provided pre-\ndictable results with strong theoretical guarantees.\n1.5The Roadto Deep Learning\nMuch of this changed with the availabilityof massiveamounts of data, thanks to the World\nWide Web, the advent of companies serving hundreds of millions of users online, a dis-\nsemination of low-cost, high-quality sensors, inexpensive data storage (Kryder\u2019s law), and\ncheap computation (Moore\u2019s law). In particular, the landscape of computation in deep\nlearning was revolutionized by advances in GPUs that were originally engineered for com-\nputer gaming. Suddenly algorithms and models that seemed computationally infeasible\nwere within reach. This is best illustrated in tab_intro_decade .\n:Dataset vs. computer memory and computational power\nTable 1.5.1: label: tab_intro_decade", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9da1c07-f300-461e-84cd-c6f7f470907d": {"__data__": {"id_": "e9da1c07-f300-461e-84cd-c6f7f470907d", "embedding": null, "metadata": {"page_label": "23", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "193bd3b5-7552-444a-b940-2d54c3606127", "node_type": "4", "metadata": {"page_label": "23", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e6545b30b6ac5362eeb7756d8e0ecd4961de79fabb1ee2c96de333dcecb8ac5f", "class_name": "RelatedNodeInfo"}}, "text": "23 The Road to Deep Learning\nDecade Dataset Mem-\noryFloating point calculations per\nsecond\n1970 100 (Iris) 1 KB 100 KF (Intel 8080)\n1980 1 K (house prices in Boston) 100\nKB1 MF (Intel 80186)\n1990 10 K (optical character recog-\nnition)10 MB 10 MF (Intel 80486)\n2000 10 M (web pages) 100\nMB1 GF (Intel Core)\n2010 10 G (advertising) 1 GB 1 TF (NVIDIA C2050)\n2020 1 T (social network) 100\nGB1 PF (NVIDIA DGX-2)\nNote that random-access memory has not kept pace with the growth in data. At the same\ntime, increases in computational power have outpaced the growth in datasets. This means\nthatstatisticalmodelsneedtobecomemorememoryefficient,andsotheyarefreetospend\nmore computer cycles optimizing parameters, thanks to the increased compute budget.\nConsequently, the sweet spot in machine learning and statistics moved from (generalized)\nlinear models and kernel methods to deep neural networks. This is also one of the rea-\nsons why many of the mainstays of deep learning, such as multilayer perceptrons ( McCul-\nloch and Pitts, 1943 ), convolutional neural networks ( LeCunetal., 1998), long short-term\nmemory( HochreiterandSchmidhuber,1997 ),andQ-Learning( WatkinsandDayan,1992 ),\nwere essentially \u201crediscovered\u201d in the past decade, after lying comparatively dormant for\nconsiderable time.\nThe recent progress in statistical models, applications, and algorithms has sometimes been\nlikenedtotheCambrianexplosion: amomentofrapidprogressintheevolutionofspecies.\nIndeed, the state of the art is not just a mere consequence of available resources applied\nto decades-old algorithms. Note that the list of ideas below barely scratches the surface of\nwhat has helped researchers achieve tremendous progress over the past decade.\n\u000fNovelmethodsforcapacitycontrol,suchas dropout (Srivastava etal.,2014),havehelped\nto mitigate overfitting. Here, noise is injected ( Bishop, 1995 ) throughout the neural\nnetwork during training.\n\u000fAttention mechanisms solved a second problem that had plagued statistics for over a\ncentury: how to increase the memory and complexity of a system without increasing\nthe number of learnable parameters. Researchers found an elegant solution by using\nwhat can only be viewed as a learnable pointer structure (Bahdanau et al., 2014).\nRather than having to remember an entire text sequence, e.g., for machine translation\nin a fixed-dimensional representation, all that needed to be stored was a pointer to the\nintermediate state of the translation process. This allowed for significantly increased\naccuracyforlongsequences,sincethemodelnolongerneededtoremembertheentire\nsequence before commencing the generation of a new one.\n\u000fBuiltsolelyonattentionmechanisms,the Transformer architecture( Vaswanietal.,2017)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e724e379-5f22-4552-aa6d-c09866d2414d": {"__data__": {"id_": "e724e379-5f22-4552-aa6d-c09866d2414d", "embedding": null, "metadata": {"page_label": "24", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d48f4050-f2ea-43ff-9c61-14a2b05dc1cf", "node_type": "4", "metadata": {"page_label": "24", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f3013835dc44245e05b8e71c8ef78d6824bf2933dedf84f4d0fe686a7f436501", "class_name": "RelatedNodeInfo"}}, "text": "24 Introduction\n30has demonstrated superior scalingbehavior: it performs better with an increase in\ndataset size, model size, and amount of training compute ( Kaplanet al., 2020). This\narchitecture has demonstrated compelling success in a wide range of areas, such as\nnaturallanguageprocessing( Brownetal.,2020,Devlinetal.,2018),computervision\n(Dosovitskiy et al., 2021,Liuet al., 2021), speech recognition ( Gulatiet al., 2020),\nreinforcement learning ( Chenet al., 2021), and graph neural networks ( Dwivedi and\nBresson,2020 ). Forexample,asingleTransformerpretrainedonmodalitiesasdiverse\nas text, images, joint torques, and button presses can play Atari, caption images, chat,\nand control a robot ( Reedetal., 2022).\n\u000fModeling probabilities of text sequences, language models can predict text given other\ntext. Scaling up the data, model, and compute has unlocked a growing number of\ncapabilities of language models to perform desired tasks via human-like text genera-\ntionbasedoninputtext( Aniletal.,2023,Brownetal.,2020,Chowdhery etal.,2022,\nHoffmann etal.,2022,OpenAI,2023 ,Raeetal.,2021,Touvronetal.,2023a,Touvron\net al., 2023b). For instance, aligning language models with human intent ( Ouyanget\nal., 2022), OpenAI\u2019s ChatGPT30allows users to interact with it in a conversational\nway to solve problems, such as code debugging and creative writing.\n\u000fMulti-stagedesigns,e.g.,viathememorynetworks( Sukhbaatar etal.,2015)andtheneu-\nralprogrammer-interpreter( ReedandDeFreitas,2015 )permittedstatisticalmodelers\ntodescribeiterativeapproachestoreasoning. Thesetoolsallowforaninternalstateof\nthe deep neural network to be modified repeatedly, thus carrying out subsequent steps\nin a chain of reasoning, just as a processor can modify memory for a computation.\n\u000fA key development in deep generative modeling was the invention of generative adver-\nsarialnetworks (Goodfellow etal.,2014). Traditionally,statisticalmethodsfordensity\nestimation and generative models focused on finding proper probability distributions\nand (often approximate) algorithms for sampling from them. As a result, these algo-\nrithms were largely limited by the lack of flexibility inherent in the statistical models.\nThe crucial innovation in generative adversarial networks was to replace the sampler\nby an arbitrary algorithm with differentiable parameters. These are then adjusted in\nsuch a way that the discriminator (effectively a two-sample test) cannot distinguish\nfake from real data. Through the ability to use arbitrary algorithms to generate data,\ndensity estimation was opened up to a wide variety of techniques. Examples of gal-\nloping zebras ( Zhuet al., 2017) and of fake celebrity faces ( Karraset al., 2017) are\neach testimony to this progress. Even amateur doodlers can produce photorealistic\nimages just based on sketches describing the layout of a scene ( Parketal., 2019).\n\u000fFurthermore, while the diffusion process gradually adds random noise to data samples,\ndiffusionmodels (Hoetal.,2020,Sohl-Dickstein etal.,2015)learnthedenoisingpro-\ncess to gradually construct data samples from random noise, reversing the diffusion\nprocess. They have started to replace generative adversarial networks in more recent\ndeep generative models, such as in DALL-E 2 ( Rameshetal., 2022) and Imagen ( Sa-\nhariaetal., 2022) for creative art and image generation based on text descriptions.\n\u000fIn many cases, a single GPU is insufficient for processing the large amounts of data", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32f0698d-a8d9-4a9d-a2a4-d60be47a111e": {"__data__": {"id_": "32f0698d-a8d9-4a9d-a2a4-d60be47a111e", "embedding": null, "metadata": {"page_label": "25", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8741c4dc-bfa3-4852-b7b1-832e8a24b54f", "node_type": "4", "metadata": {"page_label": "25", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "da3bbc77ea3ecd8989b6b9e81a31db60c701855a50facc5ee5b8dcfea1adfa33", "class_name": "RelatedNodeInfo"}}, "text": "25 Success Stories\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42availablefortraining. Overthepastdecadetheabilitytobuildparallelanddistributed\ntrainingalgorithmshasimprovedsignificantly. Oneofthekeychallengesindesigning\nscalable algorithms is that the workhorse of deep learning optimization, stochastic\ngradient descent, relies on relatively small minibatches of data to be processed. At\nthe same time, small batches limit the efficiency of GPUs. Hence, training on 1,024\nGPUs with a minibatch size of, say, 32 images per batch amounts to an aggregate\nminibatch of about 32,000 images. Work, first by Li ( 2017) and subsequently by You\netal.(2017) and Jiaetal.(2018) pushed the size up to 64,000 observations, reducing\ntraining time forthe ResNet-50model on the ImageNetdataset to less than 7 minutes.\nBy comparison, training times were initially of the order of days.\n\u000fThe ability to parallelize computation has also contributed to progress in reinforcement\nlearning . This has led to significant progress in computers achieving superhuman\nperformanceontaskslikeGo,Atarigames,Starcraft,andinphysicssimulations(e.g.,\nusing MuJoCo) where environment simulators are available. See, e.g., Silver et al.\n(2016)foradescriptionofsuchachievementsinAlphaGo. Inanutshell,reinforcement\nlearningworksbestifplentyof(state,action, reward)tuplesareavailable. Simulation\nprovides such an avenue.\n\u000fDeep learning frameworks have played a crucial role in disseminating ideas. The first\ngenerationofopen-sourceframeworksforneuralnetworkmodelingconsistedof Caffe\n31,Torch32, andTheano33. Many seminal papers were written using these tools.\nThese have now been superseded by TensorFlow34(often used via its high-level API\nKeras35),CNTK36,Caffe 237, andApache MXNet38. The third generation of\nframeworks consists of so-called imperative tools for deep learning, a trend that was\narguably ignited by Chainer39, which used a syntax similar to Python NumPy to\ndescribe models. This idea was adopted by both PyTorch40, theGluon API41of\nMXNet, and JAX42.\nThe division of labor between system researchers building better tools and statistical mod-\nelers building better neural networks has greatly simplified things. For instance, training a\nlinear logistic regression model used to be a nontrivial homework problem, worthy to give\nto new machine learning Ph.D. students at Carnegie Mellon University in 2014. By now,\nthistaskcanbeaccomplishedwithunder10linesofcode,puttingitfirmlywithinthereach\nof any programmer.\n1.6Success Stories\nArtificial intelligence has a long history of delivering results that would be difficult to ac-\ncomplish otherwise. For instance, mail sorting systems using optical character recognition\nhave been deployed since the 1990s. This is, after all, the source of the famous MNIST\ndataset of handwritten digits. The same applies to reading checks for bank deposits and\nscoring creditworthiness of applicants. Financial transactions are checked for fraud auto-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e1a139e-eb22-4214-ac35-fd7f1ea73c92": {"__data__": {"id_": "2e1a139e-eb22-4214-ac35-fd7f1ea73c92", "embedding": null, "metadata": {"page_label": "26", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adede142-f86b-49fc-b002-f2f3a5c50505", "node_type": "4", "metadata": {"page_label": "26", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "291461fd2916ff9b307180c7d7694e8eaca8ae5c06a8c66b6775481c9b4ed0a0", "class_name": "RelatedNodeInfo"}}, "text": "26 Introduction\nmatically. Thisformsthebackboneofmanye-commercepaymentsystems,suchasPayPal,\nStripe, AliPay, WeChat, Apple, Visa, and MasterCard. Computer programs for chess have\nbeen competitive for decades. Machine learning feeds search, recommendation, personal-\nization, and ranking on the Internet. In other words, machine learning is pervasive, albeit\noften hidden from sight.\nItisonlyrecentlythatAIhasbeeninthelimelight,mostlyduetosolutionstoproblemsthat\nwere considered intractable previously and that are directly related to consumers. Many of\nsuch advances are attributed to deep learning.\n\u000fIntelligent assistants, such as Apple\u2019s Siri, Amazon\u2019s Alexa, and Google\u2019s assistant, are\nable to respond to spoken requests with a reasonable degree of accuracy. This in-\ncludes menial jobs, like turning on light switches, and more complex tasks, such as\narranging barber\u2019s appointments and offering phone support dialog. This is likely the\nmost noticeable sign that AI is affecting our lives.\n\u000fA key ingredient in digital assistants is their ability to recognize speech accurately. The\naccuracyofsuchsystemshasgraduallyincreasedtothepointofachievingparitywith\nhumans for certain applications ( Xiongetal., 2018).\n\u000fObject recognition has likewise come a long way. Identifying the object in a picture was\na fairly challenging task in 2010. On the ImageNet benchmark researchers from NEC\nLabs and University of Illinois at Urbana-Champaign achieved a top-five error rate\nof 28% ( Linet al., 2010). By 2017, this error rate was reduced to 2.25% ( Huet al.,\n2018). Similarly, stunning results have been achieved for identifying birdsong and for\ndiagnosing skin cancer.\n\u000fProwess in games used to provide a measuring stick for human ability. Starting from\nTD-Gammon, a program for playing backgammon using temporal difference rein-\nforcement learning, algorithmic and computational progress has led to algorithms for\na wide range of applications. Compared with backgammon, chess has a much more\ncomplex state space and set of actions. DeepBlue beat Garry Kasparov using mas-\nsive parallelism, special-purpose hardware and efficient search through the game tree\n(Campbell etal.,2002). Goismoredifficultstill,duetoitshugestatespace. AlphaGo\nreached human parity in 2015, using deep learning combined with Monte Carlo tree\nsampling ( Silveretal., 2016). The challenge in Poker was that the state space is large\nandonlypartiallyobserved(wedonotknowtheopponents\u2019cards). Libratusexceeded\nhuman performance in Poker using efficiently structured strategies ( Brown and Sand-\nholm, 2017 ).\n\u000fAnother indication of progress in AI is the advent of self-driving vehicles. While full\nautonomy is not yet within reach, excellent progress has been made in this direction,\nwith companies such as Tesla, NVIDIA, and Waymo shipping products that enable\npartial autonomy. What makes full autonomy so challenging is that proper driving\nrequires the ability to perceive, to reason and to incorporate rules into a system. At\npresent, deep learning is used primarily in the visual aspect of these problems. The\nrest is heavily tuned by engineers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a4afd83-d4e5-4abc-8e56-138e285a0439": {"__data__": {"id_": "6a4afd83-d4e5-4abc-8e56-138e285a0439", "embedding": null, "metadata": {"page_label": "27", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed028e55-e520-470c-beea-6aafeae3f4a6", "node_type": "4", "metadata": {"page_label": "27", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "51ecbe638d67be7c383d9e1e64b94fd963ff96a2e2643b8882559efbb8386c83", "class_name": "RelatedNodeInfo"}}, "text": "27 The Essence of Deep Learning\nThis barely scratches the surface of significant applications of machine learning. For in-\nstance, robotics, logistics, computational biology, particle physics, and astronomy owe\nsome of their most impressive recent advances at least in parts to machine learning, which\nis thus becoming a ubiquitous tool for engineers and scientists.\nFrequently, questions about a coming AI apocalypse and the plausibility of a singularity\nhave been raised in non-technical articles. The fear is that somehow machine learning\nsystems will become sentient and make decisions, independently of their programmers,\nthat directly impact the lives of humans. To some extent, AI already affects the livelihood\nof humans in direct ways: creditworthiness is assessed automatically, autopilots mostly\nnavigate vehicles, decisions about whether to grant bail use statistical data as input. More\nfrivolously, we can ask Alexa to switch on the coffee machine.\nFortunately, we are far from a sentient AI system that could deliberately manipulate its\nhuman creators. First, AI systems are engineered, trained, and deployed in a specific, goal-\norientedmanner. Whiletheirbehaviormightgivetheillusionofgeneralintelligence,itisa\ncombination of rules, heuristics and statistical models that underlie the design. Second, at\npresent, there are simply no tools for artificialgeneralintelligence that are able to improve\nthemselves,reasonaboutthemselves,andthatareabletomodify,extend,andimprovetheir\nown architecture while trying to solve general tasks.\nA much more pressing concern is how AI is being used in our daily lives. It is likely that\nmany routine tasks, currently fulfilled by humans, can and will be automated. Farm robots\nwill likely reduce the costs for organic farmers but they will also automate harvesting op-\nerations. This phase of the industrial revolution may have profound consequences for large\nswaths of society, since menial jobs provide much employment in many countries. Fur-\nthermore, statistical models, when applied without care, can lead to racial, gender, or age\nbias and raise reasonable concerns about procedural fairness if automated to drive conse-\nquential decisions. It is important to ensure that these algorithms are used with care. With\nwhat we know today, this strikes us as a much more pressing concern than the potential of\nmalevolent superintelligence for destroying humanity.\n1.7The Essence of Deep Learning\nThusfar,wehavetalkedinbroadtermsaboutmachinelearning. Deeplearningisthesubset\nof machine learning concerned with models based on many-layered neural networks. It is\ndeepinpreciselythesensethatitsmodelslearnmany layersoftransformations. Whilethis\nmightsoundnarrow,deeplearninghasgivenrisetoadizzyingarrayofmodels,techniques,\nproblem formulations, and applications. Many intuitions have been developed to explain\nthe benefits of depth. Arguably, all machine learning has many layers of computation, the\nfirst consisting of feature processing steps. What differentiates deep learning is that the\noperations learned at each of the many layers of representations are learned jointly from\ndata.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15b531d9-d868-42ef-972c-9f35b91b8797": {"__data__": {"id_": "15b531d9-d868-42ef-972c-9f35b91b8797", "embedding": null, "metadata": {"page_label": "28", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d1b6549-22b7-4e24-ada5-6a9502eaca5f", "node_type": "4", "metadata": {"page_label": "28", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c0f02a632fb6d54adf4ab1170dd7592351ea0ec0acfbc219d3cc780d08a46754", "class_name": "RelatedNodeInfo"}}, "text": "28 Introduction\nTheproblemsthatwehavediscussedsofar, suchaslearningfromtherawaudiosignal, the\nraw pixel values of images, or mapping between sentences of arbitrary lengths and their\ncounterparts in foreign languages, are those where deep learning excels and traditional\nmethods falter. It turns out that these many-layered models are capable of addressing low-\nlevel perceptual data in a way that previous tools could not. Arguably the most significant\ncommonality in deep learning methods is end-to-end training . That is, rather than assem-\nbling a system based on components that are individually tuned, one builds the system and\nthen tunes their performance jointly. For instance, in computer vision scientists used to\nseparate the process of feature engineering from the process of building machine learn-\ning models. The Canny edge detector ( Canny, 1987 ) and Lowe\u2019s SIFT feature extractor\n(Lowe, 2004 ) reigned supreme for over a decade as algorithms for mapping images into\nfeature vectors. In bygone days, the crucial part of applying machine learning to these\nproblems consisted of coming up with manually-engineered ways of transforming the data\ninto some form amenable to shallow models. Unfortunately, there is only so much that\nhumans can accomplish by ingenuity in comparison with a consistent evaluation over mil-\nlions of choices carried out automatically by an algorithm. When deep learning took over,\nthese feature extractors were replaced by automatically tuned filters that yielded superior\naccuracy.\nThus, one key advantage of deep learning is that it replaces not only the shallow models at\ntheendoftraditionallearningpipelines,butalsothelabor-intensiveprocessoffeatureengi-\nneering. Moreover,byreplacingmuchofthedomain-specificpreprocessing,deeplearning\nhas eliminated many of the boundaries that previously separated computer vision, speech\nrecognition, naturallanguageprocessing, medicalinformatics, andother applicationareas,\nthereby offering a unified set of tools for tackling diverse problems.\nBeyond end-to-end training, we are experiencing a transition from parametric statistical\ndescriptions to fullynonparametric models. When datais scarce, one needsto relyonsim-\nplifyingassumptionsaboutrealityinordertoobtainusefulmodels. Whendataisabundant,\nthese can be replaced by nonparametric models that better fit the data. To some extent, this\nmirrors the progress that physics experienced in the middle of the previous century with\nthe availability of computers. Rather than solving by hand parametric approximations of\nhow electrons behave, one can now resort to numerical simulations of the associated par-\ntial differential equations. This has led to much more accurate models, albeit often at the\nexpense of interpretation.\nAnother difference from previous work is the acceptance of suboptimal solutions, dealing\nwith nonconvex nonlinear optimization problems, and the willingness to try things before\nproving them. This new-found empiricism in dealing with statistical problems, combined\nwith a rapid influx of talent has led to rapid progress in the development of practical algo-\nrithms, albeit in many cases at the expense of modifying and re-inventing tools that existed\nfor decades.\nIntheend, thedeeplearningcommunitypridesitselfonsharingtoolsacrossacademicand\ncorporate boundaries, releasing many excellent libraries, statistical models, and trained\nnetworks as open source. It is in this spirit that the notebooks forming this book are freely\navailable for distribution and use. We have worked hard to lower the barriers of access for", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70443837-b621-4d2d-9952-16940a6111d0": {"__data__": {"id_": "70443837-b621-4d2d-9952-16940a6111d0", "embedding": null, "metadata": {"page_label": "29", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a07592f2-ca99-4d0c-aaaa-90ab9fbfa598", "node_type": "4", "metadata": {"page_label": "29", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9ca3cd51715be1b0cf397c1d130ca240a1d5918d1dafb6a43df9c74d00f377bf", "class_name": "RelatedNodeInfo"}}, "text": "29 Summary\n43anyonewishingtolearnaboutdeeplearningandwehopethatourreaderswillbenefitfrom\nthis.\n1.8Summary\nMachine learning studies how computer systems can leverage experience (often data) to\nimprove performance at specific tasks. It combines ideas from statistics, data mining, and\noptimization. Often, it is used as a means of implementing AI solutions. As a class of\nmachine learning, representational learning focuses on how to automatically find the ap-\npropriate way to represent data. Considered as multi-level representation learning through\nlearning many layers of transformations, deep learning replaces not only the shallow mod-\nels at the end of traditional machinelearning pipelines, but also the labor-intensiveprocess\nof feature engineering. Much of the recent progress in deep learning has been triggered\nby an abundance of data arising from cheap sensors and Internet-scale applications, and\nby significant progress in computation, mostly through GPUs. Furthermore, the availabil-\nity of efficient deep learning frameworks has made design and implementation of whole\nsystem optimization significantly easier, and this is a key component in obtaining high\nperformance.\n1.9Exercises\n1.Which parts of code that you are currently writing could be \u201clearned\u201d, i.e., improved\nby learning and automatically determining design choices that are made in your code?\nDoes your code include heuristic design choices? What data might you need to learn\nthe desired behavior?\n2.Which problems that you encounter have many examples for their solution, yet no spe-\ncificwayforautomatingthem? Thesemaybeprimecandidatesforusingdeeplearning.\n3.Describe the relationships between algorithms, data, and computation. How do char-\nacteristics of the data and the current available computational resources influence the\nappropriateness of various algorithms?\n4.Name some settings where end-to-end training is not currently the default approach but\nwhere it might be useful.\nDiscussions43.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81f4ac52-a5bf-4771-a1a8-676eb94a99a1": {"__data__": {"id_": "81f4ac52-a5bf-4771-a1a8-676eb94a99a1", "embedding": null, "metadata": {"page_label": "30", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff1812b8-79cf-4da5-982e-7ee198ff9407", "node_type": "4", "metadata": {"page_label": "30", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "00afbc61aa60e1886e52cb318cfc248b5ec414a75b57fa4b7001bb30d4c41836", "class_name": "RelatedNodeInfo"}}, "text": "2 Preliminaries\nTo prepare for your dive into deep learning, you will need a few survival skills: (i) tech-\nniques for storing and manipulating data; (ii) libraries for ingesting and preprocessing data\nfrom a variety of sources; (iii) knowledge of the basic linear algebraic operations that we\napply to high-dimensional data elements; (iv) just enough calculus to determine which di-\nrection to adjust each parameter in order to decrease the loss function; (v) the ability to\nautomatically compute derivatives so that you can forget much of the calculus you just\nlearned; (vi) some basic fluency in probability, our primary language for reasoning under\nuncertainty; and(vii)someaptitudeforfindinganswersintheofficialdocumentationwhen\nyou get stuck.\nInshort, thischapterprovidesarapidintroductiontothebasicsthatyouwillneedtofollow\nmostof the technical content in this book.\n2.1Data Manipulation\nIn order to get anything done, we need some way to store and manipulate data. Generally,\nthere are two important things we need to do with data: (i) acquire them; and (ii) process\nthem once they are inside the computer. There is no point in acquiring data without some\nway to store it, so to start, let\u2019s get our hands dirty with \ud835\udc5b-dimensional arrays, which we\nalsocalltensors. IfyoualreadyknowtheNumPyscientificcomputingpackage,thiswillbe\na breeze. For all modern deep learning frameworks, the tensor class (ndarray in MXNet,\nTensorin PyTorch and TensorFlow) resembles NumPy\u2019s ndarray , with a few killer fea-\ntures added. First, the tensor class supports automatic differentiation. Second, it leverages\nGPUs to accelerate numerical computation, whereas NumPy only runs on CPUs. These\nproperties make neural networks both easy to code and fast to run.\n2.1.1GettingStarted\nTo start, we import the PyTorch library. Note that the package name is torch.\nimport torch\nA tensor represents a (possibly multidimensional) array of numerical values. In the one-\ndimensionalcase, i.e., whenonlyoneaxisisneededforthedata, atensoriscalleda vector.\n30", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2032, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52a0de85-65fb-4c08-9bce-2569ffeae690": {"__data__": {"id_": "52a0de85-65fb-4c08-9bce-2569ffeae690", "embedding": null, "metadata": {"page_label": "31", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9734d0af-7af2-4e4d-a5b3-19545f934a24", "node_type": "4", "metadata": {"page_label": "31", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c65ed7c8845b4e122567f1d8a97cbcfad68503db43a318cfed63d2896c25cd13", "class_name": "RelatedNodeInfo"}}, "text": "31 Data Manipulation\nWith two axes, a tensor is called a matrix. With\ud835\udc58 > 2axes, we drop the specialized names\nand just refer to the object as a \ud835\udc58th-ordertensor .\nPyTorch provides a variety of functions for creating new tensors prepopulated with values.\nForexample,byinvoking arange(n) ,wecancreateavectorofevenlyspacedvalues,start-\ning at 0 (included) and ending at n(not included). By default, the interval size is 1. Unless\notherwisespecified,newtensorsarestoredinmainmemoryanddesignatedforCPU-based\ncomputation.\nx=torch .arange( 12, dtype =torch .float32)\nx\ntensor([ 0.,1.,2.,3.,4.,5.,6.,7.,8.,9.,10.,11.])\nEach of these values is called an element of the tensor. The tensor xcontains 12 elements.\nWe can inspect the total number of elements in a tensor via its numelmethod.\nx.numel()\n12\nWecanaccessatensor\u2019s shape(thelengthalongeachaxis)byinspectingits shapeattribute.\nBecause we are dealing with a vector here, the shapecontains just a single element and is\nidentical to the size.\nx.shape\ntorch .Size([ 12])\nWecanchangetheshapeofatensorwithoutalteringitssizeorvalues,byinvoking reshape .\nFor example, we can transform our vector xwhose shape is (12,) to a matrix Xwith shape\n(3,4). Thisnewtensorretainsallelementsbutreconfiguresthemintoamatrix. Noticethat\ntheelementsofourvectorarelaidoutonerowatatimeandthus x[3] == X[0, 3] .\nX=x.reshape( 3,4)\nX\ntensor([[ 0.,1.,2.,3.],\n[4.,5.,6.,7.],\n[8.,9.,10.,11.]])\nNote that specifying every shape component to reshape is redundant. Because we already\nknow our tensor\u2019s size, we can work out one component of the shape given the rest. For\nexample, given a tensor of size \ud835\udc5band target shape ( \u210e,\ud835\udc64), we know that \ud835\udc64=\ud835\udc5b\u009d\u210e. To", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1663, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfafcf4f-5410-47a3-b048-8ef5223f6654": {"__data__": {"id_": "bfafcf4f-5410-47a3-b048-8ef5223f6654", "embedding": null, "metadata": {"page_label": "32", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2612028b-7dd1-4ec1-a359-3a3870825446", "node_type": "4", "metadata": {"page_label": "32", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "38576ef6d81072bc41479ee51df56431f155b06a9bd926db2f681c2d79e47299", "class_name": "RelatedNodeInfo"}}, "text": "32 Preliminaries\nautomaticallyinferonecomponentoftheshape,wecanplacea -1fortheshapecomponent\nthatshouldbeinferredautomatically. Inourcase, insteadofcalling x.reshape(3, 4) , we\ncould have equivalently called x.reshape(-1, 4) orx.reshape(3, -1) .\nPractitioners often need to work with tensors initialized to contain all 0s or 1s. We can\nconstruct a tensor with all elements set to 0 and a shape of (2, 3, 4) via the zerosfunc-\ntion.\ntorch .zeros(( 2,3,4))\ntensor([[[ 0.,0.,0.,0.],\n[0.,0.,0.,0.],\n[0.,0.,0.,0.]],\n[[0.,0.,0.,0.],\n[0.,0.,0.,0.],\n[0.,0.,0.,0.]]])\nSimilarly, we can create a tensor with all 1s by invoking ones.\ntorch .ones(( 2,3,4))\ntensor([[[ 1.,1.,1.,1.],\n[1.,1.,1.,1.],\n[1.,1.,1.,1.]],\n[[1.,1.,1.,1.],\n[1.,1.,1.,1.],\n[1.,1.,1.,1.]]])\nWe often wish to sample each element randomly (and independently) from a given prob-\nability distribution. For example, the parameters of neural networks are often initialized\nrandomly. The following snippet creates a tensor with elements drawn from a standard\nGaussian (normal) distribution with mean 0 and standard deviation 1.\ntorch .randn( 3,4)\ntensor([[ 0.1351 ,-0.9099 ,-0.2028 ,2.1937 ],\n[-0.3200 ,-0.7545 ,0.8086 ,-1.8730 ],\n[0.3929 ,0.4931 ,0.9114 ,-0.7072 ]])\nFinally, we can construct tensors by supplying the exact values for each element by sup-\nplying (possibly nested) Python list(s) containing numerical literals. Here, we construct a\nmatrix with a list of lists, where the outermost list corresponds to axis 0, and the inner list\ncorresponds to axis 1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1518, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc4f408b-8989-401c-ad64-a9ef412fd064": {"__data__": {"id_": "bc4f408b-8989-401c-ad64-a9ef412fd064", "embedding": null, "metadata": {"page_label": "33", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f02f67e8-15b6-4701-8f7f-672b35b4b2da", "node_type": "4", "metadata": {"page_label": "33", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7083c4c8f64f14c4abd59d7e53471f88b8cd1feaaa0290c18245757e4a3b7dcd", "class_name": "RelatedNodeInfo"}}, "text": "33 Data Manipulation\ntorch .tensor([[ 2,1,4,3], [ 1,2,3,4], [ 4,3,2,1]])\ntensor([[ 2,1,4,3],\n[1,2,3,4],\n[4,3,2,1]])\n2.1.2Indexingand Slicing\nAswithPythonlists,wecanaccesstensorelementsbyindexing(startingwith0). Toaccess\nanelementbasedonitspositionrelativetotheendofthelist,wecanusenegativeindexing.\nFinally, we can access whole ranges of indices via slicing (e.g., X[start:stop] ), where\nthe returned value includes the first index ( start)but not the last (stop). Finally, when\nonly one index (or slice) is specified for a \ud835\udc58th-order tensor, it is applied along axis 0. Thus,\nin the following code, [-1]selects the last row and [1:3]selects the second and third\nrows.\nX[-1], X[ 1:3]\n(tensor([ 8.,9.,10.,11.]),\ntensor([[ 4.,5.,6.,7.],\n[8.,9.,10.,11.]]))\nBeyondreadingthem,wecanalso writeelementsofamatrixbyspecifyingindices.\nX[1,2]=17\nX\ntensor([[ 0.,1.,2.,3.],\n[4.,5.,17.,7.],\n[8.,9.,10.,11.]])\nIf we want to assign multiple elements the same value, we apply the indexing on the left-\nhandsideoftheassignmentoperation. Forinstance, [:2, :] accessesthefirstandsecond\nrows, where :takes all the elements along axis 1 (column). While we discussed indexing\nformatrices,thisalsoworksforvectorsandfortensorsofmorethantwodimensions.\nX[:2, :] =12\nX\ntensor([[ 12.,12.,12.,12.],\n[12.,12.,12.,12.],\n[8.,9.,10.,11.]])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "33ce1c7d-dc1a-4e46-857d-8ea2cd3cf2b6": {"__data__": {"id_": "33ce1c7d-dc1a-4e46-857d-8ea2cd3cf2b6", "embedding": null, "metadata": {"page_label": "34", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4008b542-5c38-412d-a8b2-0ec2b53e812d", "node_type": "4", "metadata": {"page_label": "34", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "430de36e1a3e28031e4b3f2a4a0546f30a4bc4da12e84cb58b8c17e661925071", "class_name": "RelatedNodeInfo"}}, "text": "34 Preliminaries\n2.1.3Operations\nNow that we know how to construct tensors and how to read from and write to their ele-\nments,wecanbegintomanipulatethemwithvariousmathematicaloperations. Amongthe\nmost useful of these are the elementwise operations. These apply a standard scalar opera-\ntion to each element of a tensor. For functions that take two tensors as inputs, elementwise\noperations apply some standard binary operator on each pair of corresponding elements.\nWe can create an elementwise function from any function that maps from a scalar to a\nscalar.\nIn mathematical notation, we denote such unaryscalar operators (taking one input) by the\nsignature\ud835\udc53:R!R. This just means that the function maps from any real number onto\nsome other real number. Most standard operators, including unary ones like \ud835\udc52\ud835\udc65, can be\napplied elementwise.\ntorch .exp(x)\ntensor([ 162754.7969 ,162754.7969 ,162754.7969 ,162754.7969 ,162754.7969 ,\n162754.7969 ,162754.7969 ,162754.7969 , 2980.9580 , 8103.0840 ,\n22026.4648 ,59874.1406 ])\nLikewise, we denote binaryscalar operators, which map pairs of real numbers to a (single)\nreal number via the signature \ud835\udc53:R,R!R. Given any two vectors uandvof the\nsame shape , and a binary operator \ud835\udc53, we can produce a vector c=\ud835\udc39\u00b9u,v\u00baby setting\n\ud835\udc50\ud835\udc56 \ud835\udc53\u00b9\ud835\udc62\ud835\udc56,\ud835\udc63\ud835\udc56\u00bafor all\ud835\udc56, where\ud835\udc50\ud835\udc56,\ud835\udc62\ud835\udc56, and\ud835\udc63\ud835\udc56are the\ud835\udc56thelements of vectors c,u, andv.\nHere, we produced the vector-valued \ud835\udc39:R\ud835\udc51,R\ud835\udc51!R\ud835\udc51byliftingthe scalar function to an\nelementwise vector operation. The common standard arithmetic operators for addition ( +),\nsubtraction( -),multiplication( *),division( /),andexponentiation( **)haveallbeen lifted\nto elementwise operations for identically-shaped tensors of arbitrary shape.\nx=torch .tensor([ 1.0,2,4,8])\ny=torch .tensor([ 2,2,2,2])\nx+y, x -y, x *y, x /y, x **y\n(tensor([ 3.,4.,6.,10.]),\ntensor([ -1.,0.,2.,6.]),\ntensor([ 2.,4.,8.,16.]),\ntensor([ 0.5000 ,1.0000 ,2.0000 ,4.0000 ]),\ntensor([ 1.,4.,16.,64.]))\nIn addition to elementwise computations, we can also perform linear algebraic operations,\nsuch as dot products and matrix multiplications. We will elaborate on these in Section\n2.3.\nWe can also concatenate multiple tensors, stacking them end-to-end to form a larger one.\nWejustneedtoprovidealistoftensorsandtellthesystemalongwhichaxistoconcatenate.\nThe example below shows what happens when we concatenate two matrices along rows", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5e2a343-170a-4139-8156-ce12a9577b72": {"__data__": {"id_": "f5e2a343-170a-4139-8156-ce12a9577b72", "embedding": null, "metadata": {"page_label": "35", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66ec205c-ca01-487c-a425-779b8385ce03", "node_type": "4", "metadata": {"page_label": "35", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9234f2a5d840f2d956fd98a24f6e32ea52ea577dd8466357b27468c957052269", "class_name": "RelatedNodeInfo"}}, "text": "35 Data Manipulation\n(axis 0) instead of columns (axis 1). We can see that the first output\u2019s axis-0 length ( 6) is\nthe sum of the two input tensors\u2019 axis-0 lengths ( 3\u00b83); while the second output\u2019s axis-1\nlength ( 8) is the sum of the two input tensors\u2019 axis-1 lengths ( 4\u00b84).\nX=torch .arange( 12, dtype =torch .float32) .reshape(( 3,4))\nY=torch .tensor([[ 2.0,1,4,3], [ 1,2,3,4], [ 4,3,2,1]])\ntorch .cat((X, Y), dim =0), torch .cat((X, Y), dim =1)\n(tensor([[ 0.,1.,2.,3.],\n[4.,5.,6.,7.],\n[8.,9.,10.,11.],\n[2.,1.,4.,3.],\n[1.,2.,3.,4.],\n[4.,3.,2.,1.]]),\ntensor([[ 0.,1.,2.,3.,2.,1.,4.,3.],\n[4.,5.,6.,7.,1.,2.,3.,4.],\n[8.,9.,10.,11.,4.,3.,2.,1.]]))\nSometimes, we wantto construct a binary tensor via logicalstatements . Take X == Yas an\nexample. Foreachposition i, j,ifX[i, j] andY[i, j] areequal,thenthecorresponding\nentry in the result takes value 1, otherwise it takes value 0.\nX==Y\ntensor([[ False ,True ,False ,True ],\n[False ,False ,False ,False ],\n[False ,False ,False ,False ]])\nSumming all the elements in the tensor yields a tensor with only one element.\nX.sum()\ntensor( 66.)\n2.1.4Broadcasting\nBy now, you know how to perform elementwise binary operations on two tensors of the\nsame shape. Under certain conditions, even when shapes differ, we can still perform ele-\nmentwisebinaryoperationsbyinvokingthe broadcastingmechanism . Broadcastingworks\naccording to the following two-step procedure: (i) expand one or both arrays by copying\nelementsalongaxeswithlength1sothatafterthistransformation, thetwotensorshavethe\nsame shape; (ii) perform an elementwise operation on the resulting arrays.\na=torch .arange( 3).reshape(( 3,1))\nb=torch .arange( 2).reshape(( 1,2))\na, b", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fd92611-7fab-4441-8794-92a5910b24d5": {"__data__": {"id_": "4fd92611-7fab-4441-8794-92a5910b24d5", "embedding": null, "metadata": {"page_label": "36", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6f81479c-1371-4697-ad90-042521adbb4d", "node_type": "4", "metadata": {"page_label": "36", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b78a5982fccb80be0955de289109d10daf1ed27461bfed3170a750a8a0679a3a", "class_name": "RelatedNodeInfo"}}, "text": "36 Preliminaries\n(tensor([[ 0],\n[1],\n[2]]),\ntensor([[ 0,1]]))\nSince aandbare3\u00021and 1\u00022matrices, respectively, their shapes do not match up.\nBroadcasting produces a larger 3\u00022matrix by replicating matrix aalong the columns and\nmatrix balong the rows before adding them elementwise.\na+b\ntensor([[ 0,1],\n[1,2],\n[2,3]])\n2.1.5SavingMemory\nRunning operations can cause new memory to be allocated to host results. For example, if\nwewrite Y = X + Y ,wedereferencethetensorthat Yusedtopointtoandinsteadpoint Yat\nthe newly allocated memory. We can demonstrate this issue with Python\u2019s id()function,\nwhich gives us the exact address of the referenced object in memory. Note that after we\nrunY = Y + X ,id(Y)points to a different location. That is because Python first evaluates\nY + X, allocating new memory for the result and then points Yto this new location in\nmemory.\nbefore =id(Y)\nY=Y+X\nid(Y) ==before\nFalse\nThis might be undesirable for two reasons. First, we do not want to run around allocat-\ning memory unnecessarily all the time. In machine learning, we often have hundreds of\nmegabytes of parameters and update all of them multiple times per second. Whenever\npossible, we want to perform these updates in place. Second, we might point at the same\nparameters from multiple variables. If we do not update in place, we must be careful to\nupdate all of these references, lest we spring a memory leak or inadvertently refer to stale\nparameters.\nFortunately, performing in-place operations is easy. We can assign the result of an oper-\nation to a previously allocated array Yby using slice notation: Y[:] = <expression> .\nTo illustrate this concept, we overwrite the values of tensor Z, after initializing it, using\nzeros_like , to have the same shape as Y.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ab73145-2116-4c2e-b60e-f86a33226e5e": {"__data__": {"id_": "0ab73145-2116-4c2e-b60e-f86a33226e5e", "embedding": null, "metadata": {"page_label": "37", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bba7f5ba-dc84-4b60-83c7-eb40367fc866", "node_type": "4", "metadata": {"page_label": "37", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "579f60f7a1529fb8c982df78c013f99cdbe711a8f37fd6e6169cb23e7778a0b9", "class_name": "RelatedNodeInfo"}}, "text": "37 Data Manipulation\nZ=torch .zeros_like(Y)\nprint ('id(Z): ',id(Z))\nZ[:] =X+Y\nprint ('id(Z): ',id(Z))\nid(Z): 140381179266448\nid(Z): 140381179266448\nIf the value of Xis not reused in subsequent computations, we can also use X[:] = X + Y\norX += Yto reduce the memory overhead of the operation.\nbefore =id(X)\nX+=Y\nid(X) ==before\nTrue\n2.1.6Conversionto Other Python Objects\nConverting to a NumPy tensor ( ndarray ), or vice versa, is easy. The torch tensor and\nNumPy array will share their underlying memory, and changing one through an in-place\noperation will also change the other.\nA=X.numpy()\nB=torch .from_numpy(A)\ntype (A), type (B)\n(numpy .ndarray, torch .Tensor)\nTo convert a size-1 tensor to a Python scalar, we can invoke the itemfunction or Python\u2019s\nbuilt-in functions.\na=torch .tensor([ 3.5])\na, a .item(), float (a), int(a)\n(tensor([ 3.5000 ]), 3.5,3.5,3)\n2.1.7Summary\nThe tensor class is the main interface for storing and manipulating data in deep learning li-\nbraries. Tensorsprovideavarietyoffunctionalitiesincludingconstructionroutines; index-\ningandslicing;basicmathematicsoperations;broadcasting;memory-efficientassignment;\nand conversion to and from other Python objects.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59e90f1d-11f6-4fa6-be21-de0d8e7da99d": {"__data__": {"id_": "59e90f1d-11f6-4fa6-be21-de0d8e7da99d", "embedding": null, "metadata": {"page_label": "38", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd52669e-6b5a-49e8-af5f-d8d98e7cdedc", "node_type": "4", "metadata": {"page_label": "38", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fdbc15df3b24ae5bdbf1d2c58e53c4da9dadbab83d7ea488b8afd448af40133f", "class_name": "RelatedNodeInfo"}}, "text": "38 Preliminaries\n44\n45\n462.1.8Exercises\n1.Run the code in this section. Change the conditional statement X == YtoX < YorX >\nY, and then see what kind of tensor you can get.\n2.Replace the two tensors that operate by element in the broadcasting mechanism with\nother shapes, e.g., 3-dimensional tensors. Is the result the same as expected?\nDiscussions44.\n2.2Data Preprocessing\nSo far, we have been working with synthetic data that arrived in ready-made tensors. How-\never, to apply deep learning in the wild we must extract messy data stored in arbitrary\nformats, andpreprocessittosuitourneeds. Fortunately,the pandaslibrary45candomuch\nof the heavy lifting. This section, while no substitute for a proper pandastutorial46, will\ngive you a crash course on some of the most common routines.\n2.2.1Readingthe Dataset\nComma-separatedvalues(CSV)filesareubiquitousforthestoringoftabular(spreadsheet-\nlike) data. In them, each line corresponds to one record and consists of several (comma-\nseparated)fields,e.g.,\u201cAlbertEinstein,March141879,Ulm,Federalpolytechnicschool,field\nof gravitational physics\u201d. To demonstrate how to load CSV files with pandas, we create a\nCSV file below ../data/house_tiny.csv . This file represents a dataset of homes, where\neach row corresponds to a distinct home and the columns correspond to the number of\nrooms ( NumRooms ), the roof type ( RoofType ), and the price ( Price).\nimport os\nos.makedirs(os .path .join( '..','data '), exist_ok =True )\ndata_file =os.path .join( '..','data ','house_tiny.csv ')\nwith open (data_file, 'w')asf:\nf.write( '''NumRooms,RoofType,Price\nNA,NA,127500\n2,NA,106000\n4,Slate,178100\nNA,NA,140000 ''')\nNow let\u2019s import pandasand load the dataset with read_csv .\nimport pandas aspd\ndata =pd.read_csv(data_file)\nprint (data)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1769, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69fb3b61-5fe3-436b-bb81-e079607b99e7": {"__data__": {"id_": "69fb3b61-5fe3-436b-bb81-e079607b99e7", "embedding": null, "metadata": {"page_label": "39", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c6150d04-f7c4-4dcf-8c21-87caeda2ea2f", "node_type": "4", "metadata": {"page_label": "39", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f3ab48189a9d4b2f5285703c5ab8ee1f023de37fd29785a478e1dd7beb226a24", "class_name": "RelatedNodeInfo"}}, "text": "39 Data Preprocessing\nNumRooms RoofType Price\n0 NaN NaN 127500\n1 2.0 NaN 106000\n2 4.0 Slate 178100\n3 NaN NaN 140000\n2.2.2Data Preparation\nIn supervised learning, we train models to predict a designated targetvalue, given some\nset ofinputvalues. Our first step in processing the dataset is to separate out columns cor-\nresponding to input versus target values. We can select columns either by name or via\ninteger-location based indexing ( iloc).\nYou might have noticed that pandasreplaced all CSV entries with value NAwith a spe-\ncialNaN(not a number ) value. This can also happen whenever an entry is empty, e.g.,\n\u201c3\u201e,270000\u201d. These are called missingvalues and they are the \u201cbed bugs\u201d of data science,\na persistent menace that you will confront throughout your career. Depending upon the\ncontext, missing values might be handled either via imputation ordeletion. Imputation re-\nplaces missing values with estimates of their values while deletion simply discards either\nthose rows or those columns that contain missing values.\nHerearesomecommonimputationheuristics. Forcategoricalinputfields,wecantreat NaN\nasacategory. Sincethe RoofType columntakesvalues SlateandNaN,pandascanconvert\nthiscolumnintotwocolumns RoofType_Slate andRoofType_nan . Arowwhoserooftype\nisSlatewill set values of RoofType_Slate andRoofType_nan to 1 and 0, respectively.\nThe converse holds for a row with a missing RoofType value.\ninputs, targets =data .iloc[:, 0:2], data .iloc[:, 2]\ninputs =pd.get_dummies(inputs, dummy_na =True )\nprint (inputs)\nNumRooms RoofType_Slate RoofType_nan\n0 NaN False True\n1 2.0 False True\n2 4.0 True False\n3 NaN False True\nFor missing numerical values, one common heuristic is to replace the NaNentries with the\nmean value of the corresponding column.\ninputs =inputs .fillna(inputs .mean())\nprint (inputs)\nNumRooms RoofType_Slate RoofType_nan\n0 3.0 False True\n1 2.0 False True\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b17234be-52da-4b46-a51a-1cb5333a1d5a": {"__data__": {"id_": "b17234be-52da-4b46-a51a-1cb5333a1d5a", "embedding": null, "metadata": {"page_label": "40", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee15e2dc-1078-4e4f-a25b-31828a338d6c", "node_type": "4", "metadata": {"page_label": "40", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dfe0b84a9fffd48fe26a99626c08405cb53e4de184bed0c7a0ff6be4fc758b0e", "class_name": "RelatedNodeInfo"}}, "text": "40 Preliminaries\n47\n48\n49\n50\n51(continued from previous page)\n2 4.0 True False\n3 3.0 False True\n2.2.3Conversionto the TensorFormat\nNow that all the entries in inputsandtargets are numerical, we can load them into a\ntensor (recall Section 2.1 ).\nimport torch\nX=torch .tensor(inputs .to_numpy(dtype =float ))\ny=torch .tensor(targets .to_numpy(dtype =float ))\nX, y\n(tensor([[ 3.,0.,1.],\n[2.,0.,1.],\n[4.,1.,0.],\n[3.,0.,1.]], dtype =torch .float64),\ntensor([ 127500. ,106000. ,178100. ,140000. ], dtype =torch .float64))\n2.2.4Discussion\nYou now know how to partition data columns, impute missing variables, and load pan-\ndasdata into tensors. In Section 5.7 , you will pick up some more data processing skills.\nWhile this crash course kept things simple, data processing can get hairy. For example,\nrather than arriving in a single CSV file, our dataset might be spread across multiple files\nextractedfromarelationaldatabase. Forinstance, inane-commerceapplication, customer\naddressesmightliveinonetableandpurchasedatainanother. Moreover,practitionersface\nmyriaddatatypesbeyondcategoricalandnumeric, forexample, textstrings, images, audio\ndata, and point clouds. Oftentimes, advanced tools and efficient algorithms are required\nin order to prevent data processing from becoming the biggest bottleneck in the machine\nlearning pipeline. These problems will arise when we get to computer vision and natural\nlanguageprocessing. Finally,wemustpayattentiontodataquality. Real-worlddatasetsare\noften plagued by outliers, faulty measurements from sensors, and recording errors, which\nmust be addressed before feeding the data into any model. Data visualization tools such as\nseaborn47,Bokeh48,ormatplotlib49canhelpyoutomanuallyinspectthedataanddevelop\nintuitions about the type of problems you may need to address.\n2.2.5Exercises\n1.Try loading datasets, e.g., Abalone from the UCI Machine Learning Repository50and\ninspect their properties. What fraction of them has missing values? What fraction of\nthe variables is numerical, categorical, or text?\n2.Try indexing and selecting data columns by name rather than by column number. The\npandas documentation on indexing51has further details on how to do this.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2192, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0144706d-f33b-4f05-af65-36d213bf4dc2": {"__data__": {"id_": "0144706d-f33b-4f05-af65-36d213bf4dc2", "embedding": null, "metadata": {"page_label": "41", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cb7f96df-28be-4cdd-bbec-203a49fc8c6e", "node_type": "4", "metadata": {"page_label": "41", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "de2016e2db6c9fab2465116000f1d0bfb0d48322ecc3384badee61542e4ce528", "class_name": "RelatedNodeInfo"}}, "text": "41 Linear Algebra\n52\n53\n543.How large a dataset do you think you could load this way? What might be the limita-\ntions? Hint: consider the time to read the data, representation, processing, and memory\nfootprint. Try this out on your laptop. What happens if you try it out on a server?\n4.How would you deal with data that has a very large number of categories? What if the\ncategory labels are all unique? Should you include the latter?\n5.What alternatives to pandas can you think of? How about loading NumPy tensors from\na file52? Check out Pillow53, the Python Imaging Library.\nDiscussions54.\n2.3LinearAlgebra\nBy now, we can load datasets into tensors and manipulate these tensors with basic math-\nematical operations. To start building sophisticated models, we will also need a few tools\nfromlinearalgebra. Thissectionoffersagentleintroductiontothemostessentialconcepts,\nstarting from scalar arithmetic and ramping up to matrix multiplication.\nimport torch\n2.3.1Scalars\nMost everyday mathematics consists of manipulating numbers one at a time. Formally, we\ncall these values scalars. For example, the temperature in Palo Alto is a balmy 72degrees\nFahrenheit. If you wanted to convert the temperature to Celsius you would evaluate the\nexpression\ud835\udc50=5\n9\u00b9\ud835\udc53\u000032\u00ba, setting\ud835\udc53to72. In this equation, the values 5,9, and 32are\nconstant scalars. The variables \ud835\udc50and\ud835\udc53in general represent unknown scalars.\nWe denote scalars by ordinary lower-cased letters (e.g., \ud835\udc65,\ud835\udc66, and\ud835\udc67) and the space of all\n(continuous) real-valued scalars by R. For expedience, we will skip past rigorous defini-\ntions ofspaces: just remember that the expression \ud835\udc652Ris a formal way to say that \ud835\udc65is\na real-valued scalar. The symbol 2(pronounced \u201cin\u201d) denotes membership in a set. For\nexample,\ud835\udc65,\ud835\udc662f0,1gindicates that \ud835\udc65and\ud835\udc66are variables that can only take values 0or\n1.\nScalars are implemented as tensors that contain only one element. Below, we assign two\nscalars and perform the familiar addition, multiplication, division, and exponentiation op-\nerations.\nx=torch .tensor( 3.0)\ny=torch .tensor( 2.0)\nx+y, x *y, x /y, x **y", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2078, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f21c069b-d7e1-413d-9e34-146fab6365b0": {"__data__": {"id_": "f21c069b-d7e1-413d-9e34-146fab6365b0", "embedding": null, "metadata": {"page_label": "42", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a8981be-7da6-461d-a7a7-fcf1249dce5d", "node_type": "4", "metadata": {"page_label": "42", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a94a7aae5330cb5ec10270a2e18b02b06032e1cbf117e4e0312b85d371ce8ae7", "class_name": "RelatedNodeInfo"}}, "text": "42 Preliminaries\n(tensor( 5.), tensor( 6.), tensor( 1.5000 ), tensor( 9.))\n2.3.2Vectors\nFor current purposes, you can think of a vector as a fixed-length array of scalars. As with\ntheir code counterparts, we call these scalars the elements of the vector (synonyms include\nentriesandcomponents ). When vectors represent examples from real-world datasets, their\nvalues hold some real-world significance. For example, if we were training a model to\npredicttheriskofaloandefaulting, wemightassociateeachapplicantwithavectorwhose\ncomponentscorrespondtoquantitiesliketheirincome,lengthofemployment,ornumberof\nprevious defaults. If we were studying the risk of heart attack, each vector might represent\na patient and its components might correspond to their most recent vital signs, cholesterol\nlevels, minutes of exercise per day, etc. We denote vectors by bold lowercase letters, (e.g.,\nx,y, andz).\nVectors are implemented as 1st-order tensors. In general, such tensors can have arbitrary\nlengths, subject to memory limitations. Caution: in Python, as in most programming lan-\nguages, vector indices start at 0, also known as zero-based indexing , whereas in linear\nalgebra subscripts begin at 1(one-based indexing).\nx=torch .arange( 3)\nx\ntensor([ 0,1,2])\nWe can refer to an element of a vector by using a subscript. For example, \ud835\udc652denotes the\nsecondelementof x. Since\ud835\udc652isascalar,wedonotboldit. Bydefault,wevisualizevectors\nby stacking their elements vertically.\nx=2666664\ud835\udc651\n...\n\ud835\udc65\ud835\udc5b3777775, (2.3.1)\nHere\ud835\udc651,...,\ud835\udc65\ud835\udc5bare elements of the vector. Later on, we will distinguish between such\ncolumn vectors androw vectors whose elements are stacked horizontally. Recall that we\naccess a tensor\u2019s elements via indexing.\nx[2]\ntensor( 2)\nTo indicate that a vector contains \ud835\udc5belements, we write x2R\ud835\udc5b. Formally, we call \ud835\udc5bthe\ndimensionality ofthevector. Incode,thiscorrespondstothetensor\u2019slength,accessiblevia\nPython\u2019s built-in lenfunction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05075055-abf5-447f-b648-c7ee9dfadc97": {"__data__": {"id_": "05075055-abf5-447f-b648-c7ee9dfadc97", "embedding": null, "metadata": {"page_label": "43", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79e09c87-7906-4c1a-9b88-d93cc0ef91dd", "node_type": "4", "metadata": {"page_label": "43", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d1c80860a6a983a908d2338ca311774e343f32531fd6720f8480887849c5fa35", "class_name": "RelatedNodeInfo"}}, "text": "43 Linear Algebra\nlen(x)\n3\nWe can also access the length via the shapeattribute. The shape is a tuple that indicates\na tensor\u2019s length along each axis. Tensors with just one axis have shapes with just one\nelement.\nx.shape\ntorch .Size([ 3])\nOftentimes,theword\u201cdimension\u201dgetsoverloadedtomeanboththenumberofaxesandthe\nlength along a particular axis. To avoid this confusion, we use orderto refer to the number\nof axes and dimensionality exclusively to refer to the number of components.\n2.3.3Matrices\nJustasscalarsare 0th-ordertensorsandvectorsare 1st-ordertensors,matricesare 2nd-order\ntensors. Wedenotematricesbyboldcapitalletters(e.g., X,Y, andZ), andrepresentthem\nin code by tensors with two axes. The expression A2R\ud835\udc5a\u0002\ud835\udc5bindicates that a matrix A\ncontains\ud835\udc5a\u0002\ud835\udc5breal-valued scalars, arranged as \ud835\udc5arows and\ud835\udc5bcolumns. When \ud835\udc5a=\ud835\udc5b, we\nsay that a matrix is square. Visually, we can illustrate any matrix as a table. To refer to an\nindividualelement,wesubscriptboththerowandcolumnindices,e.g., \ud835\udc4e\ud835\udc56\ud835\udc57isthevaluethat\nbelongs to A\u2019s\ud835\udc56throw and\ud835\udc57thcolumn:\nA=266666664\ud835\udc4e11\ud835\udc4e12\u0001\u0001\u0001\ud835\udc4e1\ud835\udc5b\n\ud835\udc4e21\ud835\udc4e22\u0001\u0001\u0001\ud835\udc4e2\ud835\udc5b\n............\n\ud835\udc4e\ud835\udc5a1\ud835\udc4e\ud835\udc5a2\u0001\u0001\u0001\ud835\udc4e\ud835\udc5a\ud835\udc5b377777775. (2.3.2)\nIn code, we represent a matrix A2R\ud835\udc5a\u0002\ud835\udc5bby a 2nd-order tensor with shape ( \ud835\udc5a,\ud835\udc5b). We can\nconvert any appropriately sized \ud835\udc5a\u0002\ud835\udc5btensor into an \ud835\udc5a\u0002\ud835\udc5bmatrix by passing the desired\nshape to reshape :\nA=torch .arange( 6).reshape( 3,2)\nA\ntensor([[ 0,1],\n[2,3],\n[4,5]])\nSometimes we want to flip the axes. When we exchange a matrix\u2019s rows and columns, the\nresult is called its transpose . Formally, we signify a matrix A\u2019s transpose by A>and if", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1541, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c77e64e3-6c4a-420f-b00a-3edd2cb474f1": {"__data__": {"id_": "c77e64e3-6c4a-420f-b00a-3edd2cb474f1", "embedding": null, "metadata": {"page_label": "44", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed575f07-71c8-4abc-8fcb-b626e8a8e240", "node_type": "4", "metadata": {"page_label": "44", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bc96b40c095808c653180ddabcb2ecba35aa3568162a271a0bb9bf44477badcb", "class_name": "RelatedNodeInfo"}}, "text": "44 Preliminaries\nB=A>, then\ud835\udc4f\ud835\udc56\ud835\udc57=\ud835\udc4e\ud835\udc57\ud835\udc56for all\ud835\udc56and\ud835\udc57. Thus, the transpose of an \ud835\udc5a\u0002\ud835\udc5bmatrix is an\ud835\udc5b\u0002\ud835\udc5a\nmatrix:\nA>=266666664\ud835\udc4e11\ud835\udc4e21... \ud835\udc4e\ud835\udc5a1\n\ud835\udc4e12\ud835\udc4e22... \ud835\udc4e\ud835\udc5a2\n............\n\ud835\udc4e1\ud835\udc5b\ud835\udc4e2\ud835\udc5b... \ud835\udc4e\ud835\udc5a\ud835\udc5b377777775. (2.3.3)\nIn code, we can access any matrix\u2019s transpose as follows:\nA.T\ntensor([[ 0,2,4],\n[1,3,5]])\nSymmetricmatricesarethesubsetofsquarematricesthatareequaltotheirowntransposes:\nA=A>. The following matrix is symmetric:\nA=torch .tensor([[ 1,2,3], [ 2,0,4], [ 3,4,5]])\nA==A.T\ntensor([[ True ,True ,True ],\n[True ,True ,True ],\n[True ,True ,True ]])\nMatrices are useful for representing datasets. Typically, rows correspond to individual\nrecords and columns correspond to distinct attributes.\n2.3.4Tensors\nWhile you can go far in your machine learning journey with only scalars, vectors, and\nmatrices, eventually you may need to work with higher-order tensors. Tensors give us\na generic way of describing extensions to \ud835\udc5bth-order arrays. We call software objects of\nthetensor class \u201ctensors\u201d precisely because they too can have arbitrary numbers of axes.\nWhile it may be confusing to use the word tensorfor both the mathematical object and its\nrealization in code, our meaning should usually be clear from context. We denote general\ntensors by capital letters with a special font face (e.g., X,Y, andZ) and their indexing\nmechanism (e.g., \ud835\udc65\ud835\udc56\ud835\udc57\ud835\udc58and\u00bbX\u00bc1,2\ud835\udc56\u00001,3) follows naturally from that of matrices.\nTensors will become more important when we start working with images. Each image\narrives as a 3rd-order tensor with axes corresponding to the height, width, and channel. At\neachspatiallocation,theintensitiesofeachcolor(red,green,andblue)arestackedalongthe\nchannel. Furthermore, a collection of images is represented in code by a 4th-order tensor,\nwheredistinctimagesareindexedalongthefirstaxis. Higher-ordertensorsareconstructed,\nas were vectors and matrices, by growing the number of shape components.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f863320a-4fc3-4296-8916-c7da2406d2fa": {"__data__": {"id_": "f863320a-4fc3-4296-8916-c7da2406d2fa", "embedding": null, "metadata": {"page_label": "45", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42c4ac4b-cef1-4d0a-8fce-d4e40198749b", "node_type": "4", "metadata": {"page_label": "45", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bd33d778978a6983c57fe4854e3fb192c8aaf26086abff6bbf9a65eea04d4cc0", "class_name": "RelatedNodeInfo"}}, "text": "45 Linear Algebra\ntorch .arange( 24).reshape( 2,3,4)\ntensor([[[ 0,1,2,3],\n[4,5,6,7],\n[8,9,10,11]],\n[[12,13,14,15],\n[16,17,18,19],\n[20,21,22,23]]])\n2.3.5BasicPropertiesof Tensor Arithmetic\nScalars,vectors,matrices,andhigher-ordertensorsallhavesomehandyproperties. Forex-\nample,elementwiseoperationsproduceoutputsthathavethesameshapeastheiroperands.\nA=torch .arange( 6, dtype =torch .float32) .reshape( 2,3)\nB=A.clone() # Assign a copy of A to B by allocating new memory\nA, A +B\n(tensor([[ 0.,1.,2.],\n[3.,4.,5.]]),\ntensor([[ 0.,2.,4.],\n[6.,8.,10.]]))\nThe elementwise product of two matrices is called their Hadamard product (denoted\f).\nWecanspellouttheentriesoftheHadamardproductoftwomatrices A,B2R\ud835\udc5a\u0002\ud835\udc5b:\nA\fB=266666664\ud835\udc4e11\ud835\udc4f11\ud835\udc4e12\ud835\udc4f12... \ud835\udc4e 1\ud835\udc5b\ud835\udc4f1\ud835\udc5b\n\ud835\udc4e21\ud835\udc4f21\ud835\udc4e22\ud835\udc4f22... \ud835\udc4e 2\ud835\udc5b\ud835\udc4f2\ud835\udc5b\n............\n\ud835\udc4e\ud835\udc5a1\ud835\udc4f\ud835\udc5a1\ud835\udc4e\ud835\udc5a2\ud835\udc4f\ud835\udc5a2... \ud835\udc4e\ud835\udc5a\ud835\udc5b\ud835\udc4f\ud835\udc5a\ud835\udc5b377777775. (2.3.4)\nA*B\ntensor([[ 0.,1.,4.],\n[9.,16.,25.]])\nAdding or multiplying a scalar and a tensor produces a result with the same shape as\nthe original tensor. Here, each element of the tensor is added to (or multiplied by) the\nscalar.\na=2\nX=torch .arange( 24).reshape( 2,3,4)\na+X, (a *X).shape", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1096, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2ed04a1-98db-46e5-9381-634f0f2565ba": {"__data__": {"id_": "c2ed04a1-98db-46e5-9381-634f0f2565ba", "embedding": null, "metadata": {"page_label": "46", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2399847-6c4d-41d4-bcd1-9721215e597e", "node_type": "4", "metadata": {"page_label": "46", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4c2bc640160d8f45ca1a6f499d92fcc4cbf828c8ef257c71c72b308bc165ce99", "class_name": "RelatedNodeInfo"}}, "text": "46 Preliminaries\n(tensor([[[ 2,3,4,5],\n[6,7,8,9],\n[10,11,12,13]],\n[[14,15,16,17],\n[18,19,20,21],\n[22,23,24,25]]]),\ntorch .Size([ 2,3,4]))\n2.3.6Reduction\nOften, we wish to calculate the sum of a tensor\u2019s elements. To express the sum of the\nelementsinavector xoflength\ud835\udc5b,wewrite\u00cd\ud835\udc5b\n\ud835\udc56=1\ud835\udc65\ud835\udc56. Thereisasimplefunctionforit:\nx=torch .arange( 3, dtype =torch .float32)\nx, x .sum()\n(tensor([ 0.,1.,2.]), tensor( 3.))\nTo express sums over the elements of tensors of arbitrary shape, we simply sum over all\nits axes. For example, the sum of the elements of an \ud835\udc5a\u0002\ud835\udc5bmatrix Acould be written\u00cd\ud835\udc5a\n\ud835\udc56=1\u00cd\ud835\udc5b\n\ud835\udc57=1\ud835\udc4e\ud835\udc56\ud835\udc57.\nA.shape, A .sum()\n(torch .Size([ 2,3]), tensor( 15.))\nBy default, invoking the sum function reduces a tensor along all of its axes, eventually\nproducing a scalar. Our libraries also allow us to specify the axes along which the tensor\nshould be reduced. To sum over all elements along the rows (axis 0), we specify axis=0in\nsum. Since the input matrix reduces along axis 0 to generate the output vector, this axis is\nmissing from the shape of the output.\nA.shape, A .sum(axis =0).shape\n(torch .Size([ 2,3]), torch .Size([ 3]))\nSpecifying axis=1will reduce the column dimension (axis 1) by summing up elements of\nall the columns.\nA.shape, A .sum(axis =1).shape\n(torch .Size([ 2,3]), torch .Size([ 2]))", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9a90cea-6a16-4cff-a549-447cee2b9d0a": {"__data__": {"id_": "a9a90cea-6a16-4cff-a549-447cee2b9d0a", "embedding": null, "metadata": {"page_label": "47", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11d4ef51-b189-4038-9718-c28622a03dbf", "node_type": "4", "metadata": {"page_label": "47", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cedd72b3672baf82b4408f725dd7165213c13ce6d2449dc8c82167a291d581ab", "class_name": "RelatedNodeInfo"}}, "text": "47 Linear Algebra\nReducing a matrix along both rows and columns via summation is equivalent to summing\nup all the elements of the matrix.\nA.sum(axis =[0,1])==A.sum() # Same as A.sum()\ntensor( True )\nA related quantity is the mean, also called the average. We calculate the mean by dividing\nthe sum by the total number of elements. Because computing the mean is so common, it\ngets a dedicated library function that works analogously to sum.\nA.mean(), A .sum() /A.numel()\n(tensor( 2.5000 ), tensor( 2.5000 ))\nLikewise, the function for calculating the mean can also reduce a tensor along specific\naxes.\nA.mean(axis =0), A .sum(axis =0)/A.shape[ 0]\n(tensor([ 1.5000 ,2.5000 ,3.5000 ]), tensor([ 1.5000 ,2.5000 ,3.5000 ]))\n2.3.7Non-ReductionSum\nSometimes it can be useful to keep the number of axesunchanged when invoking the func-\ntion for calculating the sum or mean. This matters when we want to use the broadcast\nmechanism.\nsum_A =A.sum(axis =1, keepdims =True )\nsum_A, sum_A .shape\n(tensor([[ 3.],\n[12.]]),\ntorch .Size([ 2,1]))\nFor instance, since sum_Akeeps its two axes after summing each row, we can divide Aby\nsum_Awith broadcasting to create a matrix where each row sums up to 1.\nA/sum_A\ntensor([[ 0.0000 ,0.3333 ,0.6667 ],\n[0.2500 ,0.3333 ,0.4167 ]])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1257, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7148b9e3-1f6c-4e4a-b15e-94d4daf221a3": {"__data__": {"id_": "7148b9e3-1f6c-4e4a-b15e-94d4daf221a3", "embedding": null, "metadata": {"page_label": "48", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "787d7e78-b7e6-4afc-a43b-b08a2208eca7", "node_type": "4", "metadata": {"page_label": "48", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e0a12d914b9c128f7f4ce203856e30d56d7a13822f8ec1fa378f5f836d4deee6", "class_name": "RelatedNodeInfo"}}, "text": "48 Preliminaries\nIf we want to calculate the cumulative sum of elements of Aalong some axis, say axis=0\n(row by row), we can call the cumsumfunction. By design, this function does not reduce\nthe input tensor along any axis.\nA.cumsum(axis =0)\ntensor([[ 0.,1.,2.],\n[3.,5.,7.]])\n2.3.8DotProducts\nSofar,wehaveonlyperformedelementwiseoperations,sums,andaverages. Andifthiswas\nallwecoulddo,linearalgebrawouldnotdeserveitsownsection. Fortunately,thisiswhere\nthings get more interesting. One of the most fundamental operations is the dot product.\nGiven two vectors x,y2R\ud835\udc51, theirdotproduct x>y(also known as innerproduct ,hx,yi)\nis a sum over the products of the elements at the same position: x>y=\u00cd\ud835\udc51\n\ud835\udc56=1\ud835\udc65\ud835\udc56\ud835\udc66\ud835\udc56.\ny=torch .ones( 3, dtype =torch .float32)\nx, y, torch .dot(x, y)\n(tensor([ 0.,1.,2.]), tensor([ 1.,1.,1.]), tensor( 3.))\nEquivalently,wecancalculatethedotproductoftwovectorsbyperforminganelementwise\nmultiplication followed by a sum:\ntorch .sum(x *y)\ntensor( 3.)\nDot products are useful in a wide range of contexts. For example, given some set of val-\nues, denoted by a vector x2R\ud835\udc5b, and a set of weights, denoted by w2R\ud835\udc5b, the weighted\nsum of the values in xaccording to the weights wcould be expressed as the dot product\nx>w. When the weights are nonnegative and sum to 1, i.e.,\u0000\u00cd\ud835\udc5b\n\ud835\udc56=1\ud835\udc64\ud835\udc56=1\u0001, the dot prod-\nuct expresses a weighted average . After normalizing two vectors to have unit length, the\ndot products express the cosine of the angle between them. Later in this section, we will\nformally introduce this notion of length.\n2.3.9Matrix\u2013VectorProducts\nNow that we know how to calculate dot products, we can begin to understand the product\nbetween an\ud835\udc5a\u0002\ud835\udc5bmatrix Aand an\ud835\udc5b-dimensional vector x. To start off, we visualize our", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60ffbdef-f205-4c67-8592-b086ddcdb97f": {"__data__": {"id_": "60ffbdef-f205-4c67-8592-b086ddcdb97f", "embedding": null, "metadata": {"page_label": "49", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5a07e84-b24d-48e3-bdc4-869410dbe674", "node_type": "4", "metadata": {"page_label": "49", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7f6e786aa42ff252125ba78eef39b0717ac3c808d3530206e016f2790b0bf4bf", "class_name": "RelatedNodeInfo"}}, "text": "49 Linear Algebra\nmatrix in terms of its row vectors\nA=266666664a>\n1\na>\n2...\na>\n\ud835\udc5a377777775, (2.3.5)\nwhere each a>\n\ud835\udc562R\ud835\udc5bis a row vector representing the \ud835\udc56throw of the matrix A.\nThe matrix\u2013vector product Axis simply a column vector of length \ud835\udc5a, whose\ud835\udc56thelement\nis the dot product a>\n\ud835\udc56x:\nAx=266666664a>\n1\na>\n2...\na>\n\ud835\udc5a377777775x=266666664a>\n1x\na>\n2x\n...\na>\n\ud835\udc5ax377777775. (2.3.6)\nWe can think of multiplication with a matrix A2R\ud835\udc5a\u0002\ud835\udc5bas a transformation that projects\nvectorsfrom R\ud835\udc5btoR\ud835\udc5a. Thesetransformationsareremarkablyuseful. Forexample,wecan\nrepresent rotations as multiplications by certain square matrices. Matrix\u2013vector products\nalsodescribethekeycalculationinvolvedincomputingtheoutputsofeachlayerinaneural\nnetwork given the outputs from the previous layer.\nTo express a matrix\u2013vector product in code, we use the mvfunction. Note that the column\ndimensionof A(itslengthalongaxis1)mustbethesameasthedimensionof x(itslength).\nPythonhasaconvenienceoperator @thatcanexecutebothmatrix\u2013vectorandmatrix\u2013matrix\nproducts (depending on its arguments). Thus we can write A@x.\nA.shape, x .shape, torch .mv(A, x), A @x\n(torch .Size([ 2,3]), torch .Size([ 3]), tensor([ 5.,14.]), tensor([ 5.,14.]))\n2.3.10Matrix\u2013MatrixMultiplication\nOnce you have gotten the hang of dot products and matrix\u2013vector products, then matrix\u2013\nmatrixmultiplication should be straightforward.\nSay that we have two matrices A2R\ud835\udc5b\u0002\ud835\udc58andB2R\ud835\udc58\u0002\ud835\udc5a:\nA=266666664\ud835\udc4e11\ud835\udc4e12\u0001\u0001\u0001\ud835\udc4e1\ud835\udc58\n\ud835\udc4e21\ud835\udc4e22\u0001\u0001\u0001\ud835\udc4e2\ud835\udc58\n............\n\ud835\udc4e\ud835\udc5b1\ud835\udc4e\ud835\udc5b2\u0001\u0001\u0001\ud835\udc4e\ud835\udc5b\ud835\udc58377777775,B=266666664\ud835\udc4f11\ud835\udc4f12\u0001\u0001\u0001\ud835\udc4f1\ud835\udc5a\n\ud835\udc4f21\ud835\udc4f22\u0001\u0001\u0001\ud835\udc4f2\ud835\udc5a\n............\n\ud835\udc4f\ud835\udc581\ud835\udc4f\ud835\udc582\u0001\u0001\u0001\ud835\udc4f\ud835\udc58\ud835\udc5a377777775. (2.3.7)\nLeta>\n\ud835\udc562R\ud835\udc58denotetherowvectorrepresentingthe \ud835\udc56throwofthematrix Aandlet b\ud835\udc572R\ud835\udc58", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1627, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf3209ba-d448-4555-880d-53162fe979f9": {"__data__": {"id_": "cf3209ba-d448-4555-880d-53162fe979f9", "embedding": null, "metadata": {"page_label": "50", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1e526fc-e769-4f49-99e4-3e1247d91071", "node_type": "4", "metadata": {"page_label": "50", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d4d65f82976188270fe272e3cb253dd786f3a9087990af2387eff8fdfb57c8a6", "class_name": "RelatedNodeInfo"}}, "text": "50 Preliminaries\ndenote the column vector from the \ud835\udc57thcolumn of the matrix B:\nA=266666664a>\n1\na>\n2...\na>\n\ud835\udc5b377777775,B=\u0002\nb1b2\u0001\u0001\u0001b\ud835\udc5a\u0003\n. (2.3.8)\nTo form the matrix product C2R\ud835\udc5b\u0002\ud835\udc5a, we simply compute each element \ud835\udc50\ud835\udc56\ud835\udc57as the dot\nproduct between the \ud835\udc56throw of Aand the\ud835\udc57thcolumn of B, i.e.,a>\n\ud835\udc56b\ud835\udc57:\nC=AB=266666664a>\n1\na>\n2...\na>\n\ud835\udc5b377777775\u0002\nb1b2\u0001\u0001\u0001b\ud835\udc5a\u0003\n=266666664a>\n1b1a>\n1b2\u0001\u0001\u0001a>\n1b\ud835\udc5a\na>\n2b1a>\n2b2\u0001\u0001\u0001a>\n2b\ud835\udc5a\n............\na>\n\ud835\udc5bb1a>\n\ud835\udc5bb2\u0001\u0001\u0001a>\n\ud835\udc5bb\ud835\udc5a377777775. (2.3.9)\nWecanthinkofthematrix\u2013matrixmultiplication ABasperforming \ud835\udc5amatrix\u2013vectorprod-\nucts or\ud835\udc5a\u0002\ud835\udc5bdot products and stitching the results together to form an \ud835\udc5b\u0002\ud835\udc5amatrix. In the\nfollowing snippet, we perform matrix multiplication on AandB. Here, Ais a matrix with\ntwo rows and three columns, and Bis a matrix with three rows and four columns. After\nmultiplication, we obtain a matrix with two rows and four columns.\nB=torch .ones( 3,4)\ntorch .mm(A, B), A @B\n(tensor([[ 3.,3.,3.,3.],\n[12.,12.,12.,12.]]),\ntensor([[ 3.,3.,3.,3.],\n[12.,12.,12.,12.]]))\nThe term matrix\u2013matrix multiplication is often simplified to matrix multiplication , and\nshould not be confused with the Hadamard product.\n2.3.11Norms\nSome of the most useful operators in linear algebra are norms. Informally, the norm of a\nvector tells us how bigit is. For instance, the \u21132norm measures the (Euclidean) length of a\nvector. Here, we are employing a notion of sizethat concerns the magnitude of a vector\u2019s\ncomponents (not its dimensionality).\nA norm is a function k\u0001kthat maps a vector to a scalar and satisfies the following three\nproperties:\n1.Given any vector x, if we scale (all elements of) the vector by a scalar \ud835\udefc2R, its norm\nscales accordingly:\nk\ud835\udefcxk=j\ud835\udefcjkxk. (2.3.10)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1673, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "617bf87d-2e1e-4115-867f-cbae3746b6e4": {"__data__": {"id_": "617bf87d-2e1e-4115-867f-cbae3746b6e4", "embedding": null, "metadata": {"page_label": "51", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93848c46-d556-4e4e-8e0a-0241df0fa7fc", "node_type": "4", "metadata": {"page_label": "51", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f5b1737bf3e4fb47d42c9c0fb1cb402f1f8cbea85a51244f7dbdb4310c6ca2af", "class_name": "RelatedNodeInfo"}}, "text": "51 Linear Algebra\n2.For any vectors xandy: norms satisfy the triangle inequality:\nkx\u00b8yk\u0014kxk\u00b8kyk. (2.3.11)\n3.The norm of a vector is nonnegative and it only vanishes if the vector is zero:\nkxk>0for allx\u22600. (2.3.12)\nMany functions are valid norms and different norms encode different notions of size. The\nEuclidean norm that we all learned in elementary school geometry when calculating the\nhypotenuseofarighttriangleisthesquarerootofthesumofsquaresofavector\u2019selements.\nFormally, this is called the \u21132normand expressed as\nkxk2=vt\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc652\n\ud835\udc56. (2.3.13)\nThe method normcalculates the \u21132norm.\nu=torch .tensor([ 3.0,-4.0])\ntorch .norm(u)\ntensor( 5.)\nThe\u21131norm is also common and the associated measure is called the Manhattan distance.\nBy definition, the \u21131norm sums the absolute values of a vector\u2019s elements:\nkxk1=\ud835\udc5b\u00d5\n\ud835\udc56=1j\ud835\udc65\ud835\udc56j. (2.3.14)\nCompared to the \u21132norm, it is less sensitive to outliers. To compute the \u21131norm, we\ncompose the absolute value with the sum operation.\ntorch .abs(u) .sum()\ntensor( 7.)\nBoth the\u21132and\u21131norms are special cases of the more general \u2113\ud835\udc5dnorms:\nkxk\ud835\udc5d= \ud835\udc5b\u00d5\n\ud835\udc56=1j\ud835\udc65\ud835\udc56j\ud835\udc5d!1\u009d\ud835\udc5d\n. (2.3.15)\nIn the case of matrices, matters are more complicated. After all, matrices can be viewed\nbothascollectionsofindividualentries andasobjectsthatoperateonvectorsandtransform\nthem into other vectors. For instance, we can ask by how much longer the matrix\u2013vector\nproduct Xvcould be relative to v. This line of thought leads to what is called the spectral", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1447, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b9933bd6-f123-4258-95b8-f385b5513fc1": {"__data__": {"id_": "b9933bd6-f123-4258-95b8-f385b5513fc1", "embedding": null, "metadata": {"page_label": "52", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39fc460d-a7b7-4bd7-8ee1-459e7470c5df", "node_type": "4", "metadata": {"page_label": "52", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f0ebdc34ba4873b7d05ba4f09199056dd05558e4480a2b6680dbd394e08dc3db", "class_name": "RelatedNodeInfo"}}, "text": "52 Preliminaries\nnorm. For now, we introduce the Frobenius norm , which is much easier to compute and\ndefined as the square root of the sum of the squares of a matrix\u2019s elements:\nkXkF=vut\ud835\udc5a\u00d5\n\ud835\udc56=1\ud835\udc5b\u00d5\n\ud835\udc57=1\ud835\udc652\n\ud835\udc56\ud835\udc57. (2.3.16)\nThe Frobenius norm behaves as if it were an \u21132norm of a matrix-shaped vector. Invoking\nthe following function will calculate the Frobenius norm of a matrix.\ntorch .norm(torch .ones(( 4,9)))\ntensor( 6.)\nWhile we do not want to get too far ahead of ourselves, we already can plant some intu-\nition about why these concepts are useful. In deep learning, we are often trying to solve\noptimization problems: maximize the probability assigned to observed data; maximize the\nrevenue associated with a recommender model; minimize the distance between predictions\nandthegroundtruthobservations; minimize thedistancebetweenrepresentationsofphotos\nofthesamepersonwhile maximizing thedistancebetweenrepresentationsofphotosofdif-\nferentpeople. Thesedistances,whichconstitutetheobjectivesofdeeplearningalgorithms,\nare often expressed as norms.\n2.3.12Discussion\nIn this section, we have reviewed all the linear algebra that you will need to understand a\nsignificant chunk of modern deep learning. There is a lot more to linear algebra, though,\nand much of it is useful for machine learning. For example, matrices can be decomposed\ninto factors, and these decompositions can reveal low-dimensional structure in real-world\ndatasets. There are entire subfields of machine learning that focus on using matrix decom-\npositions and their generalizations to high-order tensors to discover structure in datasets\nand solve prediction problems. But this book focuses on deep learning. And we believe\nyouwillbemoreinclinedtolearnmoremathematicsonceyouhavegottenyourhandsdirty\napplyingmachinelearningtorealdatasets. Sowhilewereservetherighttointroducemore\nmathematics later on, we wrap up this section here.\nIf you are eager to learn more linear algebra, there are many excellent books and online\nresources. For a more advanced crash course, consider checking out Strang ( 1993), Kolter\n(2008), and Petersen and Pedersen ( 2008).\nTo recap:\n\u000fScalars, vectors, matrices, and tensors are the basic mathematical objects used in linear\nalgebra and have zero, one, two, and an arbitrary number of axes, respectively.\n\u000fTensors can be sliced or reduced along specified axes via indexing, or operations such\nassumandmean, respectively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2414, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7927626e-d9a0-4bbe-bbf8-81369eb40728": {"__data__": {"id_": "7927626e-d9a0-4bbe-bbf8-81369eb40728", "embedding": null, "metadata": {"page_label": "53", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e7968dcd-5cd1-405b-8bf7-0d5323556670", "node_type": "4", "metadata": {"page_label": "53", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "53ec3ec33b8526db323a6f05e6972b8f859b1198b29d911e625fdb2d1e0eec53", "class_name": "RelatedNodeInfo"}}, "text": "53 Linear Algebra\n\u000fElementwiseproductsarecalledHadamardproducts. Bycontrast,dotproducts,matrix\u2013\nvector products, and matrix\u2013matrix products are not elementwise operations and in\ngeneral return objects having shapes that are different from the the operands.\n\u000fCompared to Hadamard products, matrix\u2013matrix products take considerably longer to\ncompute (cubic rather than quadratic time).\n\u000fNorms capture various notions of the magnitude of a vector (or matrix), and are com-\nmonly applied to the difference of two vectors to measure their distance apart.\n\u000fCommon vector norms include the \u21131and\u21132norms, and common matrix norms include\nthespectral andFrobenius norms.\n2.3.13Exercises\n1.Prove that the transpose of the transpose of a matrix is the matrix itself: \u00b9A>\u00ba>=A.\n2.Given two matrices AandB, show that sum and transposition commute: A>\u00b8B>=\n\u00b9A\u00b8B\u00ba>.\n3.Given any square matrix A, isA\u00b8A>always symmetric? Can you prove the result by\nusing only the results of the previous two exercises?\n4.We defined the tensor Xof shape (2, 3, 4) in this section. What is the output of len(X)?\nWrite your answer without implementing any code, then check your answer using code.\n5.For a tensor Xof arbitrary shape, does len(X) always correspond to the length of a\ncertain axis of X? What is that axis?\n6.Run A / A.sum(axis=1) and see what happens. Can you analyze the results?\n7.When traveling between two points in downtown Manhattan, what is the distance that\nyou need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can\nyou travel diagonally?\n8.Consider a tensor of shape (2, 3, 4). What are the shapes of the summation outputs\nalong axes 0, 1, and 2?\n9.Feed a tensor with three or more axes to the linalg.norm function and observe its\noutput. What does this function compute for tensors of arbitrary shape?\n10.Consider three large matrices, say A2R210\u0002216,B2R216\u000225andC2R25\u0002214, ini-\ntialized with Gaussian random variables. You want to compute the product ABC. Is\nthereanydifferenceinmemoryfootprintandspeed,dependingonwhetheryoucompute\n\u00b9AB\u00baCorA\u00b9BC\u00ba. Why?\n11.Consider three large matrices, say A2R210\u0002216,B2R216\u000225andC2R25\u0002216. Is there\nany difference in speed depending on whether you compute ABorAC>? Why? What\nchanges if you initialize C=B>without cloning memory? Why?\n12.Consider three matrices, say A,B,C2R100\u0002200. Construct a tensor with three axes by", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2370, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb2d9984-67b7-4d8a-82b8-91da3e8d5148": {"__data__": {"id_": "bb2d9984-67b7-4d8a-82b8-91da3e8d5148", "embedding": null, "metadata": {"page_label": "54", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1bb0de0-db2a-4655-af02-fb158c4efc55", "node_type": "4", "metadata": {"page_label": "54", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ea601a4fbd71a74c8c30c8d3df6e2c64fd2ce20da3fe3d3f830f72ec9a11139b", "class_name": "RelatedNodeInfo"}}, "text": "54 Preliminaries\n55stacking\u00bbA,B,C\u00bc. What is the dimensionality? Slice out the second coordinate of the\nthird axis to recover B. Check that your answer is correct.\nDiscussions55.\n2.4Calculus\nFor a long time, how to calculate the area of a circle remained a mystery. Then, in Ancient\nGreece, the mathematician Archimedes came up with the clever idea to inscribe a series of\npolygons with increasing numbers of vertices on the inside of a circle ( Fig. 2.4.1 ). For a\npolygon with \ud835\udc5bvertices, we obtain \ud835\udc5btriangles. The height of each triangle approaches the\nradius\ud835\udc5faswepartitionthecirclemorefinely. Atthesametime,itsbaseapproaches 2\ud835\udf0b\ud835\udc5f\u009d\ud835\udc5b,\nsince the ratio between arc and secant approaches 1 for a large number of vertices. Thus,\nthe area of the polygon approaches \ud835\udc5b\u0001\ud835\udc5f\u00011\n2\u00b92\ud835\udf0b\ud835\udc5f\u009d\ud835\udc5b\u00ba=\ud835\udf0b\ud835\udc5f2.\ntFig. 2.4.1 Finding the area of a circle as a limit procedure.\nThislimitingprocedureisattherootofboth differentialcalculus andintegralcalculus . The\nformer can tell us how to increase or decrease a function\u2019s value by manipulating its argu-\nments. This comes in handy for the optimization problems that we face in deep learning,\nwhere we repeatedly update our parameters in order to decrease the loss function. Opti-\nmizationaddresseshowtofitourmodelstotrainingdata,andcalculusisitskeyprerequisite.\nHowever, do not forget that our ultimate goal is to perform well on previouslyunseen data.\nThat problem is called generalization and will be a key focus of other chapters.\n%matplotlib inline\nimport numpy asnp\nfrom matplotlib_inline import backend_inline\nfrom d2l import torch asd2l\n2.4.1Derivativesand Differentiation\nPut simply, a derivative is the rate of change in a function with respect to changes in its\narguments. Derivatives can tell us how rapidly a loss function would increase or decrease\nwere we to increase ordecrease each parameter by an infinitesimally small amount. For-\nmally, for functions \ud835\udc53:R!R, that map from scalars to scalars, the derivative of\ud835\udc53at a\npoint\ud835\udc65is defined as\n\ud835\udc530\u00b9\ud835\udc65\u00ba=lim\n\u210e!0\ud835\udc53\u00b9\ud835\udc65\u00b8\u210e\u00ba\u0000\ud835\udc53\u00b9\ud835\udc65\u00ba\n\u210e. (2.4.1)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cad6b4c-576d-4218-8ec2-fa461df72811": {"__data__": {"id_": "0cad6b4c-576d-4218-8ec2-fa461df72811", "embedding": null, "metadata": {"page_label": "55", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a032f26-cfb9-483a-b87b-d2fe24205f08", "node_type": "4", "metadata": {"page_label": "55", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "09675834fd07bf2c300d0b42125958690cf034733c23023d4422d26febb98964", "class_name": "RelatedNodeInfo"}}, "text": "55 Calculus\nThis term on the right hand side is called a limitand it tells us what happens to the value of\nan expression as a specified variable approaches a particular value. This limit tells us what\nthe ratio between a perturbation \u210eand the change in the function value \ud835\udc53\u00b9\ud835\udc65\u00b8\u210e\u00ba\u0000\ud835\udc53\u00b9\ud835\udc65\u00ba\nconverges to as we shrink its size to zero.\nWhen\ud835\udc530\u00b9\ud835\udc65\u00baexists,\ud835\udc53is said to be differentiable at\ud835\udc65; and when\ud835\udc530\u00b9\ud835\udc65\u00baexists for all \ud835\udc65on a\nset, e.g., the interval \u00bb\ud835\udc4e,\ud835\udc4f\u00bc, we say that \ud835\udc53is differentiable on this set. Not all functions are\ndifferentiable,includingmanythatwewishtooptimize,suchasaccuracyandtheareaunder\nthe receiving operating characteristic (AUC). However, because computing the derivative\nof the loss is a crucial step in nearly all algorithms for training deep neural networks, we\noften optimize a differentiable surrogate instead.\nWe can interpret the derivative \ud835\udc530\u00b9\ud835\udc65\u00baas theinstantaneous rate of change of \ud835\udc53\u00b9\ud835\udc65\u00bawith\nrespect to\ud835\udc65. Let\u2019s develop some intuition with an example. Define \ud835\udc62=\ud835\udc53\u00b9\ud835\udc65\u00ba=3\ud835\udc652\u0000\n4\ud835\udc65.\ndef f(x):\nreturn 3*x**2-4*x\nSetting\ud835\udc65=1, we see that\ud835\udc53\u00b9\ud835\udc65\u00b8\u210e\u00ba\u0000\ud835\udc53\u00b9\ud835\udc65\u00ba\n\u210eapproaches 2as\u210eapproaches 0. While this ex-\nperiment lacks the rigor of a mathematical proof, we can quickly see that indeed \ud835\udc530\u00b91\u00ba=\n2.\nfor hin10.0 **np.arange( -1,-6,-1):\nprint (f'h={h:.5f}, numerical limit= {(f(1+h)-f(1))/h:.5f}')\nh=0.10000 , numerical limit =2.30000\nh=0.01000 , numerical limit =2.03000\nh=0.00100 , numerical limit =2.00300\nh=0.00010 , numerical limit =2.00030\nh=0.00001 , numerical limit =2.00003\nThere are several equivalent notational conventions for derivatives. Given \ud835\udc66=\ud835\udc53\u00b9\ud835\udc65\u00ba, the\nfollowing expressions are equivalent:\n\ud835\udc530\u00b9\ud835\udc65\u00ba=\ud835\udc660=\ud835\udc51\ud835\udc66\n\ud835\udc51\ud835\udc65=\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65=\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc37\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc37\ud835\udc65\ud835\udc53\u00b9\ud835\udc65\u00ba, (2.4.2)\nwhere the symbols\ud835\udc51\n\ud835\udc51\ud835\udc65and\ud835\udc37aredifferentiation operators . Below, we present the deriva-\ntives of some common functions:\n\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc36=0 for any constant \ud835\udc36\n\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc65\ud835\udc5b=\ud835\udc5b\ud835\udc65\ud835\udc5b\u00001for\ud835\udc5b\u22600\n\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc52\ud835\udc65=\ud835\udc52\ud835\udc65\n\ud835\udc51\n\ud835\udc51\ud835\udc65ln\ud835\udc65=\ud835\udc65\u00001.(2.4.3)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e421b952-d4f7-48da-9fbd-bcb756e1b7ca": {"__data__": {"id_": "e421b952-d4f7-48da-9fbd-bcb756e1b7ca", "embedding": null, "metadata": {"page_label": "56", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b4abbce-a384-41fa-8fae-4031aeca7d2a", "node_type": "4", "metadata": {"page_label": "56", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "90f7ff2d9c10273b32dac06be88f04afb97b40211d96861791a08c92f1393277", "class_name": "RelatedNodeInfo"}}, "text": "56 Preliminaries\nFunctions composed from differentiable functions are often themselves differentiable. The\nfollowing rules come in handy for working with compositions of any differentiable func-\ntions\ud835\udc53and\ud835\udc54, and constant \ud835\udc36.\n\ud835\udc51\n\ud835\udc51\ud835\udc65\u00bb\ud835\udc36\ud835\udc53\u00b9\ud835\udc65\u00ba\u00bc=\ud835\udc36\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc53\u00b9\ud835\udc65\u00ba Constant multiple rule\n\ud835\udc51\n\ud835\udc51\ud835\udc65\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc54\u00b9\ud835\udc65\u00ba\u00bc=\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc54\u00b9\ud835\udc65\u00ba Sum rule\n\ud835\udc51\n\ud835\udc51\ud835\udc65\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc54\u00b9\ud835\udc65\u00ba\u00bc=\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc54\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc54\u00b9\ud835\udc65\u00ba\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc53\u00b9\ud835\udc65\u00baProduct rule\n\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc53\u00b9\ud835\udc65\u00ba\n\ud835\udc54\u00b9\ud835\udc65\u00ba=\ud835\udc54\u00b9\ud835\udc65\u00ba\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc53\u00b9\ud835\udc65\u00ba\u0000\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc54\u00b9\ud835\udc65\u00ba\n\ud835\udc542\u00b9\ud835\udc65\u00baQuotient rule(2.4.4)\nUsing this, we can apply the rules to find the derivative of 3\ud835\udc652\u00004\ud835\udc65via\n\ud835\udc51\n\ud835\udc51\ud835\udc65\u00bb3\ud835\udc652\u00004\ud835\udc65\u00bc=3\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc652\u00004\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc65=6\ud835\udc65\u00004. (2.4.5)\nPlugging in \ud835\udc65=1shows that, indeed, the derivative equals 2at this location. Note that\nderivatives tell us the slopeof a function at a particular location.\n2.4.2VisualizationUtilities\nWe can visualize the slopes of functions using the matplotlib library. We need to de-\nfine a few functions. As its name indicates, use_svg_display tells matplotlib to output\ngraphicsinSVGformatforcrisperimages. Thecomment #@saveisaspecialmodifierthat\nallows us to save any function, class, or other code block to the d2lpackage so that we can\ninvoke it later without repeating the code, e.g., via d2l.use_svg_display() .\ndef use_svg_display (): #@save\n\"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\nbackend_inline .set_matplotlib_formats( 'svg')\nConveniently, we can set figure sizes with set_figsize . Since the import statement from\nmatplotlib import pyplot as plt was marked via #@savein the d2lpackage, we can\ncalld2l.plt .\ndef set_figsize (figsize =(3.5,2.5)): #@save\n\"\"\"Set the figure size for matplotlib.\"\"\"\nuse_svg_display()\nd2l.plt.rcParams[ 'figure.figsize ']=figsize\nThe set_axes function can associate axes with properties, including labels, ranges, and\nscales.\n#@save\ndef set_axes (axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n\"\"\"Set the axes for matplotlib.\"\"\"\naxes .set_xlabel(xlabel), axes .set_ylabel(ylabel)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12ad2c3f-fd69-4ab4-b29d-3d46696cbb8e": {"__data__": {"id_": "12ad2c3f-fd69-4ab4-b29d-3d46696cbb8e", "embedding": null, "metadata": {"page_label": "57", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f8abe648-a4f2-4069-a5f8-4a6f68112cf1", "node_type": "4", "metadata": {"page_label": "57", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6fad08d98b89dc6db3f4ed279d3e6e038f573ec9456332ac7e85399a5ee31399", "class_name": "RelatedNodeInfo"}}, "text": "57 Calculus\n(continued from previous page)\naxes .set_xscale(xscale), axes .set_yscale(yscale)\naxes .set_xlim(xlim), axes .set_ylim(ylim)\niflegend:\naxes .legend(legend)\naxes .grid()\nWiththesethreefunctions,wecandefinea plotfunctiontooverlaymultiplecurves. Much\nof the code here is just ensuring that the sizes and shapes of inputs match.\n#@save\ndef plot (X, Y =None , xlabel =None , ylabel =None , legend =[], xlim =None ,\nylim =None , xscale ='linear ', yscale ='linear ',\nfmts =('-','m--','g-.','r:'), figsize =(3.5,2.5), axes =None ):\n\"\"\"Plot data points.\"\"\"\ndef has_one_axis (X): # True if X (tensor or list) has 1 axis\nreturn (hasattr (X, \"ndim \")and X.ndim ==1orisinstance (X, list )\nand not hasattr (X[0],\"__len__ \"))\nifhas_one_axis(X): X =[X]\nifYisNone :\nX, Y =[[]] *len(X), X\nelif has_one_axis(Y):\nY=[Y]\niflen(X) !=len(Y):\nX=X*len(Y)\nset_figsize(figsize)\nifaxes isNone :\naxes =d2l.plt.gca()\naxes .cla()\nfor x, y, fmt inzip(X, Y, fmts):\naxes .plot(x,y,fmt) iflen(x) else axes .plot(y,fmt)\nset_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\nNow we can plot the function \ud835\udc62=\ud835\udc53\u00b9\ud835\udc65\u00baand its tangent line \ud835\udc66=2\ud835\udc65\u00003at\ud835\udc65=1, where the\ncoefficient 2is the slope of the tangent line.\nx=np.arange( 0,3,0.1)\nplot(x, [f(x), 2*x-3],'x','f(x) ', legend =['f(x) ','Tangent line (x=1) '])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed4790d1-f0ea-4240-ba29-ee1f4c0b5a7f": {"__data__": {"id_": "ed4790d1-f0ea-4240-ba29-ee1f4c0b5a7f", "embedding": null, "metadata": {"page_label": "58", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1cda2b2-4e87-4c41-8b20-d709a342f985", "node_type": "4", "metadata": {"page_label": "58", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "99061047515944ce8fb308bba820bb1b482be783bf575105f5a3ab597ad71639", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f40f765b-1d64-4843-9d95-effd0747b318", "node_type": "1", "metadata": {}, "hash": "aa34d84f3ccec51d12d013cb116587bed5bc72ac0af0ce74617b74e0d6b6cce8", "class_name": "RelatedNodeInfo"}}, "text": "58 Preliminaries\n2.4.3PartialDerivativesand Gradients\nThus far, we have been differentiating functions of just one variable. In deep learning, we\nalso need to work with functions of manyvariables. We briefly introduce notions of the\nderivative that apply to such multivariate functions.\nLet\ud835\udc66=\ud835\udc53\u00b9\ud835\udc651,\ud835\udc652,...,\ud835\udc65\ud835\udc5b\u00babe a function with \ud835\udc5bvariables. The partial derivative of\ud835\udc66with\nrespect to its \ud835\udc56thparameter\ud835\udc65\ud835\udc56is\n\ud835\udf15\ud835\udc66\n\ud835\udf15\ud835\udc65\ud835\udc56=lim\n\u210e!0\ud835\udc53\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc56\u00001,\ud835\udc65\ud835\udc56\u00b8\u210e,\ud835\udc65\ud835\udc56\u00b81,...,\ud835\udc65\ud835\udc5b\u00ba\u0000\ud835\udc53\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc56,...,\ud835\udc65\ud835\udc5b\u00ba\n\u210e. (2.4.6)\nTocalculate\ud835\udf15\ud835\udc66\n\ud835\udf15\ud835\udc65\ud835\udc56,wecantreat \ud835\udc651,...,\ud835\udc65\ud835\udc56\u00001,\ud835\udc65\ud835\udc56\u00b81,...,\ud835\udc65\ud835\udc5basconstantsandcalculatethederiva-\ntive of\ud835\udc66with respect to \ud835\udc65\ud835\udc56. The following notational conventions for partial derivatives are\nall common and all mean the same thing:\n\ud835\udf15\ud835\udc66\n\ud835\udf15\ud835\udc65\ud835\udc56=\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc65\ud835\udc56=\ud835\udf15\ud835\udc65\ud835\udc56\ud835\udc53=\ud835\udf15\ud835\udc56\ud835\udc53=\ud835\udc53\ud835\udc65\ud835\udc56=\ud835\udc53\ud835\udc56=\ud835\udc37\ud835\udc56\ud835\udc53=\ud835\udc37\ud835\udc65\ud835\udc56\ud835\udc53. (2.4.7)\nWe can concatenate partial derivatives of a multivariate function with respect to all its\nvariables to obtain a vector that is called the gradient of the function. Suppose that the\ninput of function \ud835\udc53:R\ud835\udc5b!Ris an\ud835\udc5b-dimensional vector x=\u00bb\ud835\udc651,\ud835\udc652,...,\ud835\udc65\ud835\udc5b\u00bc>and the\noutput is a scalar. The gradient of the function \ud835\udc53with respect to xis a vector of \ud835\udc5bpartial\nderivatives:\nrx\ud835\udc53\u00b9x\u00ba=\u0002\n\ud835\udf15\ud835\udc651\ud835\udc53\u00b9x\u00ba,\ud835\udf15\ud835\udc652\ud835\udc53\u00b9x\u00ba,...\ud835\udf15\ud835\udc65\ud835\udc5b\ud835\udc53\u00b9x\u00ba\u0003>. (2.4.8)\nWhen there is no ambiguity, rx\ud835\udc53\u00b9x\u00bais typically replaced by r\ud835\udc53\u00b9x\u00ba. The following rules\ncome in handy for differentiating multivariate functions:\n\u000fFor all A2R\ud835\udc5a\u0002\ud835\udc5bwe haverxAx=A>andrxx>A=A.\n\u000fFor square matrices A2R\ud835\udc5b\u0002\ud835\udc5bwe have thatrxx>Ax=\u00b9A\u00b8A>\u00baxand in particular\nrxkxk2=rxx>x=2x.\nSimilarly, for any matrix X, we haverXkXk2\nF=2X.\n2.4.4Chain Rule\nIn deep learning, the gradients of concern are often difficult to calculate because we are\nworkingwithdeeplynestedfunctions(offunctions(offunctions\u2026)). Fortunately,the chain\nruletakescareofthis. Returningtofunctionsofasinglevariable,supposethat \ud835\udc66=\ud835\udc53\u00b9\ud835\udc54\u00b9\ud835\udc65\u00ba\u00ba\nand that the underlying functions \ud835\udc66=\ud835\udc53\u00b9\ud835\udc62\u00baand\ud835\udc62=\ud835\udc54\u00b9\ud835\udc65\u00baare both differentiable. The chain\nrule states that\n\ud835\udc51\ud835\udc66\n\ud835\udc51\ud835\udc65=\ud835\udc51\ud835\udc66\n\ud835\udc51\ud835\udc62\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc65.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1863, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f40f765b-1d64-4843-9d95-effd0747b318": {"__data__": {"id_": "f40f765b-1d64-4843-9d95-effd0747b318", "embedding": null, "metadata": {"page_label": "58", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1cda2b2-4e87-4c41-8b20-d709a342f985", "node_type": "4", "metadata": {"page_label": "58", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "99061047515944ce8fb308bba820bb1b482be783bf575105f5a3ab597ad71639", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed4790d1-f0ea-4240-ba29-ee1f4c0b5a7f", "node_type": "1", "metadata": {"page_label": "58", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a39d24b15b918f668e9f3fedd051ba900fb38cd5ac44e4b0c265c24508bbeb3d", "class_name": "RelatedNodeInfo"}}, "text": "Similarly, for any matrix X, we haverXkXk2\nF=2X.\n2.4.4Chain Rule\nIn deep learning, the gradients of concern are often difficult to calculate because we are\nworkingwithdeeplynestedfunctions(offunctions(offunctions\u2026)). Fortunately,the chain\nruletakescareofthis. Returningtofunctionsofasinglevariable,supposethat \ud835\udc66=\ud835\udc53\u00b9\ud835\udc54\u00b9\ud835\udc65\u00ba\u00ba\nand that the underlying functions \ud835\udc66=\ud835\udc53\u00b9\ud835\udc62\u00baand\ud835\udc62=\ud835\udc54\u00b9\ud835\udc65\u00baare both differentiable. The chain\nrule states that\n\ud835\udc51\ud835\udc66\n\ud835\udc51\ud835\udc65=\ud835\udc51\ud835\udc66\n\ud835\udc51\ud835\udc62\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc65. (2.4.9)\nTurningbacktomultivariatefunctions,supposethat \ud835\udc66=\ud835\udc53\u00b9u\u00bahasvariables \ud835\udc621,\ud835\udc622,...,\ud835\udc62\ud835\udc5a,\nwhere each\ud835\udc62\ud835\udc56=\ud835\udc54\ud835\udc56\u00b9x\u00bahas variables \ud835\udc651,\ud835\udc652,...,\ud835\udc65\ud835\udc5b, i.e.,u=\ud835\udc54\u00b9x\u00ba. Then the chain rule\nstates that\n\ud835\udf15\ud835\udc66\n\ud835\udf15\ud835\udc65\ud835\udc56=\ud835\udf15\ud835\udc66\n\ud835\udf15\ud835\udc621\ud835\udf15\ud835\udc621\n\ud835\udf15\ud835\udc65\ud835\udc56\u00b8\ud835\udf15\ud835\udc66\n\ud835\udf15\ud835\udc622\ud835\udf15\ud835\udc622\n\ud835\udf15\ud835\udc65\ud835\udc56\u00b8...\u00b8\ud835\udf15\ud835\udc66\n\ud835\udf15\ud835\udc62\ud835\udc5a\ud835\udf15\ud835\udc62\ud835\udc5a\n\ud835\udf15\ud835\udc65\ud835\udc56and sorx\ud835\udc66=Aru\ud835\udc66, (2.4.10)", "mimetype": "text/plain", "start_char_idx": 1425, "end_char_idx": 2119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1727c6a-636b-45ca-8997-946b790d86ca": {"__data__": {"id_": "f1727c6a-636b-45ca-8997-946b790d86ca", "embedding": null, "metadata": {"page_label": "59", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c29adde6-2d1a-4aab-bc2a-c6cad078aafa", "node_type": "4", "metadata": {"page_label": "59", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a2a88acf8acb8a750c1a4da110ddbccddbc4354facc4d1a3821e6eefc3da414f", "class_name": "RelatedNodeInfo"}}, "text": "59 Calculus\n56whereA2R\ud835\udc5b\u0002\ud835\udc5ais amatrixthat contains the derivative of vector uwith respect to vector\nx. Thus, evaluating the gradient requires computing a vector\u2013matrix product. This is one\nof the key reasons why linear algebra is such an integral building block in building deep\nlearning systems.\n2.4.5Discussion\nWhilewehavejustscratchedthesurfaceofadeeptopic,anumberofconceptsalreadycome\nintofocus: first, thecompositionrulesfordifferentiationcanbeappliedroutinely, enabling\nus to compute gradients automatically . This task requires no creativity and thus we can\nfocus our cognitive powers elsewhere. Second, computing the derivatives of vector-valued\nfunctions requires us to multiply matrices as we trace the dependency graph of variables\nfrom output to input. In particular, this graph is traversed in a forward direction when\nwe evaluate a function and in a backwards direction when we compute gradients. Later\nchapters will formally introduce backpropagation, a computational procedure for applying\nthe chain rule.\nFrom the viewpoint of optimization, gradients allow us to determine how to move the pa-\nrametersofamodelinordertolowertheloss, andeachstepoftheoptimizationalgorithms\nused throughout this book will require calculating the gradient.\n2.4.6Exercises\n1.Sofarwetooktherulesforderivativesforgranted. Usingthedefinitionandlimitsprove\nthe properties for (i) \ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc50, (ii)\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc65\ud835\udc5b, (iii)\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc52\ud835\udc65and (iv)\ud835\udc53\u00b9\ud835\udc65\u00ba=log\ud835\udc65.\n2.In the same vein, prove the product, sum, and quotient rule from first principles.\n3.Prove that the constant multiple rule follows as a special case of the product rule.\n4.Calculate the derivative of \ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc65\ud835\udc65.\n5.What does it mean that \ud835\udc530\u00b9\ud835\udc65\u00ba=0for some\ud835\udc65? Give an example of a function \ud835\udc53and a\nlocation\ud835\udc65for which this might hold.\n6.Plot the function \ud835\udc66=\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc653\u00001\n\ud835\udc65and plot its tangent line at \ud835\udc65=1.\n7.Find the gradient of the function \ud835\udc53\u00b9x\u00ba=3\ud835\udc652\n1\u00b85\ud835\udc52\ud835\udc652.\n8.What is the gradient of the function \ud835\udc53\u00b9x\u00ba=kxk2? What happens for x=0?\n9.Can you write out the chain rule for the case where \ud835\udc62=\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66,\ud835\udc67\u00baand\ud835\udc65=\ud835\udc65\u00b9\ud835\udc4e,\ud835\udc4f\u00ba,\n\ud835\udc66=\ud835\udc66\u00b9\ud835\udc4e,\ud835\udc4f\u00ba, and\ud835\udc67=\ud835\udc67\u00b9\ud835\udc4e,\ud835\udc4f\u00ba?\n10.Given a function \ud835\udc53\u00b9\ud835\udc65\u00bathat is invertible, compute the derivative of its inverse \ud835\udc53\u00001\u00b9\ud835\udc65\u00ba.\nHere we have that \ud835\udc53\u00001\u00b9\ud835\udc53\u00b9\ud835\udc65\u00ba\u00ba=\ud835\udc65and conversely \ud835\udc53\u00b9\ud835\udc53\u00001\u00b9\ud835\udc66\u00ba\u00ba=\ud835\udc66. Hint: use these\nproperties in your derivation.\nDiscussions56.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2248, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05b4a805-7dea-4c47-928e-8aeed6583602": {"__data__": {"id_": "05b4a805-7dea-4c47-928e-8aeed6583602", "embedding": null, "metadata": {"page_label": "60", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f3e80c2-dfac-4a3a-acdc-bf2563ffc259", "node_type": "4", "metadata": {"page_label": "60", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bbbfd9bb666c7af60b15dc693f594f0f3ae67a6a49c410832cbd5dc0d196766f", "class_name": "RelatedNodeInfo"}}, "text": "60 Preliminaries\n2.5AutomaticDifferentiation\nRecallfrom Section2.4 thatcalculatingderivativesisthecrucialstepinalltheoptimization\nalgorithms that we will use to train deep networks. While the calculations are straightfor-\nward,workingthemoutbyhandcanbetediousanderror-prone,andtheseissuesonlygrow\nas our models become more complex.\nFortunately all modern deep learning frameworks take this work off our plates by offering\nautomatic differentiation (often shortened to autograd ). As we pass data through each\nsuccessivefunction,theframeworkbuildsa computationalgraph thattrackshoweachvalue\ndepends on others. To calculate derivatives, automatic differentiation works backwards\nthrough this graph applying the chain rule. The computational algorithm for applying the\nchain rule in this fashion is called backpropagation .\nWhile autograd libraries have become a hot concern over the past decade, they have a\nlong history. In fact the earliest references to autograd date back over half of a century\n(Wengert, 1964 ). The core ideas behind modern backpropagation date to a PhD thesis\nfrom 1980 ( Speelpenning, 1980 ) and were further developed in the late 1980s ( Griewank,\n1989). While backpropagation has become the default method for computing gradients,\nit is not the only option. For instance, the Julia programming language employs forward\npropagation ( Revelsetal., 2016). Before exploring methods, let\u2019s first master the autograd\npackage.\nimport torch\n2.5.1ASimpleFunction\nLet\u2019s assume that we are interested in differentiating the function \ud835\udc66=2x>xwith respect to\nthe column vector x. To start, we assign xan initial value.\nx=torch .arange( 4.0)\nx\ntensor([ 0.,1.,2.,3.])\nBefore we calculate the gradient of \ud835\udc66with respect to x, we need a place to store it. In\ngeneral, we avoid allocating new memory every time we take a derivative because deep\nlearning requires successively computing derivatives with respect to the same parameters\na great many times, and we might risk running out of memory. Note that the gradient of\na scalar-valued function with respect to a vector xis vector-valued with the same shape as\nx.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b4ddc73-e2c4-427d-a387-a1fb531a33ed": {"__data__": {"id_": "4b4ddc73-e2c4-427d-a387-a1fb531a33ed", "embedding": null, "metadata": {"page_label": "61", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27e666d4-ce96-485a-b671-d3281a6b8c4f", "node_type": "4", "metadata": {"page_label": "61", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c0e77a208e0df099dec0dcaa14c67a24f4e8ee3a518c567ade53dec00728526e", "class_name": "RelatedNodeInfo"}}, "text": "61 Automatic Differentiation\n# Can also create x = torch.arange(4.0, requires_grad=True)\nx.requires_grad_( True )\nx.grad # The gradient is None by default\nWe now calculate our function of xand assign the result to y.\ny=2*torch .dot(x, x)\ny\ntensor( 28., grad_fn =<MulBackward0 >)\nWe can now take the gradient of ywith respect to xby calling its backward method. Next,\nwe can access the gradient via x\u2019sgradattribute.\ny.backward()\nx.grad\ntensor([ 0.,4.,8.,12.])\nWe already know that the gradient of the function \ud835\udc66=2x>xwith respect to xshould be\n4x. Wecannowverify thattheautomaticgradientcomputationandtheexpectedresultare\nidentical.\nx.grad ==4*x\ntensor([ True ,True ,True ,True ])\nNow let\u2019s calculate another function of xand take its gradient. Note that PyTorch does not\nautomatically reset the gradient buffer when we record a new gradient. Instead, the new\ngradient is added to the already-stored gradient. This behavior comes in handy when we\nwant to optimize the sum of multiple objective functions. To reset the gradient buffer, we\ncan call x.grad.zero_() as follows:\nx.grad .zero_() # Reset the gradient\ny=x.sum()\ny.backward()\nx.grad\ntensor([ 1.,1.,1.,1.])\n2.5.2BackwardforNon-ScalarVariables\nWhen yis a vector, the most natural representation of the derivative of ywith respect\nto a vector xis a matrix called the Jacobian that contains the partial derivatives of each", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d79ded35-b431-4d0d-b983-885f8e797e06": {"__data__": {"id_": "d79ded35-b431-4d0d-b983-885f8e797e06", "embedding": null, "metadata": {"page_label": "62", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d2a4fc0-a7b2-45b4-8cc4-a0e55285a204", "node_type": "4", "metadata": {"page_label": "62", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7c8bbe17e3785c33b254c392b9bf2c89ab09f38567092c759837af2b01f7607f", "class_name": "RelatedNodeInfo"}}, "text": "62 Preliminaries\n57component of ywith respect to each component of x. Likewise, for higher-order yandx,\nthe result of differentiation could be an even higher-order tensor.\nWhile Jacobians do show up in some advanced machine learning techniques, more com-\nmonly we want to sum up the gradients of each component of ywith respect to the full\nvector x, yielding a vector of the same shape as x. For example, we often have a vector\nrepresenting the value of our loss function calculated separately for each example among a\nbatchof training examples. Here, we just want to sum up the gradients computed individ-\nually for each example.\nBecause deep learning frameworks vary in how they interpret gradients of non-scalar ten-\nsors, PyTorch takes some steps to avoid confusion. Invoking backward on a non-scalar\nelicits an error unless we tell PyTorch how to reduce the object to a scalar. More formally,\nwe need to provide some vector vsuch that backward will compute v>\ud835\udf15xyrather than\n\ud835\udf15xy. This next part may be confusing, but for reasons that will become clear later, this\nargument (representing v) is named gradient . For a more detailed description, see Yang\nZhang\u2019s Medium post57.\nx.grad .zero_()\ny=x*x\ny.backward(gradient =torch .ones( len(y))) # Faster: y.sum().backward()\nx.grad\ntensor([ 0.,2.,4.,6.])\n2.5.3DetachingComputation\nSometimes, we wish to move some calculations outside of the recorded computational\ngraph. For example, say that we use the input to create some auxiliary intermediate terms\nfor which we do not want to compute a gradient. In this case, we need to detachthe re-\nspective computational graph from the final result. The following toy example makes this\nclearer: suppose we have z = x * y andy = x * x but we want to focus on the direct\ninfluence of xonzrather than the influence conveyed via y. In this case, we can create a\nnew variable uthat takes the same value as ybut whose provenance (how it was created)\nhasbeenwipedout. Thus uhasnoancestorsinthegraphandgradientsdonotflowthrough\nutox. For example, taking the gradient of z = x * u will yield the result u, (not 3 * x\n* xas you might have expected since z = x * x * x ).\nx.grad .zero_()\ny=x*x\nu=y.detach()\nz=u*x\nz.sum() .backward()\nx.grad ==u", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed3a7d16-b2a6-453a-b0b8-d93bd8855ec8": {"__data__": {"id_": "ed3a7d16-b2a6-453a-b0b8-d93bd8855ec8", "embedding": null, "metadata": {"page_label": "63", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8f043b9-8820-458e-aba2-87c03b8ab06e", "node_type": "4", "metadata": {"page_label": "63", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4d9088f9345afa59fe3b3d5b30cc581d16b806249bde190fd2575455b054fa7f", "class_name": "RelatedNodeInfo"}}, "text": "63 Automatic Differentiation\ntensor([ True ,True ,True ,True ])\nNotethatwhilethisproceduredetaches y\u2019sancestorsfromthegraphleadingto z, thecom-\nputational graph leading to ypersists and thus we can calculate the gradient of ywith\nrespect to x.\nx.grad .zero_()\ny.sum() .backward()\nx.grad ==2*x\ntensor([ True ,True ,True ,True ])\n2.5.4Gradientsand Python ControlFlow\nSo far we reviewed cases where the path from input to output was well defined via a func-\ntion such as z = x * x * x . Programming offers us a lot more freedom in how we\ncompute results. For instance, we can make them depend on auxiliary variables or condi-\ntion choices on intermediate results. One benefit of using automatic differentiation is that\neven if building the computational graph of a function required passing through a maze\nof Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still\ncalculate the gradient of the resulting variable. To illustrate this, consider the following\ncode snippet where the number of iterations of the whileloop and the evaluation of the if\nstatement both depend on the value of the input a.\ndef f(a):\nb=a*2\nwhile b.norm() <1000 :\nb=b*2\nifb.sum() >0:\nc=b\nelse :\nc=100 *b\nreturn c\nBelow, we call this function, passing in a random value, as input. Since the input is a\nrandom variable, we do not know what form the computational graph will take. However,\nwhenever we execute f(a)on a specific input, we realize a specific computational graph\nand can subsequently run backward .\na=torch .randn(size =(), requires_grad =True )\nd=f(a)\nd.backward()\nEven though our function fis, for demonstration purposes, a bit contrived, its dependence\non the input is quite simple: it is a linearfunction of awith piecewise defined scale. As", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "575e57ca-c73c-4351-b8e9-f2e0bbf09631": {"__data__": {"id_": "575e57ca-c73c-4351-b8e9-f2e0bbf09631", "embedding": null, "metadata": {"page_label": "64", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a6d4ada6-5953-431d-954c-d724b8e51203", "node_type": "4", "metadata": {"page_label": "64", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "95ef466bbb241be89fca2d19ac25e844b570b2523b0524d7e53810c23f956e6f", "class_name": "RelatedNodeInfo"}}, "text": "64 Preliminaries\nsuch, f(a) / a is a vector of constant entries and, moreover, f(a) / a needs to match the\ngradient of f(a)with respect to a.\na.grad ==d/a\ntensor( True )\nDynamic control flow is very common in deep learning. For instance, when processing\ntext, the computational graph depends on the length of the input. In these cases, automatic\ndifferentiation becomes vital for statistical modeling since it is impossible to compute the\ngradienta priori.\n2.5.5Discussion\nYou have now gotten a taste of the power of automatic differentiation. The development of\nlibraries for calculating derivatives both automatically and efficiently has been a massive\nproductivity booster for deep learning practitioners, liberating them so they can focus on\nless menial. Moreover, autograd lets us design massive models for which pen and paper\ngradient computations would be prohibitively time consuming. Interestingly, while we use\nautograd to optimize models (in a statistical sense) the optimization of autograd libraries\nthemselves(inacomputationalsense)isarichsubjectofvitalinteresttoframeworkdesign-\ners. Here, tools from compilers and graph manipulation are leveraged to compute results\nin the most expedient and memory-efficient manner.\nFor now, try to remember these basics: (i) attach gradients to those variables with respect\nto which we desire derivatives; (ii) record the computation of the target value; (iii) execute\nthe backpropagation function; and (iv) access the resulting gradient.\n2.5.6Exercises\n1.Whyisthesecondderivativemuchmoreexpensivetocomputethanthefirstderivative?\n2.After running the function for backpropagation, immediately run it again and see what\nhappens. Investigate.\n3.In the control flow example where we calculate the derivative of dwith respect to a,\nwhat would happen if we changed the variable ato a random vector or a matrix? At\nthis point, the result of the calculation f(a)is no longer a scalar. What happens to the\nresult? How do we analyze this?\n4.Let\ud835\udc53\u00b9\ud835\udc65\u00ba=sin\u00b9\ud835\udc65\u00ba. Plot the graph of \ud835\udc53and of its derivative \ud835\udc530. Do not exploit the fact\nthat\ud835\udc530\u00b9\ud835\udc65\u00ba=cos\u00b9\ud835\udc65\u00babut rather use automatic differentiation to get the result.\n5.Let\ud835\udc53\u00b9\ud835\udc65\u00ba=\u00b9\u00b9log\ud835\udc652\u00ba\u0001sin\ud835\udc65\u00ba\u00b8\ud835\udc65\u00001. Write out a dependency graph tracing results from\n\ud835\udc65to\ud835\udc53\u00b9\ud835\udc65\u00ba.\n6.Usethechainruletocomputethederivative\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65oftheaforementionedfunction,placing\neach term on the dependency graph that you constructed previously.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2106326d-5ce7-4f94-af00-bcab066aa183": {"__data__": {"id_": "2106326d-5ce7-4f94-af00-bcab066aa183", "embedding": null, "metadata": {"page_label": "65", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56155bc6-0e0f-4be7-b8ac-3b92c68e2770", "node_type": "4", "metadata": {"page_label": "65", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6c5e4bc6219cc700a0329f18bb9362f8498f3223f422bb00ac875251dbbcba49", "class_name": "RelatedNodeInfo"}}, "text": "65 Probability and Statistics\n587.Given the graph and the intermediate derivative results, you have a number of options\nwhen computing the gradient. Evaluate the result once starting from \ud835\udc65to\ud835\udc53and once\nfrom\ud835\udc53tracing back to \ud835\udc65. The path from \ud835\udc65to\ud835\udc53is commonly known as forward differ-\nentiation , whereas the path from \ud835\udc53to\ud835\udc65is known as backward differentiation.\n8.When might you want to use forward, and when backward, differentiation? Hint: con-\nsider the amount of intermediate data needed, the ability to parallelize steps, and the\nsize of matrices and vectors involved.\nDiscussions58.\n2.6Probabilityand Statistics\nOne way or another, machine learning is all about uncertainty. In supervised learning, we\nwanttopredictsomethingunknown(the target)givensomethingknown(the features). De-\npending on our objective, we might attempt to predict the most likely value of the target.\nOr we might predict the value with the smallest expected distance from the target. And\nsometimes we wish not only to predict a specific value but to quantifyouruncertainty . For\nexample, given some features describing a patient, we might want to know howlikely they\nare to suffer a heart attack in the next year. In unsupervised learning, we often care about\nuncertainty. To determine whether a set of measurements are anomalous, it helps to know\nhow likely one is to observe values in a population of interest. Furthermore, in reinforce-\nment learning, we wish to develop agents that act intelligently in various environments.\nThis requires reasoning about how an environment might be expected to change and what\nrewards one might expect to encounter in response to each of the available actions.\nProbability is the mathematical field concerned with reasoning under uncertainty. Given a\nprobabilistic model of some process, we can reason about the likelihood of various events.\nTheuseofprobabilitiestodescribethefrequenciesofrepeatableevents(likecointosses)is\nfairlyuncontroversial. Infact, frequentist scholarsadheretoaninterpretationofprobability\nthat applies onlyto such repeatable events. By contrast Bayesian scholars use the language\nofprobabilitymorebroadlytoformalizereasoningunderuncertainty. Bayesianprobability\nis characterized by two unique features: (i) assigning degrees of belief to non-repeatable\nevents, e.g., what is the probability that a dam will collapse?; and (ii) subjectivity. While\nBayesianprobabilityprovidesunambiguousrulesforhowoneshouldupdatetheirbeliefsin\nlight of new evidence, it allows for different individuals to start off with different priorbe-\nliefs.Statistics helps us to reason backwards, starting off with collection and organization\nof data and backing out to what inferences we might draw about the process that generated\nthe data. Whenever we analyze a dataset, hunting for patterns that we hope might charac-\nterize a broader population, we are employing statistical thinking. Many courses, majors,\ntheses, careers, departments, companies, and institutions have been devoted to the study of\nprobabilityand statistics. While thissection onlyscratchesthe surface, wewill providethe\nfoundation that you need to begin building models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "736add59-f22f-4992-a026-4f582564cff0": {"__data__": {"id_": "736add59-f22f-4992-a026-4f582564cff0", "embedding": null, "metadata": {"page_label": "66", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28ceeab6-898e-4fc2-9a56-bb198d4cf3b2", "node_type": "4", "metadata": {"page_label": "66", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b344b9b442c786ec3914b4606b35a721d50b5045f702b63338688276ee9172b8", "class_name": "RelatedNodeInfo"}}, "text": "66 Preliminaries\n%matplotlib inline\nimport random\nimport torch\nfrom torch .distributions .multinomial import Multinomial\nfrom d2l import torch asd2l\n2.6.1A SimpleExample: TossingCoins\nImagine that we plan to toss a coin and want to quantify how likely we are to see heads\n(vs. tails). If the coin is fair, then both outcomes (heads and tails), are equally likely.\nMoreover if we plan to toss the coin \ud835\udc5btimes then the fraction of heads that we expectto\nsee should exactly match the expected fraction of tails. One intuitive way to see this is\nby symmetry: for every possible outcome with \ud835\udc5bhheads and\ud835\udc5bt=\u00b9\ud835\udc5b\u0000\ud835\udc5bh\u00batails, there is\nan equally likely outcome with \ud835\udc5btheads and\ud835\udc5bhtails. Note that this is only possible if on\naverage we expect to see 1\u009d2of tosses come up heads and 1\u009d2come up tails. Of course, if\nyou conduct this experiment many times with \ud835\udc5b=1000000 tosses each, you might never\nsee a trial where \ud835\udc5bh=\ud835\udc5btexactly.\nFormally, the quantity 1\u009d2is called a probability and here it captures the certainty with\nwhich any given toss will come up heads. Probabilities assign scores between 0and1to\noutcomes of interest, called events. Here the event of interest is heads and we denote the\ncorrespondingprobability \ud835\udc43\u00b9heads\u00ba. Aprobabilityof 1indicatesabsolutecertainty(imag-\nine a trick coin where both sides were heads) and a probability of 0indicates impossibility\n(e.g.,ifbothsidesweretails). Thefrequencies \ud835\udc5bh\u009d\ud835\udc5band\ud835\udc5bt\u009d\ud835\udc5barenotprobabilitiesbutrather\nstatistics . Probabilities are theoretical quantities that underly the data generating process.\nHere,theprobability 1\u009d2isapropertyofthecoinitself. Bycontrast,statisticsare empirical\nquantities that are computed as functions of the observed data. Our interests in probabilis-\ntic and statistical quantities are inextricably intertwined. We often design special statistics\ncalledestimators that,givenadataset,produce estimates ofmodelparameterssuchasprob-\nabilities. Moreover, when those estimators satisfy a nice property called consistency , our\nestimates will converge to the corresponding probability. In turn, these inferred probabili-\ntiestellaboutthelikelystatisticalpropertiesofdatafromthesamepopulationthatwemight\nencounter in the future.\nSuppose that we stumbled upon a real coin for which we did not know the true \ud835\udc43\u00b9heads\u00ba.\nTo investigate this quantity with statistical methods, we need to (i) collect some data; and\n(ii) design an estimator. Data acquisition here is easy; we can toss the coin many times\nandrecordalltheoutcomes. Formally, drawingrealizationsfromsomeunderlyingrandom\nprocess is called sampling . As you might have guessed, one natural estimator is the ratio\nof the number of observed headsto the total number of tosses.\nNow, suppose that the coin was in fact fair, i.e., \ud835\udc43\u00b9heads\u00ba=0.5. To simulate tosses of a\nfair coin, we can invoke any random number generator. There are some easy ways to draw\nsamples of an event with probability 0.5. For example Python\u2019s random.random yields\nnumbers in the interval \u00bb0,1\u00bcwhere the probability of lying in any sub-interval \u00bb\ud835\udc4e,\ud835\udc4f\u00bc\u001a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f0428e4-b33d-410f-ad9f-7361b3ebd78d": {"__data__": {"id_": "4f0428e4-b33d-410f-ad9f-7361b3ebd78d", "embedding": null, "metadata": {"page_label": "67", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "803921da-05a0-4faa-ba0f-829ad3c86c23", "node_type": "4", "metadata": {"page_label": "67", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7b0c1152567a18bc33e1e2d056013e9b6b32993e7b964a54d4f218e2455d0b88", "class_name": "RelatedNodeInfo"}}, "text": "67 Probability and Statistics\n\u00bb0,1\u00bcis equal to\ud835\udc4f\u0000\ud835\udc4e. Thus we can get out 0and1with probability 0.5each by testing\nwhether the returned float number is greater than 0.5:\nnum_tosses =100\nheads =sum([random .random() >0.5 for _inrange (num_tosses)])\ntails =num_tosses -heads\nprint (\"heads, tails: \", [heads, tails])\nheads, tails: [ 44,56]\nMore generally, we can simulate multiple draws from any variable with a finite number\nof possible outcomes (like the toss of a coin or roll of a die) by calling the multinomial\nfunction, setting the first argument to the number of draws and the second as a list of prob-\nabilitiesassociatedwitheachofthepossibleoutcomes. Tosimulatetentossesofafaircoin,\nweassignprobabilityvector [0.5, 0.5] ,interpretingindex0asheadsandindex1astails.\nThe function returns a vector with length equal to the number of possible outcomes (here,\n2), where the first component tells us the number of occurrences of heads and the second\ncomponent tells us the number of occurrences of tails.\nfair_probs =torch .tensor([ 0.5,0.5])\nMultinomial( 100, fair_probs) .sample()\ntensor([ 50.,50.])\nEach time you run this sampling process, you will receive a new random value that may\ndiffer from the previous outcome. Dividing by the number of tosses gives us the frequency\nofeachoutcomeinourdata. Notethatthesefrequencies,justliketheprobabilitiesthatthey\nare intended to estimate, sum to 1.\nMultinomial( 100, fair_probs) .sample() /100\ntensor([ 0.4800 ,0.5200 ])\nHere, even though our simulated coin is fair (we ourselves set the probabilities [0.5, 0.\n5]), the counts of heads and tails may not be identical. That is because we only drew a\nrelatively small number of samples. If we did not implement the simulation ourselves, and\nonlysawtheoutcome,howwouldweknowifthecoinwereslightlyunfairorifthepossible\ndeviation from 1\u009d2was just an artifact of the small sample size? Let\u2019s see what happens\nwhen we simulate 10,000 tosses.\ncounts =Multinomial( 10000 , fair_probs) .sample()\ncounts /10000", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0d298e3-4d87-4d4c-949d-783d564177eb": {"__data__": {"id_": "e0d298e3-4d87-4d4c-949d-783d564177eb", "embedding": null, "metadata": {"page_label": "68", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf91f90a-a791-4855-9d9c-a3a341d4ac7d", "node_type": "4", "metadata": {"page_label": "68", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6e32dcd183e5e1f48a0b988477e371c5df6670ad0868e6bcc34c0982da31cede", "class_name": "RelatedNodeInfo"}}, "text": "68 Preliminaries\ntensor([ 0.4966 ,0.5034 ])\nIn general, for averages of repeated events (like coin tosses), as the number of repetitions\ngrows, our estimates are guaranteed to converge to the true underlying probabilities. The\nmathematical formulation of this phenomenon is called the law of large numbers and the\ncentral limit theorem tells us that in many situations, as the sample size \ud835\udc5bgrows, these\nerrors should go down at a rate of \u00b91\u009dp\ud835\udc5b\u00ba. Let\u2019s get some more intuition by studying how\nour estimate evolves as we grow the number of tosses from 1 to 10,000.\ncounts =Multinomial( 1, fair_probs) .sample(( 10000 ,))\ncum_counts =counts .cumsum(dim =0)\nestimates =cum_counts /cum_counts .sum(dim =1, keepdims =True )\nestimates =estimates .numpy()\nd2l.set_figsize(( 4.5,3.5))\nd2l.plt.plot(estimates[:, 0], label =(\"P(coin=heads) \"))\nd2l.plt.plot(estimates[:, 1], label =(\"P(coin=tails) \"))\nd2l.plt.axhline(y =0.5, color ='black ', linestyle ='dashed ')\nd2l.plt.gca() .set_xlabel( 'Samples ')\nd2l.plt.gca() .set_ylabel( 'Estimated probability ')\nd2l.plt.legend();\nEach solid curve corresponds to one of the two values of the coin and gives our estimated\nprobability that the coin turns up that value after each group of experiments. The dashed\nblack line gives the true underlying probability. As we get more data by conducting more\nexperiments, the curves converge towards the true probability. You might already begin to\nsee the shape of some of the more advanced questions that preoccupy statisticians: How\nquickly does this convergence happen? If we had already tested many coins manufactured\nat the same plant, how might we incorporate this information?\n2.6.2A MoreFormalTreatment\nWe have already gotten pretty far: posing a probabilistic model, generating synthetic data,\nrunning a statistical estimator, empirically assessing convergence, and reporting error met-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bffa3e25-34bd-48b2-826e-eb11dd21685f": {"__data__": {"id_": "bffa3e25-34bd-48b2-826e-eb11dd21685f", "embedding": null, "metadata": {"page_label": "69", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e2f61ef-8c36-4930-a332-3d35c3b57010", "node_type": "4", "metadata": {"page_label": "69", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5f89acae03b1c25722b20c20e3221c39a1af56f81a204999a9415039f7bb095d", "class_name": "RelatedNodeInfo"}}, "text": "69 Probability and Statistics\nrics (checking the deviation). However, to go much further, we will need to be more pre-\ncise.\nWhen dealing with randomness, we denote the set of possible outcomes Sand call it the\nsample space oroutcome space . Here, each element is a distinct possible outcome . In\nthe case of rolling a single coin, S=fheads,tailsg. For a single die,S=f1,2,3,4,5,6g.\nWhenflippingtwocoins,possibleoutcomesare f\u00b9heads,heads\u00ba,\u00b9heads,tails\u00ba,\u00b9tails,heads\u00ba,\u00b9tails,tails\u00bag.\nEventsare subsets of the sample space. For instance, the event \u201cthe first coin toss comes\nup heads\u201d corresponds to the set f\u00b9heads,heads\u00ba,\u00b9heads,tails\u00bag. Whenever the outcome\n\ud835\udc67of a random experiment satisfies \ud835\udc672A, then eventAhas occurred. For a single roll\nof a die, we could define the events \u201cseeing a 5\u201d (A=f5g) and \u201cseeing an odd number\u201d\n(B=f1,3,5g). Inthiscase,ifthediecameup 5,wewouldsaythatboth AandBoccurred.\nOn the other hand, if \ud835\udc67=3, thenAdid not occur butBdid.\nAprobability function maps events onto real values \ud835\udc43:A\u0012S!\u00bb 0,1\u00bc. The probabil-\nity, denoted \ud835\udc43\u00b9A\u00ba, of an eventAin the given sample space S, has the following proper-\nties:\n\u000fThe probability of any event Ais a nonnegative real number, i.e., \ud835\udc43\u00b9A\u00ba\u0015 0;\n\u000fThe probability of the entire sample space is 1, i.e.,\ud835\udc43\u00b9S\u00ba=1;\n\u000fFor any countable sequence of events A1,A2,...that aremutuallyexclusive (i.e.,A\ud835\udc56\\\nA\ud835\udc57=;for all\ud835\udc56\u2260\ud835\udc57), the probability that any of them happens is equal to the sum of\ntheir individual probabilities, i.e., \ud835\udc43\u00b9\u00d01\n\ud835\udc56=1A\ud835\udc56\u00ba=\u00cd1\n\ud835\udc56=1\ud835\udc43\u00b9A\ud835\udc56\u00ba.\nThese axioms of probability theory, proposed by Kolmogorov ( 1933), can be applied to\nrapidly derive a number of important consequences. For instance, it follows immediately\nthattheprobabilityofanyevent AoritscomplementA0occurringis1(because A[A0=\nS). We can also prove that \ud835\udc43\u00b9;\u00ba=0because 1=\ud835\udc43\u00b9S[S0\u00ba=\ud835\udc43\u00b9S[;\u00ba =\ud835\udc43\u00b9S\u00ba\u00b8\ud835\udc43\u00b9;\u00ba=\n1\u00b8\ud835\udc43\u00b9;\u00ba. Consequently, the probability of any event Aandits complementA0occurring\nsimultaneouslyis \ud835\udc43\u00b9A\\A0\u00ba=0. Informally,thistellsusthatimpossibleeventshavezero\nprobability of occurring.\n2.6.3RandomVariables\nWhen we spoke about events like the roll of a die coming up odds or the first coin toss\ncoming up heads, we were invoking the idea of a random variable . Formally, random\nvariablesaremappingsfromanunderlyingsamplespacetoasetof(possiblymany)values.\nYoumightwonderhowarandomvariableisdifferentfromthesamplespace,sincebothare\ncollections of outcomes. Importantly, random variables can be much coarser than the raw\nsample space. We can define a binary random variable like \u201cgreater than 0.5\u201d even when\nthe underlying sample space is infinite, e.g., points on the line segment between 0and1.\nAdditionally, multiple random variables can share the same underlying sample space. For\nexample\u201cwhethermyhomealarmgoesoff\u201dand\u201cwhethermyhousewasburgled\u201dareboth\nbinaryrandomvariablesthatshareanunderlyingsamplespace. Consequently,knowingthe\nvalue taken by one random variable can tell us something about the likely value of another", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c3848f2-be86-48a8-b9f2-de60fb766da9": {"__data__": {"id_": "2c3848f2-be86-48a8-b9f2-de60fb766da9", "embedding": null, "metadata": {"page_label": "70", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25f44172-c490-46ae-be42-a56b3b190c37", "node_type": "4", "metadata": {"page_label": "70", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "37462e96eeb013e7c40470c5e8d1a55efde97fb6583b18b0b3e857d2d7a4434a", "class_name": "RelatedNodeInfo"}}, "text": "70 Preliminaries\nrandom variable. Knowing that the alarm went off, we might suspect that the house was\nlikely burgled.\nEvery value taken by a random variable corresponds to a subset of the underlying sample\nspace. Thus the occurrence where the random variable \ud835\udc4btakes value\ud835\udc63, denoted by \ud835\udc4b=\ud835\udc63,\nis aneventand\ud835\udc43\u00b9\ud835\udc4b=\ud835\udc63\u00badenotes its probability. Sometimes this notation can get clunky,\nand we can abuse notation when the context is clear. For example, we might use \ud835\udc43\u00b9\ud835\udc4b\u00bato\nrefer broadly to the distribution of\ud835\udc4b, i.e., the function that tells us the probability that \ud835\udc4b\ntakes any given value. Other times we write expressions like \ud835\udc43\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\ud835\udc43\u00b9\ud835\udc4b\u00ba\ud835\udc43\u00b9\ud835\udc4c\u00ba, as a\nshorthand to express a statement that is true for all of the values that the random variables\n\ud835\udc4band\ud835\udc4ccan take, i.e., for all \ud835\udc56,\ud835\udc57it holds that \ud835\udc43\u00b9\ud835\udc4b=\ud835\udc56and\ud835\udc4c=\ud835\udc57\u00ba=\ud835\udc43\u00b9\ud835\udc4b=\ud835\udc56\u00ba\ud835\udc43\u00b9\ud835\udc4c=\ud835\udc57\u00ba.\nOthertimes, weabusenotationbywriting \ud835\udc43\u00b9\ud835\udc63\u00bawhentherandomvariableisclearfromthe\ncontext. Since an event in probability theory is a set of outcomes from the sample space,\nwecanspecifyarangeofvaluesforarandomvariabletotake. Forexample, \ud835\udc43\u00b91\u0014\ud835\udc4b\u00143\u00ba\ndenotes the probability of the event f1\u0014\ud835\udc4b\u00143g.\nNote that there is a subtle difference between discrete random variables, like flips of a coin\nor tosses of a die, and continuous ones, like the weight and the height of a person sampled\nat random from the population. In this case we seldom really care about someone\u2019s exact\nheight. Moreover, if we took precise enough measurements, we would find that no two\npeople on the planet have the exact same height. In fact, with fine enough measurements,\nyou would never have the same height when you wake up and when you go to sleep. There\nislittlepointinaskingabouttheexactprobabilitythatsomeoneis1.801392782910287192\nmeters tall. Instead, we typically care more about being able to say whether someone\u2019s\nheightfallsintoagiveninterval, saybetween1.79and1.81meters. Inthesecaseswework\nwithprobability densities . Theheightofexactly1.80metershasnoprobability,butnonzero\ndensity. Toworkouttheprobabilityassignedtoaninterval, wemusttakean integralofthe\ndensity over that interval.\n2.6.4MultipleRandom Variables\nYoumighthavenoticedthatwecouldnotevenmakeitthroughtheprevioussectionwithout\nmakingstatementsinvolvinginteractionsamongmultiplerandomvariables(recall \ud835\udc43\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\n\ud835\udc43\u00b9\ud835\udc4b\u00ba\ud835\udc43\u00b9\ud835\udc4c\u00ba). Mostofmachinelearningisconcernedwithsuchrelationships. Here,thesam-\nple space would be the population of interest, say customers who transact with a business,\nphotographs on the Internet, or proteins known to biologists. Each random variable would\nrepresent the (unknown) value of a different attribute. Whenever we sample an individual\nfrom the population, we observe a realization of each of the random variables. Because\nthe values taken by random variables correspond to subsets of the sample space that could\nbe overlapping, partially overlapping, or entirely disjoint, knowing the value taken by one\nrandom variable can cause us to update our beliefs about which values of another random\nvariable are likely. If a patient walks into a hospital and we observe that they are having\ntrouble breathing and have lost their sense of smell, then we believe that they are more\nlikely to have COVID-19 than we might if they had no trouble breathing and a perfectly\nordinary sense of smell.\nWhen working with multiple random variables, we can construct events corresponding to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3334, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29a3574e-bef1-4177-8feb-021d55c608a2": {"__data__": {"id_": "29a3574e-bef1-4177-8feb-021d55c608a2", "embedding": null, "metadata": {"page_label": "71", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d28a70d-bf62-4b79-9a26-c305d73f9dcb", "node_type": "4", "metadata": {"page_label": "71", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7dfbbbfac5fd80600618e6331fe1e308ca905feb55c933391a64d19feea80f31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9f0aa0db-2a4f-4c48-80fc-150e8d50b22a", "node_type": "1", "metadata": {}, "hash": "d41b7d35e36df5bf9646fb9dced1f5b92c6930ca8e6e1903d3c7a49906b6f942", "class_name": "RelatedNodeInfo"}}, "text": "71 Probability and Statistics\nevery combination of values that the variables can jointly take. The probability function\nthatassignsprobabilitiestoeachofthesecombinations(e.g. \ud835\udc34=\ud835\udc4eand\ud835\udc35=\ud835\udc4f)iscalledthe\njoint probability function and simply returns the probability assigned to the intersection\nof the corresponding subsets of the sample space. The joint probability assigned to the\nevent where random variables \ud835\udc34and\ud835\udc35take values\ud835\udc4eand\ud835\udc4f, respectively, is denoted \ud835\udc43\u00b9\ud835\udc34=\n\ud835\udc4e,\ud835\udc35=\ud835\udc4f\u00ba, where the comma indicates \u201cand\u201d. Note that for any values \ud835\udc4eand\ud835\udc4f, it follows\nthat\n\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e,\ud835\udc35=\ud835\udc4f\u00ba\u0014\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e\u00baand\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e,\ud835\udc35=\ud835\udc4f\u00ba\u0014\ud835\udc43\u00b9\ud835\udc35=\ud835\udc4f\u00ba, (2.6.1)\nsince for\ud835\udc34=\ud835\udc4eand\ud835\udc35=\ud835\udc4fto happen,\ud835\udc34=\ud835\udc4ehas to happen and\ud835\udc35=\ud835\udc4falso has to\nhappen. Interestingly,thejointprobabilitytellsusallthatwecanknowabouttheserandom\nvariables in a probabilistic sense, and can be used to derive many other useful quantities,\nincluding recovering the individual distributions \ud835\udc43\u00b9\ud835\udc34\u00baand\ud835\udc43\u00b9\ud835\udc35\u00ba. To recover \ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e\u00ba\nwe simply sum up \ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e,\ud835\udc35=\ud835\udc63\u00baover all values \ud835\udc63that the random variable \ud835\udc35can take:\n\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e\u00ba=\u00cd\n\ud835\udc63\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e,\ud835\udc35=\ud835\udc63\u00ba.\nThe ratio\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e,\ud835\udc35=\ud835\udc4f\u00ba\n\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e\u00ba\u00141turns out to be extremely important. It is called the conditional\nprobability , and is denoted via the \u201c j\u201d symbol:\n\ud835\udc43\u00b9\ud835\udc35=\ud835\udc4fj\ud835\udc34=\ud835\udc4e\u00ba=\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e,\ud835\udc35=\ud835\udc4f\u00ba\u009d\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e\u00ba. (2.6.2)\nIt tells us the new probability associated with the event \ud835\udc35=\ud835\udc4f, once we condition on the\nfact\ud835\udc34=\ud835\udc4etook place. We can think of this conditional probability as restricting attention\nonlytothesubsetofthesamplespaceassociatedwith \ud835\udc34=\ud835\udc4eandthenrenormalizingsothat\nall probabilities sum to 1. Conditional probabilities are in fact just ordinary probabilities\nand thus respect all of the axioms, as long as we condition all terms on the same event and\nthus restrict attention to the same sample space. For instance, for disjoint events BandB0,\nwe have that \ud835\udc43\u00b9B[B0j\ud835\udc34=\ud835\udc4e\u00ba=\ud835\udc43\u00b9Bj\ud835\udc34=\ud835\udc4e\u00ba\u00b8\ud835\udc43\u00b9B0j\ud835\udc34=\ud835\udc4e\u00ba.\nUsing the definition of conditional probabilities, we can derive the famous result called\nBayes\u2019 theorem . By construction, we have that \ud835\udc43\u00b9\ud835\udc34,\ud835\udc35\u00ba=\ud835\udc43\u00b9\ud835\udc35j\ud835\udc34\u00ba\ud835\udc43\u00b9\ud835\udc34\u00baand\ud835\udc43\u00b9\ud835\udc34,\ud835\udc35\u00ba=\n\ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00ba\ud835\udc43\u00b9\ud835\udc35\u00ba. Combining both equations yields \ud835\udc43\u00b9\ud835\udc35j\ud835\udc34\u00ba\ud835\udc43\u00b9\ud835\udc34\u00ba=\ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00ba\ud835\udc43\u00b9\ud835\udc35\u00baand\nhence\n\ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00ba=\ud835\udc43\u00b9\ud835\udc35j\ud835\udc34\u00ba\ud835\udc43\u00b9\ud835\udc34\u00ba\n\ud835\udc43\u00b9\ud835\udc35\u00ba. (2.6.3)\nThis simple equation has profound implications because it allows us to reverse the order of\nconditioning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f0aa0db-2a4f-4c48-80fc-150e8d50b22a": {"__data__": {"id_": "9f0aa0db-2a4f-4c48-80fc-150e8d50b22a", "embedding": null, "metadata": {"page_label": "71", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d28a70d-bf62-4b79-9a26-c305d73f9dcb", "node_type": "4", "metadata": {"page_label": "71", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7dfbbbfac5fd80600618e6331fe1e308ca905feb55c933391a64d19feea80f31", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "29a3574e-bef1-4177-8feb-021d55c608a2", "node_type": "1", "metadata": {"page_label": "71", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "620091fb1e6a5409bb5e42dd88b261ff0415637cd7b9ac407f4f30b0566bf7a7", "class_name": "RelatedNodeInfo"}}, "text": "By construction, we have that \ud835\udc43\u00b9\ud835\udc34,\ud835\udc35\u00ba=\ud835\udc43\u00b9\ud835\udc35j\ud835\udc34\u00ba\ud835\udc43\u00b9\ud835\udc34\u00baand\ud835\udc43\u00b9\ud835\udc34,\ud835\udc35\u00ba=\n\ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00ba\ud835\udc43\u00b9\ud835\udc35\u00ba. Combining both equations yields \ud835\udc43\u00b9\ud835\udc35j\ud835\udc34\u00ba\ud835\udc43\u00b9\ud835\udc34\u00ba=\ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00ba\ud835\udc43\u00b9\ud835\udc35\u00baand\nhence\n\ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00ba=\ud835\udc43\u00b9\ud835\udc35j\ud835\udc34\u00ba\ud835\udc43\u00b9\ud835\udc34\u00ba\n\ud835\udc43\u00b9\ud835\udc35\u00ba. (2.6.3)\nThis simple equation has profound implications because it allows us to reverse the order of\nconditioning. Ifweknowhowtoestimate \ud835\udc43\u00b9\ud835\udc35j\ud835\udc34\u00ba,\ud835\udc43\u00b9\ud835\udc34\u00ba,and\ud835\udc43\u00b9\ud835\udc35\u00ba,thenwecanestimate\n\ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00ba. We often find it easier to estimate one term directly but not the other and Bayes\u2019\ntheoremcancometotherescuehere. Forinstance,ifweknowtheprevalenceofsymptoms\nfor a given disease, and the overall prevalences of the disease and symptoms, respectively,\nwe can determine how likely someone is to have the disease based on their symptoms. In\nsome cases we might not have direct access to \ud835\udc43\u00b9\ud835\udc35\u00ba, such as the prevalence of symptoms.\nIn this case a simplified version of Bayes\u2019 theorem comes in handy:\n\ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00ba/\ud835\udc43\u00b9\ud835\udc35j\ud835\udc34\u00ba\ud835\udc43\u00b9\ud835\udc34\u00ba. (2.6.4)", "mimetype": "text/plain", "start_char_idx": 1878, "end_char_idx": 2736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51117917-f541-4cc7-8ecd-4a4e0657e508": {"__data__": {"id_": "51117917-f541-4cc7-8ecd-4a4e0657e508", "embedding": null, "metadata": {"page_label": "72", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cac64a08-aa40-4215-b4b4-d4913d584a9c", "node_type": "4", "metadata": {"page_label": "72", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "61b865a5c542051bc76e365fb0d591852b46de7c950da5f1221a8ab65bf5abea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8ba4b377-fb2e-467f-acf2-01cf80f810f8", "node_type": "1", "metadata": {}, "hash": "3a368a8b677fec63cead16b3242c9ff7e4da4f6ffdde75bd7bbf2bcebbd26a3a", "class_name": "RelatedNodeInfo"}}, "text": "72 Preliminaries\nSince we know that \ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00bamust be normalized to 1, i.e.,\u00cd\n\ud835\udc4e\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4ej\ud835\udc35\u00ba=1, we can\nuse it to compute\n\ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00ba=\ud835\udc43\u00b9\ud835\udc35j\ud835\udc34\u00ba\ud835\udc43\u00b9\ud835\udc34\u00ba\u00cd\n\ud835\udc4e\ud835\udc43\u00b9\ud835\udc35j\ud835\udc34=\ud835\udc4e\u00ba\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e\u00ba. (2.6.5)\nIn Bayesian statistics, we think of an observer as possessing some (subjective) prior be-\nliefs about the plausibility of the available hypotheses encoded in the prior\ud835\udc43\u00b9\ud835\udc3b\u00ba, and a\nlikelihood function that says how likely one is to observe any value of the collected evi-\ndence for each of the hypotheses in the class \ud835\udc43\u00b9\ud835\udc38j\ud835\udc3b\u00ba. Bayes\u2019 theorem is then interpreted\nas telling us how to update the initial prior\ud835\udc43\u00b9\ud835\udc3b\u00bain light of the available evidence \ud835\udc38to\nproduceposterior beliefs\ud835\udc43\u00b9\ud835\udc3bj\ud835\udc38\u00ba=\ud835\udc43\u00b9\ud835\udc38j\ud835\udc3b\u00ba\ud835\udc43\u00b9\ud835\udc3b\u00ba\n\ud835\udc43\u00b9\ud835\udc38\u00ba. Informally, this can be stated as \u201cpos-\nterior equals prior times likelihood, divided by the evidence\u201d. Now, because the evidence\n\ud835\udc43\u00b9\ud835\udc38\u00bais the same for all hypotheses, we can get away with simply normalizing over the\nhypotheses.\nNote that\u00cd\n\ud835\udc4e\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4ej\ud835\udc35\u00ba=1also allows us to marginalize over random variables.\nThat is, we can drop variables from a joint distribution such as \ud835\udc43\u00b9\ud835\udc34,\ud835\udc35\u00ba. After all, we have\nthat\n\u00d5\n\ud835\udc4e\ud835\udc43\u00b9\ud835\udc35j\ud835\udc34=\ud835\udc4e\u00ba\ud835\udc43\u00b9\ud835\udc34=\ud835\udc4e\u00ba=\u00d5\n\ud835\udc4e\ud835\udc43\u00b9\ud835\udc35,\ud835\udc34=\ud835\udc4e\u00ba=\ud835\udc43\u00b9\ud835\udc35\u00ba.(2.6.6)\nIndependenceisanotherfundamentallyimportantconceptthatformsthebackboneofmany\nimportant ideas in statistics. In short, two variables are independent if conditioning on the\nvalue of\ud835\udc34does not cause any change to the probability distribution associated with \ud835\udc35and\nvice versa. More formally, independence, denoted \ud835\udc34?\ud835\udc35, requires that \ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00ba=\ud835\udc43\u00b9\ud835\udc34\u00ba\nand, consequently, that \ud835\udc43\u00b9\ud835\udc34,\ud835\udc35\u00ba=\ud835\udc43\u00b9\ud835\udc34j\ud835\udc35\u00ba\ud835\udc43\u00b9\ud835\udc35\u00ba=\ud835\udc43\u00b9\ud835\udc34\u00ba\ud835\udc43\u00b9\ud835\udc35\u00ba. Independence is often\nan appropriate assumption. For example, if the random variable \ud835\udc34represents the outcome\nfrom tossing one fair coin and the random variable \ud835\udc35represents the outcome from tossing\nanother, then knowing whether \ud835\udc34came up heads should not influence the probability of \ud835\udc35\ncoming up heads.\nIndependence is especially useful when it holds among the successive draws of our data\nfrom some underlying distribution (allowing us to make strong statistical conclusions) or\nwhenitholdsamongvariousvariablesinourdata,allowingustoworkwithsimplermodels\nthat encode this independence structure. On the other hand, estimating the dependencies\namongrandomvariablesisoftentheveryaimoflearning. Wecaretoestimatetheprobabil-\nity of disease given symptoms specifically because we believe that diseases and symptoms\narenotindependent.\nNote that because conditional probabilities are proper probabilities, the concepts of inde-\npendenceanddependencealsoapplytothem. Tworandomvariables \ud835\udc34and\ud835\udc35arecondition-\nally independent given a third variable \ud835\udc36if and only if \ud835\udc43\u00b9\ud835\udc34,\ud835\udc35j\ud835\udc36\u00ba=\ud835\udc43\u00b9\ud835\udc34j\ud835\udc36\u00ba\ud835\udc43\u00b9\ud835\udc35j\ud835\udc36\u00ba.\nInterestingly, two variables can be independent in general but become dependent when\nconditioning on a third.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2668, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ba4b377-fb2e-467f-acf2-01cf80f810f8": {"__data__": {"id_": "8ba4b377-fb2e-467f-acf2-01cf80f810f8", "embedding": null, "metadata": {"page_label": "72", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cac64a08-aa40-4215-b4b4-d4913d584a9c", "node_type": "4", "metadata": {"page_label": "72", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "61b865a5c542051bc76e365fb0d591852b46de7c950da5f1221a8ab65bf5abea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51117917-f541-4cc7-8ecd-4a4e0657e508", "node_type": "1", "metadata": {"page_label": "72", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5381a035539638c8d7935f63ca30f4630ffe2afcdf38930ac88d9ef59dd674d9", "class_name": "RelatedNodeInfo"}}, "text": "On the other hand, estimating the dependencies\namongrandomvariablesisoftentheveryaimoflearning. Wecaretoestimatetheprobabil-\nity of disease given symptoms specifically because we believe that diseases and symptoms\narenotindependent.\nNote that because conditional probabilities are proper probabilities, the concepts of inde-\npendenceanddependencealsoapplytothem. Tworandomvariables \ud835\udc34and\ud835\udc35arecondition-\nally independent given a third variable \ud835\udc36if and only if \ud835\udc43\u00b9\ud835\udc34,\ud835\udc35j\ud835\udc36\u00ba=\ud835\udc43\u00b9\ud835\udc34j\ud835\udc36\u00ba\ud835\udc43\u00b9\ud835\udc35j\ud835\udc36\u00ba.\nInterestingly, two variables can be independent in general but become dependent when\nconditioning on a third. This often occurs when the two random variables \ud835\udc34and\ud835\udc35cor-\nrespond to causes of some third variable \ud835\udc36. For example, broken bones and lung cancer\nmight be independent in the general population but if we condition on being in the hospital\nthen we might find that broken bones are negatively correlated with lung cancer. That is", "mimetype": "text/plain", "start_char_idx": 2079, "end_char_idx": 2993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6edaedb-a003-4b8d-bab2-a55c8ac57c39": {"__data__": {"id_": "e6edaedb-a003-4b8d-bab2-a55c8ac57c39", "embedding": null, "metadata": {"page_label": "73", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97a80c91-b707-48ee-ab1e-5d5278ecc716", "node_type": "4", "metadata": {"page_label": "73", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0d44af07b2498fd03d6598b87fc830bb0b90d76e3e0541b2e47b2f159c307fcf", "class_name": "RelatedNodeInfo"}}, "text": "73 Probability and Statistics\nbecause the broken bone explainsaway why some person is in the hospital and thus lowers\nthe probability that they are hospitalized because of having lung cancer.\nAndconversely,twodependentrandomvariablescanbecomeindependentuponcondition-\ning on a third. This often happens when two otherwise unrelated events have a common\ncause. Shoesizeandreadinglevelarehighlycorrelatedamongelementaryschoolstudents,\nbut this correlation disappears if we condition on age.\n2.6.5An Example\nLet\u2019s put our skills to the test. Assume that a doctor administers an HIV test to a patient.\nThis test is fairly accurate and fails only with 1% probability if the patient is healthy but\nreported as diseased, i.e., healthy patients test positive in 1% of cases. Moreover, it never\nfailstodetectHIVifthepatientactuallyhasit. Weuse \ud835\udc3712f0,1gtoindicatethediagnosis\n(0if negative and 1if positive) and \ud835\udc3b2f0,1gto denote the HIV status.\nConditional probability \ud835\udc3b=1\ud835\udc3b=0\n\ud835\udc43\u00b9\ud835\udc371=1j\ud835\udc3b\u00ba 1 0.01\n\ud835\udc43\u00b9\ud835\udc371=0j\ud835\udc3b\u00ba 0 0.99\nNote that the column sums are all 1 (but the row sums do not), since they are conditional\nprobabilities. Let\u2019s compute the probability of the patient having HIV if the test comes\nback positive, i.e., \ud835\udc43\u00b9\ud835\udc3b=1j\ud835\udc371=1\u00ba. Intuitively this is going to depend on how common\nthe disease is, since it affects the number of false alarms. Assume that the population is\nfairly free of the disease, e.g., \ud835\udc43\u00b9\ud835\udc3b=1\u00ba=0.0015. To apply Bayes\u2019 theorem, we need to\napply marginalization to determine\n\ud835\udc43\u00b9\ud835\udc371=1\u00ba=\ud835\udc43\u00b9\ud835\udc371=1,\ud835\udc3b=0\u00ba\u00b8\ud835\udc43\u00b9\ud835\udc371=1,\ud835\udc3b=1\u00ba\n=\ud835\udc43\u00b9\ud835\udc371=1j\ud835\udc3b=0\u00ba\ud835\udc43\u00b9\ud835\udc3b=0\u00ba\u00b8\ud835\udc43\u00b9\ud835\udc371=1j\ud835\udc3b=1\u00ba\ud835\udc43\u00b9\ud835\udc3b=1\u00ba\n=0.011485.(2.6.7)\nThis leads us to\n\ud835\udc43\u00b9\ud835\udc3b=1j\ud835\udc371=1\u00ba=\ud835\udc43\u00b9\ud835\udc371=1j\ud835\udc3b=1\u00ba\ud835\udc43\u00b9\ud835\udc3b=1\u00ba\n\ud835\udc43\u00b9\ud835\udc371=1\u00ba=0.1306. (2.6.8)\nIn other words, there is only a 13.06% chance that the patient actually has HIV, despite the\ntestbeingprettyaccurate. Aswecansee,probabilitycanbecounterintuitive. Whatshoulda\npatientdouponreceivingsuchterrifyingnews? Likely,thepatientwouldaskthephysician\nto administer another test to get clarity. The second test has different characteristics and it\nis not as good as the first one.\nConditional probability \ud835\udc3b=1\ud835\udc3b=0\n\ud835\udc43\u00b9\ud835\udc372=1j\ud835\udc3b\u00ba 0.98 0.03\n\ud835\udc43\u00b9\ud835\udc372=0j\ud835\udc3b\u00ba 0.02 0.97", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9a05212-276c-47b7-bffe-24e43ae969c2": {"__data__": {"id_": "a9a05212-276c-47b7-bffe-24e43ae969c2", "embedding": null, "metadata": {"page_label": "74", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69ede226-5ed6-4900-8d9f-28e503e72a09", "node_type": "4", "metadata": {"page_label": "74", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "65c126ea5e81b0163feef1054c13a9fafd232f2c010257d13973d6dfdf21bc1a", "class_name": "RelatedNodeInfo"}}, "text": "74 Preliminaries\nUnfortunately, the second test comes back positive, too. Let\u2019s calculate the requisite prob-\nabilities to invoke Bayes\u2019 theorem by assuming conditional independence:\n\ud835\udc43\u00b9\ud835\udc371=1,\ud835\udc372=1j\ud835\udc3b=0\u00ba=\ud835\udc43\u00b9\ud835\udc371=1j\ud835\udc3b=0\u00ba\ud835\udc43\u00b9\ud835\udc372=1j\ud835\udc3b=0\u00ba=0.0003,\n\ud835\udc43\u00b9\ud835\udc371=1,\ud835\udc372=1j\ud835\udc3b=1\u00ba=\ud835\udc43\u00b9\ud835\udc371=1j\ud835\udc3b=1\u00ba\ud835\udc43\u00b9\ud835\udc372=1j\ud835\udc3b=1\u00ba= 0.98.\n(2.6.9)\nNow we can apply marginalization to obtain the probability that both tests come back pos-\nitive:\n\ud835\udc43\u00b9\ud835\udc371=1,\ud835\udc372=1\u00ba\n=\ud835\udc43\u00b9\ud835\udc371=1,\ud835\udc372=1,\ud835\udc3b=0\u00ba\u00b8\ud835\udc43\u00b9\ud835\udc371=1,\ud835\udc372=1,\ud835\udc3b=1\u00ba\n=\ud835\udc43\u00b9\ud835\udc371=1,\ud835\udc372=1j\ud835\udc3b=0\u00ba\ud835\udc43\u00b9\ud835\udc3b=0\u00ba\u00b8\ud835\udc43\u00b9\ud835\udc371=1,\ud835\udc372=1j\ud835\udc3b=1\u00ba\ud835\udc43\u00b9\ud835\udc3b=1\u00ba\n=0.00176955.\n(2.6.10)\nFinally,theprobabilityofthepatienthavingHIVgiventhatbothtestsarepositiveis\n\ud835\udc43\u00b9\ud835\udc3b=1j\ud835\udc371=1,\ud835\udc372=1\u00ba=\ud835\udc43\u00b9\ud835\udc371=1,\ud835\udc372=1j\ud835\udc3b=1\u00ba\ud835\udc43\u00b9\ud835\udc3b=1\u00ba\n\ud835\udc43\u00b9\ud835\udc371=1,\ud835\udc372=1\u00ba=0.8307.(2.6.11)\nThat is, the second test allowed us to gain much higher confidence that not all is well. De-\nspite the second test being considerably less accurate than the first one, it still significantly\nimproved our estimate. The assumption of both tests being conditionally independent of\neach other was crucial for our ability to generate a more accurate estimate. Take the ex-\ntreme case where we run the same test twice. In this situation we would expect the same\noutcomebothtimes,hencenoadditionalinsightisgainedfromrunningthesametestagain.\nThe astute reader might have noticed that the diagnosis behaved like a classifier hiding in\nplain sight where our ability to decide whether a patient is healthy increases as we obtain\nmore features (test outcomes).\n2.6.6Expectations\nOften, making decisions requires not just looking at the probabilities assigned to individ-\nual events but composing them together into useful aggregates that can provide us with\nguidance. For example, when random variables take continuous scalar values, we often\ncare about knowing what value to expect on average . This quantity is formally called an\nexpectation . If we are making investments, the first quantity of interest might be the return\nwe can expect, averaging over all the possible outcomes (and weighting by the appropri-\nate probabilities). For instance, say that with 50% probability, an investment might fail\naltogether, with 40% probability it might provide a 2 \u0002return, and with 10% probability\nit might provide a 10 \u0002return 10\u0002. To calculate the expected return, we sum over all re-\nturns, multiplying each by the probability that they will occur. This yields the expectation\n0.5\u00010\u00b80.4\u00012\u00b80.1\u000110=1.8. Hence the expected return is 1.8 \u0002.\nIn general, the expectation (or average) of the random variable \ud835\udc4bis defined as\n\ud835\udc38\u00bb\ud835\udc4b\u00bc=\ud835\udc38\ud835\udc65\u0018\ud835\udc43\u00bb\ud835\udc65\u00bc=\u00d5\n\ud835\udc65\ud835\udc65\ud835\udc43\u00b9\ud835\udc4b=\ud835\udc65\u00ba.(2.6.12)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2478, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "969b1d9c-a631-40ec-b3a1-ce660e929dee": {"__data__": {"id_": "969b1d9c-a631-40ec-b3a1-ce660e929dee", "embedding": null, "metadata": {"page_label": "75", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "848d32f0-ee97-43bf-aabd-06716aa69d77", "node_type": "4", "metadata": {"page_label": "75", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "feaab0f300f0da5eaffa61d4f400a4cd0d67e11b56d4996199054ba4a5cea8b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8666de96-70ca-4cfd-8440-d0f5c0a7f153", "node_type": "1", "metadata": {}, "hash": "cac9cb876badd78bdd5ec59a0270761330c8c90a0db2799dde7d34b68f863266", "class_name": "RelatedNodeInfo"}}, "text": "75 Probability and Statistics\nLikewise, for densities we obtain \ud835\udc38\u00bb\ud835\udc4b\u00bc=\u00af\n\ud835\udc65 \ud835\udc51\ud835\udc5d\u00b9\ud835\udc65\u00ba. Sometimes we are interested in the\nexpected value of some function of \ud835\udc65. We can calculate these expectations as\n\ud835\udc38\ud835\udc65\u0018\ud835\udc43\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba\u00bc=\u00d5\n\ud835\udc65\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc43\u00b9\ud835\udc65\u00baand\ud835\udc38\ud835\udc65\u0018\ud835\udc43\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba\u00bc=\u00b9\n\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc5d\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65 (2.6.13)\nfor discrete probabilities and densities, respectively. Returning to the investment exam-\nple from above, \ud835\udc53might be the utility(happiness) associated with the return. Behavior\neconomists have long noted that people associate greater disutility with losing money than\nthe utility gained from earning one dollar relative to their baseline. Moreover, the value\nof money tends to be sub-linear. Possessing 100k dollars versus zero dollars can make the\ndifferencebetweenpayingtherent, eatingwell, andenjoyingqualityhealthcareversussuf-\nfering through homelessness. On the other hand, the gains due to possessing 200k versus\n100k are less dramatic. Reasoning like this motivates the clich\u00e9 that \u201cthe utility of money\nis logarithmic\u201d.\nIftheutilityassociatedwithatotallosswere \u00001, andtheutilitiesassociatedwithreturnsof\n1,2, and 10were 1,2and4, respectively, then the expected happiness of investing would\nbe0.5\u0001\u00b9\u00001\u00ba\u00b80.4\u00012\u00b80.1\u00014=0.7(an expected loss of utility of 30%). If indeed this were\nyour utility function, you might be best off keeping the money in the bank.\nForfinancialdecisions,wemightalsowanttomeasurehow riskyaninvestmentis. Here,we\ncare not just about the expected value but how much the actual values tend to varyrelative\nto this value. Note that we cannot just take the expectation of the difference between the\nactual and expected values. This is because the expectation of a difference is the difference\nof the expectations, i.e., \ud835\udc38\u00bb\ud835\udc4b\u0000\ud835\udc38\u00bb\ud835\udc4b\u00bc\u00bc=\ud835\udc38\u00bb\ud835\udc4b\u00bc\u0000\ud835\udc38\u00bb\ud835\udc38\u00bb\ud835\udc4b\u00bc\u00bc=0. However, we can look at\nthe expectation of any non-negative function of this difference. The variance of a random\nvariable is calculated by looking at the expected value of the squared differences:\nVar\u00bb\ud835\udc4b\u00bc=\ud835\udc38\u0002\n\u00b9\ud835\udc4b\u0000\ud835\udc38\u00bb\ud835\udc4b\u00bc\u00ba2\u0003\n=\ud835\udc38\u00bb\ud835\udc4b2\u00bc\u0000\ud835\udc38\u00bb\ud835\udc4b\u00bc2. (2.6.14)\nHere the equality follows by expanding \u00b9\ud835\udc4b\u0000\ud835\udc38\u00bb\ud835\udc4b\u00bc\u00ba2=\ud835\udc4b2\u00002\ud835\udc4b\ud835\udc38\u00bb\ud835\udc4b\u00bc\u00b8\ud835\udc38\u00bb\ud835\udc4b\u00bc2and taking\nexpectationsforeachterm. Thesquarerootofthevarianceisanotherusefulquantitycalled\nthestandarddeviation . Whilethisandthevarianceconveythesameinformation(eithercan\nbecalculatedfromtheother),thestandarddeviationhasthenicepropertythatitisexpressed\nin the same units as the original quantity represented by the random variable.\nLastly, the variance of a function of a random variable is defined analogously as\nVar\ud835\udc65\u0018\ud835\udc43\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba\u00bc=\ud835\udc38\ud835\udc65\u0018\ud835\udc43\u00bb\ud835\udc532\u00b9\ud835\udc65\u00ba\u00bc\u0000\ud835\udc38\ud835\udc65\u0018\ud835\udc43\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba\u00bc2. (2.6.15)\nReturning to our investment example, we can now compute the variance of the investment.\nIt is given by 0.5\u00010\u00b80.4\u000122\u00b80.1\u0001102\u00001.82=8.36. For all intents and purposes this\nis a risky investment. Note that by mathematical convention mean and variance are often\nreferenced as \ud835\udf07and\ud835\udf0e2. This is particularly the case whenever we use it to parametrize a\nGaussian distribution.\nIn the same way as we introduced expectations and variance for scalarrandom variables,\nwe can do so for vector-valued ones.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2976, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8666de96-70ca-4cfd-8440-d0f5c0a7f153": {"__data__": {"id_": "8666de96-70ca-4cfd-8440-d0f5c0a7f153", "embedding": null, "metadata": {"page_label": "75", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "848d32f0-ee97-43bf-aabd-06716aa69d77", "node_type": "4", "metadata": {"page_label": "75", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "feaab0f300f0da5eaffa61d4f400a4cd0d67e11b56d4996199054ba4a5cea8b1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "969b1d9c-a631-40ec-b3a1-ce660e929dee", "node_type": "1", "metadata": {"page_label": "75", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "539aab6d66d34fa844d04947de3a7608f54a0a391a099d744d00075ab8331ede", "class_name": "RelatedNodeInfo"}}, "text": "(2.6.15)\nReturning to our investment example, we can now compute the variance of the investment.\nIt is given by 0.5\u00010\u00b80.4\u000122\u00b80.1\u0001102\u00001.82=8.36. For all intents and purposes this\nis a risky investment. Note that by mathematical convention mean and variance are often\nreferenced as \ud835\udf07and\ud835\udf0e2. This is particularly the case whenever we use it to parametrize a\nGaussian distribution.\nIn the same way as we introduced expectations and variance for scalarrandom variables,\nwe can do so for vector-valued ones. Expectations are easy, since we can apply them el-\nementwise. For instance, \ud835\udf41def=\ud835\udc38x\u0018\ud835\udc43\u00bbx\u00bchas coordinates \ud835\udf07\ud835\udc56=\ud835\udc38x\u0018\ud835\udc43\u00bb\ud835\udc65\ud835\udc56\u00bc.Covariances", "mimetype": "text/plain", "start_char_idx": 2476, "end_char_idx": 3104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06f2257d-a580-4a11-a926-8e5f7ec6a48e": {"__data__": {"id_": "06f2257d-a580-4a11-a926-8e5f7ec6a48e", "embedding": null, "metadata": {"page_label": "76", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a8aeaa5-ec81-4fa6-b412-56143a889c20", "node_type": "4", "metadata": {"page_label": "76", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "95298a9a6508e64ff8302d9cd89b0e57db5a2d2621b7cf4f7d52e74199e20ea4", "class_name": "RelatedNodeInfo"}}, "text": "76 Preliminaries\n59are more complicated. We define them by taking expectations of the outer product of the\ndifference between random variables and their mean:\n\ud835\udebadef=Covx\u0018\ud835\udc43\u00bbx\u00bc=\ud835\udc38x\u0018\ud835\udc43\u0002\n\u00b9x\u0000\ud835\udf41\u00ba\u00b9x\u0000\ud835\udf41\u00ba>\u0003\n. (2.6.16)\nThis matrix \ud835\udebais referred to as the covariance matrix. An easy way to see its effect is to\nconsider some vector vof the same size as x. It follows that\nv>\ud835\udebav=\ud835\udc38x\u0018\ud835\udc43\u0002\nv>\u00b9x\u0000\ud835\udf41\u00ba\u00b9x\u0000\ud835\udf41\u00ba>v\u0003\n=Var\ud835\udc65\u0018\ud835\udc43\u00bbv>x\u00bc. (2.6.17)\nAs such, \ud835\udebaallows us to compute the variance for any linear function of xby a simple\nmatrix multiplication. The off-diagonal elements tell us how correlated the coordinates\nare: a value of 0 means no correlation, where a larger positive value means that they are\nmore strongly correlated.\n2.6.7Discussion\nIn machine learning, there are many things to be uncertain about! We can be uncertain\nabout the value of a label given an input. We can be uncertain about the estimated value of\na parameter. We can even be uncertain about whether data arriving at deployment is even\nfrom the same distribution as the training data.\nByaleatoric uncertainty , we mean uncertainty that is intrinsic to the problem, and due to\ngenuine randomness unaccounted for by the observed variables. By epistemicuncertainty ,\nwe mean uncertainty over a model\u2019s parameters, the sort of uncertainty that we can hope\nto reduce by collecting more data. We might have epistemic uncertainty concerning the\nprobability that a coin turns up heads, but even once we know this probability, we are left\nwith aleatoric uncertainty about the outcome of any future toss. No matter how long we\nwatch someone tossing a fair coin, we will never be more or less than 50% certain that\nthe next toss will come up heads. These terms come from mechanical modeling, (see e.g.,\nDer Kiureghian and Ditlevsen ( 2009) for a review on this aspect of uncertainty quantifica-\ntion59). It is worth noting, however, that these terms constitute a slight abuse of language.\nThe term epistemic refers to anything concerning knowledge and thus, in the philosophical\nsense, all uncertainty is epistemic.\nWesawthatsamplingdatafromsomeunknownprobabilitydistributioncanprovideuswith\ninformation that can be used to estimate the parameters of the data generating distribution.\nThat said, the rate at which this is possible can be quite slow. In our coin tossing example\n(and many others) we can do no better than to design estimators that converge at a rate of\n1\u009dp\ud835\udc5b, where\ud835\udc5bis the sample size (e.g., the number of tosses). This means that by going\nfrom10to1000observations(usuallyaveryachievabletask)weseeatenfoldreductionof\nuncertainty, whereas the next 1000 observations help comparatively little, offering only a\n1.41timesreduction. Thisisapersistentfeatureofmachinelearning: whilethereareoften\neasy gains, it takes a very large amount of data, and often with it an enormous amount of\ncomputation, to make further gains. For an empirical review of this fact for large scale\nlanguage models see Revels etal.(2016).\nWe also sharpened our language and tools for statistical modeling. In the process of that", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3041, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eac017ef-b37f-400a-b548-37df4aff5052": {"__data__": {"id_": "eac017ef-b37f-400a-b548-37df4aff5052", "embedding": null, "metadata": {"page_label": "77", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9552ac9c-417b-4475-bb3e-88ab9a6ad5bd", "node_type": "4", "metadata": {"page_label": "77", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f1b6d7a783d439c7fe12ec1880210d1a712d64cf52eed228a90928654d44dce2", "class_name": "RelatedNodeInfo"}}, "text": "77 Probability and Statistics\n60\n61\n62we learned about conditional probabilities and about one of the most important equations\nin statistics\u2014Bayes\u2019 theorem. It is an effective tool for decoupling information conveyed\nby data through a likelihood term \ud835\udc43\u00b9\ud835\udc35j\ud835\udc34\u00bathat addresses how well observations \ud835\udc35match\na choice of parameters \ud835\udc34, and a prior probability \ud835\udc43\u00b9\ud835\udc34\u00bawhich governs how plausible a par-\nticular choice of \ud835\udc34was in the first place. In particular, we saw how this rule can be applied\nto assign probabilities to diagnoses, based on the efficacy of the test andthe prevalence of\nthe disease itself (i.e., our prior).\nLastly, we introduced a first set of nontrivial questions about the effect of a specific proba-\nbilitydistribution,namelyexpectationsandvariances. Whiletherearemanymorethanjust\nlinear and quadratic expectations for a probability distribution, these two already provide\na good deal of knowledge about the possible behavior of the distribution. For instance,\nChebyshev\u2019s inequality60states that\ud835\udc43\u00b9j\ud835\udc4b\u0000\ud835\udf07j\u0015\ud835\udc58\ud835\udf0e\u00ba\u0014 1\u009d\ud835\udc582, where\ud835\udf07is the expecta-\ntion,\ud835\udf0e2is the variance of the distribution, and \ud835\udc58 > 1is a confidence parameter of our\nchoosing. It tells us that draws from a distribution lie with at least 50% probability within\na\u00bb\u0000p\n2\ud835\udf0e,p\n2\ud835\udf0e\u00bcinterval centered on the expectation.\n2.6.8Exercises\n1.Giveanexamplewhereobservingmoredatacanreducetheamountofuncertaintyabout\nthe outcome to an arbitrarily low level.\n2.Giveanexamplewhereobservingmoredatawillonlyreducetheamountofuncertainty\nuptoapointandthennofurther. Explainwhythisisthecaseandwhereyouexpectthis\npoint to occur.\n3.We empirically demonstrated convergence to the mean for the toss of a coin. Calculate\nthevarianceoftheestimateoftheprobabilitythatweseeaheadafterdrawing \ud835\udc5bsamples.\n1.How does the variance scale with the number of observations?\n2.Use Chebyshev\u2019s inequality to bound the deviation from the expectation.\n3.How does it relate to the central limit theorem?\n4.Assume that we draw \ud835\udc5asamples\ud835\udc65\ud835\udc56from a probability distribution with zero mean and\nunit variance. Compute the averages \ud835\udc67\ud835\udc5adef=\ud835\udc5a\u00001\u00cd\ud835\udc5a\n\ud835\udc56=1\ud835\udc65\ud835\udc56. Can we apply Chebyshev\u2019s\ninequality for every \ud835\udc67\ud835\udc5aindependently? Why not?\n5.Given two events with probability \ud835\udc43\u00b9A\u00baand\ud835\udc43\u00b9B\u00ba, compute upper and lower bounds\non\ud835\udc43\u00b9A[B\u00ba and\ud835\udc43\u00b9A\\B\u00ba . Hint: graph the situation using a Venn diagram61.\n6.Assumethatwehaveasequenceofrandomvariables,say \ud835\udc34,\ud835\udc35,and\ud835\udc36,where\ud835\udc35onlyde-\npendson\ud835\udc34,and\ud835\udc36onlydependson \ud835\udc35,canyousimplifythejointprobability \ud835\udc43\u00b9\ud835\udc34,\ud835\udc35,\ud835\udc36\u00ba?\nHint: this is a Markov chain62.\n7.InSection 2.6.5 , assume that the outcomes of the two tests are not independent. In\nparticular assume that either test on its own has a false positive rate of 10% and a false\nnegative rate of 1%. That is, assume that \ud835\udc43\u00b9\ud835\udc37=1j\ud835\udc3b=0\u00ba=0.1and that\ud835\udc43\u00b9\ud835\udc37=\n0j\ud835\udc3b=1\u00ba=0.01. Moreover, assume that for \ud835\udc3b=1(infected) the test outcomes are", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb5ccd20-5864-46d7-b36d-e3439f394294": {"__data__": {"id_": "fb5ccd20-5864-46d7-b36d-e3439f394294", "embedding": null, "metadata": {"page_label": "78", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d11e5500-70e5-4394-960b-8f754e82f6ba", "node_type": "4", "metadata": {"page_label": "78", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "181aed24d7fa52175bcff9a1d65ddfd1a9247dafd41178c356296b4bdaf64ca2", "class_name": "RelatedNodeInfo"}}, "text": "78 Preliminaries\n63\n64\n65\n66conditionally independent, i.e., that \ud835\udc43\u00b9\ud835\udc371,\ud835\udc372j\ud835\udc3b=1\u00ba=\ud835\udc43\u00b9\ud835\udc371j\ud835\udc3b=1\u00ba\ud835\udc43\u00b9\ud835\udc372j\ud835\udc3b=\n1\u00babut that for healthy patients the outcomes are coupled via \ud835\udc43\u00b9\ud835\udc371=\ud835\udc372=1j\ud835\udc3b=\n0\u00ba=0.02.\n1.Work out the joint probability table for \ud835\udc371and\ud835\udc372, given\ud835\udc3b=0based on the infor-\nmation you have so far.\n2.Derive the probability that the patient is diseased ( \ud835\udc3b=1) after one test returns\npositive. You can assume the same baseline probability \ud835\udc43\u00b9\ud835\udc3b=1\u00ba=0.0015as\nbefore.\n3.Derive the probability that the patient is diseased ( \ud835\udc3b=1) after both tests return\npositive.\n8.Assume that you are an asset manager for an investment bank and you have a choice of\nstocks\ud835\udc60\ud835\udc56toinvestin. Yourportfolioneedstoaddupto 1withweights \ud835\udefc\ud835\udc56foreachstock.\nThe stocks have an average return \ud835\udf41=\ud835\udc38s\u0018\ud835\udc43\u00bbs\u00bcand covariance \ud835\udeba=Covs\u0018\ud835\udc43\u00bbs\u00bc.\n1.Compute the expected return for a given portfolio \ud835\udf36.\n2.If you wanted to maximize the return of the portfolio, how should you choose your\ninvestment?\n3.Compute the variance of the portfolio.\n4.Formulateanoptimizationproblemofmaximizingthereturnwhilekeepingthevari-\nanceconstrainedtoanupperbound. ThisistheNobel-Prizewinning Markovitzport-\nfolio63(Mangram, 2013 ). Tosolveityouwillneedaquadraticprogrammingsolver,\nsomething way beyond the scope of this book.\nDiscussions64.\n2.7Documentation\nWhilewecannotpossiblyintroduceeverysinglePyTorchfunctionandclass(andtheinfor-\nmation might become outdated quickly), the API documentation65and additional tutorials\n66and examples provide such documentation. This section provides some guidance for\nhow to explore the PyTorch API.\nimport torch\n2.7.1Functionsand Classes in a Module\nTo know which functions and classes can be called in a module, we invoke the dirfunc-\ntion. For instance, we can query all properties in the module for generating random num-\nbers:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09aefb07-ba76-4618-8084-06f8842fbcf7": {"__data__": {"id_": "09aefb07-ba76-4618-8084-06f8842fbcf7", "embedding": null, "metadata": {"page_label": "79", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2bae5020-5188-4ec2-ac64-e013c7128923", "node_type": "4", "metadata": {"page_label": "79", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7a925fa77f164af66ae5f65adc2f2a35583a21ceff7530957fffc3316d3b8b47", "class_name": "RelatedNodeInfo"}}, "text": "79 Documentation\nprint (dir(torch .distributions))\n['AbsTransform ','AffineTransform ','Bernoulli ','Beta ','Binomial ',\n\u21a9!'CatTransform ','Categorical ','Cauchy ','Chi2 ','ComposeTransform ',\n\u21a9!'ContinuousBernoulli ','CorrCholeskyTransform ',\n\u21a9!'CumulativeDistributionTransform ','Dirichlet ','Distribution ','ExpTransform\n\u21a9!','Exponential ','ExponentialFamily ','FisherSnedecor ','Gamma ','Geometric\n\u21a9!','Gumbel ','HalfCauchy ','HalfNormal ','Independent ','IndependentTransform\n\u21a9!','Kumaraswamy ','LKJCholesky ','Laplace ','LogNormal ','LogisticNormal ',\n\u21a9!'LowRankMultivariateNormal ','LowerCholeskyTransform ','MixtureSameFamily ',\n\u21a9!'Multinomial ','MultivariateNormal ','NegativeBinomial ','Normal ',\n\u21a9!'OneHotCategorical ','OneHotCategoricalStraightThrough ','Pareto ','Poisson ',\n\u21a9!'PositiveDefiniteTransform ','PowerTransform ','RelaxedBernoulli ',\n\u21a9!'RelaxedOneHotCategorical ','ReshapeTransform ','SigmoidTransform ',\n\u21a9!'SoftmaxTransform ','SoftplusTransform ','StackTransform ',\n\u21a9!'StickBreakingTransform ','StudentT ','TanhTransform ','Transform ',\n\u21a9!'TransformedDistribution ','Uniform ','VonMises ','Weibull ','Wishart ','__\n\u21a9!all__ ','__builtins__ ','__cached__ ','__doc__ ','__file__ ','__loader__ ','_\n\u21a9!_name__ ','__package__ ','__path__ ','__spec__ ','bernoulli ','beta ',\n\u21a9!'biject_to ','binomial ','categorical ','cauchy ','chi2 ','constraint_\n\u21a9!registry ','constraints ','continuous_bernoulli ','dirichlet ','distribution\n\u21a9!','exp_family ','exponential ','fishersnedecor ','gamma ','geometric ',\n\u21a9!'gumbel ','half_cauchy ','half_normal ','identity_transform ','independent ',\n\u21a9!'kl','kl_divergence ','kumaraswamy ','laplace ','lkj_cholesky ','log_normal\n\u21a9!','logistic_normal ','lowrank_multivariate_normal ','mixture_same_family ',\n\u21a9!'multinomial ','multivariate_normal ','negative_binomial ','normal ','one_\n\u21a9!hot_categorical ','pareto ','poisson ','register_kl ','relaxed_bernoulli ',\n\u21a9!'relaxed_categorical ','studentT ','transform_to ','transformed_distribution\n\u21a9!','transforms ','uniform ','utils ','von_mises ','weibull ','wishart ']\nGenerally, we can ignore functions that start and end with __(special objects in Python) or\nfunctionsthatstartwithasingle _(usuallyinternalfunctions). Basedontheremainingfunc-\ntionorattributenames,wemighthazardaguessthatthismoduleoffersvariousmethodsfor\ngenerating random numbers, including sampling from the uniform distribution ( uniform ),\nnormal distribution ( normal), and multinomial distribution ( multinomial ).\n2.7.2SpecificFunctions and Classes\nFor specific instructions on how to use a given function or class, we can invoke the help\nfunction. Asanexample,let\u2019sexploretheusageinstructionsfortensors\u2019 onesfunction.\nhelp(torch .ones)\nHelp on built-in function ones in module torch:\nones(...)\nones( *size, *, out=None, dtype=None, layout=torch.strided, device=None,\n\u21a9!requires_grad=False) -> Tensor\nReturns a tensor filled with the scalar value 1, with the shape defined", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7974e510-bc8c-4e80-839a-0461fce8a316": {"__data__": {"id_": "7974e510-bc8c-4e80-839a-0461fce8a316", "embedding": null, "metadata": {"page_label": "80", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45f1c665-9493-4da1-b1d5-86c00104e08e", "node_type": "4", "metadata": {"page_label": "80", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a87595be22640d83e4129b26258723b491defb224b695f2493f9df4a15d65f9d", "class_name": "RelatedNodeInfo"}}, "text": "80 Preliminaries\nby the variable argument size.\nArgs:\nsize (int...): a sequence of integers defining the shape of the \u2423\n\u21a9!output tensor.\nCan be a variable number of arguments or a collection like a \u2423\n\u21a9!list or tuple.\nKeyword arguments:\nout (Tensor, optional): the output tensor.\ndtype (torch.dtype, optional): the desired data type of returned \u2423\n\u21a9!tensor.\nDefault: if None, uses a global default (see torch.set_default_\n\u21a9!tensor_type()).\nlayout (torch.layout, optional): the desired layout of returned \u2423\n\u21a9!Tensor.\nDefault: torch.strided.\ndevice (torch.device, optional): the desired device of returned \u2423\n\u21a9!tensor.\nDefault: if None, uses the current device for the default tensor \u2423\n\u21a9!type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor \u2423\n\u21a9!types.\nrequires_grad (bool, optional): If autograd should record operations \u2423\n\u21a9!on the\nreturned tensor. Default: False.\nExample::\n>>> torch.ones(2, 3)\ntensor([[ 1., 1., 1.],\n[ 1., 1., 1.]])\n>>> torch.ones(5)\ntensor([ 1., 1., 1., 1., 1.])\nFrom the documentation, we can see that the onesfunction creates a new tensor with the\nspecified shape and sets all the elements to the value of 1. Whenever possible, you should\nrun a quick test to confirm your interpretation:\ntorch .ones( 4)\ntensor([ 1.,1.,1.,1.])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6470f9c7-42a3-4a6b-ace0-c105308081dc": {"__data__": {"id_": "6470f9c7-42a3-4a6b-ace0-c105308081dc", "embedding": null, "metadata": {"page_label": "81", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e55930c-82a8-4720-977e-8af40392cc43", "node_type": "4", "metadata": {"page_label": "81", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "43e3839f57399f1e47fd6f4140e2c79ad02aed508febd1f6d1189fd89cfd5713", "class_name": "RelatedNodeInfo"}}, "text": "81 Documentation\n67In the Jupyter notebook, we can use ?to display the document in another window. For\nexample, list?will create content that is almost identical to help(list) , displaying it\nin a new browser window. In addition, if we use two question marks, such as list??, the\nPython code implementing the function will also be displayed.\nThe official documentation provides plenty of descriptions and examples that are beyond\nthis book. We emphasize important use cases that will get you started quickly with prac-\ntical problems, rather than completeness of coverage. We also encourage you to study the\nsource code of the libraries to see examples of high-quality implementations of production\ncode. By doing this you will become a better engineer in addition to becoming a better\nscientist.\nDiscussions67.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6e8128e-529b-4793-a39d-26ac11e59d8b": {"__data__": {"id_": "d6e8128e-529b-4793-a39d-26ac11e59d8b", "embedding": null, "metadata": {"page_label": "82", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a6174e1-6ee8-4abe-b476-7a563e647b47", "node_type": "4", "metadata": {"page_label": "82", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2978a6a3687b31a39d771c8dd79dd83899fe19f1d347cfdfb861507eb8e97644", "class_name": "RelatedNodeInfo"}}, "text": "3 Linear Neural Networks for Regression\nBefore we worry about making our neural networks deep, it will be helpful to implement\nsomeshallowones,forwhichtheinputsconnectdirectlytotheoutputs. Thiswillproveim-\nportant for a few reasons. First, rather than getting distracted by complicated architectures,\nwe can focus on the basics of neural network training, including parametrizing the output\nlayer, handling data, specifying a loss function, and training the model. Second, this class\nof shallow networks happens to comprise the set of linear models, which subsumes many\nclassical methods of statistical prediction, including linear and softmax regression. Un-\nderstanding these classical tools is pivotal because they are widely used in many contexts\nand we will often need to use them as baselines when justifying the use of fancier archi-\ntectures. This chapter will focus narrowly on linear regression and the next one will extend\nour modeling repertoire by developing linear neural networks for classification.\n3.1LinearRegression\nRegression problems pop up whenever we want to predict a numerical value. Common ex-\namples include predicting prices (of homes, stocks, etc.), predicting the length of stay (for\npatientsinthehospital),forecastingdemand(forretailsales),amongnumerousothers. Not\neverypredictionproblemisoneofclassicalregression. Lateron,wewillintroduceclassifi-\ncationproblems,wherethegoalistopredictmembershipamongasetofcategories.\nAs a running example, suppose that we wish to estimate the prices of houses (in dollars)\nbased on their area (in square feet) and age (in years). To develop a model for predicting\nhouse prices, we need to get our hands on data, including the sales price, area, and age for\neach home. In the terminology of machine learning, the dataset is called a trainingdataset\nortraining set , and each row (containing the data corresponding to one sale) is called an\nexample (ordata point ,instance,sample). The thing we are trying to predict (price) is\ncalled alabel(ortarget). The variables (age and area) upon which the predictions are\nbased are called features (orcovariates ).\n%matplotlib inline\nimport math\nimport time\nimport numpy asnp\n(continues on next page)\n82", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c4dd99f-c5cc-42b3-925d-206818b37ba3": {"__data__": {"id_": "4c4dd99f-c5cc-42b3-925d-206818b37ba3", "embedding": null, "metadata": {"page_label": "83", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2700a2ec-d2c4-42b3-af73-4dce9696eb4a", "node_type": "4", "metadata": {"page_label": "83", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4de6e7234726fdc61901af2e55b37050539e1d170d4fd7c9c871edd7b18d3c39", "class_name": "RelatedNodeInfo"}}, "text": "83 Linear Regression\n(continued from previous page)\nimport torch\nfrom d2l import torch asd2l\n3.1.1Basics\nLinearregression is both the simplest and most popular among the standard tools for tack-\nling regression problems. Dating back to the dawn of the 19th century ( Gauss, 1809 ,Leg-\nendre, 1805 ), linear regression flows from a few simple assumptions. First, we assume that\nthe relationship between features xand target\ud835\udc66is approximately linear, i.e., that the con-\nditional mean \ud835\udc38\u00bb\ud835\udc4cj\ud835\udc4b=x\u00bccan be expressed as a weighted sum of the features x. This\nsetup allows that the target value may still deviate from its expected value on account of\nobservationnoise. Next,wecanimposetheassumptionthatanysuchnoiseiswellbehaved,\nfollowing a Gaussian distribution. Typically, we will use \ud835\udc5bto denote the number of exam-\nples in our dataset. We use superscripts to enumerate samples and targets, and subscripts\nto index coordinates. More concretely, x\u00b9\ud835\udc56\u00badenotes the\ud835\udc56thsample and\ud835\udc65\u00b9\ud835\udc56\u00ba\n\ud835\udc57denotes its\ud835\udc57th\ncoordinate.\nModel\nAt the heart of every solution is a model that describes how features can be transformed\ninto an estimate of the target. The assumption of linearity means that the expected value of\nthe target(price) can be expressedas a weighted sum of the features(area and age):\nprice=\ud835\udc64area\u0001area\u00b8\ud835\udc64age\u0001age\u00b8\ud835\udc4f. (3.1.1)\nHere\ud835\udc64areaand\ud835\udc64ageare called weights, and\ud835\udc4fis called a bias(oroffsetorintercept ). The\nweights determine the influence of each feature on our prediction. The bias determines the\nvalue of the estimate when all features are zero. Even though we will never see any newly-\nbuilt homes with precisely zero area, we still need the bias because it allows us to express\nall linear functions of our features (rather than restricting us to lines that pass through the\norigin). Strictly speaking, (3.1.1 )is ana\ufb00ine transformation of input features, which is\ncharacterized by a linear transformation of features via a weighted sum, combined with a\ntranslation via the added bias. Given a dataset, our goal is to choose the weights wand\nthe bias\ud835\udc4fthat, on average, make our model\u2019s predictions fit the true prices observed in the\ndata as closely as possible.\nIn disciplines where it is common to focus on datasets with just a few features, explicitly\nexpressing models long-form, as in (3.1.1 ), is common. In machine learning, we usually\nwork with high-dimensional datasets, where it is more convenient to employ compact lin-\near algebra notation. When our inputs consist of \ud835\udc51features, we can assign each an index\n(between 1and\ud835\udc51) and express our prediction \u02c6\ud835\udc66(in general the \u201chat\u201d symbol denotes an\nestimate) as\n\u02c6\ud835\udc66=\ud835\udc641\ud835\udc651\u00b8\u0001\u0001\u0001\u00b8\ud835\udc64\ud835\udc51\ud835\udc65\ud835\udc51\u00b8\ud835\udc4f. (3.1.2)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8521df9-447c-4cf0-b5f7-9be268d98b2d": {"__data__": {"id_": "c8521df9-447c-4cf0-b5f7-9be268d98b2d", "embedding": null, "metadata": {"page_label": "84", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0366dec6-afdd-45a9-922a-34422fc937c7", "node_type": "4", "metadata": {"page_label": "84", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d326aab69c54d4867518150ba24140570b07d4e7f29dfae6d264c2bf40e7cd5b", "class_name": "RelatedNodeInfo"}}, "text": "84 Linear Neural Networks for Regression\nCollecting all features into a vector x2R\ud835\udc51and all weights into a vector w2R\ud835\udc51, we can\nexpress our model compactly via the dot product between wandx:\n\u02c6\ud835\udc66=w>x\u00b8\ud835\udc4f. (3.1.3)\nIn(3.1.3 ), the vector xcorresponds to the features of a single example. We will often\nfind it convenient to refer to features of our entire dataset of \ud835\udc5bexamples via the design\nmatrix X2R\ud835\udc5b\u0002\ud835\udc51. Here, Xcontains one row for every example and one column for every\nfeature. For a collection of features X, the predictions \u02c6y2R\ud835\udc5bcan be expressed via the\nmatrix\u2013vector product:\n\u02c6y=Xw\u00b8\ud835\udc4f, (3.1.4)\nwhere broadcasting ( Section 2.1.4 ) is applied during the summation. Given features of a\ntraining dataset Xand corresponding (known) labels y, the goal of linear regression is to\nfindtheweightvector wandthebiasterm \ud835\udc4fsuchthat,givenfeaturesofanewdataexample\nsampled from the same distribution as X, the new example\u2019s label will (in expectation) be\npredicted with the smallest error.\nEven if we believe that the best model for predicting \ud835\udc66givenxis linear, we would not\nexpect to find a real-world dataset of \ud835\udc5bexamples where \ud835\udc66\u00b9\ud835\udc56\u00baexactly equals w>x\u00b9\ud835\udc56\u00ba\u00b8\ud835\udc4f\nfor all 1\u0014\ud835\udc56\u0014\ud835\udc5b. For example, whatever instruments we use to observe the features X\nand labels y, there might be a small amount of measurement error. Thus, even when we\nare confident that the underlying relationship is linear, we will incorporate a noise term to\naccount for such errors.\nBefore we can go about searching for the best parameters (ormodelparameters )wand\ud835\udc4f,\nwe will need two more things: (i) a measure of the quality of some given model; and (ii) a\nprocedure for updating the model to improve its quality.\nLoss Function\nNaturally, fitting our model to the data requires that we agree on some measure of fitness\n(or, equivalently, of unfitness ).Loss functions quantify the distance between the realand\npredicted valuesofthetarget. Thelosswillusuallybeanonnegativenumberwheresmaller\nvaluesarebetterandperfectpredictionsincuralossof0. Forregressionproblems,themost\ncommon loss function is the squared error. When our prediction for an example \ud835\udc56is\u02c6\ud835\udc66\u00b9\ud835\udc56\u00ba\nand the corresponding true label is \ud835\udc66\u00b9\ud835\udc56\u00ba, thesquarederror is given by:\n\ud835\udc59\u00b9\ud835\udc56\u00ba\u00b9w,\ud835\udc4f\u00ba=1\n2\u0010\n\u02c6\ud835\udc66\u00b9\ud835\udc56\u00ba\u0000\ud835\udc66\u00b9\ud835\udc56\u00ba\u00112\n. (3.1.5)\nThe constant1\n2makes no real difference but proves to be notationally convenient, since it\ncancels out when we take the derivative of the loss. Because the training dataset is given\nto us, and thus is out of our control, the empirical error is only a function of the model\nparameters. In Fig. 3.1.1 , we visualize the fit of a linear regression model in a problem\nwith one-dimensional inputs.\nNotethatlargedifferencesbetweenestimates \u02c6\ud835\udc66\u00b9\ud835\udc56\u00baandtargets\ud835\udc66\u00b9\ud835\udc56\u00baleadtoevenlargercontri-\nbutions to the loss, due to its quadratic form (this quadraticity can be a double-edge sword;", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2784, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "294f9388-866e-4131-ab0c-6aa77ec13715": {"__data__": {"id_": "294f9388-866e-4131-ab0c-6aa77ec13715", "embedding": null, "metadata": {"page_label": "85", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d28071a5-dc63-4768-9fa2-42a47d76b49a", "node_type": "4", "metadata": {"page_label": "85", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dcc16b769ba1507a4e3940d3af00c3329b5e75958ab21ebc74fb5c1e05420401", "class_name": "RelatedNodeInfo"}}, "text": "85 Linear Regression\ntFig. 3.1.1 Fitting a linear regression model to one-dimensional data.\nwhile it encourages the model to avoid large errors it can also lead to excessive sensitivity\nto anomalous data). To measure the quality of a model on the entire dataset of \ud835\udc5bexamples,\nwe simply average (or equivalently, sum) the losses on the training set:\n\ud835\udc3f\u00b9w,\ud835\udc4f\u00ba=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc59\u00b9\ud835\udc56\u00ba\u00b9w,\ud835\udc4f\u00ba=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=11\n2\u0010\nw>x\u00b9\ud835\udc56\u00ba\u00b8\ud835\udc4f\u0000\ud835\udc66\u00b9\ud835\udc56\u00ba\u00112\n. (3.1.6)\nWhen training the model, we seek parameters ( w\u0003,\ud835\udc4f\u0003) that minimize the total loss across\nall training examples:\nw\u0003,\ud835\udc4f\u0003=argmin\nw,\ud835\udc4f\ud835\udc3f\u00b9w,\ud835\udc4f\u00ba.(3.1.7)\nAnalyticSolution\nUnlike most of the models that we will cover, linear regression presents us with a surpris-\ningly easy optimization problem. In particular, we can find the optimal parameters (as\nassessed on the training data) analytically by applying a simple formula as follows. First,\nwe can subsume the bias \ud835\udc4finto the parameter wby appending a column to the design ma-\ntrix consisting of all 1s. Then our prediction problem is to minimize ky\u0000Xwk2. As long\nas the design matrix Xhas full rank (no feature is linearly dependent on the others), then\nthere will be just one critical point on the loss surface and it corresponds to the minimum\nof the loss over the entire domain. Taking the derivative of the loss with respect to wand\nsetting it equal to zero yields:\n\ud835\udf15wky\u0000Xwk2=2X>\u00b9Xw\u0000y\u00ba=0and hence X>y=X>Xw. (3.1.8)\nSolving for wprovides us with the optimal solution for the optimization problem. Note\nthat this solution\nw\u0003=\u00b9X>X\u00ba\u00001X>y (3.1.9)\nwillonlybeuniquewhenthematrix X>Xisinvertible,i.e.,whenthecolumnsofthedesign\nmatrix are linearly independent ( Golub and Van Loan, 1996 ).\nWhile simple problems like linear regression may admit analytic solutions, you should\nnotgetusedtosuchgoodfortune. Althoughanalyticsolutionsallowfornicemathematical\nanalysis,therequirementofananalyticsolutionissorestrictivethatitwouldexcludealmost\nall exciting aspects of deep learning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1926, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "509c1128-7fc7-4729-b016-f13b29faf07e": {"__data__": {"id_": "509c1128-7fc7-4729-b016-f13b29faf07e", "embedding": null, "metadata": {"page_label": "86", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54691a2f-89cb-4fbc-8a2d-6d716e7c6e19", "node_type": "4", "metadata": {"page_label": "86", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "05c57366cb9f8680e54b8d0e5b8f710f274d555066dcadcf8a15a130e23ba6a4", "class_name": "RelatedNodeInfo"}}, "text": "86 Linear Neural Networks for Regression\nMinibatchStochasticGradient Descent\nFortunately, even in cases where we cannot solve the models analytically, we can still of-\nten train models effectively in practice. Moreover, for many tasks, those hard-to-optimize\nmodels turn out to be so much better that figuring out howto train them ends up being well\nworth the trouble.\nThe key technique for optimizing nearly every deep learning model, and which we will\ncall upon throughout this book, consists of iteratively reducing the error by updating the\nparameters in the direction that incrementally lowers the loss function. This algorithm is\ncalledgradientdescent .\nThe most naive application of gradient descent consists of taking the derivative of the loss\nfunction,whichisanaverageofthelossescomputedoneverysingleexampleinthedataset.\nIn practice, this can be extremely slow: we must pass over the entire dataset before making\na single update, even if the update steps might be very powerful ( Liu and Nocedal, 1989 ).\nEven worse, if there is a lot of redundancy in the training data, the benefit of a full update\nis limited.\nThe other extreme is to consider only a single example at a time and to take update steps\nbased on one observation at a time. The resulting algorithm, stochastic gradient descent\n(SGD) can be an effective strategy ( Bottou, 2010 ), even for large datasets. Unfortunately,\nSGD has drawbacks, both computational and statistical. One problem arises from the fact\nthatprocessorsarealotfastermultiplyingandaddingnumbersthantheyareatmovingdata\nfrom main memory to processor cache. It is up to an order of magnitude more efficient\nto perform a matrix\u2013vector multiplication than a corresponding number of vector\u2013vector\noperations. Thismeansthatitcantakealotlongertoprocessonesampleatatimecompared\nto a full batch. A second problem is that some of the layers, such as batch normalization\n(to be described in Section 8.5 ), only work well when we have access to more than one\nobservation at a time.\nThe solution to both problems is to pick an intermediate strategy: rather than taking a full\nbatchoronlyasinglesampleatatime,wetakea minibatch ofobservations( Lietal.,2014).\nThe specific choice of the size of the said minibatch depends on many factors, such as the\namount of memory, the number of accelerators, the choice of layers, and the total dataset\nsize. Despite all that, a number between 32 and 256, preferably a multiple of a large power\nof2, is a good start. This leads us to minibatchstochasticgradientdescent .\nInitsmostbasicform, ineachiteration \ud835\udc61, wefirstrandomlysampleaminibatch B\ud835\udc61consist-\ning of a fixed number jBjof training examples. We then compute the derivative (gradient)\noftheaveragelossontheminibatchwithrespecttothemodelparameters. Finally,wemul-\ntiply the gradient by a predetermined small positive value \ud835\udf02, called the learning rate , and\nsubtract the resulting term from the current parameter values. We can express the update\nas follows:\n\u00b9w,\ud835\udc4f\u00ba \u00b9w,\ud835\udc4f\u00ba\u0000\ud835\udf02\njBj\u00d5\n\ud835\udc562B\ud835\udc61\ud835\udf15\u00b9w,\ud835\udc4f\u00ba\ud835\udc59\u00b9\ud835\udc56\u00ba\u00b9w,\ud835\udc4f\u00ba.(3.1.10)\nIn summary, minibatch SGD proceeds as follows: (i) initialize the values of the model", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd02a707-db52-45c7-940d-67ff2b765fa8": {"__data__": {"id_": "cd02a707-db52-45c7-940d-67ff2b765fa8", "embedding": null, "metadata": {"page_label": "87", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d623e7b1-09ec-4870-9c38-647ac09d03f1", "node_type": "4", "metadata": {"page_label": "87", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ff7968fdd250482bac31991b095708806c2a31db9d9a5cbbb1768271a9239549", "class_name": "RelatedNodeInfo"}}, "text": "87 Linear Regression\nparameters, typically at random; (ii) iteratively sample random minibatches from the data,\nupdating the parameters in the direction of the negative gradient. For quadratic losses and\naffine transformations, this has a closed-form expansion:\nw w\u0000\ud835\udf02\njBj\u00d5\n\ud835\udc562B\ud835\udc61\ud835\udf15w\ud835\udc59\u00b9\ud835\udc56\u00ba\u00b9w,\ud835\udc4f\u00ba=w\u0000\ud835\udf02\njBj\u00d5\n\ud835\udc562B\ud835\udc61x\u00b9\ud835\udc56\u00ba\u0010\nw>x\u00b9\ud835\udc56\u00ba\u00b8\ud835\udc4f\u0000\ud835\udc66\u00b9\ud835\udc56\u00ba\u0011\n\ud835\udc4f \ud835\udc4f\u0000\ud835\udf02\njBj\u00d5\n\ud835\udc562B\ud835\udc61\ud835\udf15\ud835\udc4f\ud835\udc59\u00b9\ud835\udc56\u00ba\u00b9w,\ud835\udc4f\u00ba=\ud835\udc4f\u0000\ud835\udf02\njBj\u00d5\n\ud835\udc562B\ud835\udc61\u0010\nw>x\u00b9\ud835\udc56\u00ba\u00b8\ud835\udc4f\u0000\ud835\udc66\u00b9\ud835\udc56\u00ba\u0011\n.(3.1.11)\nSince we pick a minibatch Bwe need to normalize by its size jBj. Frequently minibatch\nsize and learning rate are user-defined. Such tunable parameters that are not updated in the\ntraining loop are called hyperparameters . They can be tuned automatically by a number\nof techniques, such as Bayesian optimization ( Frazier, 2018 ). In the end, the quality of the\nsolution is typically assessed on a separate validationdataset (orvalidationset ).\nAfter training for some predetermined number of iterations (or until some other stopping\ncriterionismet),werecordtheestimatedmodelparameters,denoted \u02c6w,\u02c6\ud835\udc4f. Notethatevenif\nourfunctionistrulylinearandnoiseless, theseparameterswillnotbetheexactminimizers\nof the loss, nor even deterministic. Although the algorithm converges slowly towards the\nminimizers it typically will not find them exactly in a finite number of steps. Moreover,\nthe minibatchesBused for updating the parameters are chosen at random. This breaks\ndeterminism.\nLinear regression happens to be a learning problem with a global minimum (whenever X\nis full rank, or equivalently, whenever X>Xis invertible). However, the loss surfaces for\ndeep networks contain many saddle points and minima. Fortunately, we typically do not\ncare about finding an exact set of parameters but merely any set of parameters that leads\nto accurate predictions (and thus low loss). In practice, deep learning practitioners seldom\nstruggle to find parameters that minimize the loss on training sets (Frankle and Carbin,\n2018,Izmailov et al., 2018). The more formidable task is to find parameters that lead\nto accurate predictions on previously unseen data, a challenge called generalization . We\nreturn to these topics throughout the book.\nPredictions\nGiven the model \u02c6w>x\u00b8\u02c6\ud835\udc4f, we can now make predictions for a new example, e.g., pre-\ndicting the sales price of a previously unseen house given its area \ud835\udc651and age\ud835\udc652. Deep\nlearningpractitionershavetakentocallingthepredictionphase inference butthisisabitof\na misnomer\u2014 inference refers broadly to any conclusion reached on the basis of evidence,\nincluding both the values of the parameters and the likely label for an unseen instance. If\nanything, in the statistics literature inference more often denotes parameter inference and\nthis overloading of terminology creates unnecessary confusion when deep learning prac-\ntitioners talk to statisticians. In the following we will stick to prediction whenever possi-\nble.\n3.1.2VectorizationforSpeed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f466afd8-29bb-4f73-afb3-385acc39be72": {"__data__": {"id_": "f466afd8-29bb-4f73-afb3-385acc39be72", "embedding": null, "metadata": {"page_label": "88", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30969199-6085-4578-9221-f1f440c1ac40", "node_type": "4", "metadata": {"page_label": "88", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9ca8d32c9d8bd6f1b61d331def999dcac6ae0e809c7e3a5c11722ce1903a1225", "class_name": "RelatedNodeInfo"}}, "text": "88 Linear Neural Networks for Regression\nWhentrainingourmodels, wetypicallywanttoprocesswholeminibatchesofexamplessi-\nmultaneously. Doingthisefficientlyrequiresthatwevectorizethecalculationsandleverage\nfast linear algebra libraries rather than writing costly for-loops in Python.\nToseewhythismatterssomuch,let\u2019sconsidertwomethodsforaddingvectors. Tostart,we\ninstantiate two 10,000-dimensional vectors containing all 1s. In the first method, we loop\nover the vectors with a Python for-loop. In the second, we rely on a single call to +.\nn=10000\na=torch .ones(n)\nb=torch .ones(n)\nNow we can benchmark the workloads. First, we add them, one coordinate at a time, using\na for-loop.\nc=torch .zeros(n)\nt=time .time()\nfor iinrange (n):\nc[i] =a[i] +b[i]\nf'{time .time() -t:.5f}sec'\n'0.17802 sec '\nAlternatively, we rely on the reloaded +operator to compute the elementwise sum.\nt=time .time()\nd=a+b\nf'{time .time() -t:.5f}sec'\n'0.00036 sec '\nThesecondmethodisdramaticallyfasterthanthefirst. Vectorizingcodeoftenyieldsorder-\nof-magnitude speedups. Moreover, we push more of the mathematics to the library so we\ndo not have to write as many calculations ourselves, reducing the potential for errors and\nincreasing portability of the code.\n3.1.3The NormalDistribution and SquaredLoss\nSofarwehavegivenafairlyfunctionalmotivationofthesquaredlossobjective: theoptimal\nparameters return the conditional expectation \ud835\udc38\u00bb\ud835\udc4cj\ud835\udc4b\u00bcwhenever the underlying pattern\nis truly linear, and the loss assigns large penalties for outliers. We can also provide a more\nformalmotivationforthesquaredlossobjectivebymakingprobabilisticassumptionsabout\nthe distribution of noise.\nLinear regression was invented at the turn of the 19th century. While it has long been\ndebated whether Gauss or Legendre first thought up the idea, it was Gauss who also dis-\ncovered the normal distribution (also called the Gaussian ). It turns out that the normal", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2876c355-820e-4e31-a68b-e3e831163e1b": {"__data__": {"id_": "2876c355-820e-4e31-a68b-e3e831163e1b", "embedding": null, "metadata": {"page_label": "89", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "97858353-81ad-429d-b869-b24eca3c982d", "node_type": "4", "metadata": {"page_label": "89", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d2e0a87cc4c0e3d564402a62205d362102da0f7e2f6aa73cec7e8fc21c962fd6", "class_name": "RelatedNodeInfo"}}, "text": "89 Linear Regression\ndistribution and linear regression with squared loss share a deeper connection than com-\nmon parentage.\nTobegin,recallthatanormaldistributionwithmean \ud835\udf07andvariance \ud835\udf0e2(standarddeviation\n\ud835\udf0e) is given as\n\ud835\udc5d\u00b9\ud835\udc65\u00ba=1p\n2\ud835\udf0b\ud835\udf0e2exp\u0012\n\u00001\n2\ud835\udf0e2\u00b9\ud835\udc65\u0000\ud835\udf07\u00ba2\u0013\n. (3.1.12)\nBelow we define a function to compute the normal distribution.\ndef normal (x, mu, sigma):\np=1/math .sqrt( 2*math .pi*sigma **2)\nreturn p*np.exp( -0.5 *(x-mu)**2/sigma **2)\nWe can now visualize the normal distributions.\n# Use NumPy again for visualization\nx=np.arange( -7,7,0.01 )\n# Mean and standard deviation pairs\nparams =[(0,1), ( 0,2), ( 3,1)]\nd2l.plot(x, [normal(x, mu, sigma) for mu, sigma inparams], xlabel ='x',\nylabel ='p(x) ', figsize =(4.5,2.5),\nlegend =[f'mean {mu}, std {sigma }'for mu, sigma inparams])\nNote that changing the mean corresponds to a shift along the \ud835\udc65-axis, and increasing the\nvariance spreads the distribution out, lowering its peak.\nOnewaytomotivatelinearregressionwithsquaredlossistoassumethatobservationsarise\nfromnoisymeasurements,wherethenoise \ud835\udf16followsthenormaldistribution N\u00b90,\ud835\udf0e2\u00ba:\n\ud835\udc66=w>x\u00b8\ud835\udc4f\u00b8\ud835\udf16where\ud835\udf16\u0018N\u00b9 0,\ud835\udf0e2\u00ba. (3.1.13)\nThus, we can now write out the likelihood of seeing a particular \ud835\udc66for a given xvia\n\ud835\udc43\u00b9\ud835\udc66jx\u00ba=1p\n2\ud835\udf0b\ud835\udf0e2exp\u0012\n\u00001\n2\ud835\udf0e2\u00b9\ud835\udc66\u0000w>x\u0000\ud835\udc4f\u00ba2\u0013\n. (3.1.14)\nAs such, the likelihood factorizes. According to the principle of maximum likelihood , the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1335, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4e0783a-733b-4775-b0f9-b85c9ec0aaa5": {"__data__": {"id_": "d4e0783a-733b-4775-b0f9-b85c9ec0aaa5", "embedding": null, "metadata": {"page_label": "90", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28b424a1-5b01-4b13-81c1-c0cd1f6ae853", "node_type": "4", "metadata": {"page_label": "90", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f19a05deee7ac2431f49365fa41bcebb377af41cd55e6e4208f09eb497b87294", "class_name": "RelatedNodeInfo"}}, "text": "90 Linear Neural Networks for Regression\nbest values of parameters wand\ud835\udc4fare those that maximize the likelihood of the entire\ndataset:\n\ud835\udc43\u00b9yjX\u00ba=\ud835\udc5b\u00d6\n\ud835\udc56=1\ud835\udc5d\u00b9\ud835\udc66\u00b9\ud835\udc56\u00bajx\u00b9\ud835\udc56\u00ba\u00ba. (3.1.15)\nTheequalityfollowssinceallpairs \u00b9x\u00b9\ud835\udc56\u00ba,\ud835\udc66\u00b9\ud835\udc56\u00ba\u00baweredrawnindependentlyofeachother. Es-\ntimatorschosenaccordingtotheprincipleofmaximumlikelihoodarecalled maximumlike-\nlihood estimators . While, maximizing the product of many exponential functions, might\nlookdifficult,wecansimplifythingssignificantly,withoutchangingtheobjective,bymax-\nimizing the logarithm of the likelihood instead. For historical reasons, optimizations are\nmoreoftenexpressedasminimizationratherthanmaximization. So,withoutchangingany-\nthing,wecan minimize thenegativelog-likelihood ,whichwecanexpressasfollows:\n\u0000log\ud835\udc43\u00b9yjX\u00ba=\ud835\udc5b\u00d5\n\ud835\udc56=11\n2log\u00b92\ud835\udf0b\ud835\udf0e2\u00ba\u00b81\n2\ud835\udf0e2\u0010\n\ud835\udc66\u00b9\ud835\udc56\u00ba\u0000w>x\u00b9\ud835\udc56\u00ba\u0000\ud835\udc4f\u00112\n. (3.1.16)\nIf we assume that \ud835\udf0eis fixed, we can ignore the first term, because it does not depend on w\nor\ud835\udc4f. The second term is identical to the squared error loss introduced earlier, except for\nthe multiplicative constant1\n\ud835\udf0e2. Fortunately, the solution does not depend on \ud835\udf0eeither. It\nfollows that minimizing the mean squared error is equivalent to the maximum likelihood\nestimation of a linear model under the assumption of additive Gaussian noise.\n3.1.4Linear Regressionas a NeuralNetwork\nWhile linear models are not sufficiently rich to express the many complicated networks\nthat we will introduce in this book, (artificial) neural networks are rich enough to subsume\nlinear models as networks in which every feature is represented by an input neuron, all of\nwhich are connected directly to the output.\nFig. 3.1.2 depicts linear regression as a neural network. The diagram highlights the con-\nnectivity pattern, such as how each input is connected to the output, but not the specific\nvalues taken by the weights or biases.\ntFig. 3.1.2 Linear regression is a single-layer neural network.\nTheinputsare \ud835\udc651,...,\ud835\udc65\ud835\udc51. Wereferto \ud835\udc51asthenumberofinputs orthefeaturedimensional-\nityin the input layer. The output of the network is \ud835\udc5c1. Because we are just trying to predict\nasinglenumericalvalue,wehaveonlyoneoutputneuron. Notethattheinputvaluesareall\ngiven. There is just a single computed neuron. In summary, we can think of linear regres-\nsion as a single-layer fully connected neural network. We will encounter networks with far\nmore layers in later chapters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2350, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eeda7dda-8902-430e-a000-596106f39ab9": {"__data__": {"id_": "eeda7dda-8902-430e-a000-596106f39ab9", "embedding": null, "metadata": {"page_label": "91", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "864e8da8-17d3-4d13-aa5a-b3a70fa7a20c", "node_type": "4", "metadata": {"page_label": "91", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4d3a6a3337339bdc7d09580df4172dade398c71a319cb6dd19f9eafb1605b6e3", "class_name": "RelatedNodeInfo"}}, "text": "91 Linear Regression\nBiology\nBecause linear regression predates computational neuroscience, it might seem anachro-\nnistic to describe linear regression in terms of neural networks. Nonetheless, they were a\nnatural place to start when the cyberneticists and neurophysiologists Warren McCulloch\nand Walter Pitts began to develop models of artificial neurons. Consider the cartoonish\npicture of a biological neuron in Fig. 3.1.3 , consisting of dendrites (input terminals), the\nnucleus(CPU),the axon(outputwire),andthe axonterminals (outputterminals),enabling\nconnections to other neurons via synapses .\nDendrite\nCell bodyNode of\nRanvierAxon T erminal\nSchwann cell\nMyelin sheathAxon\nNucleus\ntFig. 3.1.3 The real neuron (source: \u201cAnatomy and Physiology\u201d by the US National Cancer\nInstitute\u2019s Surveillance, Epidemiology and End Results (SEER) Program).\nInformation \ud835\udc65\ud835\udc56arriving from other neurons (or environmental sensors) is received in the\ndendrites. In particular, that information is weighted by synaptic weights \ud835\udc64\ud835\udc56, determining\nthe effect of the inputs, e.g., activation or inhibition via the product \ud835\udc65\ud835\udc56\ud835\udc64\ud835\udc56. The weighted\ninputs arriving from multiple sources are aggregated in the nucleus as a weighted sum \ud835\udc66=\u00cd\n\ud835\udc56\ud835\udc65\ud835\udc56\ud835\udc64\ud835\udc56\u00b8\ud835\udc4f, possibly subject to some nonlinear postprocessing via a function \ud835\udf0e\u00b9\ud835\udc66\u00ba. This\ninformation is then sent via the axon to the axon terminals, where it reaches its destination\n(e.g., an actuator such as a muscle) or it is fed into another neuron via its dendrites.\nCertainly, the high-level idea that many such units could be combined, provided they have\nthe correct connectivity and learning algorithm, to produce far more interesting and com-\nplex behavior than any one neuron alone could express arises from our study of real bi-\nological neural systems. At the same time, most research in deep learning today draws\ninspiration from a much wider source. We invoke Russell and Norvig ( 2016) who pointed\nout that although airplanes might have been inspired by birds, ornithology has not been\nthe primary driver of aeronautics innovation for some centuries. Likewise, inspiration in\ndeep learning these days comes in equal or greater measure from mathematics, linguistics,\npsychology, statistics, computer science, and many other fields.\n3.1.5Summary\nInthissection,weintroducedtraditionallinearregression,wheretheparametersofalinear\nfunction are chosen to minimize squared loss on the training set. We also motivated this\nchoice of objective both via some practical considerations and through an interpretation\nof linear regression as maximimum likelihood estimation under an assumption of linearity\nandGaussiannoise. Afterdiscussingbothcomputationalconsiderationsandconnectionsto", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb341ae3-1545-4440-8aca-473cfc507e78": {"__data__": {"id_": "cb341ae3-1545-4440-8aca-473cfc507e78", "embedding": null, "metadata": {"page_label": "92", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5a0d173-0573-4744-97b4-f39dd4091f5d", "node_type": "4", "metadata": {"page_label": "92", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "96e470c6dd3275c68944e8aab7e7374fd79cdd3de2eeb5b50e4c98775510e3d4", "class_name": "RelatedNodeInfo"}}, "text": "92 Linear Neural Networks for Regression\nstatistics,weshowedhowsuchlinearmodelscouldbeexpressedassimpleneuralnetworks\nwhere the inputs are directly wired to the output(s). While we will soon move past linear\nmodels altogether, they are sufficient to introduce most of the components that all of our\nmodels require: parametric forms, differentiable objectives, optimization via minibatch\nstochastic gradient descent, and ultimately, evaluation on previously unseen data.\n3.1.6Exercises\n1.Assume that we have some data \ud835\udc651,...,\ud835\udc65\ud835\udc5b2R. Our goal is to find a constant \ud835\udc4fsuch\nthat\u00cd\n\ud835\udc56\u00b9\ud835\udc65\ud835\udc56\u0000\ud835\udc4f\u00ba2is minimized.\n1.Find an analytic solution for the optimal value of \ud835\udc4f.\n2.How does this problem and its solution relate to the normal distribution?\n3.Whatifwechangethelossfrom\u00cd\n\ud835\udc56\u00b9\ud835\udc65\ud835\udc56\u0000\ud835\udc4f\u00ba2to\u00cd\n\ud835\udc56j\ud835\udc65\ud835\udc56\u0000\ud835\udc4fj? Canyoufindtheoptimal\nsolution for\ud835\udc4f?\n2.Prove that the affine functions that can be expressed by x>w\u00b8\ud835\udc4fare equivalent to linear\nfunctions on\u00b9x,1\u00ba.\n3.Assume that you want to find quadratic functions of x, i.e.,\ud835\udc53\u00b9x\u00ba=\ud835\udc4f\u00b8\u00cd\n\ud835\udc56\ud835\udc64\ud835\udc56\ud835\udc65\ud835\udc56\u00b8\u00cd\n\ud835\udc57\u0014\ud835\udc56\ud835\udc64\ud835\udc56\ud835\udc57\ud835\udc65\ud835\udc56\ud835\udc65\ud835\udc57. How would you formulate this in a deep network?\n4.Recall that one of the conditions for the linear regression problem to be solvable was\nthat the design matrix X>Xhas full rank.\n1.What happens if this is not the case?\n2.How could you fix it? What happens if you add a small amount of coordinate-wise\nindependent Gaussian noise to all entries of X?\n3.What is the expected value of the design matrix X>Xin this case?\n4.What happens with stochastic gradient descent when X>Xdoes not have full rank?\n5.Assume that the noise model governing the additive noise \ud835\udf16is the exponential distribu-\ntion. That is, \ud835\udc5d\u00b9\ud835\udf16\u00ba=1\n2exp\u00b9\u0000j\ud835\udf16j\u00ba.\n1.Write out the negative log-likelihood of the data under the model \u0000log\ud835\udc43\u00b9yjX\u00ba.\n2.Can you find a closed form solution?\n3.Suggest a minibatch stochastic gradient descent algorithm to solve this problem.\nWhat could possibly go wrong (hint: what happens near the stationary point as we\nkeep on updating the parameters)? Can you fix this?\n6.Assume that we want to design a neural network with two layers by composing two\nlinear layers. That is, the output of the first layer becomes the input of the second layer.\nWhy would such a naive composition not work?\n7.What happens if you want to use regression for realistic price estimation of houses or\nstock prices?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8da91da0-7a85-4577-b293-3efd2f617353": {"__data__": {"id_": "8da91da0-7a85-4577-b293-3efd2f617353", "embedding": null, "metadata": {"page_label": "93", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9bb7a012-7163-443f-ab82-2ad62999d93a", "node_type": "4", "metadata": {"page_label": "93", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2e3cc52d5ccc7f0b64d0153018381ee7d2b93bfb0aba6d395a590ee0c1408971", "class_name": "RelatedNodeInfo"}}, "text": "93 Object-Oriented Design for Implementation\n68\n69\n701.Show that the additive Gaussian noise assumption is not appropriate. Hint: can we\nhave negative prices? What about fluctuations?\n2.Whywouldregressiontothelogarithmofthepricebemuchbetter,i.e., \ud835\udc66=logprice?\n3.Whatdoyouneedtoworryaboutwhendealingwithpennystock,i.e.,stockwithvery\nlow prices? Hint: can you trade at all possible prices? Why is this a bigger problem\nfor cheap stock? For more information review the celebrated Black\u2013Scholes model\nfor option pricing ( Black and Scholes, 1973 ).\n8.Suppose we want to use regression to estimate the number of apples sold in a grocery\nstore.\n1.What are the problems with a Gaussian additive noise model? Hint: you are selling\napples, not oil.\n2.ThePoisson distribution68captures distributions over counts. It is given by \ud835\udc5d\u00b9\ud835\udc58j\n\ud835\udf06\u00ba=\ud835\udf06\ud835\udc58\ud835\udc52\u0000\ud835\udf06\u009d\ud835\udc58!. Here\ud835\udf06is the rate function and \ud835\udc58is the number of events you see.\nProve that\ud835\udf06is the expected value of counts \ud835\udc58.\n3.Design a loss function associated with the Poisson distribution.\n4.Design a loss function for estimating log\ud835\udf06instead.\nDiscussions69.\n3.2Object-Oriented Design forImplementation\nIn our introduction to linear regression, we walked through various components including\nthe data, the model, the loss function, and the optimization algorithm. Indeed, linear re-\ngressionisoneofthesimplestmachinelearningmodels. Trainingit,however,usesmanyof\nthe same components that other models in this book require. Therefore, before diving into\nthe implementation details it is worth designing some of the APIs that we use throughout.\nTreating components in deep learning as objects, we can start by defining classes for these\nobjects and their interactions. This object-oriented design for implementation will greatly\nstreamline the presentation and you might even want to use it in your projects.\nInspired by open-source libraries such as PyTorch Lightning70, at a high level we wish\nto have three classes: (i) Modulecontains models, losses, and optimization methods; (ii)\nDataModule provides data loaders for training and validation; (iii) both classes are com-\nbined using the Trainer class, which allows us to train models on a variety of hardware\nplatforms. Most code in this book adapts ModuleandDataModule . We will touch upon\ntheTrainer class only when we discuss GPUs, CPUs, parallel training, and optimization\nalgorithms.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2360, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf7bbc38-4df6-45d5-835b-2989e2d1c95f": {"__data__": {"id_": "cf7bbc38-4df6-45d5-835b-2989e2d1c95f", "embedding": null, "metadata": {"page_label": "94", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0790221-ca80-4f64-b6ea-69e125aa6c0c", "node_type": "4", "metadata": {"page_label": "94", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "07e37db990f1a1ca89758d97a8f1a8cf1cfcc3def23e6f62a17ff82bbec2b1e2", "class_name": "RelatedNodeInfo"}}, "text": "94 Linear Neural Networks for Regression\nimport time\nimport numpy asnp\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n3.2.1Utilities\nWeneedafewutilitiestosimplifyobject-orientedprogramminginJupyternotebooks. One\nof the challenges is that class definitions tend to be fairly long blocks of code. Notebook\nreadability demands short code fragments, interspersed with explanations, a requirement\nincompatible with the style of programming common for Python libraries. The first utility\nfunctionallowsustoregisterfunctionsasmethodsinaclass aftertheclasshasbeencreated.\nIn fact, we can do so evenafter we have created instances of the class! It allows us to split\nthe implementation of a class into multiple code blocks.\ndef add_to_class (Class): #@save\n\"\"\"Register functions as methods in created class.\"\"\"\ndef wrapper (obj):\nsetattr (Class, obj .__name__ , obj)\nreturn wrapper\nLet\u2019s have a quick look at how to use it. We plan to implement a class Awith a method do.\nInstead of having code for both Aanddoin the same code block, we can first declare the\nclass Aand create an instance a.\nclass A:\ndef __init__ (self ):\nself .b=1\na=A()\nNext we define the method doas we normally would, but not in class A\u2019s scope. Instead,\nwe decorate this method by add_to_class with class Aas its argument. In doing so, the\nmethod is able to access the member variables of Ajust as we would expect had it been\nincludedaspartof A\u2019sdefinition. Let\u2019sseewhathappenswhenweinvokeitfortheinstance\na.\n@add_to_class (A)\ndef do(self ):\nprint ('Class attribute \"b\"is',self .b)\na.do()\nClass attribute \"b\"is1\nThe second one is a utility class that saves all arguments in a class\u2019s __init__ method", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1677, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9978a6f-0c1a-4705-8679-7207ffec10d9": {"__data__": {"id_": "e9978a6f-0c1a-4705-8679-7207ffec10d9", "embedding": null, "metadata": {"page_label": "95", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9cb6792-fd5e-4a72-8726-52d9d4173f2b", "node_type": "4", "metadata": {"page_label": "95", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "03f7d432ed69b0f432dd3d62cf69d70a5a33480d1512d652e793c159cbd572fe", "class_name": "RelatedNodeInfo"}}, "text": "95 Object-Oriented Design for Implementation\n71as class attributes. This allows us to extend constructor call signatures implicitly without\nadditional code.\nclass HyperParameters :#@save\n\"\"\"The base class of hyperparameters.\"\"\"\ndef save_hyperparameters (self , ignore =[]):\nraise NotImplemented\nWe defer its implementation into Section B.7 . To use it, we define our class that inherits\nfrom HyperParameters andcalls save_hyperparameters inthe __init__ method.\n# Call the fully implemented HyperParameters class saved in d2l\nclass B(d2l .HyperParameters):\ndef __init__ (self , a, b, c):\nself .save_hyperparameters(ignore =['c'])\nprint ('self.a = ',self .a,'self.b = ',self .b)\nprint ('There is no self.c = ',not hasattr (self ,'c'))\nb=B(a=1, b=2, c=3)\nself .a=1self .b=2\nThere isnoself .c=True\nThe final utility allows us to plot experiment progress interactively while it is going on.\nIn deference to the much more powerful (and complex) TensorBoard71we name it Pro-\ngressBoard . The implementation is deferred to Section B.7 . For now, let\u2019s simply see it\nin action.\nThedrawmethod plots a point (x, y) in the figure, with labelspecified in the legend.\nThe optional every_n smooths the line by only showing 1\u009d\ud835\udc5bpoints in the figure. Their\nvalues are averaged from the \ud835\udc5bneighbor points in the original figure.\nclass ProgressBoard (d2l .HyperParameters): #@save\n\"\"\"The board that plots data points in animation.\"\"\"\ndef __init__ (self , xlabel =None , ylabel =None , xlim =None ,\nylim =None , xscale ='linear ', yscale ='linear ',\nls=['-','--','-.',':'], colors =['C0','C1','C2','C3'],\nfig=None , axes =None , figsize =(3.5,2.5), display =True ):\nself .save_hyperparameters()\ndef draw (self , x, y, label, every_n =1):\nraise NotImplemented\nInthefollowingexample,wedraw sinandcoswithadifferentsmoothness. Ifyourunthis\ncode block, you will see the lines grow in animation.\nboard =d2l.ProgressBoard( 'x')\nfor xinnp.arange( 0,10,0.1):\nboard .draw(x, np .sin(x), 'sin', every_n =2)\nboard .draw(x, np .cos(x), 'cos', every_n =10)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc6804f9-92c5-43d0-beb2-2fb8bca17404": {"__data__": {"id_": "bc6804f9-92c5-43d0-beb2-2fb8bca17404", "embedding": null, "metadata": {"page_label": "96", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ceb790f-eaa2-4f07-bd38-bb4386e9833b", "node_type": "4", "metadata": {"page_label": "96", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e4eb9532108080deb81cd2d363392ff8d3478f4a98f465c52db50338db821584", "class_name": "RelatedNodeInfo"}}, "text": "96 Linear Neural Networks for Regression\n3.2.2Models\nThe Module class is the base class of all models we will implement. At the very least\nwe need three methods. The first, __init__ , stores the learnable parameters, the train-\ning_step methodacceptsadatabatchtoreturnthelossvalue,andfinally, configure_optimizers\nreturns the optimization method, or a list of them, that is used to update the learnable pa-\nrameters. Optionally we can define validation_step to report the evaluation measures.\nSometimes we put the code for computing the output into a separate forward method to\nmake it more reusable.\nclass Module (nn.Module, d2l .HyperParameters): #@save\n\"\"\"The base class of models.\"\"\"\ndef __init__ (self , plot_train_per_epoch =2, plot_valid_per_epoch =1):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .board =ProgressBoard()\ndef loss (self , y_hat, y):\nraise NotImplementedError\ndef forward (self , X):\nassert hasattr (self ,'net'),'Neural network is defined '\nreturn self .net(X)\ndef plot (self , key, value, train):\n\"\"\"Plot a point in animation.\"\"\"\nassert hasattr (self ,'trainer '),'Trainer is not inited '\nself .board .xlabel ='epoch '\niftrain:\nx=self .trainer .train_batch_idx /\\\nself .trainer .num_train_batches\nn=self .trainer .num_train_batches /\\\nself .plot_train_per_epoch\nelse :\nx=self .trainer .epoch +1\nn=self .trainer .num_val_batches /\\\nself .plot_valid_per_epoch\nself .board .draw(x, value .to(d2l .cpu()) .detach() .numpy(),\n('train_ 'iftrain else 'val_ ')+key,\nevery_n =int(n))\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1534, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86665916-c47c-43a5-b17b-6b5987b7e8a6": {"__data__": {"id_": "86665916-c47c-43a5-b17b-6b5987b7e8a6", "embedding": null, "metadata": {"page_label": "97", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e6ee11b-09c9-4283-beb3-dab424a3b7e5", "node_type": "4", "metadata": {"page_label": "97", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f74d86942e980f90baf6cff6bdabb6ecc00a942780aab69723271bc63a54d8c0", "class_name": "RelatedNodeInfo"}}, "text": "97 Object-Oriented Design for Implementation\n(continued from previous page)\ndef training_step (self , batch):\nl=self .loss( self (*batch[: -1]), batch[ -1])\nself .plot( 'loss ', l, train =True )\nreturn l\ndef validation_step (self , batch):\nl=self .loss( self (*batch[: -1]), batch[ -1])\nself .plot( 'loss ', l, train =False )\ndef configure_optimizers (self ):\nraise NotImplementedError\nYou may notice that Moduleis a subclass of nn.Module , the base class of neural networks\nin PyTorch. It provides convenient features for handling neural networks. For example, if\nwe define a forward method, such as forward(self, X) , then for an instance awe can\ninvoke this method by a(X). This works since it calls the forward method in the built-in\n__call__ method. You can find more details and examples about nn.Module inSection\n6.1.\n3.2.3Data\nTheDataModule class is the base class for data. Quite frequently the __init__ method is\nused to prepare the data. This includes downloading and preprocessing if needed. The\ntrain_dataloader returns the data loader for the training dataset. A data loader is a\n(Python) generator that yields a data batch each time it is used. This batch is then fed\ninto the training_step method of Module to compute the loss. There is an optional\nval_dataloader to return the validation dataset loader. It behaves in the same manner,\nexcept that it yields data batches for the validation_step method in Module.\nclass DataModule (d2l .HyperParameters): #@save\n\"\"\"The base class of data.\"\"\"\ndef __init__ (self , root ='../data ', num_workers =4):\nself .save_hyperparameters()\ndef get_dataloader (self , train):\nraise NotImplementedError\ndef train_dataloader (self ):\nreturn self .get_dataloader(train =True )\ndef val_dataloader (self ):\nreturn self .get_dataloader(train =False )\n3.2.4Training\nTheTrainer class trains the learnable parameters in the Moduleclass with data specified\ninDataModule . The key method is fit, which accepts two arguments: model, an instance\nofModule, and data, an instance of DataModule . It then iterates over the entire dataset", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2072, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b634dba-961f-4f7b-a842-4c67be2401c0": {"__data__": {"id_": "4b634dba-961f-4f7b-a842-4c67be2401c0", "embedding": null, "metadata": {"page_label": "98", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "569b404b-52eb-45bc-8e55-42687ac31a94", "node_type": "4", "metadata": {"page_label": "98", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e72b28cac7c88b159453227a4e17e7070996c23e571f913d32ee077f7d76e624", "class_name": "RelatedNodeInfo"}}, "text": "98 Linear Neural Networks for Regression\n72\n73max_epochs times to train the model. As before, we will defer the implementation of this\nmethod to later chapters.\nclass Trainer (d2l .HyperParameters): #@save\n\"\"\"The base class for training models with data.\"\"\"\ndef __init__ (self , max_epochs, num_gpus =0, gradient_clip_val =0):\nself .save_hyperparameters()\nassert num_gpus ==0,'No GPU support yet '\ndef prepare_data (self , data):\nself .train_dataloader =data .train_dataloader()\nself .val_dataloader =data .val_dataloader()\nself .num_train_batches =len(self .train_dataloader)\nself .num_val_batches =(len(self .val_dataloader)\nifself .val_dataloader isnot None else 0)\ndef prepare_model (self , model):\nmodel .trainer =self\nmodel .board .xlim =[0,self .max_epochs]\nself .model =model\ndef fit(self , model, data):\nself .prepare_data(data)\nself .prepare_model(model)\nself .optim =model .configure_optimizers()\nself .epoch =0\nself .train_batch_idx =0\nself .val_batch_idx =0\nfor self .epoch inrange (self .max_epochs):\nself .fit_epoch()\ndef fit_epoch (self ):\nraise NotImplementedError\n3.2.5Summary\nTo highlight the object-oriented design for our future deep learning implementation, the\nabove classes simply show how their objects store data and interact with each other. We\nwill keep enriching implementations of these classes, such as via @add_to_class , in the\nrest of the book. Moreover, these fully implemented classes are saved in the D2L library72\n, alightweighttoolkit that makes structured modeling for deep learning easy. In particular,\nit facilitates reusing many components between projects without changing much at all. For\ninstance, we can replace just the optimizer, just the model, just the dataset, etc.; this degree\nof modularity pays dividends throughout the book in terms of conciseness and simplicity\n(this is why we added it) and it can do the same for your own projects.\n3.2.6Exercises\n1.Locate full implementations of the above classes that are saved in the D2L library73\n. We strongly recommend that you look at the implementation in detail once you have\ngained some more familiarity with deep learning modeling.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2134, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89c4e52b-dc97-4b33-990b-32116886e6e7": {"__data__": {"id_": "89c4e52b-dc97-4b33-990b-32116886e6e7", "embedding": null, "metadata": {"page_label": "99", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6be64de5-ba13-4db8-9654-11e5f88ef360", "node_type": "4", "metadata": {"page_label": "99", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f82645c247c2a56e41d3ef500bf4d55ecded316123fbec2478fe50152012691b", "class_name": "RelatedNodeInfo"}}, "text": "99 Synthetic Regression Data\n742.Removethe save_hyperparameters statementinthe Bclass. Canyoustillprint self.a\nandself.b? Optional: if you have dived into the full implementation of the HyperPa-\nrameters class, can you explain why?\nDiscussions74.\n3.3SyntheticRegressionData\nMachine learning is all about extracting information from data. So you might wonder,\nwhat could we possibly learn from synthetic data? While we might not care intrinsically\nabout the patterns that we ourselves baked into an artificial data generating model, such\ndatasets are nevertheless useful for didactic purposes, helping us to evaluate the properties\nof our learning algorithms and to confirm that our implementations work as expected. For\nexample, if we create data for which the correct parameters are known a priori, then we\ncan check that our model can in fact recover them.\n%matplotlib inline\nimport random\nimport torch\nfrom d2l import torch asd2l\n3.3.1Generating the Dataset\nFor this example, we will work in low dimension for succinctness. The following code\nsnippet generates 1000 examples with 2-dimensional features drawn from a standard nor-\nmaldistribution. Theresultingdesignmatrix Xbelongsto R1000\u00022. Wegenerateeachlabel\nby applying a groundtruth linear function, corrupting them via additive noise \ud835\udf50, drawn in-\ndependently and identically for each example:\ny=Xw\u00b8\ud835\udc4f\u00b8\ud835\udf50. (3.3.1)\nFor convenience we assume that \ud835\udf50is drawn from a normal distribution with mean \ud835\udf07=0\nand standard deviation \ud835\udf0e=0.01. Note that for object-oriented design we add the code to\nthe__init__ methodofasubclassof d2l.DataModule (introducedin Section3.2.3 ). Itis\ngood practice to allow the setting of any additional hyperparameters. We accomplish this\nwith save_hyperparameters() . The batch_size will be determined later.\nclass SyntheticRegressionData (d2l .DataModule): #@save\n\"\"\"Synthetic data for linear regression.\"\"\"\ndef __init__ (self , w, b, noise =0.01 , num_train =1000 , num_val =1000 ,\nbatch_size =32):\nsuper ().__init__ ()\nself .save_hyperparameters()\nn=num_train +num_val\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2068, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbf8f5f1-bb56-40fa-8ea7-5dc974aca21e": {"__data__": {"id_": "bbf8f5f1-bb56-40fa-8ea7-5dc974aca21e", "embedding": null, "metadata": {"page_label": "100", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1bde34fa-52f4-4540-903a-12463f03952f", "node_type": "4", "metadata": {"page_label": "100", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1a7dbf1ef51d566ee6add7788c3f781cbd98b47e21ff4acf1d33b4585c0129f4", "class_name": "RelatedNodeInfo"}}, "text": "100 Linear Neural Networks for Regression\n(continued from previous page)\nself .X=torch .randn(n, len(w))\nnoise =torch .randn(n, 1)*noise\nself .y=torch .matmul( self .X, w .reshape(( -1,1))) +b+noise\nBelow, we set the true parameters to w=\u00bb2,\u00003.4\u00bc>and\ud835\udc4f=4.2. Later, we can check our\nestimated parameters against these groundtruth values.\ndata =SyntheticRegressionData(w =torch .tensor([ 2,-3.4]), b =4.2)\nEach row in features consists of a vector in R2and each row in labelsis a scalar. Let\u2019s\nhave a look at the first entry.\nprint ('features: ', data .X[0],'\\nlabel: ', data .y[0])\nfeatures: tensor([ 0.9026 ,1.0264 ])\nlabel: tensor([ 2.5148 ])\n3.3.2Readingthe Dataset\nTraining machine learning models often requires multiple passes over a dataset, grabbing\none minibatch of examples at a time. This data is then used to update the model. To\nillustrate how this works, we implement the get_dataloader method, registering it in\ntheSyntheticRegressionData class via add_to_class (introduced in Section 3.2.1 ). It\ntakesa batchsize, a matrix offeatures, anda vectorof labels, and generatesminibatchesof\nsizebatch_size . As such, each minibatch consists of a tuple of features and labels. Note\nthat we need to be mindful of whether we\u2019re in training or validation mode: in the former,\nwewillwanttoreadthedatainrandomorder,whereasforthelatter,beingabletoreaddata\nin a pre-defined order may be important for debugging purposes.\n@d2l .add_to_class(SyntheticRegressionData)\ndef get_dataloader (self , train):\niftrain:\nindices =list (range (0,self .num_train))\n# The examples are read in random order\nrandom .shuffle(indices)\nelse :\nindices =list (range (self .num_train, self .num_train +self .num_val))\nfor iinrange (0,len(indices), self .batch_size):\nbatch_indices =torch .tensor(indices[i: i +self .batch_size])\nyield self .X[batch_indices], self .y[batch_indices]\nTo build some intuition, let\u2019s inspect the first minibatch of data. Each minibatch of fea-\ntures provides us with both its size and the dimensionality of input features. Likewise, our\nminibatch of labels will have a matching shape given by batch_size .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0aa3d46-2046-42af-85c6-94ca5e8e19f6": {"__data__": {"id_": "a0aa3d46-2046-42af-85c6-94ca5e8e19f6", "embedding": null, "metadata": {"page_label": "101", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4c1b0ff-79d1-4310-953d-e4bf3b9c6b66", "node_type": "4", "metadata": {"page_label": "101", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "94280836e0177c86b6468e70b25298c5349acef7765ffd6ead859e94e7a5ebec", "class_name": "RelatedNodeInfo"}}, "text": "101 Synthetic Regression Data\nX, y =next (iter (data .train_dataloader()))\nprint ('X shape: ', X.shape, '\\ny shape: ', y.shape)\nX shape: torch .Size([ 32,2])\ny shape: torch .Size([ 32,1])\nWhile seemingly innocuous, the invocation of iter(data.train_dataloader()) illus-\ntrates the power of Python\u2019s object-oriented design. Note that we added a method to the\nSyntheticRegressionData classaftercreating the dataobject. Nonetheless, the object\nbenefits from the ex postfacto addition of functionality to the class.\nThroughout the iteration we obtain distinct minibatches until the entire dataset has been\nexhausted (try this). While the iteration implemented above is good for didactic purposes,\nit is inefficient in ways that might get us into trouble with real problems. For example, it\nrequires that we load all the data in memory and that we perform lots of random memory\naccess. The built-in iterators implemented in a deep learning framework are considerably\nmore efficient and they can deal with sources such as data stored in files, data received via\na stream, and data generated or processed on the fly. Next let\u2019s try to implement the same\nmethod using built-in iterators.\n3.3.3Concise Implementation of the Data Loader\nRather than writing our own iterator, we can call the existing API in a framework to load\ndata. As before, we need a dataset with features Xand labels y. Beyond that, we set\nbatch_size in the built-in data loader and let it take care of shuffling examples effi-\nciently.\n@d2l .add_to_class(d2l .DataModule) #@save\ndef get_tensorloader (self , tensors, train, indices =slice (0,None )):\ntensors =tuple (a[indices] for aintensors)\ndataset =torch .utils .data .TensorDataset( *tensors)\nreturn torch .utils .data .DataLoader(dataset, self .batch_size,\nshuffle =train)\n@d2l .add_to_class(SyntheticRegressionData) #@save\ndef get_dataloader (self , train):\ni=slice (0,self .num_train) iftrain else slice (self .num_train, None )\nreturn self .get_tensorloader(( self .X,self .y), train, i)\nThe new data loader behaves just like the previous one, except that it is more efficient and\nhas some added functionality.\nX, y =next (iter (data .train_dataloader()))\nprint ('X shape: ', X.shape, '\\ny shape: ', y.shape)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eafde4f0-0216-4015-ad84-bd56047bc7d5": {"__data__": {"id_": "eafde4f0-0216-4015-ad84-bd56047bc7d5", "embedding": null, "metadata": {"page_label": "102", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56dde7c9-f983-4dcf-86f0-c60ad35facb4", "node_type": "4", "metadata": {"page_label": "102", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4f1066da63c9d25beb7db6ffa5039320e9d5b1ac9875d410e63a578de98894e1", "class_name": "RelatedNodeInfo"}}, "text": "102 Linear Neural Networks for Regression\n75\n76X shape: torch .Size([ 32,2])\ny shape: torch .Size([ 32,1])\nFor instance, the data loader provided by the framework API supports the built-in __len__\nmethod, so we can query its length, i.e., the number of batches.\nlen(data .train_dataloader())\n32\n3.3.4Summary\nData loaders are a convenient way of abstracting out the process of loading and manipu-\nlating data. This way the same machine learning algorithm is capable of processing many\ndifferenttypesandsourcesofdatawithouttheneedformodification. Oneofthenicethings\nabout data loaders is that they can be composed. For instance, we might be loading images\nand then have a postprocessing filter that crops them or modifies them in other ways. As\nsuch, data loaders can be used to describe an entire data processing pipeline.\nAs for the model itself, the two-dimensional linear model is about the simplest we might\nencounter. It lets us test out the accuracy of regression models without worrying about\nhaving insufficient amounts of data or an underdetermined system of equations. We will\nput this to good use in the next section.\n3.3.5Exercises\n1.What will happen if the number of examples cannot be divided by the batch size. How\nwould you change this behavior by specifying a different argument by using the frame-\nwork\u2019s API?\n2.Suppose that we want to generate a huge dataset, where both the size of the parameter\nvector wand the number of examples num_examples are large.\n1.What happens if we cannot hold all data in memory?\n2.Howwouldyoushufflethedataifitisheldondisk? Yourtaskistodesignan e\ufb00icient\nalgorithm that does not require too many random reads or writes. Hint: pseudoran-\ndom permutation generators75allow you to design a reshuffle without the need to\nstore the permutation table explicitly ( Naor and Reingold, 1999 ).\n3.Implement a data generator that produces new data on the fly, every time the iterator is\ncalled.\n4.How would you design a random data generator that generates thesame data each time\nit is called?\nDiscussions76.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1dfb7ead-0bb3-4f09-9088-e6dc45de257c": {"__data__": {"id_": "1dfb7ead-0bb3-4f09-9088-e6dc45de257c", "embedding": null, "metadata": {"page_label": "103", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "971983bf-9542-4e21-8260-63bff691c029", "node_type": "4", "metadata": {"page_label": "103", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c66c62e367edffcd92ebb9094722d511981c3eaf640a016a6ef86ffeb067a61a", "class_name": "RelatedNodeInfo"}}, "text": "103 Linear Regression Implementation from Scratch\n3.4LinearRegressionImplementationfromScratch\nWearenowreadytoworkthroughafullyfunctioningimplementationoflinearregression.\nIn this section, we will implement the entire method from scratch, including (i) the model;\n(ii) the loss function; (iii) a minibatch stochastic gradient descent optimizer; and (iv) the\ntrainingfunctionthatstitchesallofthesepiecestogether. Finally, wewillrunoursynthetic\ndatageneratorfrom Section3.3 andapplyourmodelontheresultingdataset. Whilemodern\ndeep learning frameworks can automate nearly all of this work, implementing things from\nscratch is the only way to make sure that you really know what you are doing. Moreover,\nwhenitistimetocustomizemodels, definingourownlayersorlossfunctions, understand-\ning how things work under the hood will prove handy. In this section, we will rely only\non tensors and automatic differentiation. Later, we will introduce a more concise imple-\nmentation, taking advantage of the bells and whistles of deep learning frameworks while\nretaining the structure of what follows below.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\n3.4.1Definingthe Model\nBefore we can begin optimizing our model\u2019s parameters by minibatch SGD, we need to\nhave some parameters in the first place. In the following we initialize weights by drawing\nrandom numbers from a normal distribution with mean 0 and a standard deviation of 0.01.\nThe magic number 0.01 often works well in practice, but you can specify a different value\nthrough the argument sigma. Moreover we set the bias to 0. Note that for object-oriented\ndesign we add the code to the __init__ method of a subclass of d2l.Module (introduced\ninSection 3.2.2 ).\nclass LinearRegressionScratch (d2l .Module): #@save\n\"\"\"The linear regression model implemented from scratch.\"\"\"\ndef __init__ (self , num_inputs, lr, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .w=torch .normal( 0, sigma, (num_inputs, 1), requires_grad =True )\nself .b=torch .zeros( 1, requires_grad =True )\nNext we must define our model, relating its input and parameters to its output. Using the\nsame notation as (3.1.4 )for our linear model we simply take the matrix\u2013vector product of\nthe input features Xand the model weights w, and add the offset \ud835\udc4fto each example. The\nproduct Xwis a vector and \ud835\udc4fis a scalar. Because of the broadcasting mechanism (see\nSection2.1.4 ), whenweaddavectorandascalar, thescalarisaddedtoeachcomponentof\nthe vector. The resulting forward method is registered in the LinearRegressionScratch\nclass via add_to_class (introduced in Section 3.2.1 ).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96d906e9-1754-4878-85b7-63c17c3ad108": {"__data__": {"id_": "96d906e9-1754-4878-85b7-63c17c3ad108", "embedding": null, "metadata": {"page_label": "104", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5cdc269-d563-481b-8d71-1a0cfcfb3e0f", "node_type": "4", "metadata": {"page_label": "104", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "32398bb07d51015cce033576f5e33a104c5dd071c80d8ae5f5ef44f266ab6711", "class_name": "RelatedNodeInfo"}}, "text": "104 Linear Neural Networks for Regression\n@d2l .add_to_class(LinearRegressionScratch) #@save\ndef forward (self , X):\nreturn torch .matmul(X, self .w)+self .b\n3.4.2Defining the Loss Function\nSince updating our model requires taking the gradient of our loss function, we ought to\ndefine the loss function first. Here we use the squared loss function in (3.1.5 ). In the\nimplementation, we need to transform the true value yinto the predicted value\u2019s shape\ny_hat. Theresultreturnedbythefollowingmethodwillalsohavethesameshapeas y_hat.\nWe also return the averaged loss value among all examples in the minibatch.\n@d2l .add_to_class(LinearRegressionScratch) #@save\ndef loss (self , y_hat, y):\nl=(y_hat -y)**2/2\nreturn l.mean()\n3.4.3Definingthe OptimizationAlgorithm\nAs discussed in Section 3.1 , linear regression has a closed-form solution. However, our\ngoal here is to illustrate how to train more general neural networks, and that requires that\nwe teach you how to use minibatch SGD. Hence we will take this opportunity to introduce\nyour first working example of SGD. At each step, using a minibatch randomly drawn from\nour dataset, we estimate the gradient of the loss with respect to the parameters. Next, we\nupdate the parameters in the direction that may reduce the loss.\nThe following code applies the update, given a set of parameters, a learning rate lr. Since\nourlossiscomputedasanaverageovertheminibatch,wedonotneedtoadjustthelearning\nrate against the batch size. In later chapters we will investigate how learning rates should\nbe adjusted for very large minibatches as they arise in distributed large-scale learning. For\nnow, we can ignore this dependency.\nWedefineour SGDclass,asubclassof d2l.HyperParameters (introducedin Section3.2.1 ),\nto have a similar API as the built-in SGD optimizer. We update the parameters in the step\nmethod. The zero_grad method sets all gradients to 0, which must be run before a back-\npropagation step.\nclass SGD(d2l .HyperParameters): #@save\n\"\"\"Minibatch stochastic gradient descent.\"\"\"\ndef __init__ (self , params, lr):\nself .save_hyperparameters()\ndef step (self ):\nfor param inself .params:\nparam -=self .lr*param .grad\ndef zero_grad (self ):\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "319c6702-0378-4b39-a163-0b5f428d362f": {"__data__": {"id_": "319c6702-0378-4b39-a163-0b5f428d362f", "embedding": null, "metadata": {"page_label": "105", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1e6b0a2-cdda-4bc3-86e0-e204b8181237", "node_type": "4", "metadata": {"page_label": "105", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "61248c59140b6c9454b5e1adaefa74976f4a355a32fa1ca411ebd04e0cf15132", "class_name": "RelatedNodeInfo"}}, "text": "105 Linear Regression Implementation from Scratch\n(continued from previous page)\nfor param inself .params:\nifparam .grad isnot None :\nparam .grad .zero_()\nWe next define the configure_optimizers method, which returns an instance of the SGD\nclass.\n@d2l .add_to_class(LinearRegressionScratch) #@save\ndef configure_optimizers (self ):\nreturn SGD([ self .w,self .b], self .lr)\n3.4.4Training\nNowthatwehaveallofthepartsinplace(parameters,lossfunction,model,andoptimizer),\nwe are ready to implement the main training loop. It is crucial that you understand this\ncodefullysinceyouwillemploysimilartrainingloopsforeveryotherdeeplearningmodel\ncovered in this book. In each epoch, we iterate through the entire training dataset, passing\nonce through every example (assuming that the number of examples is divisible by the\nbatch size). In each iteration , we grab a minibatch of training examples, and compute its\nloss through the model\u2019s training_step method. Then we compute the gradients with\nrespect to each parameter. Finally, we will call the optimization algorithm to update the\nmodel parameters. In summary, we will execute the following loop:\n\u000fInitialize parameters \u00b9w,\ud835\udc4f\u00ba\n\u000fRepeat until done\n\u2013Compute gradient g \ud835\udf15\u00b9w,\ud835\udc4f\u00ba1\njBj\u00cd\n\ud835\udc562B\ud835\udc59\u00b9x\u00b9\ud835\udc56\u00ba,\ud835\udc66\u00b9\ud835\udc56\u00ba,w,\ud835\udc4f\u00ba\n\u2013Update parameters \u00b9w,\ud835\udc4f\u00ba \u00b9w,\ud835\udc4f\u00ba\u0000\ud835\udf02g\nRecallthatthesyntheticregressiondatasetthatwegeneratedin Section3.3 doesnotprovide\na validation dataset. In most cases, however, we will want a validation dataset to measure\nour model quality. Here we pass the validation dataloader once in each epoch to mea-\nsure the model performance. Following our object-oriented design, the prepare_batch\nandfit_epoch methods are registered in the d2l.Trainer class (introduced in Section\n3.2.4).\n@d2l .add_to_class(d2l .Trainer) #@save\ndef prepare_batch (self , batch):\nreturn batch\n@d2l .add_to_class(d2l .Trainer) #@save\ndef fit_epoch (self ):\nself .model .train()\nfor batch inself .train_dataloader:\nloss =self .model .training_step( self .prepare_batch(batch))\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42271eb6-dca6-4e99-acc4-d80224cadcb7": {"__data__": {"id_": "42271eb6-dca6-4e99-acc4-d80224cadcb7", "embedding": null, "metadata": {"page_label": "106", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84050f23-07dc-4fe5-a089-277cd68a87bc", "node_type": "4", "metadata": {"page_label": "106", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "718844ebf9d6c48fe8400b6bff6e6939e5599d8fdc330402761a32e578e86b50", "class_name": "RelatedNodeInfo"}}, "text": "106 Linear Neural Networks for Regression\n(continued from previous page)\nself .optim .zero_grad()\nwith torch .no_grad():\nloss .backward()\nifself .gradient_clip_val >0:# To be discussed later\nself .clip_gradients( self .gradient_clip_val, self .model)\nself .optim .step()\nself .train_batch_idx +=1\nifself .val_dataloader isNone :\nreturn\nself .model .eval()\nfor batch inself .val_dataloader:\nwith torch .no_grad():\nself .model .validation_step( self .prepare_batch(batch))\nself .val_batch_idx +=1\nWe are almost ready to train the model, but first we need some training data. Here we use\ntheSyntheticRegressionData class and pass in some ground truth parameters. Then\nwe train our model with the learning rate lr=0.03 and set max_epochs=3 . Note that in\ngeneral, both the number of epochs and the learning rate are hyperparameters. In general,\nsetting hyperparameters is tricky and we will usually want to use a three-way split, one\nset for training, a second for hyperparameter selection, and the third reserved for the final\nevaluation. We elide these details for now but will revise them later.\nmodel =LinearRegressionScratch( 2, lr =0.03 )\ndata =d2l.SyntheticRegressionData(w =torch .tensor([ 2,-3.4]), b =4.2)\ntrainer =d2l.Trainer(max_epochs =3)\ntrainer .fit(model, data)\nBecause we synthesized the dataset ourselves, we know precisely what the true parameters\nare. Thus, we can evaluate our success in training by comparing the true parameters with\nthose that we learned through our training loop. Indeed they turn out to be very close to\neach other.\nwith torch .no_grad():\nprint (f'error in estimating w: {data .w-model .w.reshape(data .w.shape) }')\nprint (f'error in estimating b: {data .b-model .b}')", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1706, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3add66e8-617f-4a99-b0e8-7c1d93cbecac": {"__data__": {"id_": "3add66e8-617f-4a99-b0e8-7c1d93cbecac", "embedding": null, "metadata": {"page_label": "107", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99126150-4bad-4350-8ce9-920e7cbd7884", "node_type": "4", "metadata": {"page_label": "107", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "25c617a74baa54fc4c8641685d701d68949db0706333db1cdfc583d9e22aad1e", "class_name": "RelatedNodeInfo"}}, "text": "107 Linear Regression Implementation from Scratch\n77\n78error inestimating w: tensor([ 0.1408 ,-0.1493 ])\nerror inestimating b: tensor([ 0.2130 ])\nWe should not take the ability to exactly recover the ground truth parameters for granted.\nIn general, for deep models unique solutions for the parameters do not exist, and even\nfor linear models, exactly recovering the parameters is only possible when no feature is\nlinearlydependentontheothers. However,inmachinelearning,weareoftenlessconcerned\nwith recovering true underlying parameters, but rather with parameters that lead to highly\naccurate prediction ( Vapnik, 1992 ). Fortunately, even on difficult optimization problems,\nstochastic gradient descent can often find remarkably good solutions, owing partly to the\nfact that, for deep networks, there exist many configurations of the parameters that lead to\nhighly accurate prediction.\n3.4.5Summary\nIn this section, we took a significant step towards designing deep learning systems by im-\nplementing a fully functional neural network model and training loop. In this process, we\nbuilt a data loader, a model, a loss function, an optimization procedure, and a visualization\nand monitoring tool. We did this by composing a Python object that contains all relevant\ncomponentsfortrainingamodel. Whilethisisnotyetaprofessional-gradeimplementation\nit is perfectly functional and code like this could already help you to solve small problems\nquickly. In the coming sections, we will see how to do this both more concisely (avoiding\nboilerplate code) and moree\ufb00iciently (using our GPUs to their full potential).\n3.4.6Exercises\n1.What would happen if we were to initialize the weights to zero. Would the algorithm\nstill work? What if we initialized the parameters with variance 1000rather than 0.01?\n2.Assume that you are Georg Simon Ohm77trying to come up with a model for resis-\ntancethatrelatesvoltageandcurrent. Canyouuseautomaticdifferentiationtolearnthe\nparameters of your model?\n3.Can you use Planck\u2019s Law78to determine the temperature of an object using spectral\nenergy density? For reference, the spectral density \ud835\udc35of radiation emanating from a\nblack body is \ud835\udc35\u00b9\ud835\udf06,\ud835\udc47\u00ba=2\u210e\ud835\udc502\n\ud835\udf065\u0001\u0010\nexp\u210e\ud835\udc50\n\ud835\udf06\ud835\udc58\ud835\udc47\u00001\u0011\u00001\n. Here\ud835\udf06is the wavelength, \ud835\udc47is the\ntemperature, \ud835\udc50is the speed of light, \u210eis Planck\u2019s constant, and \ud835\udc58is the Boltzmann\nconstant. You measure the energy for different wavelengths \ud835\udf06and you now need to fit\nthe spectral density curve to Planck\u2019s law.\n4.Whataretheproblemsyoumightencounterifyouwantedtocomputethesecondderiva-\ntives of the loss? How would you fix them?\n5.Why is the reshape method needed in the lossfunction?\n6.Experimentusingdifferentlearningratestofindouthowquicklythelossfunctionvalue\ndrops. Can you reduce the error by increasing the number of epochs of training?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0cee30a-50d7-4f98-9e40-756259482c1f": {"__data__": {"id_": "f0cee30a-50d7-4f98-9e40-756259482c1f", "embedding": null, "metadata": {"page_label": "108", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75c67326-a331-4e8d-8832-f162f63b444e", "node_type": "4", "metadata": {"page_label": "108", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5f5b8f08a9d5c6437dcd75e4737a8994660da043bc64acec9c5cef3a67ca6bb3", "class_name": "RelatedNodeInfo"}}, "text": "108 Linear Neural Networks for Regression\n797.Ifthenumberofexamplescannotbedividedbythebatchsize,whathappensto data_iter\nat the end of an epoch?\n8.Try implementing a different loss function, such as the absolute value loss (y_hat -\nd2l.reshape(y, y_hat.shape)).abs().sum() .\n1.Check what happens for regular data.\n2.Check whether there is a difference in behavior if you actively perturb some entries,\nsuch as\ud835\udc665=10000, ofy.\n3.Can you think of a cheap solution for combining the best aspects of squared loss and\nabsolute value loss? Hint: how can you avoid really large gradient values?\n9.Why do we need to reshuffle the dataset? Can you design a case where a maliciously\nconstructed dataset would break the optimization algorithm otherwise?\nDiscussions79.\n3.5ConciseImplementation of Linear Regression\nDeep learning has witnessed a sort of Cambrian explosion over the past decade. The sheer\nnumber of techniques, applications and algorithms by far surpasses the progress of pre-\nvious decades. This is due to a fortuitous combination of multiple factors, one of which\nis the powerful free tools offered by a number of open-source deep learning frameworks.\nTheano ( Bergstraet al., 2010), DistBelief ( Deanet al., 2012), and Caffe ( Jiaet al., 2014)\narguably represent the first generation of such models that found widespread adoption.\nIn contrast to earlier (seminal) works like SN2 (Simulateur Neuristique) ( Bottou and Le\nCun, 1988 ), which provided a Lisp-like programming experience, modern frameworks of-\nfer automatic differentiation and the convenience of Python. These frameworks allow us\nto automate and modularize the repetitive work of implementing gradient-based learning\nalgorithms.\nInSection 3.4 , we relied only on (i) tensors for data storage and linear algebra; and (ii)\nautomatic differentiation for calculating gradients. In practice, because data iterators, loss\nfunctions, optimizers, and neural network layers are so common, modern libraries imple-\nment these components for us as well. In this section, we will show you how to implement\nthe linear regression model from Section 3.4 concisely by using high-level APIs of deep\nlearning frameworks.\nimport numpy asnp\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44351a43-2794-4c66-af29-ddb2480e68a9": {"__data__": {"id_": "44351a43-2794-4c66-af29-ddb2480e68a9", "embedding": null, "metadata": {"page_label": "109", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ca6cce4-e72a-444f-ac2a-43deff0205ad", "node_type": "4", "metadata": {"page_label": "109", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c0f36ae1a74357026f0e675fee47f23f8c6f017218df6dbb6cd757248b637297", "class_name": "RelatedNodeInfo"}}, "text": "109 Concise Implementation of Linear Regression\n3.5.1Definingthe Model\nWhenweimplementedlinearregressionfromscratchin Section3.4 , wedefinedourmodel\nparameters explicitly and coded up the calculations to produce output using basic linear\nalgebra operations. You shouldknow how to do this. But once your models get more\ncomplex, and once you have to do this nearly every day, you will be glad of the assistance.\nThe situation is similar to coding up your own blog from scratch. Doing it once or twice\nis rewarding and instructive, but you would be a lousy web developer if you spent a month\nreinventing the wheel.\nFor standard operations, we can use a framework\u2019s predefined layers, which allow us to\nfocus on the layersused to construct the model rather than worrying about their implemen-\ntation. Recall the architecture of a single-layer network as described in Fig. 3.1.2 . The\nlayer is called fully connected , since each of its inputs is connected to each of its outputs\nby means of a matrix\u2013vector multiplication.\nIn PyTorch, the fully connected layer is defined in LinearandLazyLinear classes (avail-\nable since version 1.8.0). The latter allows users to specify merelythe output dimension,\nwhile the former additionally asks for how many inputs go into this layer. Specifying input\nshapes is inconvenient and may require nontrivial calculations (such as in convolutional\nlayers). Thus, for simplicity, we will use such \u201clazy\u201d layers whenever we can.\nclass LinearRegression (d2l .Module): #@save\n\"\"\"The linear regression model implemented with high-level APIs.\"\"\"\ndef __init__ (self , lr):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.LazyLinear( 1)\nself .net.weight .data .normal_( 0,0.01 )\nself .net.bias .data .fill_( 0)\nIn the forward method we just invoke the built-in __call__ method of the predefined\nlayers to compute the outputs.\n@d2l .add_to_class(LinearRegression) #@save\ndef forward (self , X):\nreturn self .net(X)\n3.5.2Defining the Loss Function\nTheMSELoss class computes the mean squared error (without the 1\u009d2factor in (3.1.5 )).\nBy default, MSELoss returns the average loss over examples. It is faster (and easier to use)\nthan implementing our own.\n@d2l .add_to_class(LinearRegression) #@save\ndef loss (self , y_hat, y):\nfn=nn.MSELoss()\nreturn fn(y_hat, y)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2298, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f7b4159-17aa-4ce0-9224-598c462466e1": {"__data__": {"id_": "0f7b4159-17aa-4ce0-9224-598c462466e1", "embedding": null, "metadata": {"page_label": "110", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "82bc8d08-c1d9-4a3c-aa47-14524138935f", "node_type": "4", "metadata": {"page_label": "110", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ea7aa181edb54c12a8bb6bed459afe8d4bf442226e76722d2740241774c11538", "class_name": "RelatedNodeInfo"}}, "text": "110 Linear Neural Networks for Regression\n3.5.3Definingthe OptimizationAlgorithm\nMinibatch SGD is a standard tool for optimizing neural networks and thus PyTorch sup-\nports it alongside a number of variations on this algorithm in the optimmodule. When we\ninstantiate an SGDinstance, we specify the parameters to optimize over, obtainable from\nour model via self.parameters() , and the learning rate ( self.lr ) required by our opti-\nmization algorithm.\n@d2l .add_to_class(LinearRegression) #@save\ndef configure_optimizers (self ):\nreturn torch .optim .SGD( self .parameters(), self .lr)\n3.5.4Training\nYoumighthavenoticedthatexpressingourmodelthroughhigh-levelAPIsofadeeplearn-\ning framework requires fewer lines of code. We did not have to allocate parameters indi-\nvidually, define our loss function, or implement minibatch SGD. Once we start working\nwith much more complex models, the advantages of the high-level API will grow consid-\nerably.\nNow that we have all the basic pieces in place, the training loop itself is the same as the\none we implemented from scratch. So we just call the fitmethod (introduced in Section\n3.2.4),whichreliesontheimplementationofthe fit_epoch methodin Section3.4 ,totrain\nour model.\nmodel =LinearRegression(lr =0.03 )\ndata =d2l.SyntheticRegressionData(w =torch .tensor([ 2,-3.4]), b =4.2)\ntrainer =d2l.Trainer(max_epochs =3)\ntrainer .fit(model, data)\nBelow, we compare the model parameters learned by training on finite data and the actual\nparametersthatgeneratedourdataset. Toaccessparameters,weaccesstheweightsandbias\nof the layer that we need. As in our implementation from scratch, note that our estimated\nparameters are close to their true counterparts.\n@d2l .add_to_class(LinearRegression) #@save\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "036deeaa-89ce-4b84-87ae-4b6aec4c3906": {"__data__": {"id_": "036deeaa-89ce-4b84-87ae-4b6aec4c3906", "embedding": null, "metadata": {"page_label": "111", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2d3d345-8fa6-4374-b0c3-288706de72bc", "node_type": "4", "metadata": {"page_label": "111", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a0e9cd64ca849577e73c54a2e0b932d2dda9becc80b9510908d344d1a5ae9c78", "class_name": "RelatedNodeInfo"}}, "text": "111 Concise Implementation of Linear Regression\n(continued from previous page)\ndef get_w_b (self ):\nreturn (self .net.weight .data, self .net.bias .data)\nw, b =model .get_w_b()\nprint (f'error in estimating w: {data .w-w.reshape(data .w.shape) }')\nprint (f'error in estimating b: {data .b-b}')\nerror inestimating w: tensor([ 0.0094 ,-0.0030 ])\nerror inestimating b: tensor([ 0.0137 ])\n3.5.5Summary\nThis section contains the first implementation of a deep network (in this book) to tap into\nthe conveniences afforded by modern deep learning frameworks, such as MXNet ( Chen\net al., 2015), JAX ( Frostiget al., 2018), PyTorch ( Paszkeet al., 2019), and Tensorflow\n(Abadiet al., 2016). We used framework defaults for loading data, defining a layer, a loss\nfunction,anoptimizerandatrainingloop. Whenevertheframeworkprovidesallnecessary\nfeatures, it is generally a good idea to use them, since the library implementations of these\ncomponentstendtobeheavilyoptimizedforperformanceandproperlytestedforreliability.\nAt the same time, try not to forget that these modules canbe implemented directly. This is\nespeciallyimportantforaspiringresearcherswhowishtoliveontheleadingedgeofmodel\ndevelopment, where you will be inventing new components that cannot possibly exist in\nany current library.\nIn PyTorch, the datamodule provides tools for data processing, the nnmodule defines a\nlargenumberofneuralnetworklayersandcommonlossfunctions. Wecaninitializethepa-\nrametersbyreplacingtheirvalueswithmethodsendingwith _. Notethatweneedtospecify\nthe input dimensions of the network. While this is trivial for now, it can have significant\nknock-on effects when we want to design complex networks with many layers. Careful\nconsiderations of how to parametrize these networks is needed to allow portability.\n3.5.6Exercises\n1.How would you need to change the learning rate if you replace the aggregate loss over\nthe minibatch with an average over the loss on the minibatch?\n2.Review the framework documentation to see which loss functions are provided. In par-\nticular, replace the squared loss with Huber\u2019s robust loss function. That is, use the loss\nfunction\n\ud835\udc59\u00b9\ud835\udc66,\ud835\udc660\u00ba=(\nj\ud835\udc66\u0000\ud835\udc660j\u0000\ud835\udf0e\n2ifj\ud835\udc66\u0000\ud835\udc660j>\ud835\udf0e\n1\n2\ud835\udf0e\u00b9\ud835\udc66\u0000\ud835\udc660\u00ba2otherwise(3.5.1)\n3.How do you access the gradient of the weights of the model?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2004dbb0-5b90-45bc-94cc-60e9002ad618": {"__data__": {"id_": "2004dbb0-5b90-45bc-94cc-60e9002ad618", "embedding": null, "metadata": {"page_label": "112", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "faba9fee-404d-4c59-874f-f57f0d4703d8", "node_type": "4", "metadata": {"page_label": "112", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "72021d015fbbaa11ca40a78b8076e730f8d1b729c17f67e03bc5f9aad97366c8", "class_name": "RelatedNodeInfo"}}, "text": "112 Linear Neural Networks for Regression\n804.What is the effect on the solution if you change the learning rate and the number of\nepochs? Does it keep on improving?\n5.How does the solution change as you vary the amount of data generated?\n1.Plot the estimation error for \u02c6w\u0000wand \u02c6\ud835\udc4f\u0000\ud835\udc4fas a function of the amount of data.\nHint: increase the amount of data logarithmically rather than linearly, i.e., 5, 10, 20,\n50, \u2026, 10,000 rather than 1000, 2000, \u2026, 10,000.\n2.Why is the suggestion in the hint appropriate?\nDiscussions80.\n3.6Generalization\nConsider two college students diligently preparing for their final exam. Commonly, this\npreparation will consist of practicing and testing their abilities by taking exams adminis-\nteredinpreviousyears. Nonetheless,doingwellonpastexamsisnoguaranteethattheywill\nexcelwhenitmatters. Forinstance,imagineastudent,ExtraordinaryEllie,whoseprepara-\ntion consisted entirely of memorizing the answers to previous years\u2019 exam questions. Even\nifElliewereendowedwithanextraordinarymemory,andthuscouldperfectlyrecallthean-\nswer to any previouslyseen question, she might nevertheless freeze when faced with a new\n(previously unseen ) question. By comparison, imagine another student, Inductive Irene,\nwith comparably poor memorization skills, but a knack for picking up patterns. Note that\nif the exam truly consisted of recycled questions from a previous year, Ellie would handily\noutperform Irene. Even if Irene\u2019s inferred patterns yielded 90% accurate predictions, they\ncouldnevercompetewithEllie\u2019s100%recall. However,eveniftheexamconsistedentirely\nof fresh questions, Irene might maintain her 90% average.\nAsmachinelearningscientists,ourgoalistodiscover patterns. Buthowcanwebesurethat\nwehavetrulydiscovereda generalpatternandnotsimplymemorizedourdata? Mostofthe\ntime, our predictions are only useful if our model discovers such a pattern. We do not want\nto predict yesterday\u2019s stock prices, but tomorrow\u2019s. We do not need to recognize already\ndiagnoseddiseasesforpreviouslyseenpatients,butratherpreviouslyundiagnosedailments\nin previously unseen patients. This problem\u2014how to discover patterns that generalize \u2014is\nthe fundamental problem of machine learning, and arguably of all of statistics. We might\ncast this problem as just one slice of a far grander question that engulfs all of science:\nwhen are we ever justified in making the leap from particular observations to more general\nstatements?\nIn real life, we must fit our models using a finite collection of data. The typical scales\nof that data vary wildly across domains. For many important medical problems, we can\nonly access a few thousand data points. When studying rare diseases, we might be lucky to\naccesshundreds. Bycontrast,thelargestpublicdatasetsconsistingoflabeledphotographs,\ne.g., ImageNet( Dengetal., 2009), containmillionsofimages. Andsomeunlabeledimage", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17be11f2-534d-4e92-891f-8a9549880701": {"__data__": {"id_": "17be11f2-534d-4e92-891f-8a9549880701", "embedding": null, "metadata": {"page_label": "113", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "804d66b5-09ea-46e4-9561-6ad0cca74770", "node_type": "4", "metadata": {"page_label": "113", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1dc416d0a1c3d4db54d0472fdbb363b70d99447c133eb47ddb1040413382ac69", "class_name": "RelatedNodeInfo"}}, "text": "113 Generalization\ncollections such as the Flickr YFC100M dataset can be even larger, containing over 100\nmillion images ( Thomeeet al., 2016). However, even at this extreme scale, the number of\navailable data points remains infinitesimally small compared to the space of all possible\nimages at a megapixel resolution. Whenever we work with finite samples, we must keep in\nmind the risk that we might fit our training data, only to discover that we failed to discover\na generalizable pattern.\nThe phenomenon of fitting closer to our training data than to the underlying distribution is\ncalledoverfitting , and techniques for combatting overfitting are often called regularization\nmethods. While it is no substitute for a proper introduction to statistical learning theory\n(see Boucheron etal.(2005), Vapnik ( 1998)), we will give you just enough intuition to get\ngoing. Wewillrevisitgeneralizationinmanychaptersthroughoutthebook,exploringboth\nwhat is known about the principles underlying generalization in various models, and also\nheuristictechniquesthathavebeenfound(empirically)toyieldimprovedgeneralizationon\ntasks of practical interest.\n3.6.1Training Errorand Generalization Error\nIn the standard supervised learning setting, we assume that the training data and the test\ndata are drawn independently fromidentical distributions. This is commonly called the\nIID assumption . While this assumption is strong, it is worth noting that, absent any such\nassumption, we would be dead in the water. Why should we believe that training data\nsampled from distribution \ud835\udc43\u00b9\ud835\udc4b,\ud835\udc4c\u00bashould tell us how to make predictions on test data\ngeneratedbya differentdistribution \ud835\udc44\u00b9\ud835\udc4b,\ud835\udc4c\u00ba? Makingsuchleapsturnsouttorequirestrong\nassumptions about how \ud835\udc43and\ud835\udc44are related. Later on we will discuss some assumptions\nthat allow for shifts in distribution but first we need to understand the IID case, where\n\ud835\udc43\u00b9\u0001\u00ba=\ud835\udc44\u00b9\u0001\u00ba.\nTobeginwith, weneedtodifferentiatebetweenthe trainingerror \ud835\udc45emp, whichisa statistic\ncalculated on the training dataset, and the generalization error \ud835\udc45, which is an expectation\ntaken with respect to the underlying distribution. You can think of the generalization error\nas what you would see if you applied your model to an infinite stream of additional data\nexamples drawn from the same underlying data distribution. Formally the training error is\nexpressed as a sum(with the same notation as Section 3.1 ):\n\ud835\udc45emp\u00bbX,y, \ud835\udc53\u00bc=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc59\u00b9x\u00b9\ud835\udc56\u00ba,\ud835\udc66\u00b9\ud835\udc56\u00ba, \ud835\udc53\u00b9x\u00b9\ud835\udc56\u00ba\u00ba\u00ba, (3.6.1)\nwhile the generalization error is expressed as an integral:\n\ud835\udc45\u00bb\ud835\udc5d, \ud835\udc53\u00bc=\ud835\udc38\u00b9x,\ud835\udc66\u00ba\u0018\ud835\udc43\u00bb\ud835\udc59\u00b9x,\ud835\udc66, \ud835\udc53\u00b9x\u00ba\u00ba\u00bc=\u00b9 \u00b9\n\ud835\udc59\u00b9x,\ud835\udc66, \ud835\udc53\u00b9x\u00ba\u00ba\ud835\udc5d\u00b9x,\ud835\udc66\u00ba\ud835\udc51x\ud835\udc51\ud835\udc66. (3.6.2)\nProblematically, we can never calculate the generalization error \ud835\udc45exactly. Nobody ever\ntells us the precise form of the density function \ud835\udc5d\u00b9x,\ud835\udc66\u00ba. Moreover, we cannot sample\nan infinite stream of data points. Thus, in practice, we must estimate the generalization\nerror by applying our model to an independent test set constituted of a random selection\nof examples X0and labels y0that were withheld from our training set. This consists of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73591ad6-2183-4757-870d-aaf315d238f6": {"__data__": {"id_": "73591ad6-2183-4757-870d-aaf315d238f6", "embedding": null, "metadata": {"page_label": "114", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fafc2ac-86cb-4ea6-bae7-ace0563ccf3a", "node_type": "4", "metadata": {"page_label": "114", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2bf9d2716d6129dee6a4c77b62ceb1b13cdac1cf45101243aa956dc28573fab6", "class_name": "RelatedNodeInfo"}}, "text": "114 Linear Neural Networks for Regression\napplying the same formula that was used for calculating the empirical training error but to\na test set X0,y0.\nCrucially,whenweevaluateourclassifieronthetestset,weareworkingwitha fixedclassi-\nfier(itdoesnotdependonthesampleofthetestset), andthusestimatingitserrorissimply\ntheproblemofmeanestimation. Howeverthesamecannotbesaidforthetrainingset. Note\nthat the model we wind up with depends explicitly on the selection of the training set and\nthus the training error will in general be a biased estimate of the true error on the underly-\ning population. The central question of generalization is then when should we expect our\ntraining error to be close to the population error (and thus the generalization error).\nModelComplexity\nIn classical theory, when we have simple models and abundant data, the training and gen-\neralization errors tend to be close. However, when we work with more complex models\nand/or fewer examples, we expect the training error to go down but the generalization gap\nto grow. This should not be surprising. Imagine a model class so expressive that for any\ndataset of\ud835\udc5bexamples, we can find a set of parameters that can perfectly fit arbitrary labels,\neven if randomly assigned. In this case, even if we fit our training data perfectly, how can\nwe conclude anything about the generalization error? For all we know, our generalization\nerror might be no better than random guessing.\nIn general, absent any restriction on our model class, we cannot conclude, based on fitting\nthe training data alone, that our model has discovered any generalizable pattern ( Vapniket\nal., 1994). On the other hand, if our model class was not capable of fitting arbitrary labels,\nthen it must have discovered a pattern. Learning-theoretic ideas about model complexity\nderived some inspiration from the ideas of Karl Popper, an influential philosopher of sci-\nence, who formalized the criterion of falsifiability. According to Popper, a theory that can\nexplainanyandallobservationsisnotascientifictheoryatall! Afterall,whathasittoldus\nabouttheworldifithasnotruledoutanypossibility? Inshort,whatwewantisahypothesis\nthatcould not explain any observations we might conceivably make and yet nevertheless\nhappens to be compatible with those observations that we in factmake.\nNow what precisely constitutes an appropriate notion of model complexity is a complex\nmatter. Often, models with more parameters are able to fit a greater number of arbitrarily\nassignedlabels. However,thisisnotnecessarilytrue. Forinstance,kernelmethodsoperate\nin spaces with infinite numbers of parameters, yet their complexity is controlled by other\nmeans (Sch\u00f6lkopf and Smola, 2002 ). One notion of complexity that often proves useful\nis the range of values that the parameters can take. Here, a model whose parameters are\npermitted to take arbitrary values would be more complex. We will revisit this idea in the\nnextsection,whenweintroduce weightdecay ,yourfirstpracticalregularizationtechnique.\nNotably,itcanbedifficulttocomparecomplexityamongmembersofsubstantiallydifferent\nmodel classes (say, decision trees vs. neural networks).\nAt this point, we must stress another important point that we will revisit when introducing\ndeep neural networks. When a model is capable of fitting arbitrary labels, low training\nerror does not necessarily imply low generalization error. However,itdoesnotnecessarily", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3417, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13f15cef-a770-4857-95e0-2f3119d4eb8b": {"__data__": {"id_": "13f15cef-a770-4857-95e0-2f3119d4eb8b", "embedding": null, "metadata": {"page_label": "115", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fba1c0eb-4ac1-4887-8978-dcddbf45a5c1", "node_type": "4", "metadata": {"page_label": "115", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "da30f5075eddd5a1e5ee316be2dce3c28b0bb67bd6a81b39949d53bb1b761153", "class_name": "RelatedNodeInfo"}}, "text": "115 Generalization\nimplyhighgeneralizationerroreither! All we can say with confidence is that low training\nerror alone is not enough to certify low generalization error. Deep neural networks turn\nout to be just such models: while they generalize well in practice, they are too powerful\nto allow us to conclude much on the basis of training error alone. In these cases we must\nrely more heavily on our holdout data to certify generalization after the fact. Error on the\nholdout data, i.e., validation set, is called the validationerror .\n3.6.2Underfittingor Overfitting?\nWhen we compare the training and validation errors, we want to be mindful of two com-\nmonsituations. First,wewanttowatchoutforcaseswhenourtrainingerrorandvalidation\nerror are both substantial but there is a little gap between them. If the model is unable to\nreduce the training error, that could mean that our model is too simple (i.e., insufficiently\nexpressive) to capture the pattern that we are trying to model. Moreover, since the gener-\nalization gap (\ud835\udc45emp\u0000\ud835\udc45) between our training and generalization errors is small, we have\nreason to believe that we could get away with a more complex model. This phenomenon is\nknown as underfitting .\nOn the other hand, as we discussed above, we want to watch out for the cases when our\ntraining error is significantly lower than our validation error, indicating severe overfitting .\nNote that overfitting is not always a bad thing. In deep learning especially, the best pre-\ndictive models often perform far better on training data than on holdout data. Ultimately,\nwe usually care about driving the generalization error lower, and only care about the gap\ninsofar as it becomes an obstacle to that end. Note that if the training error is zero, then the\ngeneralization gap is precisely equal to the generalization error and we can make progress\nonly by reducing the gap.\nPolynomialCurveFitting\nTo illustrate some classical intuition about overfitting and model complexity, consider the\nfollowing: given training data consisting of a single feature \ud835\udc65and a corresponding real-\nvalued label \ud835\udc66, we try to find the polynomial of degree \ud835\udc51\n\u02c6\ud835\udc66=\ud835\udc51\u00d5\n\ud835\udc56=0\ud835\udc65\ud835\udc56\ud835\udc64\ud835\udc56 (3.6.3)\nfor estimating the label \ud835\udc66. This is just a linear regression problem where our features are\ngiven by the powers of \ud835\udc65, the model\u2019s weights are given by \ud835\udc64\ud835\udc56, and the bias is given by \ud835\udc640\nsince\ud835\udc650=1for all\ud835\udc65. Since this is just a linear regression problem, we can use the squared\nerror as our loss function.\nA higher-order polynomial function is more complex than a lower-order polynomial func-\ntion,sincethehigher-orderpolynomialhasmoreparametersandthemodelfunction\u2019sselec-\ntion range is wider. Fixing the training dataset, higher-order polynomial functions should\nalways achieve lower (at worst, equal) training error relative to lower-degree polynomials.\nIn fact, whenever each data example has a distinct value of \ud835\udc65, a polynomial function with\ndegree equal to the number of data examples can fit the training set perfectly. We compare", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d06b098-2efc-4e00-8176-e3317085491a": {"__data__": {"id_": "0d06b098-2efc-4e00-8176-e3317085491a", "embedding": null, "metadata": {"page_label": "116", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcc7dafd-c7b5-43a1-90d1-057288eaac32", "node_type": "4", "metadata": {"page_label": "116", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5dd72531d63886c42c582dd97dbe4aa258198090423393b9e4f634e587b88c6d", "class_name": "RelatedNodeInfo"}}, "text": "116 Linear Neural Networks for Regression\nthe relationship between polynomial degree (model complexity) and both underfitting and\noverfitting in Fig. 3.6.1 .\ntFig. 3.6.1 In\ufb02uence of model complexity on under\ufb01tting and over\ufb01tting.\nDatasetSize\nAs the above bound already indicates, another big consideration to bear in mind is dataset\nsize. Fixing our model, the fewer samples we have in the training dataset, the more likely\n(and more severely) we are to encounter overfitting. As we increase the amount of training\ndata, the generalization error typically decreases. Moreover, in general, more data never\nhurts. For a fixed task and data distribution, model complexity should not increase more\nrapidly than the amount of data. Given more data, we might attempt to fit a more complex\nmodel. Absent sufficient data, simpler models may be more difficult to beat. For many\ntasks, deep learning only outperforms linear models when many thousands of training ex-\namplesareavailable. In part, thecurrent successof deeplearning owesconsiderablytothe\nabundance of massive datasets arising from Internet companies, cheap storage, connected\ndevices, and the broad digitization of the economy.\n3.6.3Model Selection\nTypically,weselectourfinalmodelonlyafterevaluatingmultiplemodelsthatdifferinvari-\nous ways (different architectures, training objectives, selected features, data preprocessing,\nlearning rates, etc.). Choosing among manymodels is aptlycalled modelselection .\nIn principle, we should not touch our test set until after we have chosen all our hyperpa-\nrameters. Werewetousethetestdatainthemodelselectionprocess, thereisariskthatwe\nmight overfit the test data. Then we would be in serious trouble. If we overfit our training\ndata, there is always the evaluation on test data to keep us honest. But if we overfit the test\ndata, how would we ever know? See Ong etal.(2005) for an example of how this can lead\nto absurd results even for models where the complexity can be tightly controlled.\nThus, we should never rely on the test data for model selection. And yet we cannot rely\nsolely on the training data for model selection either because we cannot estimate the gen-\neralization error on the very data that we use to train the model.\nIn practical applications, the picture gets muddier. While ideally we would only touch the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba999abb-a9de-4d90-8330-975f4054dc3c": {"__data__": {"id_": "ba999abb-a9de-4d90-8330-975f4054dc3c", "embedding": null, "metadata": {"page_label": "117", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be20631e-77f0-4d7b-9fbc-f9a796bdb73c", "node_type": "4", "metadata": {"page_label": "117", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a5ff9d345db74894b5a266e51a3667535faacd39b671ac2b30050a650aa406c6", "class_name": "RelatedNodeInfo"}}, "text": "117 Generalization\n81\n82test data once, to assess the very best model or to compare a small number of models with\neach other, real-world test data is seldom discarded after just one use. We can seldom\nafford a new test set for each round of experiments. In fact, recycling benchmark data for\ndecades can have a significant impact on the development of algorithms, e.g., for image\nclassification81andoptical character recognition82.\nThe common practice for addressing the problem of training on the test set is to split our\ndata three ways, incorporating a validation set in addition to the training and test datasets.\nThe result is a murky business where the boundaries between validation and test data are\nworryingly ambiguous. Unless explicitly stated otherwise, in the experiments in this book\nwe are really working with what should rightly be called training data and validation data,\nwith no true test sets. Therefore, the accuracy reported in each experiment of the book is\nreally the validation accuracy and not a true test set accuracy.\nCross-Validation\nWhentrainingdataisscarce,wemightnotevenbeabletoaffordtoholdoutenoughdatato\nconstituteapropervalidationset. Onepopularsolutiontothisproblemistoemploy \ud835\udc3e-fold\ncross-validation . Here, the original training data is split into \ud835\udc3enon-overlapping subsets.\nThen model training and validation are executed \ud835\udc3etimes, each time training on \ud835\udc3e\u00001\nsubsets and validating on a different subset (the one not used for training in that round).\nFinally, the training and validation errors are estimated by averaging over the results from\nthe\ud835\udc3eexperiments.\n3.6.4Summary\nThis section explored some of the underpinnings of generalization in machine learning.\nSomeoftheseideasbecomecomplicatedandcounterintuitivewhenwegettodeepermod-\nels; here, modelsarecapableofoverfittingdatabadly, andtherelevantnotionsofcomplex-\nitycanbebothimplicitandcounterintuitive(e.g.,largerarchitectureswithmoreparameters\ngeneralizing better). We leave you with a few rules of thumb:\n1.Use validation sets (or \ud835\udc3e-foldcross-validation ) for model selection;\n2.More complex models often require more data;\n3.Relevant notions of complexity include both the number of parameters and the range of\nvalues that they are allowed to take;\n4.Keeping all else equal, more data almost always leads to better generalization;\n5.This entire talk of generalization is all predicated on the IID assumption. If we relax\nthis assumption, allowing for distributions to shift between the train and testing peri-\nods, then we cannot say anything about generalization absent a further (perhaps milder)\nassumption.\n3.6.5Exercises\n1.When can you solve the problem of polynomial regression exactly?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62a8f625-ca96-45c3-9693-dd39c109f849": {"__data__": {"id_": "62a8f625-ca96-45c3-9693-dd39c109f849", "embedding": null, "metadata": {"page_label": "118", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a983d791-6e5b-408b-9c31-46bfd637b3bb", "node_type": "4", "metadata": {"page_label": "118", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b2107c83fa23a846330afaafd172298ab202e6c97602dca7af9f5d9be92be436", "class_name": "RelatedNodeInfo"}}, "text": "118 Linear Neural Networks for Regression\n832.Giveatleastfiveexampleswheredependentrandomvariablesmaketreatingtheproblem\nas IID data inadvisable.\n3.Can you ever expect to see zero training error? Under which circumstances would you\nsee zero generalization error?\n4.Why is\ud835\udc3e-fold cross-validation very expensive to compute?\n5.Why is the\ud835\udc3e-fold cross-validation error estimate biased?\n6.The VC dimension is defined as the maximum number of points that can be classified\nwith arbitrary labels f\u00061gby a function of a class of functions. Why might this not be\na good idea for measuring how complex the class of functions is? Hint: consider the\nmagnitude of the functions.\n7.Your manager gives you a difficult dataset on which your current algorithm does not\nperform so well. How would you justify to him that you need more data? Hint: you\ncannot increase the data but you can decrease it.\nDiscussions83.\n3.7WeightDecay\nNow that we have characterized the problem of overfitting, we can introduce our first reg-\nularization technique. Recall that we can always mitigate overfitting by collecting more\ntraining data. However, that can be costly, time consuming, or entirely out of our control,\nmaking it impossible in the short run. For now, we can assume that we already have as\nmuchhigh-qualitydataasourresourcespermitandfocusthetoolsatourdisposalwhenthe\ndataset is taken as a given.\nRecallthatinourpolynomialregressionexample( Section3.6.2 )wecouldlimitourmodel\u2019s\ncapacity by tweaking the degree of the fitted polynomial. Indeed, limiting the number of\nfeatures is a popular technique for mitigating overfitting. However, simply tossing aside\nfeatures can be too blunt an instrument. Sticking with the polynomial regression example,\nconsider what might happen with high-dimensional input. The natural extensions of poly-\nnomials to multivariate data are called monomials , which are simply products of powers\nof variables. The degree of a monomial is the sum of the powers. For example, \ud835\udc652\n1\ud835\udc652, and\n\ud835\udc653\ud835\udc652\n5are both monomials of degree 3.\nNote that the number of terms with degree \ud835\udc51blows up rapidly as \ud835\udc51grows larger. Given \ud835\udc58\nvariables, the number of monomials of degree \ud835\udc51is\u0000\ud835\udc58\u00001\u00b8\ud835\udc51\n\ud835\udc58\u00001\u0001. Even small changes in degree,\nsay from 2to3, dramatically increase the complexity of our model. Thus we often need a\nmore fine-grained tool for adjusting function complexity.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b81eff3-a9e4-4974-9511-ecf9212c381b": {"__data__": {"id_": "1b81eff3-a9e4-4974-9511-ecf9212c381b", "embedding": null, "metadata": {"page_label": "119", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bd754d5-a894-4bfc-aa05-9debd265d1e0", "node_type": "4", "metadata": {"page_label": "119", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0f1e25472273e8b7fd64ffe64f3080b3dab5b40cf6c50dfc7543dbceb90642a1", "class_name": "RelatedNodeInfo"}}, "text": "119 Weight Decay\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n3.7.1Normsand WeightDecay\nRather than directly manipulating the number of parameters, weight decay , operates by\nrestrictingthevaluesthattheparameterscantake. Morecommonlycalled \u21132regularization\noutside of deep learning circles when optimized by minibatch stochastic gradient descent,\nweightdecaymightbethemostwidelyusedtechniqueforregularizingparametricmachine\nlearningmodels. Thetechniqueismotivatedbythebasicintuitionthatamongallfunctions\n\ud835\udc53, the function \ud835\udc53=0(assigningthevalue 0toallinputs)isin somesensethe simplest, and\nthat we can measure the complexity of a function by the distance of its parameters from\nzero. But how precisely should we measure the distance between a function and zero?\nThere is no single right answer. In fact, entire branches of mathematics, including parts\nof functional analysis and the theory of Banach spaces, are devoted to addressing such\nissues.\nOne simple interpretation might be to measure the complexity of a linear function \ud835\udc53\u00b9x\u00ba=\nw>xby some norm of its weight vector, e.g., kwk2. Recall that we introduced the \u21132norm\nand\u21131norm, which are special cases of the more general \u2113\ud835\udc5dnorm, in Section 2.3.11 . The\nmost common method for ensuring a small weight vector is to add its norm as a penalty\nterm to the problem of minimizing the loss. Thus we replace our original objective, min-\nimizing the prediction loss on the training labels , with new objective, minimizing the sum\nof the prediction loss and the penalty term . Now, if our weight vector grows too large, our\nlearningalgorithmmightfocusonminimizingtheweightnorm kwk2ratherthanminimiz-\ning the training error. That is exactly what we want. To illustrate things in code, we revive\nour previous example from Section 3.1 for linear regression. There, our loss was given\nby\n\ud835\udc3f\u00b9w,\ud835\udc4f\u00ba=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=11\n2\u0010\nw>x\u00b9\ud835\udc56\u00ba\u00b8\ud835\udc4f\u0000\ud835\udc66\u00b9\ud835\udc56\u00ba\u00112\n. (3.7.1)\nRecall that x\u00b9\ud835\udc56\u00baare the features, \ud835\udc66\u00b9\ud835\udc56\u00bais the label for any data example \ud835\udc56, and\u00b9w,\ud835\udc4f\u00baare\nthe weight and bias parameters, respectively. To penalize the size of the weight vector,\nwe must somehow add kwk2to the loss function, but how should the model trade off the\nstandard loss for this new additive penalty? In practice, we characterize this trade-off via\ntheregularization constant \ud835\udf06, a nonnegative hyperparameter that we fit using validation\ndata:\n\ud835\udc3f\u00b9w,\ud835\udc4f\u00ba\u00b8\ud835\udf06\n2kwk2. (3.7.2)\nFor\ud835\udf06=0, we recover our original loss function. For \ud835\udf06 > 0, we restrict the size of kwk.\nWe divide by 2by convention: when we take the derivative of a quadratic function, the\n2and 1\u009d2cancel out, ensuring that the expression for the update looks nice and simple.\nThe astute reader might wonder why we work with the squared norm and not the standard", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2726, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95f21d77-6be0-46c9-935a-8c42d92c0d94": {"__data__": {"id_": "95f21d77-6be0-46c9-935a-8c42d92c0d94", "embedding": null, "metadata": {"page_label": "120", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2493d4d1-7e7c-45b6-b8b0-2733e1314d1e", "node_type": "4", "metadata": {"page_label": "120", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5176bb83e517665ca7744d7341b6d3de1d9f1e5565fc32de7db5958972c624d6", "class_name": "RelatedNodeInfo"}}, "text": "120 Linear Neural Networks for Regression\nnorm(i.e.,theEuclideandistance). Wedothisforcomputationalconvenience. Bysquaring\nthe\u21132norm, we remove the square root, leaving the sum of squares of each component of\nthe weight vector. This makes the derivative of the penalty easy to compute: the sum of\nderivatives equals the derivative of the sum.\nMoreover, you might ask why we work with the \u21132norm in the first place and not, say,\nthe\u21131norm. In fact, other choices are valid and popular throughout statistics. While \u21132-\nregularized linear models constitute the classic ridge regression algorithm,\u21131-regularized\nlinear regression is a similarly fundamental method in statistics, popularly known as lasso\nregression . Onereasontoworkwiththe \u21132normisthatitplacesanoutsizepenaltyonlarge\ncomponents of the weight vector. This biases our learning algorithm towards models that\ndistribute weight evenly across a larger number of features. In practice, this might make\nthem more robust to measurement error in a single variable. By contrast, \u21131penalties lead\nto models that concentrate weights on a small set of features by clearing the other weights\nto zero. This gives us an effective method for featureselection , which may be desirable for\nother reasons. For example, if our model only relies on a few features, then we may not\nneed to collect, store, or transmit data for the other (dropped) features.\nUsing the same notation in (3.1.11 ), minibatch stochastic gradient descent updates for \u21132-\nregularized regression as follows:\nw \u00b91\u0000\ud835\udf02\ud835\udf06\u00baw\u0000\ud835\udf02\njBj\u00d5\n\ud835\udc562Bx\u00b9\ud835\udc56\u00ba\u0010\nw>x\u00b9\ud835\udc56\u00ba\u00b8\ud835\udc4f\u0000\ud835\udc66\u00b9\ud835\udc56\u00ba\u0011\n.(3.7.3)\nAs before, we update wbased on the amount by which our estimate differs from the ob-\nservation. However, we also shrink the size of wtowards zero. That is why the method is\nsometimescalled\u201cweightdecay\u201d: giventhepenaltytermalone,ouroptimizationalgorithm\ndecaysthe weight at each step of training. In contrast to feature selection, weight decay\noffers us a mechanism for continuously adjusting the complexity of a function. Smaller\nvalues of\ud835\udf06correspond to less constrained w, whereas larger values of \ud835\udf06constrain wmore\nconsiderably. Whether we include a corresponding bias penalty \ud835\udc4f2can vary across imple-\nmentations, and may vary across layers of a neural network. Often, we do not regularize\nthe bias term. Besides, although \u21132regularization may not be equivalent to weight decay\nfor other optimization algorithms, the idea of regularization through shrinking the size of\nweights still holds true.\n3.7.2High-Dimensional Linear Regression\nWe can illustrate the benefits of weight decay through a simple synthetic example.\nFirst, we generate some data as before:\n\ud835\udc66=0.05\u00b8\ud835\udc51\u00d5\n\ud835\udc56=10.01\ud835\udc65\ud835\udc56\u00b8\ud835\udf16where\ud835\udf16\u0018N\u00b9 0,0.012\u00ba. (3.7.4)\nIn this synthetic dataset, our label is given by an underlying linear function of our inputs,\ncorrupted by Gaussian noise with zero mean and standard deviation 0.01. For illustrative\npurposes, we can make the effects of overfitting pronounced, by increasing the dimen-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c036cec1-d482-4a2c-8bdc-ca84eba44926": {"__data__": {"id_": "c036cec1-d482-4a2c-8bdc-ca84eba44926", "embedding": null, "metadata": {"page_label": "121", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf099bf1-4332-4afb-ad82-a6275a7eb773", "node_type": "4", "metadata": {"page_label": "121", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8ef39840a5b268d0a6cb5d33cbf8dc44290251bf339416c2ce7f71a488f5e379", "class_name": "RelatedNodeInfo"}}, "text": "121 Weight Decay\nsionality of our problem to \ud835\udc51=200and working with a small training set with only 20\nexamples.\nclass Data (d2l .DataModule):\ndef __init__ (self , num_train, num_val, num_inputs, batch_size):\nself .save_hyperparameters()\nn=num_train +num_val\nself .X=torch .randn(n, num_inputs)\nnoise =torch .randn(n, 1)*0.01\nw, b =torch .ones((num_inputs, 1))*0.01 ,0.05\nself .y=torch .matmul( self .X, w) +b+noise\ndef get_dataloader (self , train):\ni=slice (0,self .num_train) iftrain else slice (self .num_train, None )\nreturn self .get_tensorloader([ self .X,self .y], train, i)\n3.7.3Implementation fromScratch\nNow,let\u2019stryimplementingweightdecayfromscratch. Sinceminibatchstochasticgradient\ndescent is our optimizer, we just need to add the squared \u21132penalty to the original loss\nfunction.\nDefining\u21132NormPenalty\nPerhaps the most convenient way of implementing this penalty is to square all terms in\nplace and sum them.\ndef l2_penalty (w):\nreturn (w**2).sum() /2\nDefiningthe Model\nInthefinalmodel,thelinearregressionandthesquaredlosshavenotchangedsince Section\n3.4,sowewilljustdefineasubclassof d2l.LinearRegressionScratch . Theonlychange\nhere is that our loss now includes the penalty term.\nclass WeightDecayScratch (d2l .LinearRegressionScratch):\ndef __init__ (self , num_inputs, lambd, lr, sigma =0.01 ):\nsuper ().__init__ (num_inputs, lr, sigma)\nself .save_hyperparameters()\ndef loss (self , y_hat, y):\nreturn (super ().loss(y_hat, y) +\nself .lambd *l2_penalty( self .w))\nThe following code fits our model on the training set with 20 examples and evaluates it on\nthe validation set with 100 examples.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1606, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "747483d0-42fb-4fba-885f-3d954df4f25a": {"__data__": {"id_": "747483d0-42fb-4fba-885f-3d954df4f25a", "embedding": null, "metadata": {"page_label": "122", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7274fe8-3501-49ce-8f1c-0bdb2aa36caf", "node_type": "4", "metadata": {"page_label": "122", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e0dc9173818bcd8c58f0827c362d964c1c9c1e810b9952726f6ace3624738ed1", "class_name": "RelatedNodeInfo"}}, "text": "122 Linear Neural Networks for Regression\ndata =Data(num_train =20, num_val =100, num_inputs =200, batch_size =5)\ntrainer =d2l.Trainer(max_epochs =10)\ndef train_scratch (lambd):\nmodel =WeightDecayScratch(num_inputs =200, lambd =lambd, lr =0.01 )\nmodel .board .yscale ='log'\ntrainer .fit(model, data)\nprint ('L2 norm of w: ',float (l2_penalty(model .w)))\nTrainingwithout Regularization\nWe now run this code with lambd = 0 , disabling weight decay. Note that we overfit\nbadly, decreasing the training error but not the validation error\u2014a textbook case of over-\nfitting.\ntrain_scratch( 0)\nL2 norm of w: 0.009948714636266232\nUsing WeightDecay\nBelow, we run with substantial weight decay. Note that the training error increases but\nthe validation error decreases. This is precisely the effect we expect from regulariza-\ntion.\ntrain_scratch( 3)\nL2 norm of w: 0.0017270983662456274\n3.7.4Concise Implementation\nBecause weight decay is ubiquitous in neural network optimization, the deep learning\nframework makes it especially convenient, integrating weight decay into the optimization", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f48e5092-4abf-4d85-a15a-0d2726f7ac58": {"__data__": {"id_": "f48e5092-4abf-4d85-a15a-0d2726f7ac58", "embedding": null, "metadata": {"page_label": "123", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b4fd42a2-63c0-436a-95f4-2a50666a1fcf", "node_type": "4", "metadata": {"page_label": "123", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fe9364649e2ce64c0563ad0eb75ea9c364b504b5b1b416800deb7e6460562244", "class_name": "RelatedNodeInfo"}}, "text": "123 Weight Decay\n84algorithm itself for easy use in combination with anyloss function. Moreover, this integra-\ntion serves a computational benefit, allowing implementation tricks to add weight decay\nto the algorithm, without any additional computational overhead. Since the weight decay\nportion of the update depends only on the current value of each parameter, the optimizer\nmust touch each parameter once anyway.\nBelow, we specify the weight decay hyperparameter directly through weight_decay when\ninstantiating our optimizer. By default, PyTorch decays both weights and biases simulta-\nneously, but we can configure the optimizer to handle different parameters according to\ndifferent policies. Here, we only set weight_decay for the weights (the net.weight pa-\nrameters), hence the bias (the net.bias parameter) will not decay.\nclass WeightDecay (d2l .LinearRegression):\ndef __init__ (self , wd, lr):\nsuper ().__init__ (lr)\nself .save_hyperparameters()\nself .wd=wd\ndef configure_optimizers (self ):\nreturn torch .optim .SGD([\n{'params ':self .net.weight, 'weight_decay ':self .wd},\n{'params ':self .net.bias}], lr =self .lr)\nThe plot looks similar to that when we implemented weight decay from scratch. How-\never, this version runs faster and is easier to implement, benefits that will become more\npronounced as you address larger problems and this work becomes more routine.\nmodel =WeightDecay(wd =3, lr =0.01 )\nmodel .board .yscale ='log'\ntrainer .fit(model, data)\nprint ('L2 norm of w: ',float (l2_penalty(model .get_w_b()[ 0])))\nL2 norm of w: 0.013779522851109505\nSofar,wehavetouchedupononenotionofwhatconstitutesasimplelinearfunction. How-\never, even for simple nonlinear functions, the situation can be much more complex. To see\nthis, the concept of reproducing kernel Hilbert space (RKHS)84allows one to apply tools", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2117805d-9a58-4a7e-b576-004b404a0bec": {"__data__": {"id_": "2117805d-9a58-4a7e-b576-004b404a0bec", "embedding": null, "metadata": {"page_label": "124", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e0dded5-3b0c-44a5-b22b-7fa94b52cbba", "node_type": "4", "metadata": {"page_label": "124", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9f064d8ecb7fe54ed10a25a6c24929c1c64c504a3a863291504f592c447f05fe", "class_name": "RelatedNodeInfo"}}, "text": "124 Linear Neural Networks for Regression\n85introduced for linear functions in a nonlinear context. Unfortunately, RKHS-based algo-\nrithms tend to scale poorly to large, high-dimensional data. In this book we will often\nadopt the common heuristic whereby weight decay is applied to all layers of a deep net-\nwork.\n3.7.5Summary\nRegularization is a common method for dealing with overfitting. Classical regularization\ntechniquesaddapenaltytermtothelossfunction(whentraining)toreducethecomplexity\nof the learned model. One particular choice for keeping the model simple is using an \u21132\npenalty. This leads to weight decay in the update steps of the minibatch stochastic gradient\ndescent algorithm. In practice, the weight decay functionality is provided in optimizers\nfrom deep learning frameworks. Different sets of parameters can have different update\nbehaviors within the same training loop.\n3.7.6Exercises\n1.Experiment with the value of \ud835\udf06in the estimation problem in this section. Plot training\nand validation accuracy as a function of \ud835\udf06. What do you observe?\n2.Use a validation set to find the optimal value of \ud835\udf06. Is it really the optimal value? Does\nthis matter?\n3.What would the update equations look like if instead of kwk2we used\u00cd\n\ud835\udc56j\ud835\udc64\ud835\udc56jas our\npenalty of choice ( \u21131regularization)?\n4.We know thatkwk2=w>w. Can you find a similar equation for matrices (see the\nFrobenius norm in Section 2.3.11 )?\n5.Review the relationship between training error and generalization error. In addition to\nweight decay, increased training, and the use of a model of suitable complexity, what\nother ways might help us deal with overfitting?\n6.In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior\nvia\ud835\udc43\u00b9\ud835\udc64j\ud835\udc65\u00ba/\ud835\udc43\u00b9\ud835\udc65j\ud835\udc64\u00ba\ud835\udc43\u00b9\ud835\udc64\u00ba. How can you identify \ud835\udc43\u00b9\ud835\udc64\u00bawith regularization?\nDiscussions85.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b794edc-9173-4211-bf66-fd3045ebbb01": {"__data__": {"id_": "7b794edc-9173-4211-bf66-fd3045ebbb01", "embedding": null, "metadata": {"page_label": "125", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b0f7f56-b153-4d06-8ed2-6ea9deb16314", "node_type": "4", "metadata": {"page_label": "125", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5988b9bf06e1cbec360e08e434b22d3561ef03b04ffb0bede6abe8fe9c764a00", "class_name": "RelatedNodeInfo"}}, "text": "4 Linear Neural Networks for Classi\ufb01cation\nNow that you have worked through all of the mechanics you are ready to apply the skills\nyou have learned to broader kinds of tasks. Even as we pivot towards classification, most\nof the plumbing remains the same: loading the data, passing it through the model, generat-\ning output, calculating the loss, taking gradients with respect to weights, and updating the\nmodel. However, the precise form of the targets, the parametrization of the output layer,\nand the choice of loss function will adapt to suit the classification setting.\n4.1SoftmaxRegression\nInSection 3.1 , we introduced linear regression, working through implementations from\nscratch in Section 3.4 and again using high-level APIs of a deep learning framework in\nSection 3.5 to do the heavy lifting.\nRegression is the hammer wereachforwhen wewantto answer howmuch? orhowmany?\nquestions. Ifyouwanttopredictthenumberofdollars(price)atwhichahousewillbesold,\nor the number of wins a baseball team might have, or the number of days that a patient will\nremainhospitalizedbeforebeingdischarged,thenyouareprobablylookingforaregression\nmodel. However, even within regression models, there are important distinctions. For\ninstance, the price of a house will never be negative and changes might often be relative\nto its baseline price. As such, it might be more effective to regress on the logarithm of the\nprice. Likewise, the number of days a patient spends in hospital is a discrete nonnegative\nrandom variable. As such, least mean squares might not be an ideal approach either. This\nsortoftime-to-eventmodelingcomeswithahostofothercomplicationsthataredealtwith\nin a specialized subfield called survivalmodeling .\nThe point here is not to overwhelm you but just to let you know that there is a lot more\nto estimation than simply minimizing squared errors. And more broadly, there is a lot\nmore to supervised learning than regression. In this section, we focus on classification\nproblems where we put aside how much? questions and instead focus on which category?\nquestions.\n\u000fDoes this email belong in the spam folder or the inbox?\n\u000fIs this customer more likely to sign up or not to sign up for a subscription service?\n125", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2218, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35db387d-8474-4764-8f7a-523133d79742": {"__data__": {"id_": "35db387d-8474-4764-8f7a-523133d79742", "embedding": null, "metadata": {"page_label": "126", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae499a2b-cb67-4cf1-9be7-4e4a76a3e268", "node_type": "4", "metadata": {"page_label": "126", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1fc38b55cf43928a8853762bf452b9d0f0a09dd9389bf2efc63dd4cdf87597fe", "class_name": "RelatedNodeInfo"}}, "text": "126 Linear Neural Networks for Classi\ufb01cation\n86\n87\u000fDoes this image depict a donkey, a dog, a cat, or a rooster?\n\u000fWhich movie is Aston most likely to watch next?\n\u000fWhich section of the book are you going to read next?\nColloquially, machine learning practitioners overload the word classification to describe\ntwo subtly different problems: (i) those where we are interested only in hard assignments\nofexamplestocategories(classes); and(ii)thosewherewewishtomakesoftassignments,\ni.e.,toassesstheprobabilitythateachcategoryapplies. Thedistinctiontendstogetblurred,\ninpart, becauseoften, evenwhenweonlycareabouthardassignments, westillusemodels\nthat make soft assignments.\nEven more, there are cases where more than one label might be true. For instance, a news\narticle might simultaneously cover the topics of entertainment, business, and space flight,\nbut not the topics of medicine or sports. Thus, categorizing it into one of the above cate-\ngories on their own would not be very useful. This problem is commonly known as multi-\nlabel classification86. See Tsoumakas and Katakis ( 2007) for an overview and Huang et\nal.(2015) for an effective algorithm when tagging images.\n4.1.1Classification\nTo get our feet wet, let\u2019s start with a simple image classification problem. Here, each input\nconsists of a 2\u00022grayscale image. We can represent each pixel value with a single scalar,\ngiving us four features \ud835\udc651,\ud835\udc652,\ud835\udc653,\ud835\udc654. Further, let\u2019s assume that each image belongs to one\namong the categories \u201ccat\u201d, \u201cchicken\u201d, and \u201cdog\u201d.\nNext, we have to choose how to represent the labels. We have two obvious choices. Per-\nhapsthemostnaturalimpulsewouldbetochoose \ud835\udc662f1,2,3g,wheretheintegersrepresent\nfdog,cat,chickengrespectively. This is a great way of storingsuch information on a com-\nputer. If the categories had some natural ordering among them, say if we were trying to\npredictfbaby,toddler,adolescent,young adult,adult,geriatricg, then it might even make\nsense to cast this as an ordinal regression87problem and keep the labels in this format.\nSee Moon et al.(2010) for an overview of different types of ranking loss functions and\nBeutelet al.(2014) for a Bayesian approach that addresses responses with more than one\nmode.\nIn general, classification problems do not come with natural orderings among the classes.\nFortunately, statisticians long ago invented a simple way to represent categorical data: the\none-hot encoding . A one-hot encoding is a vector with as many components as we have\ncategories. Thecomponentcorrespondingtoaparticularinstance\u2019scategoryissetto1and\nallothercomponentsaresetto0. Inourcase,alabel \ud835\udc66wouldbeathree-dimensionalvector,\nwith\u00b91,0,0\u00bacorresponding to \u201ccat\u201d, \u00b90,1,0\u00bato \u201cchicken\u201d, and\u00b90,0,1\u00bato \u201cdog\u201d:\n\ud835\udc662f\u00b91,0,0\u00ba,\u00b90,1,0\u00ba,\u00b90,0,1\u00bag. (4.1.1)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2751, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1b95781-f098-4e55-8cbf-054096b0df62": {"__data__": {"id_": "e1b95781-f098-4e55-8cbf-054096b0df62", "embedding": null, "metadata": {"page_label": "127", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dafbb433-ef7d-40eb-a3d1-4ebb61517bc5", "node_type": "4", "metadata": {"page_label": "127", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1d39c4a54ffa43f74f02d4051306e6410018b718ad2fa9ac36b0f17b8e87f768", "class_name": "RelatedNodeInfo"}}, "text": "127 Softmax Regression\nLinearModel\nIn order to estimate the conditional probabilities associated with all the possible classes,\nwe need a model with multiple outputs, one per class. To address classification with lin-\near models, we will need as many affine functions as we have outputs. Strictly speaking,\nwe only need one fewer, since the final category has to be the difference between 1and\nthe sum of the other categories, but for reasons of symmetry we use a slightly redundant\nparametrization. Each output corresponds to its own affine function. In our case, since\nwe have 4 features and 3 possible output categories, we need 12 scalars to represent the\nweights (\ud835\udc64with subscripts), and 3 scalars to represent the biases ( \ud835\udc4fwith subscripts). This\nyields:\n\ud835\udc5c1=\ud835\udc651\ud835\udc6411\u00b8\ud835\udc652\ud835\udc6412\u00b8\ud835\udc653\ud835\udc6413\u00b8\ud835\udc654\ud835\udc6414\u00b8\ud835\udc4f1,\n\ud835\udc5c2=\ud835\udc651\ud835\udc6421\u00b8\ud835\udc652\ud835\udc6422\u00b8\ud835\udc653\ud835\udc6423\u00b8\ud835\udc654\ud835\udc6424\u00b8\ud835\udc4f2,\n\ud835\udc5c3=\ud835\udc651\ud835\udc6431\u00b8\ud835\udc652\ud835\udc6432\u00b8\ud835\udc653\ud835\udc6433\u00b8\ud835\udc654\ud835\udc6434\u00b8\ud835\udc4f3.(4.1.2)\nThe corresponding neural network diagram is shown in Fig. 4.1.1 . Just as in linear regres-\nsion,weuseasingle-layerneuralnetwork. Andsincethecalculationofeachoutput, \ud835\udc5c1,\ud835\udc5c2,\nand\ud835\udc5c3, depends on every input, \ud835\udc651,\ud835\udc652,\ud835\udc653, and\ud835\udc654, the output layer can also be described as\nafullyconnectedlayer .\ntFig. 4.1.1 Softmax regression is a single-layer neural network.\nForamoreconcisenotationweusevectorsandmatrices: o=Wx\u00b8bismuchbettersuited\nformathematicsandcode. Notethatwehavegatheredallofourweightsintoa 3\u00024matrix\nand all biases b2R3in a vector.\nTheSoftmax\nAssuming a suitable loss function, we could try, directly, to minimize the difference be-\ntweenoand the labels y. While it turns out that treating classification as a vector-valued\nregressionproblemworkssurprisinglywell,itisnonethelessunsatisfactoryinthefollowing\nways:\n\u000fThere is no guarantee that the outputs \ud835\udc5c\ud835\udc56sum up to 1in the way we expect probabilities\nto behave.\n\u000fThere is no guarantee that the outputs \ud835\udc5c\ud835\udc56are even nonnegative, even if their outputs sum\nup to 1, or that they do not exceed 1.\nBoth aspects render the estimation problem difficult to solve and the solution very brittle\nto outliers. For instance, if we assume that there is a positive linear dependency between\nthe number of bedrooms and the likelihood that someone will buy a house, the probability", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc6bc613-f563-4787-b42a-22d0e744062a": {"__data__": {"id_": "fc6bc613-f563-4787-b42a-22d0e744062a", "embedding": null, "metadata": {"page_label": "128", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "987ba5bb-fb6c-438d-80c0-d289953ccd14", "node_type": "4", "metadata": {"page_label": "128", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "aed95c484f0c2aa50747d2b45fbb91a18d839bd351131f3acac3a11286e86f2b", "class_name": "RelatedNodeInfo"}}, "text": "128 Linear Neural Networks for Classi\ufb01cation\n88might exceed 1when it comes to buying a mansion! As such, we need a mechanism to\n\u201csquish\u201d the outputs.\nThere are many ways we might accomplish this goal. For instance, we could assume that\nthe outputs oare corrupted versions of y, where the corruption occurs by means of adding\nnoise \ud835\udf50drawnfrom a normal distribution. In other words, y=o\u00b8\ud835\udf50, where\ud835\udf16\ud835\udc56\u0018N\u00b9 0,\ud835\udf0e2\u00ba.\nThis is the so-called probit model88, first introduced by Fechner ( 1860). While appealing,\nit does not work quite as well nor lead to a particularly nice optimization problem, when\ncompared to the softmax.\nAnother way to accomplish this goal (and to ensure nonnegativity) is to use an exponential\nfunction\ud835\udc43\u00b9\ud835\udc66=\ud835\udc56\u00ba/exp\ud835\udc5c\ud835\udc56. This does indeed satisfy the requirement that the conditional\nclass probability increases with increasing \ud835\udc5c\ud835\udc56, it is monotonic, and all probabilities are\nnonnegative. We can then transform these values so that they add up to 1by dividing each\nby their sum. This process is called normalization . Putting these two pieces together gives\nus thesoftmax function:\n\u02c6y=softmax\u00b9o\u00bawhere \u02c6\ud835\udc66\ud835\udc56=exp\u00b9\ud835\udc5c\ud835\udc56\u00ba\u00cd\n\ud835\udc57exp\u00b9\ud835\udc5c\ud835\udc57\u00ba. (4.1.3)\nNote that the largest coordinate of ocorresponds to the most likely class according to \u02c6y.\nMoreover, because the softmax operation preserves the ordering among its arguments, we\ndonotneedtocomputethesoftmaxtodeterminewhichclasshasbeenassignedthehighest\nprobability. Thus,\nargmax\n\ud835\udc57\u02c6\ud835\udc66\ud835\udc57=argmax\n\ud835\udc57\ud835\udc5c\ud835\udc57.(4.1.4)\nTheideaofa softmaxdatesbacktoGibbs( 1902), whoadaptedideasfrom physics. Dating\neven further back, Boltzmann, the father of modern statistical physics, used this trick to\nmodel a distribution over energy states in gas molecules. In particular, he discovered that\ntheprevalenceofastateofenergyinathermodynamicensemble,suchasthemoleculesina\ngas, is proportional to exp\u00b9\u0000\ud835\udc38\u009d\ud835\udc58\ud835\udc47\u00ba. Here,\ud835\udc38is the energy of a state, \ud835\udc47is the temperature,\nand\ud835\udc58is the Boltzmann constant. When statisticians talk about increasing or decreasing\nthe \u201ctemperature\u201d of a statistical system, they refer to changing \ud835\udc47in order to favor lower\nor higher energy states. Following Gibbs\u2019 idea, energy equates to error. Energy-based\nmodels ( Ranzatoet al., 2007) use this point of view when describing problems in deep\nlearning.\nVectorization\nToimprovecomputationalefficiency,wevectorizecalculationsinminibatchesofdata. As-\nsume that we are given a minibatch X2R\ud835\udc5b\u0002\ud835\udc51of\ud835\udc5bexamples with dimensionality (number\nof inputs)\ud835\udc51. Moreover, assume that we have \ud835\udc5ecategories in the output. Then the weights\nsatisfy W2R\ud835\udc51\u0002\ud835\udc5eand the bias satisfies b2R1\u0002\ud835\udc5e.\nO=XW\u00b8b,\n\u02c6Y=softmax\u00b9O\u00ba.(4.1.5)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2561, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02a2fe03-451c-4a9d-870e-7c09af97640c": {"__data__": {"id_": "02a2fe03-451c-4a9d-870e-7c09af97640c", "embedding": null, "metadata": {"page_label": "129", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2441bad-0800-4101-8281-e9ad2a57ebfe", "node_type": "4", "metadata": {"page_label": "129", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bd37b4e865447a45d0e7ec6925969b333055cd4c014fb834d44b2aa45f97e6dc", "class_name": "RelatedNodeInfo"}}, "text": "129 Softmax Regression\nThis accelerates the dominant operation into a matrix\u2013matrix product XW. Moreover,\nsinceeachrowin Xrepresentsadataexample,thesoftmaxoperationitselfcanbecomputed\nrowwise: for each row of O, exponentiate all entries and then normalize them by the sum.\nNote,though,thatcaremustbetakentoavoidexponentiatingandtakinglogarithmsoflarge\nnumbers,sincethiscancausenumericaloverfloworunderflow. Deeplearningframeworks\ntake care of this automatically.\n4.1.2LossFunction\nNow that we have a mapping from features xto probabilities \u02c6 y, we need a way to optimize\nthe accuracy of this mapping. We will rely on maximum likelihood estimation, the very\nsamemethodthatweencounteredwhenprovidingaprobabilisticjustificationforthemean\nsquared error loss in Section 3.1.3 .\nLog-Likelihood\nThe softmax function gives us a vector \u02c6y, which we can interpret as the (estimated) con-\nditional probabilities of each class, given any input x, such as \u02c6\ud835\udc661=\ud835\udc43\u00b9\ud835\udc66=catjx\u00ba. In the\nfollowing we assume that for a dataset with features Xthe labels Yare represented using\na one-hot encoding label vector. We can compare the estimates with reality by checking\nhow probable the actual classes are according to our model, given the features:\n\ud835\udc43\u00b9YjX\u00ba=\ud835\udc5b\u00d6\n\ud835\udc56=1\ud835\udc43\u00b9y\u00b9\ud835\udc56\u00bajx\u00b9\ud835\udc56\u00ba\u00ba. (4.1.6)\nWe are allowed to use the factorization since we assume that each label is drawn indepen-\ndently from its respective distribution \ud835\udc43\u00b9yjx\u00b9\ud835\udc56\u00ba\u00ba. Since maximizing the product of terms\nisawkward,wetakethenegativelogarithmtoobtaintheequivalentproblemofminimizing\nthe negative log-likelihood:\n\u0000log\ud835\udc43\u00b9YjX\u00ba=\ud835\udc5b\u00d5\n\ud835\udc56=1\u0000log\ud835\udc43\u00b9y\u00b9\ud835\udc56\u00bajx\u00b9\ud835\udc56\u00ba\u00ba=\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc59\u00b9y\u00b9\ud835\udc56\u00ba,\u02c6y\u00b9\ud835\udc56\u00ba\u00ba, (4.1.7)\nwhere for any pair of label yand model prediction \u02c6yover\ud835\udc5eclasses, the loss function \ud835\udc59\nis\n\ud835\udc59\u00b9y,\u02c6y\u00ba=\u0000\ud835\udc5e\u00d5\n\ud835\udc57=1\ud835\udc66\ud835\udc57log \u02c6\ud835\udc66\ud835\udc57. (4.1.8)\nFor reasons explained later on, the loss function in (4.1.8 )is commonly called the cross-\nentropyloss . Since yisaone-hotvectoroflength \ud835\udc5e, thesumoverallitscoordinates \ud835\udc57van-\nishesforallbutoneterm. Notethattheloss \ud835\udc59\u00b9y,\u02c6y\u00baisboundedfrombelowby 0whenever \u02c6y\nis a probability vector: no single entry is larger than 1, hence their negative logarithm can-\nnot be lower than 0;\ud835\udc59\u00b9y,\u02c6y\u00ba=0only if we predict the actual label with certainty . This can\nnever happen for any finite setting of the weights because taking a softmax output towards\n1requires taking the corresponding input \ud835\udc5c\ud835\udc56to infinity (or all other outputs \ud835\udc5c\ud835\udc57for\ud835\udc57\u2260\ud835\udc56\nto negative infinity). Even if our model could assign an output probability of 0, any error\nmade when assigning such high confidence would incur infinite loss ( \u0000log 0=1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2515, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e0aa599-9a17-4f26-818d-a7656ced225a": {"__data__": {"id_": "7e0aa599-9a17-4f26-818d-a7656ced225a", "embedding": null, "metadata": {"page_label": "130", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3cf8e927-ead8-4e2c-b8e2-936210b5079a", "node_type": "4", "metadata": {"page_label": "130", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "eb74ab8b8fedff8e9142ffd65012cb69c023336e2408c3d57fb471e6496dcc66", "class_name": "RelatedNodeInfo"}}, "text": "130 Linear Neural Networks for Classi\ufb01cation\nSoftmax and Cross-EntropyLoss\nSince the softmax function and the corresponding cross-entropy loss are so common, it is\nworth understanding a bit better how they are computed. Plugging (4.1.3 )into the defini-\ntion of the loss in (4.1.8 )and using the definition of the softmax we obtain\n\ud835\udc59\u00b9y,\u02c6y\u00ba=\u0000\ud835\udc5e\u00d5\n\ud835\udc57=1\ud835\udc66\ud835\udc57logexp\u00b9\ud835\udc5c\ud835\udc57\u00ba\n\u00cd\ud835\udc5e\n\ud835\udc58=1exp\u00b9\ud835\udc5c\ud835\udc58\u00ba\n=\ud835\udc5e\u00d5\n\ud835\udc57=1\ud835\udc66\ud835\udc57log\ud835\udc5e\u00d5\n\ud835\udc58=1exp\u00b9\ud835\udc5c\ud835\udc58\u00ba\u0000\ud835\udc5e\u00d5\n\ud835\udc57=1\ud835\udc66\ud835\udc57\ud835\udc5c\ud835\udc57\n=log\ud835\udc5e\u00d5\n\ud835\udc58=1exp\u00b9\ud835\udc5c\ud835\udc58\u00ba\u0000\ud835\udc5e\u00d5\n\ud835\udc57=1\ud835\udc66\ud835\udc57\ud835\udc5c\ud835\udc57.(4.1.9)\nTounderstandabitbetterwhatisgoingon,considerthederivativewithrespecttoanylogit\n\ud835\udc5c\ud835\udc57. We get\n\ud835\udf15\ud835\udc5c\ud835\udc57\ud835\udc59\u00b9y,\u02c6y\u00ba=exp\u00b9\ud835\udc5c\ud835\udc57\u00ba\n\u00cd\ud835\udc5e\n\ud835\udc58=1exp\u00b9\ud835\udc5c\ud835\udc58\u00ba\u0000\ud835\udc66\ud835\udc57=softmax\u00b9o\u00ba\ud835\udc57\u0000\ud835\udc66\ud835\udc57. (4.1.10)\nIn other words, the derivative is the difference between the probability assigned by our\nmodel, as expressed by the softmax operation, and what actually happened, as expressed\nby elements in the one-hot label vector. In this sense, it is very similar to what we saw in\nregression, where the gradient was the difference between the observation \ud835\udc66and estimate\n\u02c6\ud835\udc66. This is not a coincidence. In any exponential family model, the gradients of the log-\nlikelihood are given by precisely this term. This fact makes computing gradients easy in\npractice.\nNowconsiderthecasewhereweobservenotjustasingleoutcomebutanentiredistribution\nover outcomes. We can use the same representation as before for the label y. The only dif-\nferenceisthatratherthanavectorcontainingonlybinaryentries,say \u00b90,0,1\u00ba,wenowhave\nagenericprobabilityvector, say \u00b90.1,0.2,0.7\u00ba. Themaththatweusedpreviouslytodefine\nthe loss\ud835\udc59in(4.1.8 )still works well, just that the interpretation is slightly more general. It\nis the expected value of the loss for a distribution over labels. This loss is called the cross-\nentropyloss anditisoneofthemostcommonlyusedlossesforclassificationproblems. We\ncan demystify the name by introducing just the basics of information theory. In a nutshell,\nitmeasuresthenumberofbitsneededtoencodewhatwesee, y,relativetowhatwepredict\nthat should happen, \u02c6y. We provide a very basic explanation in the following. For further\ndetails on information theory see Cover and Thomas ( 1999) or MacKay ( 2003).\n4.1.3InformationTheory Basics\nManydeeplearningpapersuseintuitionandtermsfrominformationtheory. Tomakesense\nof them, we need some common language. This is a survival guide. Information theory\ndeals with the problem of encoding, decoding, transmitting, and manipulating information\n(also known as data).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "46a4558e-5126-43d4-ac24-16dd16cf64d9": {"__data__": {"id_": "46a4558e-5126-43d4-ac24-16dd16cf64d9", "embedding": null, "metadata": {"page_label": "131", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7616701-0aa2-4f09-b512-aa7d5aa87759", "node_type": "4", "metadata": {"page_label": "131", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "754401126dda8e47602e78f41d6b6ad4e11fe73db0b1aa33905483c02cff28ab", "class_name": "RelatedNodeInfo"}}, "text": "131 Softmax Regression\nEntropy\nThe central idea in information theory is to quantify the amount of information contained\nindata. Thisplacesalimitonourabilitytocompressdata. Foradistribution \ud835\udc43itsentropy,\n\ud835\udc3b\u00bb\ud835\udc43\u00bc, is defined as:\n\ud835\udc3b\u00bb\ud835\udc43\u00bc=\u00d5\n\ud835\udc57\u0000\ud835\udc43\u00b9\ud835\udc57\u00balog\ud835\udc43\u00b9\ud835\udc57\u00ba.(4.1.11)\nOne of the fundamental theorems of information theory states that in order to encode data\ndrawn randomly from the distribution \ud835\udc43, we need at least \ud835\udc3b\u00bb\ud835\udc43\u00bc\u201cnats\u201d to encode it ( Shan-\nnon, 1948 ). If you wonder what a \u201cnat\u201d is, it is the equivalent of bit but when using a code\nwith base\ud835\udc52rather than one with base 2. Thus, one nat is1\nlog\u00b92\u00ba\u00191.44bit.\nSurprisal\nYoumightbewonderingwhatcompressionhastodowithprediction. Imaginethatwehave\na stream of data that we want to compress. If it is always easy for us to predict the next\ntoken, then this data is easy to compress. Take the extreme example where every token in\nthe stream always takes the same value. That is a very boring data stream! And not only\nit is boring, but it is also easy to predict. Because the tokens are always the same, we do\nnot have to transmit any information to communicate the contents of the stream. Easy to\npredict, easy to compress.\nHoweverifwecannotperfectlypredicteveryevent,thenwemightsometimesbesurprised.\nOursurpriseisgreaterwhenaneventisassignedlowerprobability. ClaudeShannonsettled\nonlog1\n\ud835\udc43\u00b9\ud835\udc57\u00ba=\u0000log\ud835\udc43\u00b9\ud835\udc57\u00batoquantifyone\u2019s surprisal atobservinganevent \ud835\udc57havingassigned\nit a (subjective) probability \ud835\udc43\u00b9\ud835\udc57\u00ba. The entropy defined in (4.1.11 )is then the expected\nsurprisal when one assigned the correct probabilities that truly match the data-generating\nprocess.\nCross-EntropyRevisited\nSo if entropy is the level of surprise experienced by someone who knows the true proba-\nbility, then you might be wondering, what is cross-entropy? The cross-entropy from\ud835\udc43to\n\ud835\udc44, denoted\ud835\udc3b\u00b9\ud835\udc43,\ud835\udc44\u00ba, is the expected surprisal of an observer with subjective probabilities\n\ud835\udc44upon seeing data that was actually generated according to probabilities \ud835\udc43. This is given\nby\ud835\udc3b\u00b9\ud835\udc43,\ud835\udc44\u00badef=\u00cd\n\ud835\udc57\u0000\ud835\udc43\u00b9\ud835\udc57\u00balog\ud835\udc44\u00b9\ud835\udc57\u00ba. The lowest possible cross-entropy is achieved when\n\ud835\udc43=\ud835\udc44. In this case, the cross-entropy from \ud835\udc43to\ud835\udc44is\ud835\udc3b\u00b9\ud835\udc43,\ud835\udc43\u00ba=\ud835\udc3b\u00b9\ud835\udc43\u00ba.\nIn short, we can think of the cross-entropy classification objective in two ways: (i) as max-\nimizing the likelihood of the observed data; and (ii) as minimizing our surprisal (and thus\nthe number of bits) required to communicate the labels.\n4.1.4Summaryand Discussion\nInthissection,weencounteredthefirstnontriviallossfunction,allowingustooptimizeover\ndiscreteoutputspaces. Keyinitsdesignwasthatwetookaprobabilisticapproach,treating", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d492a7a-2898-446c-b4bb-58336d18d378": {"__data__": {"id_": "5d492a7a-2898-446c-b4bb-58336d18d378", "embedding": null, "metadata": {"page_label": "132", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b0baee7-0595-43b6-ad90-0ea07d2c0955", "node_type": "4", "metadata": {"page_label": "132", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "62486071a9cb423e2138358c9330986c0114b404041193e7468fd28f4b492c82", "class_name": "RelatedNodeInfo"}}, "text": "132 Linear Neural Networks for Classi\ufb01cation\n89\n90discrete categories as instances of draws from a probability distribution. As a side effect,\nwe encountered the softmax, a convenient activation function that transforms outputs of an\nordinary neural network layer into valid discrete probability distributions. We saw that the\nderivative of the cross-entropy loss when combined with softmax behaves very similarly\nto the derivative of squared error; namely by taking the difference between the expected\nbehavior and its prediction. And, while we were only able to scratch the very surface of it,\nwe encountered exciting connections to statistical physics and information theory.\nWhile this is enough to get you on your way, and hopefully enough to whet your appetite,\nwe hardly dived deep here. Among other things, we skipped over computational con-\nsiderations. Specifically, for any fully connected layer with \ud835\udc51inputs and\ud835\udc5eoutputs, the\nparametrization and computational cost is O\u00b9\ud835\udc51\ud835\udc5e\u00ba, which can be prohibitively high in prac-\ntice. Fortunately, this cost of transforming \ud835\udc51inputs into\ud835\udc5eoutputs can be reduced through\napproximation and compression. For instance Deep Fried Convnets ( Yanget al., 2015)\nuses a combination of permutations, Fourier transforms, and scaling to reduce the cost\nfrom quadratic to log-linear. Similar techniques work for more advanced structural matrix\napproximations ( Sindhwani et al., 2015). Lastly, we can use quaternion-like decomposi-\ntions to reduce the cost to O\u00b9\ud835\udc51\ud835\udc5e\n\ud835\udc5b\u00ba, again if we are willing to trade off a small amount of\naccuracy for computational and storage cost ( Zhanget al., 2021) based on a compression\nfactor\ud835\udc5b. This is an active area of research. What makes it challenging is that we do not\nnecessarily strive for the most compact representation or the smallest number of floating\npoint operations but rather for the solution that can be executed most efficiently on modern\nGPUs.\n4.1.5Exercises\n1.Wecanexploretheconnectionbetweenexponentialfamiliesandsoftmaxinsomemore\ndepth.\n1.Compute the second derivative of the cross-entropy loss \ud835\udc59\u00b9y,\u02c6y\u00bafor softmax.\n2.Computethevarianceofthedistributiongivenby softmax\u00b9o\u00baandshowthatitmatches\nthe second derivative computed above.\n2.Assume that we have three classes which occur with equal probability, i.e., the proba-\nbility vector is\u00b91\n3,1\n3,1\n3\u00ba.\n1.What is the problem if we try to design a binary code for it?\n2.Can you design a better code? Hint: what happens if we try to encode two indepen-\ndent observations? What if we encode \ud835\udc5bobservations jointly?\n3.When encoding signals transmitted over a physical wire, engineers do not always use\nbinary codes. For instance, PAM-389uses three signal levels f\u00001,0,1gas opposed to\ntwo levelsf0,1g. How many ternary units do you need to transmit an integer in the\nrangef0,..., 7g? Why might this be a better idea in terms of electronics?\n4.TheBradley\u2013Terry model90uses a logistic model to capture preferences. For a user to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66869203-ddad-4fd7-af5b-44813d2a5e8b": {"__data__": {"id_": "66869203-ddad-4fd7-af5b-44813d2a5e8b", "embedding": null, "metadata": {"page_label": "133", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31d46ea6-d267-4628-9924-c721964817ef", "node_type": "4", "metadata": {"page_label": "133", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8d06ab6cfeeae2ddcbd4a1b2c91bb7cd59bffa2709517141b3fec08f1957c62a", "class_name": "RelatedNodeInfo"}}, "text": "133 Softmax Regression\n91\n92choose between apples and oranges one assumes scores \ud835\udc5cappleand\ud835\udc5corange. Our require-\nmentsarethatlargerscoresshouldleadtoahigherlikelihoodinchoosingtheassociated\nitemandthattheitemwiththelargestscoreisthemostlikelyonetobechosen( Bradley\nand Terry, 1952 ).\n1.Prove that softmax satisfies this requirement.\n2.What happens if you want to allow for a default option of choosing neither apples\nnor oranges? Hint: now the user has three choices.\n5.Softmaxgetsitsnamefromthefollowingmapping: RealSoftMax \u00b9\ud835\udc4e,\ud835\udc4f\u00ba=log\u00b9exp\u00b9\ud835\udc4e\u00ba\u00b8\nexp\u00b9\ud835\udc4f\u00ba\u00ba.\n1.Prove that RealSoftMax \u00b9\ud835\udc4e,\ud835\udc4f\u00ba>max\u00b9\ud835\udc4e,\ud835\udc4f\u00ba.\n2.How small can you make the difference between both functions? Hint: without loss\nof generality you can set \ud835\udc4f=0and\ud835\udc4e\u0015\ud835\udc4f.\n3.Prove that this holds for \ud835\udf06\u00001RealSoftMax\u00b9\ud835\udf06\ud835\udc4e,\ud835\udf06\ud835\udc4f\u00ba, provided that \ud835\udf06> 0.\n4.Show that for \ud835\udf06!1we have\ud835\udf06\u00001RealSoftMax\u00b9\ud835\udf06\ud835\udc4e,\ud835\udf06\ud835\udc4f\u00ba! max\u00b9\ud835\udc4e,\ud835\udc4f\u00ba.\n5.Construct an analogous softmin function.\n6.Extend this to more than two numbers.\n6.The function \ud835\udc54\u00b9x\u00badef=log\u00cd\n\ud835\udc56exp\ud835\udc65\ud835\udc56is sometimes also referred to as the log-partition\nfunction91.\n1.Prove that the function is convex. Hint: to do so, use the fact that the first derivative\namounts to the probabilities from the softmax function and show that the second\nderivative is the variance.\n2.Show that\ud835\udc54is translation invariant, i.e., \ud835\udc54\u00b9x\u00b8\ud835\udc4f\u00ba=\ud835\udc54\u00b9x\u00ba.\n3.What happens if some of the coordinates \ud835\udc65\ud835\udc56are very large? What happens if they\u2019re\nall very small?\n4.Show that if we choose \ud835\udc4f=max\ud835\udc56\ud835\udc65\ud835\udc56we end up with a numerically stable implemen-\ntation.\n7.Assume that we have some probability distribution \ud835\udc43. Suppose we pick another distri-\nbution\ud835\udc44with\ud835\udc44\u00b9\ud835\udc56\u00ba/\ud835\udc43\u00b9\ud835\udc56\u00ba\ud835\udefcfor\ud835\udefc> 0.\n1.Which choice of \ud835\udefccorresponds to doubling the temperature? Which choice corre-\nsponds to halving it?\n2.What happens if we let the temperature approach 0?\n3.What happens if we let the temperature approach 1?\nDiscussions92.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b05988b5-4961-49f6-b31f-faf5e330ba32": {"__data__": {"id_": "b05988b5-4961-49f6-b31f-faf5e330ba32", "embedding": null, "metadata": {"page_label": "134", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "282005e1-8b2e-400f-907e-6911914aebee", "node_type": "4", "metadata": {"page_label": "134", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bade993eb23b174f18e24b88eb1819f4ba35b7fcae8a5f995146455f87b33c8f", "class_name": "RelatedNodeInfo"}}, "text": "134 Linear Neural Networks for Classi\ufb01cation\n934.2The ImageClassification Dataset\nOne widely used dataset for image classification is the MNIST dataset93(LeCunet al.,\n1998) of handwritten digits. At the time of its release in the 1990s it posed a formidable\nchallenge to most machine learning algorithms, consisting of 60,000 images of 28\u000228\npixelsresolution(plusatestdatasetof10,000images). Toputthingsintoperspective,back\nin1995,aSunSPARCStation5withawhopping64MBofRAMandablistering5MFLOPs\nwasconsideredstateoftheartequipmentformachinelearningatAT&TBellLaboratories.\nAchieving high accuracy on digit recognition was a key component in automating letter\nsorting for the USPS in the 1990s. Deep networks such as LeNet-5 ( LeCunet al., 1995),\nsupport vector machines with invariances ( Sch\u00f6lkopf et al., 1996), and tangent distance\nclassifiers ( Simardetal., 1998) all could reach error rates below 1%.\nFor over a decade, MNIST served as thepoint of reference for comparing machine learn-\ning algorithms. While it had a good run as a benchmark dataset, even simple models by\ntoday\u2019sstandardsachieveclassificationaccuracyover95%, makingitunsuitablefordistin-\nguishing between strong models and weaker ones. Even more, the dataset allows for very\nhigh levels of accuracy, not typically seen in many classification problems. This skewed\nalgorithmic development towards specific families of algorithms that can take advantage\nof clean datasets, such as active set methods and boundary-seeking active set algorithms.\nToday, MNIST serves as more of a sanity check than as a benchmark. ImageNet ( Denget\nal., 2009) poses a much more relevant challenge. Unfortunately, ImageNet is too large for\nmany of the examples and illustrations in this book, as it would take too long to train to\nmake the examples interactive. As a substitute we will focus our discussion in the coming\nsectionsonthequalitativelysimilar,butmuchsmallerFashion-MNISTdataset( Xiaoetal.,\n2017)whichwasreleasedin2017. Itcontainsimagesof10categoriesofclothingat 28\u000228\npixels resolution.\n%matplotlib inline\nimport time\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom d2l import torch asd2l\nd2l.use_svg_display()\n4.2.1Loadingthe Dataset\nSince the Fashion-MNIST dataset is so useful, all major frameworks provide preprocessed\nversions of it. We can download and read it into memory using built-in framework utili-\nties.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "207acf91-d020-47df-9576-e8cac033ee35": {"__data__": {"id_": "207acf91-d020-47df-9576-e8cac033ee35", "embedding": null, "metadata": {"page_label": "135", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6b007eb-8da9-464c-8176-6de8cd446cb6", "node_type": "4", "metadata": {"page_label": "135", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6ec5af79dd74a59f0f41dd408562bf45b210c91270e90b08778adc863b5541fa", "class_name": "RelatedNodeInfo"}}, "text": "135 The Image Classi\ufb01cation Dataset\nclass FashionMNIST (d2l .DataModule): #@save\n\"\"\"The Fashion-MNIST dataset.\"\"\"\ndef __init__ (self , batch_size =64, resize =(28,28)):\nsuper ().__init__ ()\nself .save_hyperparameters()\ntrans =transforms .Compose([transforms .Resize(resize),\ntransforms .ToTensor()])\nself .train =torchvision .datasets .FashionMNIST(\nroot =self .root, train =True , transform =trans, download =True )\nself .val =torchvision .datasets .FashionMNIST(\nroot =self .root, train =False , transform =trans, download =True )\nFashion-MNIST consists of images from 10 categories, each represented by 6000 images\nin the training dataset and by 1000 in the test dataset. A test dataset is used for evaluating\nmodelperformance(itmustnotbeusedfortraining). Consequentlythetrainingsetandthe\ntest set contain 60,000 and 10,000 images, respectively.\ndata =FashionMNIST(resize =(32,32))\nlen(data .train), len(data .val)\n(60000 ,10000 )\nTheimagesaregrayscaleandupscaledto 32\u000232pixelsinresolutionabove. Thisissimilar\nto the original MNIST dataset which consisted of (binary) black and white images. Note,\nthough, that most modern image data has three channels (red, green, blue) and that hyper-\nspectral images can have in excess of 100 channels (the HyMap sensor has 126 channels).\nBy convention we store an image as a \ud835\udc50\u0002\u210e\u0002\ud835\udc64tensor, where \ud835\udc50is the number of color\nchannels,\u210eis the height and \ud835\udc64is the width.\ndata .train[ 0][0].shape\ntorch .Size([ 1,32,32])\nThecategoriesofFashion-MNISThavehuman-understandablenames. Thefollowingcon-\nvenience method converts between numeric labels and their names.\n@d2l .add_to_class(FashionMNIST) #@save\ndef text_labels (self , indices):\n\"\"\"Return text labels.\"\"\"\nlabels =['t-shirt ','trouser ','pullover ','dress ','coat ',\n'sandal ','shirt ','sneaker ','bag','ankle boot ']\nreturn [labels[ int(i)] for iinindices]\n4.2.2Readinga Minibatch\nTomakeourlifeeasierwhenreadingfromthetrainingandtestsets,weusethebuilt-indata\niterator rather than creating one from scratch. Recall that at each iteration, a data iterator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2040, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c0cce0b-db98-49c1-bb13-3c0c3e230c70": {"__data__": {"id_": "2c0cce0b-db98-49c1-bb13-3c0c3e230c70", "embedding": null, "metadata": {"page_label": "136", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff910460-6fad-4b43-8c95-f24a6fe9f6b6", "node_type": "4", "metadata": {"page_label": "136", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0648f7d3b9fd69e2ef9a589233c4dffb1c12facc4167830a352e94a6c47b13a4", "class_name": "RelatedNodeInfo"}}, "text": "136 Linear Neural Networks for Classi\ufb01cation\nreads a minibatch of data with size batch_size . We also randomly shuffle the examples\nfor the training data iterator.\n@d2l .add_to_class(FashionMNIST) #@save\ndef get_dataloader (self , train):\ndata =self .train iftrain else self .val\nreturn torch .utils .data .DataLoader(data, self .batch_size, shuffle =train,\nnum_workers =self .num_workers)\nToseehowthisworks,let\u2019sloadaminibatchofimagesbyinvokingthe train_dataloader\nmethod. It contains 64 images.\nX, y =next (iter (data .train_dataloader()))\nprint (X.shape, X .dtype, y .shape, y .dtype)\ntorch .Size([ 64,1,32,32]) torch .float32 torch .Size([ 64]) torch .int64\nLet\u2019slookatthetimeittakestoreadtheimages. Eventhoughitisabuilt-inloader,itisnot\nblazingly fast. Nonetheless, this is sufficient since processing images with a deep network\ntakes quite a bit longer. Hence it is good enough that training a network will not be I/O\nconstrained.\ntic =time .time()\nfor X, y indata .train_dataloader():\ncontinue\nf'{time .time() -tic:.2f}sec'\n'4.69 sec '\n4.2.3Visualization\nWe will often be using the Fashion-MNIST dataset. A convenience function show_images\ncan be used to visualize the images and the associated labels. Skipping implementation\ndetails, we just show the interface below: we only need to know how to invoke d2l.\nshow_images rather than how it works for such utility functions.\ndef show_images (imgs, num_rows, num_cols, titles =None , scale =1.5): #@save\n\"\"\"Plot a list of images.\"\"\"\nraise NotImplementedError\nLet\u2019sputittogooduse. Ingeneral,itisagoodideatovisualizeandinspectdatathatyouare\ntraining on. Humans are very good at spotting oddities and because of that, visualization\nserves as an additional safeguard against mistakes and errors in the design of experiments.\nHerearetheimagesandtheircorrespondinglabels(intext)forthefirstfewexamplesinthe\ntraining dataset.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6cf94ed3-8517-40ec-baad-09f03de870f1": {"__data__": {"id_": "6cf94ed3-8517-40ec-baad-09f03de870f1", "embedding": null, "metadata": {"page_label": "137", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9f842b8-ad30-4479-98c5-3845cc22a343", "node_type": "4", "metadata": {"page_label": "137", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "450cbe89bea1ac8f7255baf51f94b28cccf0e404d6dc81cbe45cf65651b02594", "class_name": "RelatedNodeInfo"}}, "text": "137 The Image Classi\ufb01cation Dataset\n94@d2l .add_to_class(FashionMNIST) #@save\ndef visualize (self , batch, nrows =1, ncols =8, labels =[]):\nX, y =batch\nifnot labels:\nlabels =self .text_labels(y)\nd2l.show_images(X .squeeze( 1), nrows, ncols, titles =labels)\nbatch =next (iter (data .val_dataloader()))\ndata .visualize(batch)\nWearenowreadytoworkwiththeFashion-MNISTdatasetinthesectionsthatfollow.\n4.2.4Summary\nWenowhaveaslightlymorerealisticdatasettouseforclassification. Fashion-MNISTisan\napparel classification dataset consisting of images representing 10 categories. We will use\nthis dataset in subsequent sections and chapters to evaluate various network designs, from\na simple linear model to advanced residual networks. As we commonly do with images,\nwereadthemasatensorofshape(batchsize,numberofchannels,height,width). Fornow,\nwe only have one channel as the images are grayscale (the visualization above uses a false\ncolor palette for improved visibility).\nLastly,dataiteratorsareakeycomponentforefficientperformance. Forinstance,wemight\nuse GPUs for efficient image decompression, video transcoding, or other preprocessing.\nWhenever possible, you should rely on well-implemented data iterators that exploit high-\nperformance computing to avoid slowing down your training loop.\n4.2.5Exercises\n1.Does reducing the batch_size (for instance, to 1) affect the reading performance?\n2.The data iterator performance is important. Do you think the current implementation\nis fast enough? Explore various options to improve it. Use a system profiler to find out\nwhere the bottlenecks are.\n3.Check out the framework\u2019s online API documentation. Which other datasets are avail-\nable?\nDiscussions94.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "867721e4-aa7a-4afc-8f84-87bd9670a49a": {"__data__": {"id_": "867721e4-aa7a-4afc-8f84-87bd9670a49a", "embedding": null, "metadata": {"page_label": "138", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9097fd4f-f17d-43e5-9f3b-05d277ea6ff0", "node_type": "4", "metadata": {"page_label": "138", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "41535aa3c802e65530499d199da9c00383a6b59e1871b6dadee4636ade5d7985", "class_name": "RelatedNodeInfo"}}, "text": "138 Linear Neural Networks for Classi\ufb01cation\n4.3TheBase Classification Model\nYou may have noticed that the implementations from scratch and the concise implementa-\ntion using framework functionality were quite similar in the case of regression. The same\nistrueforclassification. Sincemanymodelsinthisbookdealwithclassification,itisworth\naddingfunctionalitiestosupportthissettingspecifically. Thissectionprovidesabaseclass\nfor classification models to simplify future code.\nimport torch\nfrom d2l import torch asd2l\n4.3.1The Classifier Class\nWe define the Classifier class below. In the validation_step we report both the loss\nvalue and the classification accuracy on a validation batch. We draw an update for every\nnum_val_batches batches. This has the benefit of generating the averaged loss and ac-\ncuracy on the whole validation data. These average numbers are not exactly correct if the\nfinal batch contains fewer examples, but we ignore this minor difference to keep the code\nsimple.\nclass Classifier (d2l .Module): #@save\n\"\"\"The base class of classification models.\"\"\"\ndef validation_step (self , batch):\nY_hat =self (*batch[: -1])\nself .plot( 'loss ',self .loss(Y_hat, batch[ -1]), train =False )\nself .plot( 'acc',self .accuracy(Y_hat, batch[ -1]), train =False )\nBy default we use a stochastic gradient descent optimizer, operating on minibatches, just\nas we did in the context of linear regression.\n@d2l .add_to_class(d2l .Module) #@save\ndef configure_optimizers (self ):\nreturn torch .optim .SGD( self .parameters(), lr =self .lr)\n4.3.2Accuracy\nGiven the predicted probability distribution y_hat, we typically choose the class with the\nhighest predicted probability whenever we must output a hard prediction. Indeed, many\napplications require that we make a choice. For instance, Gmail must categorize an email\ninto\u201cPrimary\u201d,\u201cSocial\u201d,\u201cUpdates\u201d,\u201cForums\u201d,or\u201cSpam\u201d. Itmightestimateprobabilities\ninternally, but at the end of the day it has to choose one among the classes.\nWhen predictions are consistent with the label class y, they are correct. The classification\naccuracy is the fraction of all predictions that are correct. Although it can be difficult to\noptimizeaccuracydirectly(itisnotdifferentiable),itisoftentheperformancemeasurethat", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "998945fc-69b6-4897-8dda-8c3c098c41a9": {"__data__": {"id_": "998945fc-69b6-4897-8dda-8c3c098c41a9", "embedding": null, "metadata": {"page_label": "139", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f889c2b-dbee-4f51-accc-8ba45db25e5a", "node_type": "4", "metadata": {"page_label": "139", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cbbf557da387ece132c43846a40dc48369716aeef5a492f9eb089e9b3ec5c099", "class_name": "RelatedNodeInfo"}}, "text": "139 The Base Classi\ufb01cation Model\n95we care about the most. It is often therelevant quantity in benchmarks. As such, we will\nnearly always report it when training classifiers.\nAccuracyiscomputedasfollows. First, if y_hatisamatrix, weassumethattheseconddi-\nmensionstorespredictionscoresforeachclass. Weuse argmaxtoobtainthepredictedclass\nby the index for the largest entry in each row. Then we compare the predicted class with\nthe ground truth yelementwise. Since the equality operator ==is sensitive to data types,\nwe convert y_hat\u2019s data type to match that of y. The result is a tensor containing entries\nof 0 (false) and 1 (true). Taking the sum yields the number of correct predictions.\n@d2l .add_to_class(Classifier) #@save\ndef accuracy (self , Y_hat, Y, averaged =True ):\n\"\"\"Compute the number of correct predictions.\"\"\"\nY_hat =Y_hat .reshape(( -1, Y_hat .shape[ -1]))\npreds =Y_hat .argmax(axis =1).type(Y .dtype)\ncompare =(preds ==Y.reshape( -1)).type(torch .float32)\nreturn compare .mean() ifaveraged else compare\n4.3.3Summary\nClassification is a sufficiently common problem that it warrants its own convenience func-\ntions. Of central importance in classification is the accuracy of the classifier. Note that\nwhile we often care primarily about accuracy, we train classifiers to optimize a variety of\nother objectives for statistical and computational reasons. However, regardless of which\nloss function was minimized during training, it is useful to have a convenience method for\nassessing the accuracy of our classifier empirically.\n4.3.4Exercises\n1.Denote by\ud835\udc3fvthe validation loss, and let \ud835\udc3fq\nvbe its quick and dirty estimate computed\nby the loss function averaging in this section. Lastly, denote by \ud835\udc59b\nvthe loss on the last\nminibatch. Express \ud835\udc3fvin terms of\ud835\udc3fq\nv,\ud835\udc59b\nv, and the sample and minibatch sizes.\n2.Show that the quick and dirty estimate \ud835\udc3fq\nvis unbiased. That is, show that \ud835\udc38\u00bb\ud835\udc3fv\u00bc=\n\ud835\udc38\u00bb\ud835\udc3fq\nv\u00bc. Why would you still want to use \ud835\udc3fvinstead?\n3.Given a multiclass classification loss, denoting by \ud835\udc59\u00b9\ud835\udc66,\ud835\udc660\u00bathe penalty of estimating\n\ud835\udc660when we see \ud835\udc66and given a probabilty \ud835\udc5d\u00b9\ud835\udc66j\ud835\udc65\u00ba, formulate the rule for an optimal\nselection of\ud835\udc660. Hint: express the expected loss, using \ud835\udc59and\ud835\udc5d\u00b9\ud835\udc66j\ud835\udc65\u00ba.\nDiscussions95.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0289e801-99f3-47bf-b3e9-f0ca9829e04e": {"__data__": {"id_": "0289e801-99f3-47bf-b3e9-f0ca9829e04e", "embedding": null, "metadata": {"page_label": "140", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d176f800-d9c2-4502-8c4f-4a8ba4b98455", "node_type": "4", "metadata": {"page_label": "140", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a29c8b4e6b23affb8a403f38813cc556d930cb5c068fe2dcd345747442e70656", "class_name": "RelatedNodeInfo"}}, "text": "140 Linear Neural Networks for Classi\ufb01cation\n964.4Softmax RegressionImplementation from\nScratch\nBecause softmax regression is so fundamental, we believe that you ought to know how to\nimplement it yourself. Here, we limit ourselves to defining the softmax-specific aspects of\nthemodelandreusetheothercomponentsfromourlinearregressionsection,includingthe\ntraining loop.\nimport torch\nfrom d2l import torch asd2l\n4.4.1The Softmax\nLet\u2019s begin with the most important part: the mapping from scalars to probabilities. For a\nrefresher, recall the operation of the sum operator along specific dimensions in a tensor, as\ndiscussedin Section2.3.6 andSection2.3.7 . Givenamatrix Xwecansumoverallelements\n(by default) or only over elements in the same axis. The axisvariable lets us compute row\nand column sums:\nX=torch .tensor([[ 1.0,2.0,3.0], [ 4.0,5.0,6.0]])\nX.sum( 0, keepdims =True ), X .sum( 1, keepdims =True )\n(tensor([[ 5.,7.,9.]]),\ntensor([[ 6.],\n[15.]]))\nComputing the softmax requires three steps: (i) exponentiation of each term; (ii) a sum\nover each row to compute the normalization constant for each example; (iii) division of\neach row by its normalization constant, ensuring that the result sums to 1:\nsoftmax\u00b9X\u00ba\ud835\udc56\ud835\udc57=exp\u00b9X\ud835\udc56\ud835\udc57\u00ba\u00cd\n\ud835\udc58exp\u00b9X\ud835\udc56\ud835\udc58\u00ba. (4.4.1)\nThe (logarithm of the) denominator is called the (log) partitionfunction . It was introduced\ninstatistical physics96to sum over all possible states in a thermodynamic ensemble. The\nimplementation is straightforward:\ndef softmax (X):\nX_exp =torch .exp(X)\npartition =X_exp .sum( 1, keepdims =True )\nreturn X_exp /partition # The broadcasting mechanism is applied here\nFor any input X, we turn each element into a nonnegative number. Each row sums up to\n1, as is required for a probability. Caution: the code above is notrobust against very large\nor very small arguments. While it is sufficient to illustrate what is happening, you should", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8be326e3-1e82-4c2b-b72f-4d2c1893dc31": {"__data__": {"id_": "8be326e3-1e82-4c2b-b72f-4d2c1893dc31", "embedding": null, "metadata": {"page_label": "141", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "426b8838-6dbd-4ce1-9e62-4a7dd5d25754", "node_type": "4", "metadata": {"page_label": "141", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a491cdc182559ca187ed84987c5c4da3df39d1291b4f66b735c8b08b976a3eb8", "class_name": "RelatedNodeInfo"}}, "text": "141 Softmax Regression Implementation from Scratch\nnotuse this code verbatim for any serious purpose. Deep learning frameworks have such\nprotections built in and we will be using the built-in softmax going forward.\nX=torch .rand(( 2,5))\nX_prob =softmax(X)\nX_prob, X_prob .sum( 1)\n(tensor([[ 0.2511 ,0.1417 ,0.1158 ,0.2529 ,0.2385 ],\n[0.2004 ,0.1419 ,0.1957 ,0.2504 ,0.2117 ]]),\ntensor([ 1.,1.]))\n4.4.2TheModel\nWe now have everything that we need to implement the softmax regression model. As in\nour linear regression example, each instance will be represented by a fixed-length vector.\nSince the raw data here consists of 28\u000228pixel images, we flatten each image, treating\nthem as vectors of length 784. In later chapters, we will introduce convolutional neural\nnetworks, which exploit the spatial structure in a more satisfying way.\nIn softmax regression, the number of outputs from our network should be equal to the\nnumber of classes. Since our dataset has 10 classes, our network has an output dimension\nof 10. Consequently, our weights constitute a 784\u000210matrix plus a 1\u000210row vector for\nthe biases. As with linear regression, we initialize the weights Wwith Gaussian noise. The\nbiases are initialized as zeros.\nclass SoftmaxRegressionScratch (d2l .Classifier):\ndef __init__ (self , num_inputs, num_outputs, lr, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .W=torch .normal( 0, sigma, size =(num_inputs, num_outputs),\nrequires_grad =True )\nself .b=torch .zeros(num_outputs, requires_grad =True )\ndef parameters (self ):\nreturn [self .W,self .b]\nThecodebelowdefineshowthenetworkmapseachinputtoanoutput. Notethatweflatten\neach 28\u000228pixel image in the batch into a vector using reshape before passing the data\nthrough our model.\n@d2l .add_to_class(SoftmaxRegressionScratch)\ndef forward (self , X):\nX=X.reshape(( -1,self .W.shape[ 0]))\nreturn softmax(torch .matmul(X, self .W)+self .b)\n4.4.3The Cross-EntropyLoss\nNext we need to implement the cross-entropy loss function (introduced in Section 4.1.2 ).\nThis may be the most common loss function in all of deep learning. At the moment, appli-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2117, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c922f94-d980-4159-88ce-2671031cc20f": {"__data__": {"id_": "6c922f94-d980-4159-88ce-2671031cc20f", "embedding": null, "metadata": {"page_label": "142", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51052632-3339-4f15-8a1b-d50e5d13eb34", "node_type": "4", "metadata": {"page_label": "142", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "98e812e5e00a1a49cdafde468a41085538968ec57b108958bbef94d47eacc3a7", "class_name": "RelatedNodeInfo"}}, "text": "142 Linear Neural Networks for Classi\ufb01cation\ncations of deep learning easily cast as classification problems far outnumber those better\ntreated as regression problems.\nRecall that cross-entropy takes the negative log-likelihood of the predicted probability as-\nsigned to the true label. For efficiency we avoid Python for-loops and use indexing instead.\nIn particular, the one-hot encoding in yallows us to select the matching terms in \u02c6y.\nToseethisinactionwecreatesampledata y_hatwith2examplesofpredictedprobabilities\nover 3 classes and their corresponding labels y. The correct labels are 0and2respectively\n(i.e., the first and third class). Using yas the indices of the probabilities in y_hat, we can\npick out terms efficiently.\ny=torch .tensor([ 0,2])\ny_hat =torch .tensor([[ 0.1,0.3,0.6], [ 0.3,0.2,0.5]])\ny_hat[[ 0,1], y]\ntensor([ 0.1000 ,0.5000 ])\nNowwecanimplementthecross-entropylossfunctionbyaveragingoverthelogarithmsof\nthe selected probabilities.\ndef cross_entropy (y_hat, y):\nreturn -torch .log(y_hat[ list (range (len(y_hat))), y]) .mean()\ncross_entropy(y_hat, y)\ntensor( 1.4979 )\n@d2l .add_to_class(SoftmaxRegressionScratch)\ndef loss (self , y_hat, y):\nreturn cross_entropy(y_hat, y)\n4.4.4Training\nWereusethe fitmethoddefinedin Section3.4 totrainthemodelwith10epochs. Notethat\nthe number of epochs ( max_epochs ), the minibatch size ( batch_size ), and learning rate\n(lr) are adjustable hyperparameters. That means that while these values are not learned\nduring our primary training loop, they still influence the performance of our model, both\nvis-\u00e0-vistrainingandgeneralizationperformance. Inpracticeyouwillwanttochoosethese\nvalues based on the validation split of the data and then, ultimately, to evaluate your final\nmodelonthe testsplit. Asdiscussedin Section3.6.3 ,wewillregardthetestdataofFashion-\nMNIST as the validation set, thus reporting validation loss and validation accuracy on this\nsplit.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0c07562-2367-4994-8642-ec514c193659": {"__data__": {"id_": "d0c07562-2367-4994-8642-ec514c193659", "embedding": null, "metadata": {"page_label": "143", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab13f966-da7f-4b06-b8b9-f67d9caa8262", "node_type": "4", "metadata": {"page_label": "143", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "33414b5598c06b718dd27f11674bb308790fd5c6e9263fcc875bb72e955a721a", "class_name": "RelatedNodeInfo"}}, "text": "143 Softmax Regression Implementation from Scratch\ndata =d2l.FashionMNIST(batch_size =256)\nmodel =SoftmaxRegressionScratch(num_inputs =784, num_outputs =10, lr =0.1)\ntrainer =d2l.Trainer(max_epochs =10)\ntrainer .fit(model, data)\n4.4.5Prediction\nNow that training is complete, our model is ready to classify some images.\nX, y =next (iter (data .val_dataloader()))\npreds =model(X) .argmax(axis =1)\npreds .shape\ntorch .Size([ 256])\nWearemoreinterestedintheimageswelabel incorrectly . Wevisualizethembycomparing\ntheiractuallabels(firstlineoftextoutput)withthepredictionsfromthemodel(secondline\nof text output).\nwrong =preds .type(y .dtype) !=y\nX, y, preds =X[wrong], y[wrong], preds[wrong]\nlabels =[a+'\\n'+bfor a, b inzip(\ndata .text_labels(y), data .text_labels(preds))]\ndata .visualize([X, y], labels =labels)\n4.4.6Summary\nBy now we are starting to get some experience with solving linear regression and classifi-\ncation problems. With it, we have reached what would arguably be the state of the art of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccb78050-de0b-4ce3-9c79-8c07f7a4c288": {"__data__": {"id_": "ccb78050-de0b-4ce3-9c79-8c07f7a4c288", "embedding": null, "metadata": {"page_label": "144", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2060e13e-9074-422c-9308-ecbec79611c0", "node_type": "4", "metadata": {"page_label": "144", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "df4dc4c99fe3443bde0cf30d60f6fd7be5c768a4d3bd61652b111240b6d5b8a0", "class_name": "RelatedNodeInfo"}}, "text": "144 Linear Neural Networks for Classi\ufb01cation\n971960\u20131970s of statistical modeling. In the next section, we will show you how to leverage\ndeep learning frameworks to implement this model much more efficiently.\n4.4.7Exercises\n1.Inthissection,wedirectlyimplementedthesoftmaxfunctionbasedonthemathematical\ndefinitionofthesoftmaxoperation. Asdiscussedin Section4.1 thiscancausenumerical\ninstabilities.\n1.Test whether softmax still works correctly if an input has a value of 100.\n2.Test whether softmax still works correctly if the largest of all inputs is smaller than\n\u0000100?\n3.Implement a fix by looking at the value relative to the largest entry in the argument.\n2.Implement a cross_entropy function that follows the definition of the cross-entropy\nloss function\u00cd\n\ud835\udc56\ud835\udc66\ud835\udc56log \u02c6\ud835\udc66\ud835\udc56.\n1.Try it out in the code example of this section.\n2.Why do you think it runs more slowly?\n3.Should you use it? When would it make sense to?\n4.What do you need to be careful of? Hint: consider the domain of the logarithm.\n3.Is it always a good idea to return the most likely label? For example, would you do this\nfor medical diagnosis? How would you try to address this?\n4.Assume that we want to use softmax regression to predict the next word based on some\nfeatures. What are some problems that might arise from a large vocabulary?\n5.Experiment with the hyperparameters of the code in this section. In particular:\n1.Plot how the validation loss changes as you change the learning rate.\n2.Do the validation and training loss change as you change the minibatch size? How\nlarge or small do you need to go before you see an effect?\nDiscussions97.\n4.5ConciseImplementation of Softmax Regression\nJust as high-level deep learning frameworks made it easier to implement linear regression\n(seeSection 3.5 ), they are similarly convenient here.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68a061f8-ef31-48f9-8961-08613889e968": {"__data__": {"id_": "68a061f8-ef31-48f9-8961-08613889e968", "embedding": null, "metadata": {"page_label": "145", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4460bf1d-a326-4c70-927b-fd9e381a6eea", "node_type": "4", "metadata": {"page_label": "145", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "54ae49ce944a4b2502cb85ae31989f01b78a450b49db1356e0cd4f10e410342c", "class_name": "RelatedNodeInfo"}}, "text": "145 Concise Implementation of Softmax Regression\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n4.5.1Definingthe Model\nAsinSection3.5 ,weconstructourfullyconnectedlayerusingthebuilt-inlayer. Thebuilt-\nin__call__ methodtheninvokes forward wheneverweneedtoapplythenetworktosome\ninput.\nWe use a Flatten layer to convert the fourth-order tensor Xto second order by keeping the\ndimensionality along the first axis unchanged.\nclass SoftmaxRegression (d2l .Classifier): #@save\n\"\"\"The softmax regression model.\"\"\"\ndef __init__ (self , num_outputs, lr):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(nn .Flatten(),\nnn.LazyLinear(num_outputs))\ndef forward (self , X):\nreturn self .net(X)\n4.5.2Softmax Revisited\nInSection 4.4 we calculated our model\u2019s output and applied the cross-entropy loss. While\nthis is perfectly reasonable mathematically, it is risky computationally, because of numer-\nical underflow and overflow in the exponentiation.\nRecall that the softmax function computes probabilities via \u02c6\ud835\udc66\ud835\udc57=exp\u00b9\ud835\udc5c\ud835\udc57\u00ba\u00cd\n\ud835\udc58exp\u00b9\ud835\udc5c\ud835\udc58\u00ba. If some of the\n\ud835\udc5c\ud835\udc58are very large, i.e., very positive, then exp\u00b9\ud835\udc5c\ud835\udc58\u00bamight be larger than the largest number\nwe can have for certain data types. This is called overflow . Likewise, if every argument is\na very large negative number, we will get underflow . For instance, single precision floating\npoint numbers approximately cover the range of 10\u000038to1038. As such, if the largest term\ninolies outside the interval \u00bb\u000090,90\u00bc, the result will not be stable. A way round this\nproblem is to subtract \u00af\ud835\udc5cdef=max\ud835\udc58\ud835\udc5c\ud835\udc58from all entries:\n\u02c6\ud835\udc66\ud835\udc57=exp\ud835\udc5c\ud835\udc57\u00cd\n\ud835\udc58exp\ud835\udc5c\ud835\udc58=exp\u00b9\ud835\udc5c\ud835\udc57\u0000\u00af\ud835\udc5c\u00baexp \u00af\ud835\udc5c\u00cd\n\ud835\udc58exp\u00b9\ud835\udc5c\ud835\udc58\u0000\u00af\ud835\udc5c\u00baexp \u00af\ud835\udc5c=exp\u00b9\ud835\udc5c\ud835\udc57\u0000\u00af\ud835\udc5c\u00ba\u00cd\n\ud835\udc58exp\u00b9\ud835\udc5c\ud835\udc58\u0000\u00af\ud835\udc5c\u00ba. (4.5.1)\nBy construction we know that \ud835\udc5c\ud835\udc57\u0000\u00af\ud835\udc5c\u00140for all\ud835\udc57. As such, for a \ud835\udc5e-class classification\nproblem, the denominator is contained in the interval \u00bb1,\ud835\udc5e\u00bc. Moreover, the numerator\nnever exceeds 1, thus preventing numerical overflow. Numerical underflow only occurs\nwhen exp\u00b9\ud835\udc5c\ud835\udc57\u0000\u00af\ud835\udc5c\u00banumerically evaluates as 0. Nonetheless, a few steps down the road we\nmight find ourselves in trouble when we want to compute log \u02c6\ud835\udc66\ud835\udc57aslog 0. In particular, in\nbackpropagation, we might find ourselves faced with a screenful of the dreaded NaN(Not a\nNumber) results.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2239, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80c04846-b36e-475c-8109-32ac11cacd27": {"__data__": {"id_": "80c04846-b36e-475c-8109-32ac11cacd27", "embedding": null, "metadata": {"page_label": "146", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4296d53-305e-42c1-9629-0491912b9fe0", "node_type": "4", "metadata": {"page_label": "146", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "36253feb61319b2ee70b4040330e6e54546910a9ad1232a30d8789863b1fa114", "class_name": "RelatedNodeInfo"}}, "text": "146 Linear Neural Networks for Classi\ufb01cation\n98Fortunately, we are saved by the fact that even though we are computing exponential func-\ntions, we ultimately intend to take their log (when calculating the cross-entropy loss). By\ncombining softmax and cross-entropy, we can escape the numerical stability issues alto-\ngether. We have:\nlog \u02c6\ud835\udc66\ud835\udc57=logexp\u00b9\ud835\udc5c\ud835\udc57\u0000\u00af\ud835\udc5c\u00ba\u00cd\n\ud835\udc58exp\u00b9\ud835\udc5c\ud835\udc58\u0000\u00af\ud835\udc5c\u00ba=\ud835\udc5c\ud835\udc57\u0000\u00af\ud835\udc5c\u0000log\u00d5\n\ud835\udc58exp\u00b9\ud835\udc5c\ud835\udc58\u0000\u00af\ud835\udc5c\u00ba. (4.5.2)\nThis avoids both overflow and underflow. We will want to keep the conventional softmax\nfunction handy in case we ever want to evaluate the output probabilities by our model. But\ninstead of passing softmax probabilities into our new loss function, we just pass the logits\nandcomputethesoftmaxanditslogallatonceinsidethecross-entropylossfunction,which\ndoes smart things like the \u201cLogSumExp trick\u201d98.\n@d2l .add_to_class(d2l .Classifier) #@save\ndef loss (self , Y_hat, Y, averaged =True ):\nY_hat =Y_hat .reshape(( -1, Y_hat .shape[ -1]))\nY=Y.reshape(( -1,))\nreturn F.cross_entropy(\nY_hat, Y, reduction ='mean 'ifaveraged else 'none ')\n4.5.3Training\nNext we train our model. We use Fashion-MNIST images, flattened to 784-dimensional\nfeature vectors.\ndata =d2l.FashionMNIST(batch_size =256)\nmodel =SoftmaxRegression(num_outputs =10, lr =0.1)\ntrainer =d2l.Trainer(max_epochs =10)\ntrainer .fit(model, data)\nAsbefore,thisalgorithmconvergestoasolutionthatisreasonablyaccurate,albeitthistime\nwith fewer lines of code than before.\n4.5.4Summary\nHigh-levelAPIsareveryconvenientathidingfromtheiruserpotentiallydangerousaspects,\nsuch as numerical stability. Moreover, they allow users to design models concisely with", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a41a563f-b8a0-4304-a83c-24d02dbebca7": {"__data__": {"id_": "a41a563f-b8a0-4304-a83c-24d02dbebca7", "embedding": null, "metadata": {"page_label": "147", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5c0917c-aaa2-4077-bda8-bd5e838abc85", "node_type": "4", "metadata": {"page_label": "147", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2d58f2a362b6b6713746dc7c0d2664d731046632baa85a993ee40fde6ab95226", "class_name": "RelatedNodeInfo"}}, "text": "147 Generalization in Classi\ufb01cation\n99very few lines of code. This is both a blessing and a curse. The obvious benefit is that it\nmakesthingshighlyaccessible,eventoengineerswhonevertookasingleclassofstatistics\nin their life (in fact, they are part of the target audience of the book). But hiding the sharp\nedgesalsocomeswith aprice: a disincentivetoadd newanddifferent componentsonyour\nown, since there is little muscle memory for doing it. Moreover, it makes it more difficult\ntofixthings whenever the protective padding of a framework fails to cover all the corner\ncases entirely. Again, this is due to lack of familiarity.\nAs such, we strongly urge you to review boththe bare bones and the elegant versions of\nmany of the implementations that follow. While we emphasize ease of understanding, the\nimplementations are nonetheless usually quite performant (convolutions are the big excep-\ntionhere). Itisourintentiontoallowyoutobuildonthesewhenyouinventsomethingnew\nthat no framework can give you.\n4.5.5Exercises\n1.Deep learning uses many different number formats, including FP64 double precision\n(used extremely rarely), FP32 single precision, BFLOAT16 (good for compressed rep-\nresentations), FP16 (very unstable), TF32 (a new format from NVIDIA), and INT8.\nCompute the smallest and largest argument of the exponential function for which the\nresult does not lead to numerical underflow or overflow.\n2.INT8 is a very limited format consisting of nonzero numbers from 1to255. How could\nyou extend its dynamic range without using more bits? Do standard multiplication and\naddition still work?\n3.Increasethenumberofepochsfortraining. Whymightthevalidationaccuracydecrease\nafter a while? How could we fix this?\n4.What happens as you increase the learning rate? Compare the loss curves for several\nlearning rates. Which one works better? When?\nDiscussions99.\n4.6Generalizationin Classification\nSo far, we have focused on how to tackle multiclass classification problems by training\n(linear) neural networks with multiple outputs and softmax functions. Interpreting our\nmodel\u2019s outputs as probabilistic predictions, we motivated and derived the cross-entropy\nloss function, which calculates the negative log likelihood that our model (for a fixed set\nof parameters) assigns to the actual labels. And finally, we put these tools into practice\nby fitting our model to the training set. However, as always, our goal is to learn general\npatterns, as assessed empirically on previously unseen data (the test set). High accuracy\non the training set means nothing. Whenever each of our inputs is unique (and indeed this\nis true for most high-dimensional datasets), we can attain perfect accuracy on the training", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c2a6668-144c-4172-9d82-50e86c9ef006": {"__data__": {"id_": "1c2a6668-144c-4172-9d82-50e86c9ef006", "embedding": null, "metadata": {"page_label": "148", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3dc3e4e8-6507-42bd-ad99-1914b4e246c6", "node_type": "4", "metadata": {"page_label": "148", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "59620f14fb201d3b6aebb54a13a826a72fda8b162a71b38d9fec646202b8cf5a", "class_name": "RelatedNodeInfo"}}, "text": "148 Linear Neural Networks for Classi\ufb01cation\nset by just memorizing the dataset on the first training epoch, and subsequently looking up\nthe label whenever we see a new image. And yet, memorizing the exact labels associated\nwith the exact training examples does not tell us how to classify new examples. Absent\nfurther guidance, we might have to fall back on random guessing whenever we encounter\nnew examples.\nA number of burning questions demand immediate attention:\n1.How many test examples do we need to give a good estimate of the accuracy of our\nclassifiers on the underlying population?\n2.What happens if we keep evaluating models on the same test repeatedly?\n3.Why should we expect that fitting our linear models to the training set should fare any\nbetter than our naive memorization scheme?\nWhereas Section3.6 introducedthebasicsofoverfittingandgeneralizationinthecontextof\nlinear regression, this chapter will go a little deeper, introducing some of the foundational\nideas of statistical learning theory. It turns out that we often can guarantee generalization\na priori: for many models, and for any desired upper bound on the generalization gap \ud835\udf16,\nwe can often determine some required number of samples \ud835\udc5bsuch that if our training set\ncontains at least \ud835\udc5bsamples, our empirical error will lie within \ud835\udf16of the true error, for any\ndatageneratingdistribution . Unfortunately, it also turns out that while these sorts of guar-\nantees provide a profound set of intellectual building blocks, they are of limited practical\nutility to the deep learning practitioner. In short, these guarantees suggest that ensuring\ngeneralization of deep neural networks a priori requires an absurd number of examples\n(perhapstrillionsormore), evenwhenwefindthat, onthetaskswecareabout, deepneural\nnetworks typically generalize remarkably well with far fewer examples (thousands). Thus\ndeep learning practitioners often forgo a priori guarantees altogether, instead employing\nmethods that have generalized well on similar problems in the past, and certifying gen-\neralization post hoc through empirical evaluations. When we get to Chapter 5 , we will\nrevisit generalization and provide a light introduction to the vast scientific literature that\nhas sprung in attempts to explain why deep neural networks generalize in practice.\n4.6.1The TestSet\nSince we have already begun to rely on test sets as the gold standard method for assessing\ngeneralization error, let\u2019s get started by discussing the properties of such error estimates.\nLet\u2019s focus on a fixed classifier \ud835\udc53, without worrying about how it was obtained. Moreover\nsuppose that we possess a freshdataset of examples D=\u00b9x\u00b9\ud835\udc56\u00ba,\ud835\udc66\u00b9\ud835\udc56\u00ba\u00ba\ud835\udc5b\n\ud835\udc56=1that were not used\nto train the classifier \ud835\udc53. Theempiricalerror of our classifier \ud835\udc53onDis simply the fraction\nofinstancesforwhichtheprediction \ud835\udc53\u00b9x\u00b9\ud835\udc56\u00ba\u00badisagreeswiththetruelabel \ud835\udc66\u00b9\ud835\udc56\u00ba, andisgiven\nby the following expression:\n\ud835\udf16D\u00b9\ud835\udc53\u00ba=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=11\u00b9\ud835\udc53\u00b9x\u00b9\ud835\udc56\u00ba\u00ba\u2260\ud835\udc66\u00b9\ud835\udc56\u00ba\u00ba. (4.6.1)\nBycontrast,the populationerror istheexpected fractionofexamplesintheunderlyingpop-\nulation (some distribution \ud835\udc43\u00b9\ud835\udc4b,\ud835\udc4c\u00bacharacterized by probability density function \ud835\udc5d\u00b9x,\ud835\udc66\u00ba)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ef6463d-e027-4487-b5f9-00ec23d1bd01": {"__data__": {"id_": "4ef6463d-e027-4487-b5f9-00ec23d1bd01", "embedding": null, "metadata": {"page_label": "149", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71b68203-775b-420b-9a36-4a04646551ce", "node_type": "4", "metadata": {"page_label": "149", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0e6a9d266ad564e9ad781505620af70ceced879a7d6af4b385b147d6db9310dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6de3f18f-bb5a-496b-bc96-c1218574d6ab", "node_type": "1", "metadata": {}, "hash": "1fdfbd0fdc98c8f7441da22d3913b1bc58bb4ff70723c1f47f4448e79f0b7522", "class_name": "RelatedNodeInfo"}}, "text": "149 Generalization in Classi\ufb01cation\nfor which our classifier disagrees with the true label:\n\ud835\udf16\u00b9\ud835\udc53\u00ba=\ud835\udc38\u00b9x,\ud835\udc66\u00ba\u0018\ud835\udc431\u00b9\ud835\udc53\u00b9x\u00ba\u2260\ud835\udc66\u00ba=\u00b9 \u00b9\n1\u00b9\ud835\udc53\u00b9x\u00ba\u2260\ud835\udc66\u00ba\ud835\udc5d\u00b9x,\ud835\udc66\u00ba\ud835\udc51x\ud835\udc51\ud835\udc66. (4.6.2)\nWhile\ud835\udf16\u00b9\ud835\udc53\u00bais the quantity that we actually care about, we cannot observe it directly, just\nas we cannot directly observe the average height in a large population without measuring\nevery single person. We can only estimate this quantity based on samples. Because our\ntest setDis statistically representative of the underlying population, we can view \ud835\udf16D\u00b9\ud835\udc53\u00ba\nas a statistical estimator of the population error \ud835\udf16\u00b9\ud835\udc53\u00ba. Moreover, because our quantity of\ninterest\ud835\udf16\u00b9\ud835\udc53\u00baisanexpectation(oftherandomvariable 1\u00b9\ud835\udc53\u00b9\ud835\udc4b\u00ba\u2260\ud835\udc4c\u00ba)andthecorresponding\nestimator\ud835\udf16D\u00b9\ud835\udc53\u00baisthesampleaverage,estimatingthepopulationerrorissimplytheclassic\nproblem of mean estimation, which you may recall from Section 2.6 .\nAn important classical result from probability theory called the centrallimittheorem guar-\nantees that whenever we possess \ud835\udc5brandom samples \ud835\udc4e1,...,\ud835\udc4e\ud835\udc5bdrawn from any distribution\nwith mean\ud835\udf07and standard deviation \ud835\udf0e, then, as the number of samples \ud835\udc5bapproaches infin-\nity,thesampleaverage \u02c6\ud835\udf07approximatelytendstowardsanormaldistributioncenteredatthe\ntrue mean and with standard deviation \ud835\udf0e\u009dp\ud835\udc5b. Already, this tells us something important:\nasthenumberofexamplesgrowslarge,ourtesterror \ud835\udf16D\u00b9\ud835\udc53\u00bashouldapproachthetrueerror\n\ud835\udf16\u00b9\ud835\udc53\u00baat a rate ofO\u00b91\u009dp\ud835\udc5b\u00ba. Thus, to estimate our test error twice as precisely, we must\ncollect four times as large a test set. To reduce our test error by a factor of one hundred, we\nmust collect ten thousand times as large a test set. In general, such a rate of O\u00b91\u009dp\ud835\udc5b\u00bais\noften the best we can hope for in statistics.\nNow that we know something about the asymptotic rate at which our test error \ud835\udf16D\u00b9\ud835\udc53\u00ba\nconverges to the true error \ud835\udf16\u00b9\ud835\udc53\u00ba, we can zoom in on some important details. Recall that\nthe random variable of interest 1\u00b9\ud835\udc53\u00b9\ud835\udc4b\u00ba\u2260\ud835\udc4c\u00bacan only take values 0and 1and thus is\na Bernoulli random variable, characterized by a parameter indicating the probability that\nit takes value 1. Here, 1means that our classifier made an error, so the parameter of our\nrandomvariableisactuallythetrueerrorrate \ud835\udf16\u00b9\ud835\udc53\u00ba. Thevariance \ud835\udf0e2ofaBernoullidepends\non its parameter (here, \ud835\udf16\u00b9\ud835\udc53\u00ba) according to the expression \ud835\udf16\u00b9\ud835\udc53\u00ba\u00b91\u0000\ud835\udf16\u00b9\ud835\udc53\u00ba\u00ba. While\ud835\udf16\u00b9\ud835\udc53\u00bais\ninitially unknown, we know that it cannot be greater than 1. A little investigation of this\nfunction reveals that our variance is highest when the true error rate is close to 0.5and can\nbe far lower when it is close to 0or close to 1. This tells us that the asymptotic standard\ndeviation of our estimate \ud835\udf16D\u00b9\ud835\udc53\u00baof the error\ud835\udf16\u00b9\ud835\udc53\u00ba(over the choice of the \ud835\udc5btest samples)\ncannot be any greater thanp\n0.25\u009d\ud835\udc5b.\nIf we ignore the fact that this rate characterizes behavior as the test set size approaches\ninfinity rather than when we possess finite samples, this tells us that if we want our test\nerror\ud835\udf16D\u00b9\ud835\udc53\u00bato approximate the population error \ud835\udf16\u00b9\ud835\udc53\u00basuch that one standard deviation\ncorresponds to an interval of \u00060.01, then we should collect roughly 2500 samples.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6de3f18f-bb5a-496b-bc96-c1218574d6ab": {"__data__": {"id_": "6de3f18f-bb5a-496b-bc96-c1218574d6ab", "embedding": null, "metadata": {"page_label": "149", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71b68203-775b-420b-9a36-4a04646551ce", "node_type": "4", "metadata": {"page_label": "149", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0e6a9d266ad564e9ad781505620af70ceced879a7d6af4b385b147d6db9310dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4ef6463d-e027-4487-b5f9-00ec23d1bd01", "node_type": "1", "metadata": {"page_label": "149", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "db5dc04ca534d86dd582bdafb9d67f9db0b515c31bd3cb1361885b2c5bcce580", "class_name": "RelatedNodeInfo"}}, "text": "A little investigation of this\nfunction reveals that our variance is highest when the true error rate is close to 0.5and can\nbe far lower when it is close to 0or close to 1. This tells us that the asymptotic standard\ndeviation of our estimate \ud835\udf16D\u00b9\ud835\udc53\u00baof the error\ud835\udf16\u00b9\ud835\udc53\u00ba(over the choice of the \ud835\udc5btest samples)\ncannot be any greater thanp\n0.25\u009d\ud835\udc5b.\nIf we ignore the fact that this rate characterizes behavior as the test set size approaches\ninfinity rather than when we possess finite samples, this tells us that if we want our test\nerror\ud835\udf16D\u00b9\ud835\udc53\u00bato approximate the population error \ud835\udf16\u00b9\ud835\udc53\u00basuch that one standard deviation\ncorresponds to an interval of \u00060.01, then we should collect roughly 2500 samples. If we\nwant to fit two standard deviations in that range and thus be 95% confident that \ud835\udf16D\u00b9\ud835\udc53\u00ba2\n\ud835\udf16\u00b9\ud835\udc53\u00ba\u00060.01, then we will need 10,000 samples!\nThisturnsouttobethesizeofthetestsetsformanypopularbenchmarksinmachinelearn-\ning. You might be surprised to find out that thousands of applied deep learning papers get\npublished every year making a big deal out of error rate improvements of 0.01or less. Of", "mimetype": "text/plain", "start_char_idx": 2302, "end_char_idx": 3384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "910ca6ab-ef48-4277-989f-7459461e69a6": {"__data__": {"id_": "910ca6ab-ef48-4277-989f-7459461e69a6", "embedding": null, "metadata": {"page_label": "150", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43b2b1d8-83c4-48cb-9866-0023673d0dd9", "node_type": "4", "metadata": {"page_label": "150", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4f5c7058c15eb554804f512901713d14f2987908fb044b2b7bba7bc81e39179f", "class_name": "RelatedNodeInfo"}}, "text": "150 Linear Neural Networks for Classi\ufb01cation\ncourse, when the error rates are much closer to 0, then an improvement of 0.01can indeed\nbe a big deal.\nOne pesky feature of our analysis thus far is that it really only tells us about asymptotics,\ni.e., how the relationship between \ud835\udf16Dand\ud835\udf16evolves as our sample size goes to infinity.\nFortunately, because our random variable is bounded, we can obtain valid finite sample\nbounds by applying an inequality due to Hoeffding (1963):\n\ud835\udc43\u00b9\ud835\udf16D\u00b9\ud835\udc53\u00ba\u0000\ud835\udf16\u00b9\ud835\udc53\u00ba\u0015\ud835\udc61\u00ba<exp\u0010\n\u00002\ud835\udc5b\ud835\udc612\u0011\n. (4.6.3)\nSolving for the smallest dataset size that would allow us to conclude with 95% confidence\nthat the distance \ud835\udc61betweenour estimate \ud835\udf16D\u00b9\ud835\udc53\u00baand the true error rate \ud835\udf16\u00b9\ud835\udc53\u00badoes not exceed\n0.01, you will find that roughly 15,000 examples are required as compared to the 10,000\nexamples suggested by the asymptotic analysis above. If you go deeper into statistics you\nwill find that this trend holds generally. Guarantees that hold even in finite samples are\ntypically slightly more conservative. Note that in the scheme of things, these numbers\nare not so far apart, reflecting the general usefulness of asymptotic analysis for giving us\nballpark figures even if they are not guarantees we can take to court.\n4.6.2TestSetReuse\nIn some sense, you are now set up to succeed at conducting empirical machine learning\nresearch. Nearly all practical models are developed and validated based on test set perfor-\nmance and you are now a master of the test set. For any fixed classifier \ud835\udc53, you know how\nto evaluate its test error \ud835\udf16D\u00b9\ud835\udc53\u00ba, and know precisely what can (and cannot) be said about its\npopulation error \ud835\udf16\u00b9\ud835\udc53\u00ba.\nSo let\u2019s say that you take this knowledge and prepare to train your first model \ud835\udc531. Knowing\njusthowconfidentyouneedtobeintheperformanceofyourclassifier\u2019serrorrateyouapply\nour analysis above to determine an appropriate number of examples to set aside for the test\nset. Moreover, let\u2019s assume that you took the lessons from Section 3.6 to heart and made\nsure to preserve the sanctity of the test set by conducting all of your preliminary analysis,\nhyperparameter tuning, and even selection among multiple competing model architectures\nonavalidationset. Finallyyouevaluateyourmodel \ud835\udc531onthetestsetandreportanunbiased\nestimate of the population error with an associated confidence interval.\nSo far everything seems to be going well. However, that night you wake up at 3am with\na brilliant idea for a new modeling approach. The next day, you code up your new model,\ntune its hyperparameters on the validation set and not only are you getting your new model\n\ud835\udc532to work but its error rate appears to be much lower than \ud835\udc531\u2019s. However, the thrill of\ndiscovery suddenly fades as you prepare for the final evaluation. You do not have a test\nset!\nEventhoughtheoriginaltestset Disstillsittingonyourserver,younowfacetwoformidable\nproblems. First, when you collected your test set, you determined the required level of pre-\ncision under the assumption that you were evaluating a single classifier \ud835\udc53. However, if\nyou get into the business of evaluating multiple classifiers \ud835\udc531,..., \ud835\udc53\ud835\udc58on the same test set,\nyou must consider the problem of false discovery. Before, you might have been 95% sure", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f0673e0b-28c9-4291-937d-1d3db28782a4": {"__data__": {"id_": "f0673e0b-28c9-4291-937d-1d3db28782a4", "embedding": null, "metadata": {"page_label": "151", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca7fe14a-71f1-4eaf-92a6-9bec7f3e72b2", "node_type": "4", "metadata": {"page_label": "151", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3acfda5a5d8910d72cfe3096e1d59c89b56f28c94f6b18bf5f5c1d5f3109ef0b", "class_name": "RelatedNodeInfo"}}, "text": "151 Generalization in Classi\ufb01cation\nthat\ud835\udf16D\u00b9\ud835\udc53\u00ba2\ud835\udf16\u00b9\ud835\udc53\u00ba\u00060.01for a single classifier \ud835\udc53and thus the probability of a misleading\nresult was a mere 5%. With \ud835\udc58classifiers in the mix, it can be hard to guarantee that there\nis not even one among them whose test set performance is misleading. With 20 classifiers\nunder consideration, you might have no power at all to rule out the possibility that at least\none among them received a misleading score. This problem relates to multiple hypothesis\ntesting, which despite a vast literature in statistics, remains a persistent problem plaguing\nscientific research.\nIf that is not enough to worry you, there is a special reason to distrust the results that you\nget on subsequent evaluations. Recall that our analysis of test set performance rested on\nthe assumption that the classifier was chosen absent any contact with the test set and thus\nwe could view the test set as drawn randomly from the underlying population. Here, not\nonly are you testing multiple functions, the subsequent function \ud835\udc532was chosen after you\nobserved the test set performance of \ud835\udc531. Once information from the test set has leaked\nto the modeler, it can never be a true test set again in the strictest sense. This problem is\ncalledadaptiveoverfitting andhasrecentlyemergedasatopicofintenseinteresttolearning\ntheorists and statisticians ( Dworket al., 2015). Fortunately, while it is possible to leak all\ninformation out of a holdout set, and the theoretical worst case scenarios are bleak, these\nanalyses may be too conservative. In practice, take care to create real test sets, to consult\nthem as infrequently as possible, to account for multiple hypothesis testing when reporting\nconfidence intervals, and to dial up your vigilance more aggressively when the stakes are\nhigh and your dataset size is small. When running a series of benchmark challenges, it is\noftengoodpracticetomaintainseveraltestsetssothataftereachround, theoldtestsetcan\nbe demoted to a validation set.\n4.6.3Statistical Learning Theory\nPut simply, testsetsareallthatwereallyhave , and yet this fact seems strangely unsatisfy-\ning. First, we seldom possess a true test set \u2014unless we are the ones creating the dataset,\nsomeone else has probably already evaluated their own classifier on our ostensible \u201ctest\nset\u201d. And even when we have first dibs, we soon find ourselves frustrated, wishing we\ncould evaluate our subsequentmodeling attempts without the gnawing feeling that wecan-\nnot trust our numbers. Moreover, even a true test set can only tell us post hoc whether a\nclassifierhasinfactgeneralizedtothepopulation,notwhetherwehaveanyreasontoexpect\napriori that it should generalize.\nWith these misgivings in mind, you might now be sufficiently primed to see the appeal of\nstatistical learning theory , the mathematical subfield of machine learning whose practi-\ntioners aim to elucidate the fundamental principles that explain why/when models trained\non empirical data can/will generalize to unseen data. One of the primary aims of statistical\nlearningresearchershasbeentoboundthegeneralizationgap,relatingthepropertiesofthe\nmodel class to the number of samples in the dataset.\nLearning theorists aim to bound the difference between the empirical error \ud835\udf16S\u00b9\ud835\udc53S\u00baof a\nlearned classifier \ud835\udc53S, both trained and evaluated on the training set S, and the true error\n\ud835\udf16\u00b9\ud835\udc53S\u00baof that same classifier on the underlying population. This might look similar to\nthe evaluation problem that we just addressed but there is a major difference. Earlier, the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "508ce776-d7d8-4be4-9c09-13d13ad53257": {"__data__": {"id_": "508ce776-d7d8-4be4-9c09-13d13ad53257", "embedding": null, "metadata": {"page_label": "152", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cddbd3f0-bfbd-4f3f-aef0-876e8aeac72d", "node_type": "4", "metadata": {"page_label": "152", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b7435681b59445726d58066fd5fa6a8d503e7e921531c3a3c609553382598fed", "class_name": "RelatedNodeInfo"}}, "text": "152 Linear Neural Networks for Classi\ufb01cation\nclassifier\ud835\udc53was fixed and we only needed a dataset for evaluative purposes. And indeed,\nanyfixedclassifierdoesgeneralize: itserrorona(previouslyunseen)datasetisanunbiased\nestimate of the population error. But what can we say when a classifier is trained and\nevaluated on the same dataset? Can we ever be confident that the training error will be\nclose to the testing error?\nSuppose that our learned classifier \ud835\udc53Smust be chosen from some pre-specified set of func-\ntionsF. Recallfromourdiscussionoftestsetsthatwhileitiseasytoestimatetheerrorofa\nsingleclassifier, thingsgethairywhenwebegintoconsidercollectionsofclassifiers. Even\nif the empirical error of any one (fixed) classifier will be close to its true error with high\nprobability, once we consider a collection of classifiers, we need to worry about the possi-\nbility that justone of them will receive a badly estimated error. The worry is that we might\npick such a classifier and thereby grossly underestimate the population error. Moreover,\neven for linear models, because their parameters are continuously valued, we are typically\nchoosing from an infinite class of functions ( jFj=1).\nOne ambitious solution to the problem is to develop analytic tools for proving uniform\nconvergence, i.e., that with high probability, the empirical error rate for every classifier\nin the class \ud835\udc532 Fwillsimultaneously converge to its true error rate. In other words,\nwe seek a theoretical principle that would allow us to state that with probability at least\n1\u0000\ud835\udeff(for some small \ud835\udeff) no classifier\u2019s error rate \ud835\udf16\u00b9\ud835\udc53\u00ba(among all classifiers in the class\nF) will be misestimated by more than some small amount \ud835\udefc. Clearly, we cannot make\nsuch statements for all model classes F. Recall the class of memorization machines that\nalways achieve empirical error 0but never outperform random guessing on the underlying\npopulation.\nIn a sense the class of memorizers is too flexible. No such a uniform convergence result\ncouldpossiblyhold. Ontheotherhand,afixedclassifierisuseless\u2014itgeneralizesperfectly,\nbut fits neither the training data nor the test data. The central question of learning has\nthus historically been framed as a trade-off between more flexible (higher variance) model\nclasses that better fit the training data but risk overfitting, versus more rigid (higher bias)\nmodel classes that generalize well but risk underfitting. A central question in learning\ntheoryhasbeentodeveloptheappropriatemathematicalanalysistoquantifywhereamodel\nsits along this spectrum, and to provide the associated guarantees.\nIn a series of seminal papers, Vapnik and Chervonenkis extended the theory on the con-\nvergence of relative frequencies to more general classes of functions ( Vapnik and Cher-\nvonenkis, 1964 ,Vapnik and Chervonenkis, 1968 ,Vapnik and Chervonenkis, 1971 ,Vap-\nnik and Chervonenkis, 1981 ,Vapnik and Chervonenkis, 1991 ,Vapnik and Chervonenkis,\n1974). One of the key contributions of this line of work is the Vapnik\u2013Chervonenkis (VC)\ndimension, which measures (one notion of) the complexity (flexibility) of a model class.\nMoreover, one of their key results bounds the difference between the empirical error and\nthe population error as a function of the VC dimension and the number of samples:\n\ud835\udc43\u0000\ud835\udc45\u00bb\ud835\udc5d, \ud835\udc53\u00bc\u0000\ud835\udc45emp\u00bbX,Y, \ud835\udc53\u00bc<\ud835\udefc\u0001\u00151\u0000\ud835\udefffor\ud835\udefc\u0015\ud835\udc50p\n\u00b9VC\u0000log\ud835\udeff\u00ba\u009d\ud835\udc5b. (4.6.4)\nHere\ud835\udeff > 0is the probability that the bound is violated, \ud835\udefcis the upper bound on the\ngeneralization gap, and \ud835\udc5bis the dataset size. Lastly, \ud835\udc50 > 0is a constant that depends only", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcee35f1-d391-405c-990d-b33f8f84f4d0": {"__data__": {"id_": "fcee35f1-d391-405c-990d-b33f8f84f4d0", "embedding": null, "metadata": {"page_label": "153", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "020a85a8-2d42-4824-abfc-d2f43e8d9c90", "node_type": "4", "metadata": {"page_label": "153", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d42273a9c53c2f7fedf14c671ba5f8c861be99a77e1b96e86eb869b13be53200", "class_name": "RelatedNodeInfo"}}, "text": "153 Generalization in Classi\ufb01cation\non the scale of the loss that can be incurred. One use of the bound might be to plug in\ndesired values of \ud835\udeffand\ud835\udefcto determine how many samples to collect. The VC dimension\nquantifies the largest number of data points for which we can assign any arbitrary (binary)\nlabeling and for each find some model \ud835\udc53in the class that agrees with that labeling. For\nexample, linear models on \ud835\udc51-dimensional inputs have VC dimension \ud835\udc51\u00b81. It is easy to\nsee that a line can assign any possible labeling to three points in two dimensions, but not\nto four. Unfortunately, the theory tends to be overly pessimistic for more complex models\nand obtaining this guarantee typically requires far more examples than are actually needed\nto achieve the desired error rate. Note also that fixing the model class and \ud835\udeff, our error rate\nagain decays with the usual O\u00b91\u009dp\ud835\udc5b\u00barate. It seems unlikely that we could do better in\nterms of\ud835\udc5b. However, as we vary the model class, VC dimension can present a pessimistic\npicture of the generalization gap.\n4.6.4Summary\nThe most straightforward way to evaluate a model is to consult a test set comprised of pre-\nviously unseen data. Test set evaluations provide an unbiased estimate of the true error\nand converge at the desired O\u00b91\u009dp\ud835\udc5b\u00barate as the test set grows. We can provide approx-\nimate confidence intervals based on exact asymptotic distributions or valid finite sample\nconfidence intervals based on (more conservative) finite sample guarantees. Indeed test\nset evaluation is the bedrock of modern machine learning research. However, test sets are\nseldomtruetestsets(used bymultipleresearchersagainand again). Once thesametestset\nis used to evaluate multiple models, controlling for false discovery can be difficult. This\ncancause hugeproblemsin theory. In practice, the significanceof the problem dependson\nthe size of the holdout sets in question and whether they are merely being used to choose\nhyperparameters or if they are leaking information more directly. Nevertheless, it is good\npracticetocuraterealtestsets(ormultiple)andtobeasconservativeaspossibleabouthow\noften they are used.\nHoping to provide a more satisfying solution, statistical learning theorists have developed\nmethodsforguaranteeinguniformconvergenceoveramodelclass. Ifindeedeverymodel\u2019s\nempirical error simultaneously converges to its true error, then we are free to choose the\nmodel that performs best, minimizing the training error, knowing that it too will perform\nsimilarly well on the holdout data. Crucially, any one of such results must depend on some\npropertyofthemodelclass. VladimirVapnikandAlexeyChernovenkisintroducedtheVC\ndimension, presenting uniform convergence results that hold for all models in a VC class.\nThe training errors for all models in the class are (simultaneously) guaranteed to be close\nto their true errors, and guaranteed to grow even closer at O\u00b91\u009dp\ud835\udc5b\u00barates. Following the\nrevolutionarydiscoveryofVCdimension, numerousalternativecomplexitymeasureshave\nbeen proposed, each facilitating an analogous generalization guarantee. See Boucheron\net al.(2005) for a detailed discussion of several advanced ways of measuring function\ncomplexity. Unfortunately, while these complexity measures have become broadly useful\ntools in statistical theory, they turn out to be powerless (as straightforwardly applied) for\nexplainingwhydeepneuralnetworksgeneralize. Deepneuralnetworksoftenhavemillions\nof parameters (or more), and can easily assign random labels to large collections of points.\nNevertheless, they generalize well on practical problems and, surprisingly, they often gen-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1787ac8b-fba3-40d4-9275-67a7cfc44975": {"__data__": {"id_": "1787ac8b-fba3-40d4-9275-67a7cfc44975", "embedding": null, "metadata": {"page_label": "154", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68613df9-b843-4633-85b6-6322b0fdf762", "node_type": "4", "metadata": {"page_label": "154", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d912ae3d150143047b25db187e95b4c868af4669158fabcb5f1fc1c76bfe0370", "class_name": "RelatedNodeInfo"}}, "text": "154 Linear Neural Networks for Classi\ufb01cation\n100eralizebetter, when theyare largerand deeper, despite incurring higherVCdimensions. In\nthe next chapter, we will revisit generalization in the context of deep learning.\n4.6.5Exercises\n1.If we wish to estimate the error of a fixed model \ud835\udc53to within 0.0001with probability\ngreater than 99.9%, how many samples do we need?\n2.Supposethatsomebodyelsepossessesalabeledtestset Dandonlymakesavailablethe\nunlabeled inputs (features). Now suppose that you can only access the test set labels by\nrunning a model \ud835\udc53(with no restrictions placed on the model class) on each of the un-\nlabeled inputs and receiving the corresponding error \ud835\udf16D\u00b9\ud835\udc53\u00ba. How many models would\nyou need to evaluate before you leak the entire test set and thus could appear to have\nerror 0, regardless of your true error?\n3.What is the VC dimension of the class of fifth-order polynomials?\n4.What is the VC dimension of axis-aligned rectangles on two-dimensional data?\nDiscussions100.\n4.7Environmentand Distribution Shift\nIntheprevioussections,weworkedthroughanumberofhands-onapplicationsofmachine\nlearning, fitting models to a variety of datasets. And yet, we never stopped to contemplate\neither where data came from in the first place or what we ultimately plan to do with the\noutputs from our models. Too often, machine learning developers in possession of data\nrush to develop models without pausing to consider these fundamental issues.\nMany failed machine learning deployments can be traced back to this failure. Sometimes\nmodelsappeartoperformmarvelouslyasmeasuredbytestsetaccuracybutfailcatastroph-\nicallyindeploymentwhenthedistributionofdatasuddenlyshifts. Moreinsidiously,some-\ntimestheverydeploymentofamodelcanbethecatalystthatperturbsthedatadistribution.\nSay, for example, that we trained a model to predict who will repay rather than default on a\nloan, finding that an applicant\u2019s choice of footwear was associated with the risk of default\n(Oxfords indicate repayment, sneakers indicate default). We might be inclined thereafter\nto grant a loan to any applicant wearing Oxfords and to deny all applicants wearing sneak-\ners.\nIn this case, our ill-considered leap from pattern recognition to decision-making and our\nfailure to critically consider the environment might have disastrous consequences. For\nstarters, as soon as we began making decisions based on footwear, customers would catch\non and change their behavior. Before long, all applicants would be wearing Oxfords, with-\nout any coincident improvement in credit-worthiness. Take a minute to digest this because", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2586, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbb70f70-ef7e-4984-b68f-82863556fb55": {"__data__": {"id_": "bbb70f70-ef7e-4984-b68f-82863556fb55", "embedding": null, "metadata": {"page_label": "155", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32d46f67-6973-434f-aa47-448cacbbfc7c", "node_type": "4", "metadata": {"page_label": "155", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9a3b038b4701c144fe10bd43473ef26f253d7c3f91ab99af231d8381a45f25f7", "class_name": "RelatedNodeInfo"}}, "text": "155 Environment and Distribution Shift\nsimilarissuesaboundinmanyapplicationsofmachinelearning: byintroducingourmodel-\nbased decisions to the environment, we might break the model.\nWhilewecannotpossiblygivethesetopicsacompletetreatmentinonesection,weaimhere\nto expose some common concerns, and to stimulate the critical thinking required to detect\nsuch situations early, mitigate damage, and use machine learning responsibly. Some of the\nsolutions are simple (ask for the \u201cright\u201d data), some are technically difficult (implement a\nreinforcement learning system), and others require that we step outside the realm of sta-\ntistical prediction altogether and grapple with difficult philosophical questions concerning\nthe ethical application of algorithms.\n4.7.1Typesof Distribution Shift\nTobegin,westickwiththepassivepredictionsettingconsideringthevariouswaysthatdata\ndistributionsmightshiftandwhatmightbedonetosalvagemodelperformance. Inoneclas-\nsic setup, we assume that our training data was sampled from some distribution \ud835\udc5d\ud835\udc46\u00b9x,\ud835\udc66\u00ba\nbut that our test data will consist of unlabeled examples drawn from some different distri-\nbution\ud835\udc5d\ud835\udc47\u00b9x,\ud835\udc66\u00ba. Already, we must confront a sobering reality. Absent any assumptions on\nhow\ud835\udc5d\ud835\udc46and\ud835\udc5d\ud835\udc47relate to each other, learning a robust classifier is impossible.\nConsider a binary classification problem, where we wish to distinguish between dogs and\ncats. If the distribution can shift in arbitrary ways, then our setup permits the pathological\ncase in which the distribution over inputs remains constant: \ud835\udc5d\ud835\udc46\u00b9x\u00ba=\ud835\udc5d\ud835\udc47\u00b9x\u00ba, but the labels\nare all flipped: \ud835\udc5d\ud835\udc46\u00b9\ud835\udc66jx\u00ba=1\u0000\ud835\udc5d\ud835\udc47\u00b9\ud835\udc66jx\u00ba. In other words, if God can suddenly decide that\nin the future all \u201ccats\u201d are now dogs and what we previously called \u201cdogs\u201d are now cats\u2014\nwithout any change in the distribution of inputs \ud835\udc5d\u00b9x\u00ba, then we cannot possibly distinguish\nthis setting from one in which the distribution did not change at all.\nFortunately,undersomerestrictedassumptionsonthewaysourdatamightchangeinthefu-\nture, principledalgorithmscandetectshiftandsometimesevenadaptonthefly, improving\non the accuracy of the original classifier.\nCovariateShift\nAmong categories of distribution shift, covariate shift may be the most widely studied.\nHere, we assume that while the distribution of inputs may change over time, the labeling\nfunction, i.e., the conditional distribution \ud835\udc43\u00b9\ud835\udc66jx\u00badoes not change. Statisticians call this\ncovariate shift because the problem arises due to a shift in the distribution of the covari-\nates (features). While we can sometimes reason about distribution shift without invoking\ncausality, we note that covariate shift is the natural assumption to invoke in settings where\nwe believe that xcauses\ud835\udc66.\nConsider the challenge of distinguishing cats and dogs. Our training data might consist of\nimages of the kind in Fig. 4.7.1 .\nAt test time we are asked to classify the images in Fig. 4.7.2 .\nThe training set consists of photos, while the test set contains only cartoons. Training on a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ace0f10-9738-4194-93bf-3185237d5adf": {"__data__": {"id_": "8ace0f10-9738-4194-93bf-3185237d5adf", "embedding": null, "metadata": {"page_label": "156", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0270026-978b-4cd9-9469-10c9a2783bbc", "node_type": "4", "metadata": {"page_label": "156", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4d3bc3eab52d2f764406fdec2144eb36c15c2165cd124b0abb536eb9191de4d1", "class_name": "RelatedNodeInfo"}}, "text": "156 Linear Neural Networks for Classi\ufb01cation\ntFig. 4.7.1 Training data for distinguishing cats and dogs (illustrations: Lafeez Hossain / 500px /\nGetty Images; ilkermetinkursova / iStock / Getty Images Plus; GlobalP / iStock / Getty\nImages Plus; Musthafa Aboobakuru / 500px / Getty Images).\ntFig. 4.7.2 Test data for distinguishing cats and dogs (illustrations: SIBAS_minich / iStock / Getty\nImages Plus; Ghrzuzudu / iStock / Getty Images Plus; id-work / DigitalVision Vectors /\nGetty Images; Yime / iStock / Getty Images Plus).\ndataset with substantially different characteristics from the test set can spell trouble absent\na coherent plan for how to adapt to the new domain.\nLabel Shift\nLabel shift describes the converse problem. Here, we assume that the label marginal \ud835\udc43\u00b9\ud835\udc66\u00ba\ncan change but the class-conditional distribution \ud835\udc43\u00b9xj\ud835\udc66\u00baremains fixed across domains.\nLabel shift is a reasonable assumption to make when we believe that \ud835\udc66causes x. For ex-\nample, we may want to predict diagnoses given their symptoms (or other manifestations),\neven as the relative prevalence of diagnoses are changing over time. Label shift is the ap-\npropriateassumptionherebecausediseasescausesymptoms. Insomedegeneratecasesthe\nlabelshiftandcovariateshiftassumptionscanholdsimultaneously. Forexample,whenthe\nlabel is deterministic, the covariate shift assumption will be satisfied, even when \ud835\udc66causes\nx. Interestingly, in these cases, it is often advantageous to work with methods that flow\nfrom the label shift assumption. That is because these methods tend to involve manipulat-\ning objects that look like labels (often low-dimensional), as opposed to objects that look\nlike inputs, which tend to be high-dimensional in deep learning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1715, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb30c50e-4207-42f5-862e-b28ac7f9eeae": {"__data__": {"id_": "fb30c50e-4207-42f5-862e-b28ac7f9eeae", "embedding": null, "metadata": {"page_label": "157", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "130ca9bf-f780-4912-9725-094e49f23144", "node_type": "4", "metadata": {"page_label": "157", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9f124504bbe48f9c99cdb8fff0871c4418db08c1fb08c374a6736e23e208a146", "class_name": "RelatedNodeInfo"}}, "text": "157 Environment and Distribution Shift\nConceptShift\nWe may also encounter the related problem of concept shift , which arises when the very\ndefinitions of labels can change. This sounds weird\u2014a catis acat, no? However, other\ncategories are subject to changes in usage over time. Diagnostic criteria for mental illness,\nwhat passes for fashionable, and job titles, are all subject to considerable amounts of con-\ncept shift. It turns out that if we navigate around the United States, shifting the source of\nour data by geography, wewill find considerable concept shift regardingthe distribution of\nnames for soft drinks as shown in Fig. 4.7.3 .\ntFig. 4.7.3 Concept shift for soft drink names in the United States (CC-BY: Alan McConchie,\nPopVsSoda.com).\nIf we were to build a machine translation system, the distribution \ud835\udc43\u00b9\ud835\udc66jx\u00bamight be dif-\nferent depending on our location. This problem can be tricky to spot. We might hope to\nexploit knowledge that shift only takes place gradually either in a temporal or geographic\nsense.\n4.7.2Examplesof Distribution Shift\nBefore delving into formalism and algorithms, we can discuss some concrete situations\nwhere covariate or concept shift might not be obvious.\nMedical Diagnostics\nImaginethatyouwanttodesignanalgorithmtodetectcancer. Youcollectdatafromhealthy\nand sick people and you train your algorithm. It works fine, giving you high accuracy and\nyou conclude that you are ready for a successful career in medical diagnostics. Not so\nfast.\nThedistributionsthatgaverisetothetrainingdataandthoseyouwillencounterinthewild", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdfba9ed-f06e-4608-a36b-0e4811b68b05": {"__data__": {"id_": "cdfba9ed-f06e-4608-a36b-0e4811b68b05", "embedding": null, "metadata": {"page_label": "158", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eefd56df-5adc-4e44-9a3b-cf43bb829d9c", "node_type": "4", "metadata": {"page_label": "158", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "46b18eb434e9e8ae79bee8dd843f2a719a57ccfe9c31c93cb11496b76017d5cf", "class_name": "RelatedNodeInfo"}}, "text": "158 Linear Neural Networks for Classi\ufb01cation\nmightdifferconsiderably. Thishappenedtoanunfortunatestartupthatsomeofweauthors\nworked with years ago. They were developing a blood test for a disease that predominantly\naffects older men and hoped to study it using blood samples that they had collected from\npatients. However, it is considerably more difficult to obtain blood samples from healthy\nmen than from sick patients already in the system. To compensate, the startup solicited\nblood donations from students on a university campus to serve as healthy controls in de-\nveloping their test. Then they asked whether we could help them to build a classifier for\ndetecting the disease.\nAs we explained to them, it would indeed be easy to distinguish between the healthy and\nsick cohorts with near-perfect accuracy. However, that is because the test subjects differed\nin age, hormone levels, physical activity, diet, alcohol consumption, and many more fac-\ntors unrelated to the disease. This was unlikely to be the case with real patients. Due to\ntheir sampling procedure, we could expect to encounter extreme covariate shift. Moreover,\nthis case was unlikely to be correctable via conventional methods. In short, they wasted a\nsignificant sum of money.\nSelf-Driving Cars\nSay a company wanted to leverage machine learning for developing self-driving cars. One\nkey component here is a roadside detector. Since real annotated data is expensive to get,\nthey had the (smart and questionable) idea to use synthetic data from a game rendering\nengine as additional training data. This worked really well on \u201ctest data\u201d drawn from the\nrenderingengine. Alas, insidearealcaritwasadisaster. Asitturnedout, theroadsidehad\nbeen rendered with a very simplistic texture. More importantly, allthe roadside had been\nrendered with the sametexture and the roadside detector learned about this \u201cfeature\u201d very\nquickly.\nA similar thing happened to the US Army when they first tried to detect tanks in the forest.\nTheytookaerialphotographsoftheforestwithouttanks,thendrovethetanksintotheforest\nandtookanothersetofpictures. Theclassifierappearedtowork perfectly . Unfortunately,it\nhadmerelylearnedhowtodistinguishtreeswithshadowsfromtreeswithoutshadows\u2014the\nfirst set of pictures was taken in the early morning, the second set at noon.\nNonstationary Distributions\nA much more subtle situation arises when the distribution changes slowly (also known\nasnonstationary distribution ) and the model is not updated adequately. Below are some\ntypical cases.\n\u000fWetrainacomputationaladvertisingmodelandthenfailtoupdateitfrequently(e.g.,we\nforget to incorporate that an obscure new device called an iPad was just launched).\n\u000fWe build a spam filter. It works well at detecting all spam that we have seen so far. But\nthen the spammers wise up and craft new messages that look unlike anything we have\nseen before.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2871, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdd044c0-1499-4158-95bb-b9ffa9649cbb": {"__data__": {"id_": "bdd044c0-1499-4158-95bb-b9ffa9649cbb", "embedding": null, "metadata": {"page_label": "159", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1288119e-1daf-4d9f-bd44-30f7b889d380", "node_type": "4", "metadata": {"page_label": "159", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "04e4016a49213e6a94d860fe96c8ee70b268106d6e332e662a1267ae77096c73", "class_name": "RelatedNodeInfo"}}, "text": "159 Environment and Distribution Shift\n\u000fWe build a product recommendation system. It works throughout the winter but then\ncontinues to recommend Santa hats long after Christmas.\nMoreAnecdotes\n\u000fWe build a face detector. It works well on all benchmarks. Unfortunately it fails on test\ndata\u2014the offending examples are close-ups where the face fills the entire image (no\nsuch data was in the training set).\n\u000fWe build a web search engine for the US market and want to deploy it in the UK.\n\u000fWe train an image classifier by compiling a large dataset where each among a large set\nof classes is equally represented in the dataset, say 1000 categories, represented by\n1000 images each. Then we deploy the system in the real world, where the actual\nlabel distribution of photographs is decidedly non-uniform.\n4.7.3Correctionof Distribution Shift\nAs we have discussed, there are many cases where training and test distributions \ud835\udc43\u00b9x,\ud835\udc66\u00ba\nare different. In some cases, we get lucky and the models work despite covariate, label,\nor concept shift. In other cases, we can do better by employing principled strategies to\ncope with the shift. The remainder of this section grows considerably more technical. The\nimpatient reader could continue on to the next section as this material is not prerequisite to\nsubsequent concepts.\nEmpiricalRisk and Risk\nLet\u2019s first reflect on what exactly is happening during model training: we iterate over fea-\ntures and associated labels of training data f\u00b9x1,\ud835\udc661\u00ba,...,\u00b9x\ud835\udc5b,\ud835\udc66\ud835\udc5b\u00bagand update the param-\neters of a model \ud835\udc53after every minibatch. For simplicity we do not consider regularization,\nso we largely minimize the loss on the training:\nminimize\n\ud835\udc531\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc59\u00b9\ud835\udc53\u00b9x\ud835\udc56\u00ba,\ud835\udc66\ud835\udc56\u00ba, (4.7.1)\nwhere\ud835\udc59is the loss function measuring \u201chow bad\u201d the prediction \ud835\udc53\u00b9x\ud835\udc56\u00bais given the associ-\nated label\ud835\udc66\ud835\udc56. Statisticians call the term in (4.7.1 )empirical risk . Theempirical risk is an\naverage loss over the training data for approximating the risk, which is the expectation of\nthelossovertheentirepopulationofdatadrawnfromtheirtruedistribution \ud835\udc5d\u00b9x,\ud835\udc66\u00ba:\n\ud835\udc38\ud835\udc5d\u00b9x,\ud835\udc66\u00ba\u00bb\ud835\udc59\u00b9\ud835\udc53\u00b9x\u00ba,\ud835\udc66\u00ba\u00bc=\u00b9 \u00b9\n\ud835\udc59\u00b9\ud835\udc53\u00b9x\u00ba,\ud835\udc66\u00ba\ud835\udc5d\u00b9x,\ud835\udc66\u00ba\ud835\udc51x\ud835\udc51\ud835\udc66. (4.7.2)\nHowever, in practice we typically cannot obtain the entire population of data. Thus, em-\npirical risk minimization , which is minimizing the empirical risk in (4.7.1 ), is a practical\nstrategy for machine learning, with the hope of approximately minimizing the risk.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23260f41-50e1-49e2-a38a-9790e4193406": {"__data__": {"id_": "23260f41-50e1-49e2-a38a-9790e4193406", "embedding": null, "metadata": {"page_label": "160", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "109b9958-d9cc-4932-a62c-aa178e3a8102", "node_type": "4", "metadata": {"page_label": "160", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cd7690c08b5846703afe7673b377f5473e6a9217d3d3cabb193c721f88c19fe0", "class_name": "RelatedNodeInfo"}}, "text": "160 Linear Neural Networks for Classi\ufb01cation\nCovariateShift Correction\nAssumethatwewanttoestimatesomedependency \ud835\udc43\u00b9\ud835\udc66jx\u00baforwhichwehavelabeleddata\n\u00b9x\ud835\udc56,\ud835\udc66\ud835\udc56\u00ba. Unfortunately, the observations x\ud835\udc56are drawn from some sourcedistribution \ud835\udc5e\u00b9x\u00ba\nrather than the target distribution \ud835\udc5d\u00b9x\u00ba. Fortunately, the dependency assumption means\nthat the conditional distribution does not change: \ud835\udc5d\u00b9\ud835\udc66jx\u00ba=\ud835\udc5e\u00b9\ud835\udc66jx\u00ba. If the source\ndistribution\ud835\udc5e\u00b9x\u00bais\u201cwrong\u201d, wecan correct forthatbyusing thefollowingsimple identity\nin the risk:\n\u00b9 \u00b9\n\ud835\udc59\u00b9\ud835\udc53\u00b9x\u00ba,\ud835\udc66\u00ba\ud835\udc5d\u00b9\ud835\udc66jx\u00ba\ud835\udc5d\u00b9x\u00ba\ud835\udc51x\ud835\udc51\ud835\udc66=\u00b9 \u00b9\n\ud835\udc59\u00b9\ud835\udc53\u00b9x\u00ba,\ud835\udc66\u00ba\ud835\udc5e\u00b9\ud835\udc66jx\u00ba\ud835\udc5e\u00b9x\u00ba\ud835\udc5d\u00b9x\u00ba\n\ud835\udc5e\u00b9x\u00ba\ud835\udc51x\ud835\udc51\ud835\udc66.\n(4.7.3)\nIn other words, we need to reweigh each data example by the ratio of the probability that it\nwould have been drawn from the correct distribution to that from the wrong one:\n\ud835\udefd\ud835\udc56def=\ud835\udc5d\u00b9x\ud835\udc56\u00ba\n\ud835\udc5e\u00b9x\ud835\udc56\u00ba. (4.7.4)\nPlugging in the weight \ud835\udefd\ud835\udc56for each data example \u00b9x\ud835\udc56,\ud835\udc66\ud835\udc56\u00bawe can train our model using\nweightedempirical risk minimization :\nminimize\n\ud835\udc531\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udefd\ud835\udc56\ud835\udc59\u00b9\ud835\udc53\u00b9x\ud835\udc56\u00ba,\ud835\udc66\ud835\udc56\u00ba. (4.7.5)\nAlas, we do not know that ratio, so before we can do anything useful we need to estimate\nit. Many methods are available, including some fancy operator-theoretic approaches that\nattempt to recalibrate the expectation operator directly using a minimum-norm or a maxi-\nmumentropyprinciple. Notethatforanysuchapproach,weneedsamplesdrawnfromboth\ndistributions\u2014the \u201ctrue\u201d \ud835\udc5d, e.g., by access to test data, and the one used for generating the\ntraining set\ud835\udc5e(the latter is trivially available). Note however, that we only need features\nx\u0018\ud835\udc5d\u00b9x\u00ba; we do not need to access labels \ud835\udc66\u0018\ud835\udc5d\u00b9\ud835\udc66\u00ba.\nIn this case, there exists a very effective approach that will give almost as good results\nas the original: namely, logistic regression, which is a special case of softmax regression\n(seeSection 4.1 ) for binary classification. This is all that is needed to compute estimated\nprobability ratios. We learn a classifier to distinguish between data drawn from \ud835\udc5d\u00b9x\u00baand\ndata drawn from \ud835\udc5e\u00b9x\u00ba. If it is impossible to distinguish between the two distributions then\nit means that the associated instances are equally likely to come from either one of those\ntwo distributions. On the other hand, any instances that can be well discriminated should\nbe significantly overweighted or underweighted accordingly.\nForsimplicity\u2019ssakeassumethatwehaveanequalnumberofinstancesfrombothdistribu-\ntions\ud835\udc5d\u00b9x\u00baand\ud835\udc5e\u00b9x\u00ba, respectively. Now denote by \ud835\udc67labels that are 1for data drawn from \ud835\udc5d\nand\u00001for data drawn from \ud835\udc5e. Then the probability in a mixed dataset is given by\n\ud835\udc43\u00b9\ud835\udc67=1jx\u00ba=\ud835\udc5d\u00b9x\u00ba\n\ud835\udc5d\u00b9x\u00ba\u00b8\ud835\udc5e\u00b9x\u00baand hence\ud835\udc43\u00b9\ud835\udc67=1jx\u00ba\n\ud835\udc43\u00b9\ud835\udc67=\u00001jx\u00ba=\ud835\udc5d\u00b9x\u00ba\n\ud835\udc5e\u00b9x\u00ba. (4.7.6)\nThus, if we use a logistic regression approach, where \ud835\udc43\u00b9\ud835\udc67=1jx\u00ba=1\n1\u00b8exp\u00b9\u0000\u210e\u00b9x\u00ba\u00ba(\u210eis a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2600, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e01b0767-66e1-40e5-8feb-f605a505979d": {"__data__": {"id_": "e01b0767-66e1-40e5-8feb-f605a505979d", "embedding": null, "metadata": {"page_label": "161", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3d735f8-b4b2-44a0-b67b-2acb36a81124", "node_type": "4", "metadata": {"page_label": "161", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "aeffded2bab3cbb35a9db64492b914ae85538221ccac838267592ee2c9957294", "class_name": "RelatedNodeInfo"}}, "text": "161 Environment and Distribution Shift\nparametrized function), it follows that\n\ud835\udefd\ud835\udc56=1\u009d\u00b91\u00b8exp\u00b9\u0000\u210e\u00b9x\ud835\udc56\u00ba\u00ba\u00ba\nexp\u00b9\u0000\u210e\u00b9x\ud835\udc56\u00ba\u00ba\u009d\u00b91\u00b8exp\u00b9\u0000\u210e\u00b9x\ud835\udc56\u00ba\u00ba\u00ba=exp\u00b9\u210e\u00b9x\ud835\udc56\u00ba\u00ba. (4.7.7)\nAs a result, we need to solve two problems: the first, to distinguish between data drawn\nfrombothdistributions,andthenaweightedempiricalriskminimizationproblemin (4.7.5 )\nwhere we weigh terms by \ud835\udefd\ud835\udc56.\nNow we are ready to describe a correction algorithm. Suppose that we have a training set\nf\u00b9x1,\ud835\udc661\u00ba,...,\u00b9x\ud835\udc5b,\ud835\udc66\ud835\udc5b\u00bagand an unlabeled test set fu1,...,u\ud835\udc5ag. For covariate shift, we\nassume that x\ud835\udc56for all 1\u0014\ud835\udc56\u0014\ud835\udc5bare drawn from some source distribution and u\ud835\udc56for all\n1\u0014\ud835\udc56\u0014\ud835\udc5aare drawn from the target distribution. Here is a prototypical algorithm for\ncorrecting covariate shift:\n1.Createabinary-classificationtrainingset: f\u00b9x1,\u00001\u00ba,...,\u00b9x\ud835\udc5b,\u00001\u00ba,\u00b9u1,1\u00ba,...,\u00b9u\ud835\udc5a,1\u00bag.\n2.Train a binary classifier using logistic regression to get the function \u210e.\n3.Weigh training data using \ud835\udefd\ud835\udc56=exp\u00b9\u210e\u00b9x\ud835\udc56\u00ba\u00baor better\ud835\udefd\ud835\udc56=min\u00b9exp\u00b9\u210e\u00b9x\ud835\udc56\u00ba\u00ba,\ud835\udc50\u00bafor some\nconstant\ud835\udc50.\n4.Use weights \ud835\udefd\ud835\udc56for training onf\u00b9x1,\ud835\udc661\u00ba,...,\u00b9x\ud835\udc5b,\ud835\udc66\ud835\udc5b\u00bagin(4.7.5 ).\nNote that the above algorithm relies on a crucial assumption. For this scheme to work, we\nneed that each data example in the target (e.g., test time) distribution had nonzero proba-\nbility of occurring at training time. If we find a point where \ud835\udc5d\u00b9x\u00ba>0but\ud835\udc5e\u00b9x\u00ba=0, then\nthe corresponding importance weight should be infinity.\nLabelShift Correction\nAssume that we are dealing with a classification task with \ud835\udc58categories. Using the same\nnotation in Section 4.7.3 ,\ud835\udc5eand\ud835\udc5dare the source distribution (e.g., training time) and target\ndistribution (e.g., test time), respectively. Assume that the distribution of labels shifts over\ntime:\ud835\udc5e\u00b9\ud835\udc66\u00ba\u2260\ud835\udc5d\u00b9\ud835\udc66\u00ba, but the class-conditional distribution stays the same: \ud835\udc5e\u00b9xj\ud835\udc66\u00ba=\ud835\udc5d\u00b9xj\n\ud835\udc66\u00ba. If the source distribution \ud835\udc5e\u00b9\ud835\udc66\u00bais \u201cwrong\u201d, we can correct for that according to the\nfollowing identity in the risk as defined in (4.7.2 ):\n\u00b9 \u00b9\n\ud835\udc59\u00b9\ud835\udc53\u00b9x\u00ba,\ud835\udc66\u00ba\ud835\udc5d\u00b9xj\ud835\udc66\u00ba\ud835\udc5d\u00b9\ud835\udc66\u00ba\ud835\udc51x\ud835\udc51\ud835\udc66=\u00b9 \u00b9\n\ud835\udc59\u00b9\ud835\udc53\u00b9x\u00ba,\ud835\udc66\u00ba\ud835\udc5e\u00b9xj\ud835\udc66\u00ba\ud835\udc5e\u00b9\ud835\udc66\u00ba\ud835\udc5d\u00b9\ud835\udc66\u00ba\n\ud835\udc5e\u00b9\ud835\udc66\u00ba\ud835\udc51x\ud835\udc51\ud835\udc66.\n(4.7.8)\nHere, our importance weights will correspond to the label likelihood ratios:\n\ud835\udefd\ud835\udc56def=\ud835\udc5d\u00b9\ud835\udc66\ud835\udc56\u00ba\n\ud835\udc5e\u00b9\ud835\udc66\ud835\udc56\u00ba. (4.7.9)\nOne nice thing about label shift is that if we have a reasonably good model on the source\ndistribution, then we can get consistent estimates of these weights without ever having to\ndeal with the ambient dimension. In deep learning, the inputs tend to be high-dimensional\nobjects like images, while the labels are often simpler objects like categories.\nTo estimate the target label distribution, we first take our reasonably good off-the-shelf", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "188a4439-dbbe-4bee-a792-7200725bc32f": {"__data__": {"id_": "188a4439-dbbe-4bee-a792-7200725bc32f", "embedding": null, "metadata": {"page_label": "162", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ff465812-ef9b-474c-a26d-9b496e5a3f69", "node_type": "4", "metadata": {"page_label": "162", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c5becc275826e62710730a3ce03ce547915d8a922a18b5a3184fc388483f9d2f", "class_name": "RelatedNodeInfo"}}, "text": "162 Linear Neural Networks for Classi\ufb01cation\nclassifier (typically trained on the training data) and compute its \u201cconfusion\u201d matrix using\nthevalidationset(alsofromthetrainingdistribution). The confusionmatrix ,C,issimplya\n\ud835\udc58\u0002\ud835\udc58matrix, whereeachcolumncorrespondstothelabelcategory(groundtruth)andeach\nrow corresponds to our model\u2019s predicted category. Each cell\u2019s value \ud835\udc50\ud835\udc56\ud835\udc57is the fraction of\ntotal predictions on the validation set where the true label was \ud835\udc57and our model predicted\n\ud835\udc56.\nNow, we cannot calculate the confusion matrix on the target data directly because we do\nnot get to see the labels for the examples that we see in the wild, unless we invest in a\ncomplex real-time annotation pipeline. What we can do, however, is average all of our\nmodel\u2019s predictions at test time together, yielding the mean model outputs \ud835\udf07\u00b9\u02c6y\u00ba 2R\ud835\udc58,\nwhere the\ud835\udc56thelement\ud835\udf07\u00b9\u02c6\ud835\udc66\ud835\udc56\u00bais the fraction of the total predictions on the test set where our\nmodel predicted \ud835\udc56.\nIt turns out that under some mild conditions\u2014if our classifier was reasonably accurate in\nthe first place, and if the target data contains only categories that we have seen before, and\nif the label shift assumption holds in the first place (the strongest assumption here)\u2014we\ncan estimate the test set label distribution by solving a simple linear system\nC\ud835\udc5d\u00b9y\u00ba=\ud835\udf07\u00b9\u02c6y\u00ba, (4.7.10)\nbecause as an estimate\u00cd\ud835\udc58\n\ud835\udc57=1\ud835\udc50\ud835\udc56\ud835\udc57\ud835\udc5d\u00b9\ud835\udc66\ud835\udc57\u00ba=\ud835\udf07\u00b9\u02c6\ud835\udc66\ud835\udc56\u00baholds for all 1\u0014\ud835\udc56\u0014\ud835\udc58, where\ud835\udc5d\u00b9\ud835\udc66\ud835\udc57\u00bais\nthe\ud835\udc57thelement of the \ud835\udc58-dimensional label distribution vector \ud835\udc5d\u00b9y\u00ba. If our classifier is\nsufficiently accurate to begin with, then the confusion matrix Cwill be invertible, and we\nget a solution \ud835\udc5d\u00b9y\u00ba=C\u00001\ud835\udf07\u00b9\u02c6y\u00ba.\nBecause we observe the labels on the source data, it is easy to estimate the distribution\n\ud835\udc5e\u00b9\ud835\udc66\u00ba. Then, for any training example \ud835\udc56with label\ud835\udc66\ud835\udc56, we can take the ratio of our esti-\nmated\ud835\udc5d\u00b9\ud835\udc66\ud835\udc56\u00ba\u009d\ud835\udc5e\u00b9\ud835\udc66\ud835\udc56\u00bato calculate the weight \ud835\udefd\ud835\udc56, and plug this into weighted empirical risk\nminimization in (4.7.5 ).\nConceptShift Correction\nConceptshiftismuchhardertofixinaprincipledmanner. Forinstance,inasituationwhere\nsuddenly the problem changes from distinguishing cats from dogs to one of distinguishing\nwhite from black animals, it will be unreasonable to assume that we can do much better\nthan just collecting new labels and training from scratch. Fortunately, in practice, such\nextreme shifts are rare. Instead, what usually happens is that the task keeps on changing\nslowly. To make things more concrete, here are some examples:\n\u000fIncomputationaladvertising,newproductsarelaunched,oldproductsbecomelesspop-\nular. This means that the distribution over ads and their popularity changes gradually\nand any click-through rate predictor needs to change gradually with it.\n\u000fTraffic camera lenses degrade gradually due to environmental wear, affecting image\nquality progressively.\n\u000fNews content changes gradually (i.e., most of the news remains unchanged but new sto-\nries appear).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2860, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f8db9e9-0810-432e-8b2c-13fa9c2c4cfc": {"__data__": {"id_": "1f8db9e9-0810-432e-8b2c-13fa9c2c4cfc", "embedding": null, "metadata": {"page_label": "163", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64efa05b-1227-45cf-b1e0-f985f402d138", "node_type": "4", "metadata": {"page_label": "163", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6bffa68e22e1fdf14745284eb709619deabfe434f72f0ab68528c218ffacc724", "class_name": "RelatedNodeInfo"}}, "text": "163 Environment and Distribution Shift\nIn such cases, we can use the same approach that we used for training networks to make\nthem adapt to the change in the data. In other words, we use the existing network weights\nandsimplyperformafewupdatestepswiththenewdataratherthantrainingfromscratch.\n4.7.4A Taxonomyof Learning Problems\nArmed with knowledge about how to deal with changes in distributions, we can now con-\nsider some other aspects of machine learning problem formulation.\nBatchLearning\nInbatch learning , we have access to training features and labels f\u00b9x1,\ud835\udc661\u00ba,...,\u00b9x\ud835\udc5b,\ud835\udc66\ud835\udc5b\u00bag,\nwhichweusetotrainamodel \ud835\udc53\u00b9x\u00ba. Lateron,wedeploythismodeltoscorenewdata \u00b9x,\ud835\udc66\u00ba\ndrawn from the same distribution. This is the default assumption for any of the problems\nthat we discuss here. For instance, we might train a cat detector based on lots of pictures\nof cats and dogs. Once we have trained it, we ship it as part of a smart catdoor computer\nvision system that lets only cats in. This is then installed in a customer\u2019s home and is never\nupdated again (barring extreme circumstances).\nOnline Learning\nNow imagine that the data \u00b9x\ud835\udc56,\ud835\udc66\ud835\udc56\u00baarrives one sample at a time. More specifically, assume\nthat we first observe x\ud835\udc56, then we need to come up with an estimate \ud835\udc53\u00b9x\ud835\udc56\u00ba. Only once\nwe have done this do we observe \ud835\udc66\ud835\udc56and so receive a reward or incur a loss, given our\ndecision. Many real problems fall into this category. For example, we need to predict\ntomorrow\u2019s stock price, which allows us to trade based on that estimate and at the end\nof the day we find out whether our estimate made us a profit. In other words, in online\nlearning , we have the following cycle where we are continuously improving our model\ngiven new observations:\nmodel\ud835\udc53\ud835\udc61\u0000!datax\ud835\udc61\u0000!estimate\ud835\udc53\ud835\udc61\u00b9x\ud835\udc61\u00ba\u0000!\nobservation\ud835\udc66\ud835\udc61\u0000!loss\ud835\udc59\u00b9\ud835\udc66\ud835\udc61, \ud835\udc53\ud835\udc61\u00b9x\ud835\udc61\u00ba\u00ba\u0000!model\ud835\udc53\ud835\udc61\u00b81(4.7.11)\nBandits\nBanditsare a special case of the problem above. While in most learning problems we have\na continuously parametrized function \ud835\udc53where we want to learn its parameters (e.g., a deep\nnetwork), in a banditproblem we only have a finite number of arms that we can pull, i.e.,\na finite number of actions that we can take. It is not very surprising that for this simpler\nproblem stronger theoretical guarantees in terms of optimality can be obtained. We list\nit mainly since this problem is often (confusingly) treated as if it were a distinct learning\nsetting.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2361, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8119c2d5-58cf-42e9-8db3-653f92dad061": {"__data__": {"id_": "8119c2d5-58cf-42e9-8db3-653f92dad061", "embedding": null, "metadata": {"page_label": "164", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "74d213ba-51ce-43d9-9c4d-b33502bebad6", "node_type": "4", "metadata": {"page_label": "164", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c5c1fc56b818b290bec43f84f25d312d2fec8fb2fb1bfb416ef19b5844910f58", "class_name": "RelatedNodeInfo"}}, "text": "164 Linear Neural Networks for Classi\ufb01cation\nControl\nIn many cases the environment remembers what we did. Not necessarily in an adversarial\nmanner but it will just remember and the response will depend on what happened before.\nFor instance, a coffee boiler controller will observe different temperatures depending on\nwhether it was heating the boiler previously. PID (proportional-integral-derivative) con-\ntroller algorithms are a popular choice there. Likewise, a user\u2019s behavior on a news site\nwilldependonwhatweshowedthempreviously(e.g.,theywillreadmostnewsonlyonce).\nMany such algorithms form a model of the environment in which they act so as to make\ntheir decisions appear less random. Recently, control theory (e.g., PID variants) has also\nbeenusedtoautomaticallytunehyperparameterstoachievebetterdisentanglingandrecon-\nstruction quality, and improve the diversity of generated text and the reconstruction quality\nof generated images ( Shaoetal., 2020).\nReinforcementLearning\nIn the more general case of an environment with memory, we may encounter situations\nwhere the environment is trying to cooperate with us (cooperative games, in particular\nfor non-zero-sum games), or others where the environment will try to win. Chess, Go,\nBackgammon, or StarCraft are some of the cases in reinforcement learning . Likewise, we\nmightwanttobuildagoodcontrollerforautonomouscars. Othercarsarelikelytorespond\nto the autonomous car\u2019s driving style in nontrivial ways, e.g., trying to avoid it, trying to\ncause an accident, or trying to cooperate with it.\nConsidering the Environment\nOne key distinction between the different situations above is that a strategy that might have\nworked throughout in the case of a stationary environment, might not work throughout in\nanenvironmentthatcanadapt. Forinstance,anarbitrageopportunitydiscoveredbyatrader\nis likely to disappear once it is exploited. The speed and manner at which the environment\nchanges determines to a large extent the type of algorithms that we can bring to bear. For\ninstance, if we know that things may only change slowly, we can force any estimate to\nchange only slowly, too. If we know that the environment might change instantaneously,\nbut only very infrequently, we can make allowances for that. These types of knowledge are\ncrucial for the aspiring data scientist in dealing with concept shift, i.e., when the problem\nthat is being solved can change over time.\n4.7.5Fairness,Accountability,and Transparencyin Machine\nLearning\nFinally, it is important to remember that when you deploy machine learning systems you\nare not merely optimizing a predictive model\u2014you are typically providing a tool that will\nbe used to (partially or fully) automate decisions. These technical systems can impact the\nlives of individuals who are subject to the resulting decisions. The leap from considering\npredictions to making decisions raises not only new technical questions, but also a slew of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "26de3d5a-f4ff-4e08-b930-b73df3a5f8ae": {"__data__": {"id_": "26de3d5a-f4ff-4e08-b930-b73df3a5f8ae", "embedding": null, "metadata": {"page_label": "165", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf5d0c5b-d9fd-497d-ae87-93c4bf9b7db2", "node_type": "4", "metadata": {"page_label": "165", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "19f19a0b24916e2d7129470e0ccc535d6485576afab227d18f2437a98583bb75", "class_name": "RelatedNodeInfo"}}, "text": "165 Environment and Distribution Shift\nethicalquestionsthatmustbecarefullyconsidered. Ifwearedeployingamedicaldiagnos-\ntic system, we need to know for which populations it may work and for which it may not.\nOverlooking foreseeable risks to the welfare of a subpopulation could cause us to adminis-\nter inferior care. Moreover, once we contemplate decision-making systems, we must step\nback and reconsider how we evaluate our technology. Among other consequences of this\nchangeofscope,wewillfindthat accuracy isseldomtherightmeasure. Forinstance,when\ntranslatingpredictionsintoactions,wewilloftenwanttotakeintoaccountthepotentialcost\nsensitivity of erring in various ways. If one way of misclassifying an image could be per-\nceived as a racial sleight of hand, while misclassification to a different category would be\nharmless, then we might want to adjust our thresholds accordingly, accounting for societal\nvalues in designing the decision-making protocol. We also want to be careful about how\nprediction systems can lead to feedback loops. For example, consider predictive policing\nsystems, which allocate patrol officers to areas with high forecasted crime. It is easy to see\nhow a worrying pattern can emerge:\n1.Neighborhoods with more crime get more patrols.\n2.Consequently,morecrimesarediscoveredintheseneighborhoods,enteringthetraining\ndata available for future iterations.\n3.Exposed to more positives, the model predicts yet more crime in these neighborhoods.\n4.In the next iteration, the updated model targets the same neighborhood even more heav-\nily leading to yet more crimes discovered, etc.\nOften,thevariousmechanismsbywhichamodel\u2019spredictionsbecomecoupledtoitstrain-\ning data are unaccounted for in the modeling process. This can lead to what researchers\ncallrunaway feedback loops . Additionally, we want to be careful about whether we are\naddressing the right problem in the first place. Predictive algorithms now play an outsize\nrole in mediating the dissemination of information. Should the news that an individual en-\ncounters be determined by the set of Facebook pages they have Liked? These are just a few\namongthemanypressingethicaldilemmasthatyoumightencounterinacareerinmachine\nlearning.\n4.7.6Summary\nIn many cases training and test sets do not come from the same distribution. This is called\ndistribution shift. The risk is the expectation of the loss over the entire population of data\ndrawn from their true distribution. However, this entire population is usually unavailable.\nEmpirical risk is an average loss over the training data to approximate the risk. In practice,\nwe perform empirical risk minimization.\nUnder the corresponding assumptions, covariate and label shift can be detected and cor-\nrected for at test time. Failure to account for this bias can become problematic at test time.\nInsomecases,theenvironmentmayrememberautomatedactionsandrespondinsurprising\nways. We must account for this possibility when building models and continue to moni-\ntor live systems, open to the possibility that our models and the environment will become\nentangled in unanticipated ways.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9707589f-18f7-4e3d-9a23-f4b0fae6279b": {"__data__": {"id_": "9707589f-18f7-4e3d-9a23-f4b0fae6279b", "embedding": null, "metadata": {"page_label": "166", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "160b30d5-c10b-4dbb-81f1-dbddc080128a", "node_type": "4", "metadata": {"page_label": "166", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2a74012ca966f4770605263029566055d5de9408634fffcf8b2e82928dcb00b3", "class_name": "RelatedNodeInfo"}}, "text": "166 Linear Neural Networks for Classi\ufb01cation\n1014.7.7Exercises\n1.What could happen when we change the behavior of a search engine? What might the\nusers do? What about the advertisers?\n2.Implement a covariate shift detector. Hint: build a classifier.\n3.Implement a covariate shift corrector.\n4.Besides distribution shift, what else could affect how the empirical risk approximates\nthe risk?\nDiscussions101.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a159fd72-3c5c-4108-a74c-9d82a16ea43e": {"__data__": {"id_": "a159fd72-3c5c-4108-a74c-9d82a16ea43e", "embedding": null, "metadata": {"page_label": "167", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a684543c-2b4e-402c-8e26-134a0476151d", "node_type": "4", "metadata": {"page_label": "167", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ea220c02397f4d8fafc92d8052dd8ad6226d7aa2b861d4a3816ecd9034e092eb", "class_name": "RelatedNodeInfo"}}, "text": "5 Multilayer Perceptrons\nInthischapter,wewillintroduceyourfirsttruly deepnetwork. Thesimplestdeepnetworks\nare called multilayerperceptrons , and they consist of multiple layers of neurons each fully\nconnected to those in the layer below (from which they receive input) and those above\n(which they, in turn, influence). Although automatic differentiation significantly simplifies\nthe implementation of deep learning algorithms, we will dive deep into how these gradi-\nents are calculated in deep networks. Then we will be ready to discuss issues relating to\nnumerical stability and parameter initialization that are key to successfully training deep\nnetworks. When we train such high-capacity models we run the risk of overfitting. Thus,\nwe will revisit regularization and generalization for deep networks. Throughout, we aim to\ngiveyouafirmgraspnotjustoftheconceptsbutalsoofthepracticeofusingdeepnetworks.\nAt the end of this chapter, we apply what we have introduced so far to a real case: house\nprice prediction. We punt matters relating to the computational performance, scalability,\nand efficiency of our models to subsequent chapters.\n5.1MultilayerPerceptrons\nInSection4.1 ,weintroducedsoftmaxregression,implementingthealgorithmfromscratch\n(Section4.4 )andusinghigh-levelAPIs( Section4.5 ). Thisallowedustotrainclassifiersca-\npableofrecognizing10categoriesofclothingfromlow-resolutionimages. Alongtheway,\nwe learned how to wrangle data, coerce our outputs into a valid probability distribution,\napplyanappropriatelossfunction,andminimizeitwithrespecttoourmodel\u2019sparameters.\nNow that we have mastered these mechanics in the context of simple linear models, we\ncanlaunchourexplorationofdeepneuralnetworks,thecomparativelyrichclassofmodels\nwith which this book is primarily concerned.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\n5.1.1Hidden Layers\nWe described affine transformations in Section 3.1.1 as linear transformations with added\nbias. To begin, recall the model architecture corresponding to our softmax regression ex-\n167", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "efe732cf-652d-4340-b1a5-5a129c547507": {"__data__": {"id_": "efe732cf-652d-4340-b1a5-5a129c547507", "embedding": null, "metadata": {"page_label": "168", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f421a9b2-8b56-49da-9723-c20ca39e00d8", "node_type": "4", "metadata": {"page_label": "168", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f7e1371dfe37872668c0443f531f869431636c28a2b728176d28f8f63b04b01d", "class_name": "RelatedNodeInfo"}}, "text": "168 Multilayer Perceptrons\nample, illustrated in Fig. 4.1.1 . This model maps inputs directly to outputs via a single\naffine transformation, followed by a softmax operation. If our labels truly were related to\nthe input data by a simple affine transformation, then this approach would be sufficient.\nHowever, linearity (in affine transformations) is a strongassumption.\nLimitations of Linear Models\nFor example, linearity implies the weakerassumption of monotonicity , i.e., that any in-\ncrease in our feature must either always cause an increase in our model\u2019s output (if the\ncorresponding weight is positive), or always cause a decrease in our model\u2019s output (if\nthe corresponding weight is negative). Sometimes that makes sense. For example, if we\nweretryingtopredictwhetheranindividualwillrepayaloan,wemightreasonablyassume\nthat all other things being equal, an applicant with a higher income would always be more\nlikely to repay than one with a lower income. While monotonic, this relationship likely\nis not linearly associated with the probability of repayment. An increase in income from\n$0 to $50,000 likely corresponds to a bigger increase in likelihood of repayment than an\nincrease from $1 million to $1.05 million. One way to handle this might be to postprocess\nour outcome such that linearity becomes more plausible, by using the logistic map (and\nthus the logarithm of the probability of outcome).\nNote that we can easily come up with examples that violate monotonicity. Say for example\nthat we want to predict health as a function of body temperature. For individuals with a\nnormal body temperature above 37\u00b0C (98.6\u00b0F), higher temperatures indicate greater risk.\nHowever, if the body temperatures drops below 37\u00b0C, lower temperatures indicate greater\nrisk! Again, we might resolve the problem with some clever preprocessing, such as using\nthe distance from 37\u00b0C as a feature.\nBut what about classifying images of cats and dogs? Should increasing the intensity of the\npixelatlocation(13, 17)alwaysincrease(oralwaysdecrease)thelikelihoodthattheimage\ndepicts a dog? Reliance on a linear model corresponds to the implicit assumption that the\nonly requirement for differentiating cats and dogs is to assess the brightness of individual\npixels. This approach is doomed to fail in a world where inverting an image preserves the\ncategory.\nAnd yet despite the apparent absurdity of linearity here, as compared with our previous\nexamples, it is less obvious that we could address the problem with a simple preprocessing\nfix. That is, because the significance of any pixel depends in complex ways on its context\n(the values of the surrounding pixels). While there might exist a representation of our data\nthat would take into account the relevant interactions among our features, on top of which\na linear model wouldbe suitable, we simplydo not knowhow to calculate it byhand. With\ndeep neural networks, we used observational data to jointly learn both a representation via\nhidden layers and a linear predictor that acts upon that representation.\nThis problem of nonlinearity has been studied for at least a century ( Fisher, 1925 ). For\ninstance, decision trees in their most basic form use a sequence of binary decisions to de-\ncide upon class membership ( Quinlan, 1993 ). Likewise, kernel methods have been used\nfor many decades to model nonlinear dependencies ( Aronszajn, 1950 ). This has found its", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4629030f-1eaf-4f74-a973-8c9ff7e6fee6": {"__data__": {"id_": "4629030f-1eaf-4f74-a973-8c9ff7e6fee6", "embedding": null, "metadata": {"page_label": "169", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4d0945b2-4bc9-4368-8a51-331d967ac990", "node_type": "4", "metadata": {"page_label": "169", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2f57afa437cdc20c53c3e862330cf9a8088ab20c0d89c747072dd38665198b4d", "class_name": "RelatedNodeInfo"}}, "text": "169 Multilayer Perceptrons\nway into nonparametric spline models ( Wahba, 1990 ) and kernel methods ( Sch\u00f6lkopf and\nSmola, 2002 ). It is also something that the brain solves quite naturally. After all, neu-\nrons feed into other neurons which, in turn, feed into other neurons again ( Ram\u00f3n y Cajal\nand Azoulay, 1894 ). Consequently we have a sequence of relatively simple transforma-\ntions.\nIncorporatingHidden Layers\nWe can overcome the limitations of linear models by incorporating one or more hidden\nlayers. The easiest way to do this is to stack many fully connected layers on top of one\nanother. Eachlayerfeedsinto thelayeraboveit, untilwegenerateoutputs. Wecanthinkof\nthe first\ud835\udc3f\u00001layers as our representation and the final layer as our linear predictor. This\narchitecture is commonly called a multilayer perceptron , often abbreviated as MLP(Fig.\n5.1.1).\ntFig. 5.1.1 An MLP with a hidden layer of \ufb01ve hidden units.\nThis MLP has four inputs, three outputs, and its hidden layer contains five hidden units.\nSincetheinputlayerdoesnotinvolveanycalculations,producingoutputswiththisnetwork\nrequires implementing the computations for both the hidden and output layers; thus, the\nnumber of layers in this MLP is two. Note that both layers are fully connected. Every\ninputinfluenceseveryneuroninthehiddenlayer,andeachoftheseinturninfluencesevery\nneuron in the output layer. Alas, we are not quite done yet.\nFromLinear to Nonlinear\nAsbefore,wedenotebythematrix X2R\ud835\udc5b\u0002\ud835\udc51aminibatchof \ud835\udc5bexampleswhereeachexam-\nple has\ud835\udc51inputs (features). For a one-hidden-layer MLP whose hidden layer has \u210ehidden\nunits, we denote by H2R\ud835\udc5b\u0002\u210ethe outputs of the hidden layer, which are hidden represen-\ntations. Since the hidden and output layers are both fully connected, we have hidden-layer\nweights W\u00b91\u00ba2R\ud835\udc51\u0002\u210eand biases b\u00b91\u00ba2R1\u0002\u210eand output-layer weights W\u00b92\u00ba2R\u210e\u0002\ud835\udc5eand\nbiases b\u00b92\u00ba2R1\u0002\ud835\udc5e. This allows us to calculate the outputs O2R\ud835\udc5b\u0002\ud835\udc5eof the one-hidden-\nlayer MLP as follows:\nH=XW\u00b91\u00ba\u00b8b\u00b91\u00ba,\nO=HW\u00b92\u00ba\u00b8b\u00b92\u00ba.(5.1.1)\nNote that after adding the hidden layer, our model now requires us to track and update\nadditionalsetsofparameters. Sowhathavewegainedinexchange? Youmightbesurprised", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2141, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4978ed3-e1f1-4cdd-afcb-3f626e5b84c8": {"__data__": {"id_": "c4978ed3-e1f1-4cdd-afcb-3f626e5b84c8", "embedding": null, "metadata": {"page_label": "170", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e56e9c4c-7541-4cd0-9af1-0a4d62942f41", "node_type": "4", "metadata": {"page_label": "170", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7158d24854e6805708726824a48b4903af5683fd9de8df53b4fa767dde232a60", "class_name": "RelatedNodeInfo"}}, "text": "170 Multilayer Perceptrons\ntofindoutthat\u2014inthemodeldefinedabove\u2014 wegainnothingforourtroubles ! Thereason\nisplain. Thehiddenunitsabovearegivenbyanaffinefunctionoftheinputs,andtheoutputs\n(pre-softmax) are just an affine function of the hidden units. An affine function of an affine\nfunction is itself an affine function. Moreover, our linear model was already capable of\nrepresenting any affine function.\nToseethisformallywecanjustcollapseoutthehiddenlayerintheabovedefinition,yielding\nan equivalent single-layer model with parameters W=W\u00b91\u00baW\u00b92\u00baandb=b\u00b91\u00baW\u00b92\u00ba\u00b8\nb\u00b92\u00ba:\nO=\u00b9XW\u00b91\u00ba\u00b8b\u00b91\u00ba\u00baW\u00b92\u00ba\u00b8b\u00b92\u00ba=XW\u00b91\u00baW\u00b92\u00ba\u00b8b\u00b91\u00baW\u00b92\u00ba\u00b8b\u00b92\u00ba=XW\u00b8b.\n(5.1.2)\nIn order to realize the potential of multilayer architectures, we need one more key ingre-\ndient: a nonlinear activation function \ud835\udf0eto be applied to each hidden unit following the\naffine transformation. For instance, a popular choice is the ReLU (rectified linear unit) ac-\ntivation function ( Nair and Hinton, 2010 )\ud835\udf0e\u00b9\ud835\udc65\u00ba=max\u00b90,\ud835\udc65\u00baoperating on its arguments\nelementwise. The outputs of activation functions \ud835\udf0e\u00b9\u0001\u00baare called activations . In general,\nwith activation functions in place, it is no longer possible to collapse our MLP into a linear\nmodel:\nH=\ud835\udf0e\u00b9XW\u00b91\u00ba\u00b8b\u00b91\u00ba\u00ba,\nO=HW\u00b92\u00ba\u00b8b\u00b92\u00ba.(5.1.3)\nSince each row in Xcorresponds to an example in the minibatch, with some abuse of\nnotation, we define the nonlinearity \ud835\udf0eto apply to its inputs in a rowwise fashion, i.e., one\nexample at a time. Note that we used the same notation for softmax when we denoted a\nrowwise operation in Section 4.1.1 . Quite frequently the activation functions we use apply\nnotmerelyrowwisebutelementwise. Thatmeansthataftercomputingthelinearportionof\nthe layer, we can calculate each activation without looking at the values taken by the other\nhidden units.\nTo build more general MLPs, we can continue stacking such hidden layers, e.g., H\u00b91\u00ba=\n\ud835\udf0e1\u00b9XW\u00b91\u00ba\u00b8b\u00b91\u00ba\u00baandH\u00b92\u00ba=\ud835\udf0e2\u00b9H\u00b91\u00baW\u00b92\u00ba\u00b8b\u00b92\u00ba\u00ba,oneatopanother,yieldingevermore\nexpressive models.\nUniversalApproximators\nWe know that the brain is capable of very sophisticated statistical analysis. As such, it is\nworthasking,just howpowerful adeepnetworkcouldbe. Thisquestionhasbeenanswered\nmultiple times, e.g., in Cybenko ( 1989) in the context of MLPs, and in Micchelli ( 1984) in\nthe context of reproducing kernel Hilbert spaces in a way that could be seen as radial basis\nfunction(RBF)networkswithasinglehiddenlayer. These(andrelatedresults)suggestthat\neven with a single-hidden-layer network, given enough nodes (possibly absurdly many),\nand the right set of weights, we can model any function. Actually learning that function\nis the hard part, though. You might think of your neural network as being a bit like the\nC programming language. The language, like any other modern language, is capable of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f12838b1-4e79-482a-9104-1e25b6de5975": {"__data__": {"id_": "f12838b1-4e79-482a-9104-1e25b6de5975", "embedding": null, "metadata": {"page_label": "171", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a07d81d-6212-4c73-a92c-c0d2c08bfdcc", "node_type": "4", "metadata": {"page_label": "171", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f81002099d79d3ca91534bc5bc2e0ee7211177b2ee406aeccf2f9920b37839b9", "class_name": "RelatedNodeInfo"}}, "text": "171 Multilayer Perceptrons\nexpressing any computable program. But actually coming up with a program that meets\nyour specifications is the hard part.\nMoreover,justbecauseasingle-hidden-layernetwork canlearnanyfunctiondoesnotmean\nthat you should try to solve all of your problems with one. In fact, in this case kernel\nmethodsarewaymoreeffective,sincetheyarecapableofsolvingtheproblem exactlyeven\nin infinite dimensional spaces ( Kimeldorf and Wahba, 1971 ,Sch\u00f6lkopf et al., 2001). In\nfact, we can approximate many functions much more compactly by using deeper (rather\nthanwider)networks( SimonyanandZisserman,2014 ). Wewilltouchuponmorerigorous\narguments in subsequent chapters.\n5.1.2ActivationFunctions\nActivation functions decide whether a neuron should be activated or not by calculating the\nweighted sum and further adding bias to it. They are differentiable operators for trans-\nforming input signals to outputs, while most of them add nonlinearity. Because activation\nfunctionsarefundamentaltodeeplearning,let\u2019sbrieflysurveysomecommonones.\nReLUFunction\nThe most popular choice, due to both simplicity of implementation and its good perfor-\nmance on a variety of predictive tasks, is the rectifiedlinearunit (ReLU) (Nairand Hinton,\n2010). ReLU provides a very simple nonlinear transformation. Given an element \ud835\udc65, the\nfunction is defined as the maximum of that element and 0:\nReLU\u00b9\ud835\udc65\u00ba=max\u00b9\ud835\udc65,0\u00ba. (5.1.4)\nInformally, the ReLU function retains only positive elements and discards all negative el-\nements by setting the corresponding activations to 0. To gain some intuition, we can plot\nthe function. As you can see, the activation function is piecewise linear.\nx=torch .arange( -8.0,8.0,0.1, requires_grad =True )\ny=torch .relu(x)\nd2l.plot(x .detach(), y .detach(), 'x','relu(x) ', figsize =(5,2.5))\nWhen the input is negative, the derivative of the ReLU function is 0, and when the input\nis positive, the derivative of the ReLU function is 1. Note that the ReLU function is not", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1978, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11b75de2-618a-49d3-bce0-1846f4c980d7": {"__data__": {"id_": "11b75de2-618a-49d3-bce0-1846f4c980d7", "embedding": null, "metadata": {"page_label": "172", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c1b7c52-752b-4294-9cc2-7a5f135261e2", "node_type": "4", "metadata": {"page_label": "172", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "07074c67a899da39e8fb227a0d847a6168f2e1181c4c0374c498dc7a9cc343d2", "class_name": "RelatedNodeInfo"}}, "text": "172 Multilayer Perceptrons\ndifferentiable when the input takes value precisely equal to 0. In these cases, we default to\nthe left-hand-side derivative and say that the derivative is 0 when the input is 0. We can\ngetawaywiththisbecausetheinputmayneveractuallybezero(mathematicianswouldsay\nthat it is nondifferentiable on a set of measure zero). There is an old adage that if subtle\nboundary conditions matter, we are probably doing ( real) mathematics, not engineering.\nThat conventional wisdom may apply here, or at least, the fact that we are not performing\nconstrained optimization ( Mangasarian, 1965 ,Rockafellar, 1970 ). We plot the derivative\nof the ReLU function below.\ny.backward(torch .ones_like(x), retain_graph =True )\nd2l.plot(x .detach(), x .grad, 'x','grad of relu ', figsize =(5,2.5))\nThe reason for using ReLU is that its derivatives are particularly well behaved: either they\nvanish or they just let the argument through. This makes optimization better behaved and\nit mitigated the well-documented problem of vanishing gradients that plagued previous\nversions of neural networks (more on this later).\nNote that there are many variants to the ReLU function, including the parametrized ReLU\n(pReLU) function ( Heet al., 2015). This variation adds a linear term to ReLU, so some\ninformation still gets through, even when the argument is negative:\npReLU\u00b9\ud835\udc65\u00ba=max\u00b90,\ud835\udc65\u00ba\u00b8\ud835\udefcmin\u00b90,\ud835\udc65\u00ba. (5.1.5)\nSigmoid Function\nThesigmoidfunction transforms those inputs whose values lie in the domain R, to outputs\nthat lie on the interval (0, 1). For that reason, the sigmoid is often called a squashingfunc-\ntion: it squashes any input in the range (-inf, inf) to some value in the range (0, 1):\nsigmoid\u00b9\ud835\udc65\u00ba=1\n1\u00b8exp\u00b9\u0000\ud835\udc65\u00ba. (5.1.6)\nIn the earliest neural networks, scientists were interested in modeling biological neurons\nthat either fireordo not fire . Thus the pioneers of this field, going all the way back to\nMcCulloch and Pitts, the inventors of the artificial neuron, focused on thresholding units\n(McCulloch and Pitts, 1943 ). A thresholding activation takes value 0 when its input is\nbelow some threshold and value 1 when the input exceeds the threshold.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aba8a10a-1cbb-4fa7-8c5a-d03b2388643d": {"__data__": {"id_": "aba8a10a-1cbb-4fa7-8c5a-d03b2388643d", "embedding": null, "metadata": {"page_label": "173", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c4f8a63-5980-41d6-b3f0-979752b4e046", "node_type": "4", "metadata": {"page_label": "173", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b0ccc6a8f81d5a2a78f1f2a511a0cb0fd1da0258d2e56784b6b3fe216d992d4d", "class_name": "RelatedNodeInfo"}}, "text": "173 Multilayer Perceptrons\nWhenattentionshiftedtogradient-basedlearning,thesigmoidfunctionwasanaturalchoice\nbecause it is a smooth, differentiable approximation to a thresholding unit. Sigmoids are\nstill widely used as activation functions on the output units when we want to interpret the\noutputsasprobabilitiesforbinaryclassificationproblems: youcanthinkofthesigmoidasa\nspecialcaseofthesoftmax. However, thesigmoidhaslargelybeenreplacedbythesimpler\nand more easily trainable ReLU for most use in hidden layers. Much of this has to do with\nthe fact that the sigmoid poses challenges for optimization ( LeCunet al., 1998) since its\ngradient vanishes for large positive andnegative arguments. This can lead to plateaus that\nare difficult to escape from. Nonetheless sigmoids are important. In later chapters (e.g.,\nSection 10.1 ) on recurrent neural networks, we will describe architectures that leverage\nsigmoid units to control the flow of information across time.\nBelow, we plot the sigmoid function. Note that when the input is close to 0, the sigmoid\nfunction approaches a linear transformation.\ny=torch .sigmoid(x)\nd2l.plot(x .detach(), y .detach(), 'x','sigmoid(x) ', figsize =(5,2.5))\nThe derivative of the sigmoid function is given by the following equation:\n\ud835\udc51\n\ud835\udc51\ud835\udc65sigmoid\u00b9\ud835\udc65\u00ba=exp\u00b9\u0000\ud835\udc65\u00ba\n\u00b91\u00b8exp\u00b9\u0000\ud835\udc65\u00ba\u00ba2=sigmoid\u00b9\ud835\udc65\u00ba\u00b91\u0000sigmoid\u00b9\ud835\udc65\u00ba\u00ba. (5.1.7)\nThe derivative of the sigmoid function is plotted below. Note that when the input is 0, the\nderivative of the sigmoid function reaches a maximum of 0.25. As the input diverges from\n0 in either direction, the derivative approaches 0.\n# Clear out previous gradients\nx.grad .data .zero_()\ny.backward(torch .ones_like(x),retain_graph =True )\nd2l.plot(x .detach(), x .grad, 'x','grad of sigmoid ', figsize =(5,2.5))\nTanhFunction\nLike the sigmoid function, the tanh (hyperbolic tangent) function also squashes its inputs,\ntransforming them into elements on the interval between \u00001and1:\ntanh\u00b9\ud835\udc65\u00ba=1\u0000exp\u00b9\u00002\ud835\udc65\u00ba\n1\u00b8exp\u00b9\u00002\ud835\udc65\u00ba. (5.1.8)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6715a9e-9698-478b-b4cf-ae7a02e07d1e": {"__data__": {"id_": "f6715a9e-9698-478b-b4cf-ae7a02e07d1e", "embedding": null, "metadata": {"page_label": "174", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ced0416e-6f27-44db-850c-26d825932625", "node_type": "4", "metadata": {"page_label": "174", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "21716a0a240155d51afe481593f0b2e1e1636efda9b6c06e6c928e0ef64498dd", "class_name": "RelatedNodeInfo"}}, "text": "174 Multilayer Perceptrons\nWe plot the tanh function below. Note that as input nears 0, the tanh function approaches a\nlinear transformation. Although the shape of the function is similar to that of the sigmoid\nfunction, the tanh function exhibits point symmetry about the origin of the coordinate sys-\ntem (Kalman and Kwasny, 1992 ).\ny=torch .tanh(x)\nd2l.plot(x .detach(), y .detach(), 'x','tanh(x) ', figsize =(5,2.5))\nThe derivative of the tanh function is:\n\ud835\udc51\n\ud835\udc51\ud835\udc65tanh\u00b9\ud835\udc65\u00ba=1\u0000tanh2\u00b9\ud835\udc65\u00ba. (5.1.9)\nIt is plotted below. As the input nears 0, the derivative of the tanh function approaches a\nmaximum of 1. And as we saw with the sigmoid function, as input moves away from 0 in\neither direction, the derivative of the tanh function approaches 0.\n# Clear out previous gradients\nx.grad .data .zero_()\ny.backward(torch .ones_like(x),retain_graph =True )\nd2l.plot(x .detach(), x .grad, 'x','grad of tanh ', figsize =(5,2.5))\n5.1.3Summaryand Discussion\nWe now know how to incorporate nonlinearities to build expressive multilayer neural net-\nwork architectures. As a side note, your knowledge already puts you in command of a sim-\nilar toolkit to a practitioner circa 1990. In some ways, you have an advantage over anyone", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71841c1d-deb5-4d16-85da-f5057dbe5d65": {"__data__": {"id_": "71841c1d-deb5-4d16-85da-f5057dbe5d65", "embedding": null, "metadata": {"page_label": "175", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9789ce45-2c2e-478f-b6aa-e448fd1f89d7", "node_type": "4", "metadata": {"page_label": "175", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9e28dd364b586cb9a05b7e24305a31b76beb6c0103bd422a50ec7423ee7ac69d", "class_name": "RelatedNodeInfo"}}, "text": "175 Multilayer Perceptrons\n102working back then, because you can leverage powerful open-source deep learning frame-\nworks to build models rapidly, using only a few lines of code. Previously, training these\nnetworks required researchers to code up layers and derivatives explicitly in C, Fortran, or\neven Lisp (in the case of LeNet).\nA secondary benefit is that ReLU is significantly more amenable to optimization than the\nsigmoid or the tanh function. One could argue that this was one of the key innovations that\nhelped the resurgence of deep learning over the past decade. Note, though, that research in\nactivation functions has not stopped. For instance, the GELU (Gaussian error linear unit)\nactivationfunction \ud835\udc65\u03a6\u00b9\ud835\udc65\u00babyHendrycksandGimpel( 2016)(\u03a6\u00b9\ud835\udc65\u00baisthestandardGaussian\ncumulative distribution function) and the Swish activation function \ud835\udf0e\u00b9\ud835\udc65\u00ba=\ud835\udc65sigmoid\u00b9\ud835\udefd\ud835\udc65\u00ba\nasproposedinRamachandran etal.(2017)canyieldbetteraccuracyinmanycases.\n5.1.4Exercises\n1.Show that adding layers to a lineardeep network, i.e., a network without nonlinearity\n\ud835\udf0ecan never increase the expressive power of the network. Give an example where it\nactively reduces it.\n2.Compute the derivative of the pReLU activation function.\n3.Compute the derivative of the Swish activation function \ud835\udc65sigmoid\u00b9\ud835\udefd\ud835\udc65\u00ba.\n4.Show that an MLP using only ReLU (or pReLU) constructs a continuous piecewise\nlinear function.\n5.Sigmoid and tanh are very similar.\n1.Show that tanh\u00b9\ud835\udc65\u00ba\u00b81=2 sigmoid\u00b92\ud835\udc65\u00ba.\n2.Prove that the function classes parametrized by both nonlinearities are identical.\nHint: affine layers have bias terms, too.\n6.Assume that we have a nonlinearity that applies to one minibatch at a time, such as the\nbatch normalization ( Ioffe and Szegedy, 2015 ). What kinds of problems do you expect\nthis to cause?\n7.Provide an example where the gradients vanish for the sigmoid activation function.\nDiscussions102.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ecc5feb-aceb-42c4-9bef-40e8fc9a30f1": {"__data__": {"id_": "0ecc5feb-aceb-42c4-9bef-40e8fc9a30f1", "embedding": null, "metadata": {"page_label": "176", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55ee68cb-4244-49e7-b6d0-a9019bd95f3c", "node_type": "4", "metadata": {"page_label": "176", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0e1fae52a3c86a5fca6cb05482747e76783f9b14d12fcb39892058f1a7fab23b", "class_name": "RelatedNodeInfo"}}, "text": "176 Multilayer Perceptrons\n5.2Implementation of MultilayerPerceptrons\nMultilayerperceptrons(MLPs)arenotmuchmorecomplextoimplementthansimplelinear\nmodels. The key conceptual difference is that we now concatenate multiple layers.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n5.2.1Implementation fromScratch\nLet\u2019s begin again by implementing such a network from scratch.\nInitializingModel Parameters\nRecall that Fashion-MNIST contains 10 classes, and that each image consists of a 28\u0002\n28=784grid of grayscale pixel values. As before we will disregard the spatial structure\namong the pixels for now, so we can think of this as a classification dataset with 784 input\nfeaturesand10classes. Tobegin,wewillimplementanMLPwithonehiddenlayerand256\nhidden units. Both the number of layersand their width are adjustable(theyare considered\nhyperparameters). Typically, wechoosethelayerwidthstobedivisiblebylargerpowersof\n2. This is computationally efficient due to the way memory is allocated and addressed in\nhardware.\nAgain, we will represent our parameters with several tensors. Note that foreverylayer , we\nmust keep track of one weight matrix and one bias vector. As always, we allocate memory\nfor the gradients of the loss with respect to these parameters.\nIn the code below we use nn.Parameter to automatically register a class attribute as a\nparameter to be tracked by autograd (Section 2.5 ).\nclass MLPScratch (d2l .Classifier):\ndef __init__ (self , num_inputs, num_outputs, num_hiddens, lr, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .W1=nn.Parameter(torch .randn(num_inputs, num_hiddens) *sigma)\nself .b1=nn.Parameter(torch .zeros(num_hiddens))\nself .W2=nn.Parameter(torch .randn(num_hiddens, num_outputs) *sigma)\nself .b2=nn.Parameter(torch .zeros(num_outputs))\nModel\nTo make sure we know how everything works, we will implement the ReLU activation\nourselves rather than invoking the built-in relufunction directly.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7588a18-1179-4054-9bc4-aee080d59feb": {"__data__": {"id_": "b7588a18-1179-4054-9bc4-aee080d59feb", "embedding": null, "metadata": {"page_label": "177", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b55f1c7f-c0bb-43f5-890b-1b4d4c815996", "node_type": "4", "metadata": {"page_label": "177", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7650ba475b4339fb3a9df26cb0daf5c6a0d30b0e98e711ca0e93c112576a2c2d", "class_name": "RelatedNodeInfo"}}, "text": "177 Implementation of Multilayer Perceptrons\ndef relu (X):\na=torch .zeros_like(X)\nreturn torch .max(X, a)\nSince we are disregarding spatial structure, we reshape each two-dimensional image into\na flat vector of length num_inputs . Finally, we implement our model with just a few lines\nof code. Since we use the framework built-in autograd this is all that it takes.\n@d2l .add_to_class(MLPScratch)\ndef forward (self , X):\nX=X.reshape(( -1,self .num_inputs))\nH=relu(torch .matmul(X, self .W1) +self .b1)\nreturn torch .matmul(H, self .W2) +self .b2\nTraining\nFortunately, the training loop for MLPs is exactly the same as for softmax regression.\nWe define the model, data, and trainer, then finally invoke the fitmethod on model and\ndata.\nmodel =MLPScratch(num_inputs =784, num_outputs =10, num_hiddens =256, lr =0.1)\ndata =d2l.FashionMNIST(batch_size =256)\ntrainer =d2l.Trainer(max_epochs =10)\ntrainer .fit(model, data)\n5.2.2Concise Implementation\nAs you might expect, by relying on the high-level APIs, we can implement MLPs even\nmore concisely.\nModel\nComparedwithourconciseimplementationofsoftmaxregressionimplementation( Section\n4.5),theonlydifferenceisthatweadd twofullyconnectedlayerswherewepreviouslyadded\nonlyone. The first is the hidden layer, the second is the output layer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecb9610f-ff79-456a-b464-af4afe3fc9fd": {"__data__": {"id_": "ecb9610f-ff79-456a-b464-af4afe3fc9fd", "embedding": null, "metadata": {"page_label": "178", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7706171d-8a61-4005-8196-ff562817410a", "node_type": "4", "metadata": {"page_label": "178", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "adf6961557ba03ce41226cec063d49cc91bf11c3534ba6140ac247a6a1034f3e", "class_name": "RelatedNodeInfo"}}, "text": "178 Multilayer Perceptrons\nclass MLP(d2l .Classifier):\ndef __init__ (self , num_outputs, num_hiddens, lr):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(nn .Flatten(), nn .LazyLinear(num_hiddens),\nnn.ReLU(), nn .LazyLinear(num_outputs))\nPreviously, we defined forward methods for models to transform input using the model\nparameters. These operations are essentially a pipeline: you take an input and apply a\ntransformation (e.g., matrix multiplication with weights followed by bias addition), then\nrepetitively use the output of the current transformation as input to the next transforma-\ntion. However, you may have noticed that no forward method is defined here. In fact,\nMLPinherits the forward method from the Moduleclass (Section 3.2.2 ) to simply invoke\nself.net(X) (Xis input), which is now defined as a sequence of transformations via the\nSequential class. The Sequential class abstracts the forward process enabling us to fo-\ncus on the transformations. We will further discuss how the Sequential class works in\nSection 6.1.2 .\nTraining\nThe training loop is exactly the same as when we implemented softmax regression. This\nmodularity enables us to separate matters concerning the model architecture from orthog-\nonal considerations.\nmodel =MLP(num_outputs =10, num_hiddens =256, lr =0.1)\ntrainer .fit(model, data)\n5.2.3Summary\nNow that we have more practice in designing deep networks, the step from a single to mul-\ntiple layers of deep networks does not pose such a significant challenge any longer. In\nparticular, we can reuse the training algorithm and data loader. Note, though, that imple-\nmenting MLPs from scratch is nonetheless messy: naming and keeping track of the model\nparameters makes it difficult to extend models. For instance, imagine wanting to insert\nanotherlayerbetweenlayers42and43. Thismightnowbelayer42b,unlesswearewilling", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbfa5a24-5570-40ab-9e88-1070ff3dea05": {"__data__": {"id_": "bbfa5a24-5570-40ab-9e88-1070ff3dea05", "embedding": null, "metadata": {"page_label": "179", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fadc733f-1d1a-49d1-9d24-b20379f32cbb", "node_type": "4", "metadata": {"page_label": "179", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "49cf486d9ab603507082a165a1d2d7804a981fff94bd7dce122f56b121fb99fe", "class_name": "RelatedNodeInfo"}}, "text": "179 Implementation of Multilayer Perceptrons\n103to perform sequential renaming. Moreover, if we implement the network from scratch, it\nis much more difficult for the framework to perform meaningful performance optimiza-\ntions.\nNonetheless, you have now reached the state of the art of the late 1980s when fully con-\nnected deep networks were the method of choice for neural network modeling. Our next\nconceptual step will be to consider images. Before we do so, we need to review a number\nof statistical basics and details on how to compute models efficiently.\n5.2.4Exercises\n1.Change the number of hidden units num_hiddens and plot how its number affects the\naccuracy of the model. What is the best value of this hyperparameter?\n2.Try adding a hidden layer to see how it affects the results.\n3.Whyisitabadideatoinsertahiddenlayerwithasingleneuron? Whatcouldgowrong?\n4.Howdoeschangingthelearningratealteryourresults? Withallotherparametersfixed,\nwhich learning rate gives you the best results? How does this relate to the number of\nepochs?\n5.Let\u2019s optimize over all hyperparameters jointly, i.e., learning rate, number of epochs,\nnumber of hidden layers, and number of hidden units per layer.\n1.What is the best result you can get by optimizing over all of them?\n2.Why it is much more challenging to deal with multiple hyperparameters?\n3.Describe an efficient strategy for optimizing over multiple parameters jointly.\n6.Compare the speed of the framework and the from-scratch implementation for a chal-\nlenging problem. How does it change with the complexity of the network?\n7.Measure the speed of tensor\u2013matrix multiplications for well-aligned and misaligned\nmatrices. For instance, test for matrices with dimension 1024, 1025, 1026, 1028, and\n1032.\n1.How does this change between GPUs and CPUs?\n2.Determine the memory bus width of your CPU and GPU.\n8.Try out different activation functions. Which one works best?\n9.Is there a difference between weight initializations of the network? Does it matter?\nDiscussions103.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2017, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0487c5ae-0bb0-4326-92c6-8b0c9de1aede": {"__data__": {"id_": "0487c5ae-0bb0-4326-92c6-8b0c9de1aede", "embedding": null, "metadata": {"page_label": "180", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e84242c7-f9ba-4b13-84d3-28d371e8cbe7", "node_type": "4", "metadata": {"page_label": "180", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "90655e4e67fc6fe9832fe06535952d142b0f5bdf21c81c1d805c6d4deeed5948", "class_name": "RelatedNodeInfo"}}, "text": "180 Multilayer Perceptrons\n5.3ForwardPropagation,BackwardPropagation,\nand Computational Graphs\nSo far, we have trained our models with minibatch stochastic gradient descent. However,\nwhen we implemented the algorithm, we only worried about the calculations involved in\nforwardpropagation through the model. When it came time to calculate the gradients, we\njustinvokedthebackpropagationfunctionprovidedbythedeeplearningframework.\nThe automatic calculation of gradients profoundly simplifies the implementation of deep\nlearning algorithms. Before automatic differentiation, even small changes to complicated\nmodels required recalculating complicated derivatives by hand. Surprisingly often, aca-\ndemic papers had to allocate numerous pages to deriving update rules. While we must\ncontinue to rely on automatic differentiation so we can focus on the interesting parts, you\nought to know how these gradients are calculated under the hood if you want to go beyond\na shallow understanding of deep learning.\nIn this section, we take a deep dive into the details of backward propagation (more com-\nmonly called backpropagation ). To convey some insight for both the techniques and their\nimplementations, we rely on some basic mathematics and computational graphs. To start,\nwe focus our exposition on a one-hidden-layer MLP with weight decay ( \u21132regularization,\nto be described in subsequent chapters).\n5.3.1ForwardPropagation\nForwardpropagation (orforwardpass )referstothecalculationandstorageofintermediate\nvariables(includingoutputs)foraneuralnetworkinorderfromtheinputlayertotheoutput\nlayer. We now work step-by-step through the mechanics of a neural network with one\nhiddenlayer. ThismayseemtediousbutintheeternalwordsoffunkvirtuosoJamesBrown,\nyou must \u201cpay the cost to be the boss\u201d.\nForthesakeofsimplicity,let\u2019sassumethattheinputexampleis x2R\ud835\udc51andthatourhidden\nlayer does not include a bias term. Here the intermediate variable is:\nz=W\u00b91\u00bax, (5.3.1)\nwhere W\u00b91\u00ba2R\u210e\u0002\ud835\udc51is the weight parameter of the hidden layer. After running the inter-\nmediate variable z2R\u210ethrough the activation function \ud835\udf19we obtain our hidden activation\nvector of length \u210e:\nh=\ud835\udf19\u00b9z\u00ba. (5.3.2)\nThe hidden layer output his also an intermediate variable. Assuming that the parameters\nof the output layer possess only a weight of W\u00b92\u00ba2R\ud835\udc5e\u0002\u210e, we can obtain an output layer\nvariable with a vector of length \ud835\udc5e:\no=W\u00b92\u00bah. (5.3.3)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2379, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fc49fb2-2601-4038-ac8c-54869d048cbe": {"__data__": {"id_": "4fc49fb2-2601-4038-ac8c-54869d048cbe", "embedding": null, "metadata": {"page_label": "181", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "28e368dd-8028-423a-a280-5c48f639af16", "node_type": "4", "metadata": {"page_label": "181", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "00117ddf49fbacf553433b4debcfb038fb1d72659399cfb58e82b663dd30b30d", "class_name": "RelatedNodeInfo"}}, "text": "181 Forward Propagation, Backward Propagation, and Computational Graphs\nAssuming that the loss function is \ud835\udc59and the example label is \ud835\udc66, we can then calculate the\nloss term for a single data example,\n\ud835\udc3f=\ud835\udc59\u00b9o,\ud835\udc66\u00ba. (5.3.4)\nAs we will see the definition of \u21132regularization to be introduced later, given the hyperpa-\nrameter\ud835\udf06, the regularization term is\n\ud835\udc60=\ud835\udf06\n2\u0010\nkW\u00b91\u00bak2\nF\u00b8kW\u00b92\u00bak2\nF\u0011\n, (5.3.5)\nwhere the Frobenius norm of the matrix is simply the \u21132norm applied after flattening the\nmatrixintoavector. Finally,themodel\u2019sregularizedlossonagivendataexampleis:\n\ud835\udc3d=\ud835\udc3f\u00b8\ud835\udc60. (5.3.6)\nWe refer to\ud835\udc3das theobjectivefunction in the following discussion.\n5.3.2Computational Graph of ForwardPropagation\nPlottingcomputational graphs helps us visualize the dependencies of operators and vari-\nables within the calculation. Fig. 5.3.1 contains the graph associated with the simple net-\nwork described above, where squares denote variables and circles denote operators. The\nlower-left corner signifies the input and the upper-right corner is the output. Notice that\nthe directions of the arrows (which illustrate data flow) are primarily rightward and up-\nward.\ntFig. 5.3.1 Computational graph of forward propagation.\n5.3.3Backpropagation\nBackpropagation refers to the method of calculating the gradient of neural network param-\neters. In short, the method traverses the network in reverse order, from the output to the\ninput layer, according to the chain rule from calculus. The algorithm stores any interme-\ndiate variables (partial derivatives) required while calculating the gradient with respect to\nsome parameters. Assume that we have functions Y=\ud835\udc53\u00b9X\u00baandZ=\ud835\udc54\u00b9Y\u00ba, in which the\ninputandtheoutput X,Y,Zaretensorsofarbitraryshapes. Byusingthechainrule,wecan\ncompute the derivative of Zwith respect to Xvia\n\ud835\udf15Z\n\ud835\udf15X=prod\u0012\ud835\udf15Z\n\ud835\udf15Y,\ud835\udf15Y\n\ud835\udf15X\u0013\n. (5.3.7)\nHere we use the prod operator to multiply its arguments after the necessary operations,\nsuch as transposition and swapping input positions, have been carried out. For vectors,\nthis is straightforward: it is simply matrix\u2013matrix multiplication. For higher dimensional", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31b2022b-c507-4d4a-ba85-88aa5fa57d00": {"__data__": {"id_": "31b2022b-c507-4d4a-ba85-88aa5fa57d00", "embedding": null, "metadata": {"page_label": "182", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a699639e-bca0-4cd5-adb6-417d24d647b0", "node_type": "4", "metadata": {"page_label": "182", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "298ecfe2d450dff08a3548b4090bbf34354d2d528ec2cbd7e4dae436852c580f", "class_name": "RelatedNodeInfo"}}, "text": "182 Multilayer Perceptrons\ntensors, we use the appropriate counterpart. The operator prod hides all the notational\noverhead.\nRecall that the parameters of the simple network with one hidden layer, whose computa-\ntional graph is in Fig. 5.3.1 , areW\u00b91\u00baandW\u00b92\u00ba. The objective of backpropagation is to\ncalculate the gradients \ud835\udf15\ud835\udc3d\u009d\ud835\udf15W\u00b91\u00baand\ud835\udf15\ud835\udc3d\u009d\ud835\udf15W\u00b92\u00ba. To accomplish this, we apply the chain\nrule and calculate, in turn, the gradient of each intermediate variable and parameter. The\norderofcalculationsarereversedrelativetothoseperformedinforwardpropagation, since\nwe need to start with the outcome of the computational graph and work our way towards\ntheparameters. Thefirststepistocalculatethegradientsoftheobjectivefunction \ud835\udc3d=\ud835\udc3f\u00b8\ud835\udc60\nwith respect to the loss term \ud835\udc3fand the regularization term \ud835\udc60:\n\ud835\udf15\ud835\udc3d\n\ud835\udf15\ud835\udc3f=1and\ud835\udf15\ud835\udc3d\n\ud835\udf15\ud835\udc60=1. (5.3.8)\nNext, we compute the gradient of the objective function with respect to variable of the\noutput layer oaccording to the chain rule:\n\ud835\udf15\ud835\udc3d\n\ud835\udf15o=prod\u0012\ud835\udf15\ud835\udc3d\n\ud835\udf15\ud835\udc3f,\ud835\udf15\ud835\udc3f\n\ud835\udf15o\u0013\n=\ud835\udf15\ud835\udc3f\n\ud835\udf15o2R\ud835\udc5e. (5.3.9)\nNext, we calculate the gradients of the regularization term with respect to both parame-\nters:\n\ud835\udf15\ud835\udc60\n\ud835\udf15W\u00b91\u00ba=\ud835\udf06W\u00b91\u00baand\ud835\udf15\ud835\udc60\n\ud835\udf15W\u00b92\u00ba=\ud835\udf06W\u00b92\u00ba. (5.3.10)\nNow we are able to calculate the gradient \ud835\udf15\ud835\udc3d\u009d\ud835\udf15W\u00b92\u00ba2R\ud835\udc5e\u0002\u210eof the model parameters\nclosest to the output layer. Using the chain rule yields:\n\ud835\udf15\ud835\udc3d\n\ud835\udf15W\u00b92\u00ba=prod\u0012\ud835\udf15\ud835\udc3d\n\ud835\udf15o,\ud835\udf15o\n\ud835\udf15W\u00b92\u00ba\u0013\n\u00b8prod\u0012\ud835\udf15\ud835\udc3d\n\ud835\udf15\ud835\udc60,\ud835\udf15\ud835\udc60\n\ud835\udf15W\u00b92\u00ba\u0013\n=\ud835\udf15\ud835\udc3d\n\ud835\udf15oh>\u00b8\ud835\udf06W\u00b92\u00ba. (5.3.11)\nTo obtain the gradient with respect to W\u00b91\u00bawe need to continue backpropagation along\nthe output layer to the hidden layer. The gradient with respect to the hidden layer output\n\ud835\udf15\ud835\udc3d\u009d\ud835\udf15h2R\u210eis given by\n\ud835\udf15\ud835\udc3d\n\ud835\udf15h=prod\u0012\ud835\udf15\ud835\udc3d\n\ud835\udf15o,\ud835\udf15o\n\ud835\udf15h\u0013\n=W\u00b92\u00ba>\ud835\udf15\ud835\udc3d\n\ud835\udf15o. (5.3.12)\nSince the activation function \ud835\udf19applies elementwise, calculating the gradient \ud835\udf15\ud835\udc3d\u009d\ud835\udf15z2R\u210e\noftheintermediatevariable zrequiresthatweusetheelementwisemultiplicationoperator,\nwhich we denote by \f:\n\ud835\udf15\ud835\udc3d\n\ud835\udf15z=prod\u0012\ud835\udf15\ud835\udc3d\n\ud835\udf15h,\ud835\udf15h\n\ud835\udf15z\u0013\n=\ud835\udf15\ud835\udc3d\n\ud835\udf15h\f\ud835\udf190\u00b9z\u00ba. (5.3.13)\nFinally, we can obtain the gradient \ud835\udf15\ud835\udc3d\u009d\ud835\udf15W\u00b91\u00ba2R\u210e\u0002\ud835\udc51of the model parameters closest to\nthe input layer. According to the chain rule, we get\n\ud835\udf15\ud835\udc3d\n\ud835\udf15W\u00b91\u00ba=prod\u0012\ud835\udf15\ud835\udc3d\n\ud835\udf15z,\ud835\udf15z\n\ud835\udf15W\u00b91\u00ba\u0013\n\u00b8prod\u0012\ud835\udf15\ud835\udc3d\n\ud835\udf15\ud835\udc60,\ud835\udf15\ud835\udc60\n\ud835\udf15W\u00b91\u00ba\u0013\n=\ud835\udf15\ud835\udc3d\n\ud835\udf15zx>\u00b8\ud835\udf06W\u00b91\u00ba. (5.3.14)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17f799fc-0080-4c79-9532-021d4ed76562": {"__data__": {"id_": "17f799fc-0080-4c79-9532-021d4ed76562", "embedding": null, "metadata": {"page_label": "183", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6de7b98d-ff13-4fdd-ab73-f114d214d532", "node_type": "4", "metadata": {"page_label": "183", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1fed88e353d5e154210392cbde5eeee7408131a037a51feb77bc948b3ca52560", "class_name": "RelatedNodeInfo"}}, "text": "183 Forward Propagation, Backward Propagation, and Computational Graphs\n5.3.4TrainingNeuralNetworks\nWhen training neural networks, forward and backward propagation depend on each other.\nIn particular, for forward propagation, we traverse the computational graph in the direc-\ntion of dependencies and compute all the variables on its path. These are then used for\nbackpropagation where the compute order on the graph is reversed.\nTaketheaforementionedsimplenetworkasanillustrativeexample. Ontheonehand,com-\nputing the regularization term (5.3.5 )during forward propagation depends on the current\nvaluesofmodelparameters W\u00b91\u00baandW\u00b92\u00ba. Theyaregivenbytheoptimizationalgorithm\naccording to backpropagation in the most recent iteration. On the other hand, the gradient\ncalculationfortheparameter (5.3.11 )duringbackpropagationdependsonthecurrentvalue\nof the hidden layer output h, which is given by forward propagation.\nTherefore when training neural networks, once model parameters are initialized, we alter-\nnate forward propagation with backpropagation, updating model parameters using gradi-\nents given by backpropagation. Note that backpropagation reuses the stored intermediate\nvalues from forward propagation to avoid duplicate calculations. One of the consequences\nis that we need to retain the intermediate values until backpropagation is complete. This is\nalso one of the reasons whytraining requires significantly more memory than plain predic-\ntion. Besides, thesizeofsuchintermediatevaluesisroughlyproportionaltothenumberof\nnetwork layers and the batch size. Thus, training deeper networks using larger batch sizes\nmore easily leads to out-of-memory errors.\n5.3.5Summary\nForward propagation sequentially calculates and stores intermediate variables within the\ncomputational graph defined by the neural network. It proceeds from the input to the out-\nput layer. Backpropagation sequentially calculates and stores the gradients of intermediate\nvariables and parameters within the neural network in the reversed order. When training\ndeep learning models, forward propagation and backpropagation are interdependent, and\ntraining requires significantly more memory than prediction.\n5.3.6Exercises\n1.Assume that the inputs Xto some scalar function \ud835\udc53are\ud835\udc5b\u0002\ud835\udc5amatrices. What is the\ndimensionality of the gradient of \ud835\udc53with respect to X?\n2.Add a bias to the hidden layer of the model described in this section (you do not need\nto include bias in the regularization term).\n1.Draw the corresponding computational graph.\n2.Derive the forward and backward propagation equations.\n3.Computethememoryfootprintfortrainingandpredictioninthemodeldescribedinthis\nsection.\n4.Assume that you want to compute second derivatives. What happens to the computa-\ntional graph? How long do you expect the calculation to take?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4192164-4d6b-410d-b76a-78558d81b6b7": {"__data__": {"id_": "b4192164-4d6b-410d-b76a-78558d81b6b7", "embedding": null, "metadata": {"page_label": "184", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0942196-c0f1-4ea6-9fcc-8b32033e34f0", "node_type": "4", "metadata": {"page_label": "184", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0c01de6c358476f4d28231290105e7b868c95f208bd9d1b471e2aff15222f1cd", "class_name": "RelatedNodeInfo"}}, "text": "184 Multilayer Perceptrons\n1045.Assume that the computational graph is too large for your GPU.\n1.Can you partition it over more than one GPU?\n2.What are the advantages and disadvantages over training on a smaller minibatch?\nDiscussions104.\n5.4NumericalStability and Initialization\nThus far, every model that we have implemented required that we initialize its parameters\naccording to some pre-specified distribution. Until now, we took the initialization scheme\nfor granted, glossing over the details of how these choices are made. You might have even\ngotten the impression that these choices are not especially important. On the contrary, the\nchoice of initialization scheme plays a significant role in neural network learning, and it\ncan be crucial for maintaining numerical stability. Moreover, these choices can be tied up\nin interesting ways with the choice of the nonlinear activation function. Which function\nwe choose and how we initialize parameters can determine how quickly our optimization\nalgorithm converges. Poor choices here can cause us to encounter exploding or vanishing\ngradients while training. In this section, we delve into these topics in greater detail and\ndiscuss some useful heuristics that you will find useful throughout your career in deep\nlearning.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\n5.4.1Vanishingand Exploding Gradients\nConsider a deep network with \ud835\udc3flayers, input xand output o. With each layer \ud835\udc59defined by\na transformation \ud835\udc53\ud835\udc59parametrized by weights W\u00b9\ud835\udc59\u00ba, whose hidden layer output is h\u00b9\ud835\udc59\u00ba(let\nh\u00b90\u00ba=x), our network can be expressed as:\nh\u00b9\ud835\udc59\u00ba=\ud835\udc53\ud835\udc59\u00b9h\u00b9\ud835\udc59\u00001\u00ba\u00baand thus o=\ud835\udc53\ud835\udc3f\u000e\u0001\u0001\u0001\u000e\ud835\udc531\u00b9x\u00ba. (5.4.1)\nIf all the hidden layer output and the input are vectors, we can write the gradient of owith\nrespect to any set of parameters W\u00b9\ud835\udc59\u00baas follows:\n\ud835\udf15W\u00b9\ud835\udc59\u00bao=\ud835\udf15h\u00b9\ud835\udc3f\u00001\u00bah\u00b9\ud835\udc3f\u00ba\n|        {z        }\nM\u00b9\ud835\udc3f\u00badef=\u0001\u0001\u0001\ud835\udf15h\u00b9\ud835\udc59\u00bah\u00b9\ud835\udc59\u00b81\u00ba\n|      {z      }\nM\u00b9\ud835\udc59\u00b81\u00badef=\ud835\udf15W\u00b9\ud835\udc59\u00bah\u00b9\ud835\udc59\u00ba\n|     {z     }\nv\u00b9\ud835\udc59\u00badef=.\n(5.4.2)\nIn other words, this gradient is the product of \ud835\udc3f\u0000\ud835\udc59matrices M\u00b9\ud835\udc3f\u00ba\u0001\u0001\u0001M\u00b9\ud835\udc59\u00b81\u00baand the\ngradientvector v\u00b9\ud835\udc59\u00ba. Thuswearesusceptibletothesameproblemsofnumericalunderflow\nthat often crop up when multiplying together too many probabilities. When dealing with\nprobabilities, a common trick is to switch into log-space, i.e., shifting pressure from the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2255, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02af3693-dec2-4ef2-a225-139e5688eb6d": {"__data__": {"id_": "02af3693-dec2-4ef2-a225-139e5688eb6d", "embedding": null, "metadata": {"page_label": "185", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfeac3a4-8310-4bff-863f-1eab5428c119", "node_type": "4", "metadata": {"page_label": "185", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6f7cb40f60e3d954a2e3ad9ee63a65db58d2a460a56eb0c780f22964d5f0974e", "class_name": "RelatedNodeInfo"}}, "text": "185 Numerical Stability and Initialization\nmantissatotheexponentofthenumericalrepresentation. Unfortunately,ourproblemabove\nis more serious: initially the matrices M\u00b9\ud835\udc59\u00bamay have a wide variety of eigenvalues. They\nmight be small or large, and their product might be very large orvery small .\nThe risks posed by unstable gradients go beyond numerical representation. Gradients of\nunpredictablemagnitudealsothreatenthestabilityofouroptimizationalgorithms. Wemay\nbe facing parameter updates that are either (i) excessively large, destroying our model (the\nexploding gradient problem); or (ii) excessively small (the vanishing gradient problem),\nrendering learning impossible as parameters hardly move on each update.\nVanishingGradients\nOne frequent culprit causing the vanishing gradient problem is the choice of the activation\nfunction\ud835\udf0ethat is appended following each layer\u2019s linear operations. Historically, the sig-\nmoidfunction 1\u009d\u00b91\u00b8exp\u00b9\u0000\ud835\udc65\u00ba\u00ba(introducedin Section5.1 )waspopularbecauseitresembles\na thresholding function. Since early artificial neural networks were inspired by biological\nneural networks, the idea of neurons that fire either fullyornot at all (like biological neu-\nrons) seemed appealing. Let\u2019s take a closer look at the sigmoid to see why it can cause\nvanishing gradients.\nx=torch .arange( -8.0,8.0,0.1, requires_grad =True )\ny=torch .sigmoid(x)\ny.backward(torch .ones_like(x))\nd2l.plot(x .detach() .numpy(), [y .detach() .numpy(), x .grad .numpy()],\nlegend =['sigmoid ','gradient '], figsize =(4.5,2.5))\nAs you can see, the sigmoid\u2019s gradient vanishes both when its inputs are large and when\ntheyaresmall. Moreover, whenbackpropagatingthroughmanylayers, unlessweareinthe\nGoldilocks zone, where the inputs to many of the sigmoids are close to zero, the gradients\nof the overall product may vanish. When our network boasts many layers, unless we are\ncareful,thegradientwilllikelybecutoffatsomelayer. Indeed,thisproblemusedtoplague\ndeep network training. Consequently, ReLUs, which are more stable (but less neurally\nplausible), have emerged as the default choice for practitioners.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4b472a7-99a1-493c-83d1-d9afbec08591": {"__data__": {"id_": "b4b472a7-99a1-493c-83d1-d9afbec08591", "embedding": null, "metadata": {"page_label": "186", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4679f4f6-fa1c-4b8b-a31c-3aef76b616fb", "node_type": "4", "metadata": {"page_label": "186", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d5a3ff44a830bea999cc6d39b2562c2ce1770c3b0af858770e4df767f4c8cac1", "class_name": "RelatedNodeInfo"}}, "text": "186 Multilayer Perceptrons\nExplodingGradients\nThe opposite problem, when gradients explode, can be similarly vexing. To illustrate this\na bit better, we draw 100 Gaussian random matrices and multiply them with some initial\nmatrix. Forthescalethatwepicked(thechoiceofthevariance \ud835\udf0e2=1),thematrixproduct\nexplodes. When this happens because of the initialization of a deep network, we have no\nchance of getting a gradient descent optimizer to converge.\nM=torch .normal( 0,1, size =(4,4))\nprint ('a single matrix \\n',M)\nfor iinrange (100):\nM=M@torch .normal( 0,1, size =(4,4))\nprint ('after multiplying 100 matrices \\n', M)\na single matrix\ntensor([[ -0.8755 ,-1.2171 ,1.3316 ,0.1357 ],\n[0.4399 ,1.4073 ,-1.9131 ,-0.4608 ],\n[-2.1420 ,0.3643 ,-0.5267 ,1.0277 ],\n[-0.1734 ,-0.7549 ,2.3024 ,1.3085 ]])\nafter multiplying 100 matrices\ntensor([[ -2.9185e+23 ,1.3915e+25 ,-1.1865e+25 ,1.4354e+24 ],\n[4.9142e+23 ,-2.3430e+25 ,1.9979e+25 ,-2.4169e+24 ],\n[2.6578e+23 ,-1.2672e+25 ,1.0805e+25 ,-1.3072e+24 ],\n[-5.2223e+23 ,2.4899e+25 ,-2.1231e+25 ,2.5684e+24 ]])\nBreakingthe Symmetry\nAnother problem in neural network design is the symmetry inherent in their parametriza-\ntion. Assume that we have a simple MLP with one hidden layer and two units. In this case,\nwe could permute the weights W\u00b91\u00baof the first layer and likewise permute the weights of\nthe output layer to obtain the same function. There is nothing special differentiating the\nfirst and second hidden units. In other words, we have permutation symmetry among the\nhidden units of each layer.\nThis is more than just a theoretical nuisance. Consider the aforementioned one-hidden-\nlayer MLP with two hidden units. For illustration, suppose that the output layer transforms\nthetwohiddenunitsintoonlyoneoutputunit. Imaginewhatwouldhappenifweinitialized\nalltheparameters ofthe hiddenlayeras W\u00b91\u00ba=\ud835\udc50forsomeconstant \ud835\udc50. In thiscase, during\nforwardpropagationeitherhiddenunittakesthesameinputsandparametersproducingthe\nsameactivationwhichisfedtotheoutputunit. Duringbackpropagation,differentiatingthe\noutput unit with respect to parameters W\u00b91\u00bagives a gradient all of whose elements take\nthesamevalue. Thus,aftergradient-basediteration(e.g.,minibatchstochasticgradientde-\nscent),alltheelementsof W\u00b91\u00bastilltakethesamevalue. Suchiterationswouldnever break\nthe symmetry on their own and we might never be able to realize the network\u2019s expressive\npower. The hidden layer would behave as if it had only a single unit. Note that while mini-\nbatch stochastic gradient descent would not break this symmetry, dropout regularization\n(to be introduced later) would!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5982e40a-f318-4eca-9e3a-e56bf8621ae9": {"__data__": {"id_": "5982e40a-f318-4eca-9e3a-e56bf8621ae9", "embedding": null, "metadata": {"page_label": "187", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3096a67-e85c-4426-9183-9bce6c436400", "node_type": "4", "metadata": {"page_label": "187", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c54ebbc3d847e03cd2df80eb8c1a5f1a0e5e56b59bde6343ac0d4e1a0354597d", "class_name": "RelatedNodeInfo"}}, "text": "187 Numerical Stability and Initialization\n5.4.2ParameterInitialization\nOne way of addressing\u2014or at least mitigating\u2014the issues raised above is through care-\nful initialization. As we will see later, additional care during optimization and suitable\nregularization can further enhance stability.\nDefaultInitialization\nIn the previous sections, e.g., in Section 3.5 , we used a normal distribution to initialize the\nvalues of our weights. If we do not specify the initialization method, the framework will\nuseadefaultrandominitializationmethod,whichoftenworkswellinpracticeformoderate\nproblem sizes.\nXavierInitialization\nLet\u2019s look at the scale distribution of an output \ud835\udc5c\ud835\udc56for some fully connected layer without\nnonlinearities . With\ud835\udc5bininputs\ud835\udc65\ud835\udc57and their associated weights \ud835\udc64\ud835\udc56\ud835\udc57for this layer, an output\nis given by\n\ud835\udc5c\ud835\udc56=\ud835\udc5bin\u00d5\n\ud835\udc57=1\ud835\udc64\ud835\udc56\ud835\udc57\ud835\udc65\ud835\udc57. (5.4.3)\nTheweights \ud835\udc64\ud835\udc56\ud835\udc57arealldrawnindependentlyfromthesamedistribution. Furthermore,let\u2019s\nassume that this distribution has zero mean and variance \ud835\udf0e2. Note that this does not mean\nthat the distribution has to be Gaussian, just that the mean and variance need to exist. For\nnow,let\u2019sassumethattheinputstothelayer \ud835\udc65\ud835\udc57alsohavezeromeanandvariance \ud835\udefe2andthat\nthey are independent of \ud835\udc64\ud835\udc56\ud835\udc57and independent of each other. In this case, we can compute\nthe mean of\ud835\udc5c\ud835\udc56:\n\ud835\udc38\u00bb\ud835\udc5c\ud835\udc56\u00bc=\ud835\udc5bin\u00d5\n\ud835\udc57=1\ud835\udc38\u00bb\ud835\udc64\ud835\udc56\ud835\udc57\ud835\udc65\ud835\udc57\u00bc\n=\ud835\udc5bin\u00d5\n\ud835\udc57=1\ud835\udc38\u00bb\ud835\udc64\ud835\udc56\ud835\udc57\u00bc\ud835\udc38\u00bb\ud835\udc65\ud835\udc57\u00bc\n=0,(5.4.4)\nand the variance:\nVar\u00bb\ud835\udc5c\ud835\udc56\u00bc=\ud835\udc38\u00bb\ud835\udc5c2\n\ud835\udc56\u00bc\u0000\u00b9\ud835\udc38\u00bb\ud835\udc5c\ud835\udc56\u00bc\u00ba2\n=\ud835\udc5bin\u00d5\n\ud835\udc57=1\ud835\udc38\u00bb\ud835\udc642\n\ud835\udc56\ud835\udc57\ud835\udc652\n\ud835\udc57\u00bc\u00000\n=\ud835\udc5bin\u00d5\n\ud835\udc57=1\ud835\udc38\u00bb\ud835\udc642\n\ud835\udc56\ud835\udc57\u00bc\ud835\udc38\u00bb\ud835\udc652\n\ud835\udc57\u00bc\n=\ud835\udc5bin\ud835\udf0e2\ud835\udefe2.(5.4.5)\nOne way to keep the variance fixed is to set \ud835\udc5bin\ud835\udf0e2=1. Now consider backpropagation.\nThere we face a similar problem, albeit with gradients being propagated from the layers", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4555d321-a74f-4b1c-9226-3c1f57aaaa83": {"__data__": {"id_": "4555d321-a74f-4b1c-9226-3c1f57aaaa83", "embedding": null, "metadata": {"page_label": "188", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b01295f9-fbd5-4257-81f8-9f8e0e0d9761", "node_type": "4", "metadata": {"page_label": "188", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4c65084f62fe22ee1ccd5a084b4da9e52d599164bc7d4f1841862217a1c101c2", "class_name": "RelatedNodeInfo"}}, "text": "188 Multilayer Perceptrons\ncloser to the output. Using the same reasoning as for forward propagation, we see that the\ngradients\u2019 variance can blow up unless \ud835\udc5bout\ud835\udf0e2=1, where\ud835\udc5boutis the number of outputs\nof this layer. This leaves us in a dilemma: we cannot possibly satisfy both conditions\nsimultaneously. Instead, we simply try to satisfy:\n1\n2\u00b9\ud835\udc5bin\u00b8\ud835\udc5bout\u00ba\ud835\udf0e2=1or equivalently \ud835\udf0e=r\n2\n\ud835\udc5bin\u00b8\ud835\udc5bout. (5.4.6)\nThis is the reasoning underlying the now-standard and practically beneficial Xavierinitial-\nization,namedafterthefirstauthorofitscreators( GlorotandBengio,2010 ). Typically,the\nXavierinitializationsamplesweightsfromaGaussiandistributionwithzeromeanandvari-\nance\ud835\udf0e2=2\n\ud835\udc5bin\u00b8\ud835\udc5bout. We can also adapt this to choose the variance when sampling weights\nfrom a uniform distribution. Note that the uniform distribution \ud835\udc48\u00b9\u0000\ud835\udc4e,\ud835\udc4e\u00bahas variance\ud835\udc4e2\n3.\nPlugging\ud835\udc4e2\n3into our condition on \ud835\udf0e2prompts us to initialize according to\n\ud835\udc48\u00a9\u00ad\n\u00ab\u0000s\n6\n\ud835\udc5bin\u00b8\ud835\udc5bout,s\n6\n\ud835\udc5bin\u00b8\ud835\udc5bout\u00aa\u00ae\n\u00ac. (5.4.7)\nThough the assumption for nonexistence of nonlinearities in the above mathematical rea-\nsoning can be easily violated in neural networks, the Xavier initialization method turns out\nto work well in practice.\nBeyond\nThe reasoning above barely scratches the surface of modern approaches to parameter ini-\ntialization. A deep learning framework often implements over a dozen different heuristics.\nMoreover, parameter initialization continues to be a hot area of fundamental research in\ndeep learning. Among these are heuristics specialized for tied (shared) parameters, super-\nresolution, sequence models, and other situations. For instance, Xiao etal.(2018) demon-\nstrated the possibility of training 10,000-layer neural networks without architectural tricks\nby using a carefully-designed initialization method.\nIf the topic interests you we suggest a deep dive into this module\u2019s offerings, reading the\npapersthatproposedandanalyzedeachheuristic,andthenexploringthelatestpublications\non the topic. Perhaps you will stumble across or even invent a clever idea and contribute\nan implementation to deep learning frameworks.\n5.4.3Summary\nVanishing and exploding gradients are common issues in deep networks. Great care in\nparameter initialization is required to ensure that gradients and parameters remain well\ncontrolled. Initializationheuristicsareneededtoensurethattheinitialgradientsareneither\ntoo large nor too small. Random initialization is key to ensuring that symmetry is broken\nbefore optimization. Xavier initialization suggests that, for each layer, variance of any\noutputisnotaffectedbythenumberofinputs,andvarianceofanygradientisnotaffectedby\nthenumberofoutputs. ReLUactivationfunctionsmitigatethevanishinggradientproblem.\nThis can accelerate convergence.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7820a1b0-3956-459c-b1e9-6ada6498d8f9": {"__data__": {"id_": "7820a1b0-3956-459c-b1e9-6ada6498d8f9", "embedding": null, "metadata": {"page_label": "189", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a7cf54a-5c1b-47b2-a818-a8e69ad6c359", "node_type": "4", "metadata": {"page_label": "189", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "54fb3bcf9907fd04ce029e8b3bd07c97e35a7676cba8d24d0f64431d67dc6afa", "class_name": "RelatedNodeInfo"}}, "text": "189 Generalization in Deep Learning\n1055.4.4Exercises\n1.Can you design other cases where a neural network might exhibit symmetry that needs\nbreaking, besides the permutation symmetry in an MLP\u2019s layers?\n2.Can we initialize all weight parameters in linear regression or in softmax regression to\nthe same value?\n3.Look up analytic bounds on the eigenvalues of the product of two matrices. What does\nthis tell you about ensuring that gradients are well conditioned?\n4.If we know that some terms diverge, can we fix this after the fact? Look at the paper on\nlayerwise adaptive rate scaling for inspiration ( Youetal., 2017).\nDiscussions105.\n5.5Generalizationin Deep Learning\nInChapter 3 andChapter 4 , we tackled regression and classification problems by fitting\nlinear models to training data. In both cases, we provided practical algorithms for finding\nthe parameters that maximized the likelihood of the observed training labels. And then,\ntowards the end of each chapter, we recalled that fitting the training data was only an in-\ntermediate goal. Our real quest all along was to discover general patterns on the basis\nof which we can make accurate predictions even on new examples drawn from the same\nunderlying population. Machine learning researchers are consumers of optimization algo-\nrithms. Sometimes, we must even develop new optimization algorithms. But at the end\nof the day, optimization is merely a means to an end. At its core, machine learning is a\nstatistical discipline and we wish to optimize training loss only insofar as some statistical\nprinciple (known or unknown) leads the resulting models to generalize beyond the training\nset.\nOn the bright side, it turns out that deep neural networks trained by stochastic gradient de-\nscent generalize remarkably well across myriad prediction problems, spanning computer\nvision; natural language processing; time series data; recommender systems; electronic\nhealth records; protein folding; value function approximation in video games and board\ngames; and numerous other domains. On the downside, if you were looking for a straight-\nforward account of either the optimization story (why we can fit them to training data) or\nthegeneralizationstory(whytheresultingmodelsgeneralizetounseenexamples),thenyou\nmight want to pour yourself a drink. While our procedures for optimizing linear models\nand the statistical properties of the solutions are both described well by a comprehensive\nbody of theory, our understanding of deep learning still resembles the wild west on both\nfronts.\nBoth the theory and practice of deep learning are rapidly evolving, with theorists adopting\nnew strategies to explain what\u2019s going on, even as practitioners continue to innovate at", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2720, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42398a87-c411-4af3-be09-1d50e5a4822f": {"__data__": {"id_": "42398a87-c411-4af3-be09-1d50e5a4822f", "embedding": null, "metadata": {"page_label": "190", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f33f6653-5afd-4ff4-aef0-85f290e93a03", "node_type": "4", "metadata": {"page_label": "190", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "826de5980d031a59fc2e5ed707719d0bceed3604c69714eb589d8e83d6f31263", "class_name": "RelatedNodeInfo"}}, "text": "190 Multilayer Perceptrons\na blistering pace, building arsenals of heuristics for training deep networks and a body of\nintuitionsandfolkknowledgethatprovideguidancefordecidingwhichtechniquestoapply\nin which situations.\nThe summary of the present moment is that the theory of deep learning has produced\npromising lines of attack and scattered fascinating results, but still appears far from a com-\nprehensive account of both (i) why we are able to optimize neural networks and (ii) how\nmodelslearnedbygradientdescentmanagetogeneralizesowell,evenonhigh-dimensional\ntasks. However,inpractice,(i)isseldomaproblem(wecanalwaysfindparametersthatwill\nfit all of our training data) and thus understanding generalization is far the bigger problem.\nOn the other hand, even absent the comfort of a coherent scientific theory, practitioners\nhave developed a large collection of techniques that may help you to produce models that\ngeneralize well in practice. While no pithy summary can possibly do justice to the vast\ntopic of generalization in deep learning, and while the overall state of research is far from\nresolved, we hope, in this section, to present a broad overview of the state of research and\npractice.\n5.5.1RevisitingOverfittingand Regularization\nAccording to the \u201cno free lunch\u201d theorem of Wolpert and Macready ( 1995), any learn-\ning algorithm generalizes better on data with certain distributions, and worse with other\ndistributions. Thus, given a finite training set, a model relies on certain assumptions: to\nachieve human-level performance it may be useful to identify inductive biases that reflect\nhow humans think about the world. Such inductive biases show preferences for solutions\nwith certain properties. For example, a deep MLP has an inductive bias towards building\nup a complicated function by the composition of simpler functions.\nWith machine learning models encoding inductive biases, our approach to training them\ntypicallyconsistsoftwophases: (i)fitthetrainingdata; and(ii)estimatethe generalization\nerror(thetrueerrorontheunderlyingpopulation)byevaluatingthemodelonholdoutdata.\nThe difference between our fit on the training data and our fit on the test data is called the\ngeneralizationgap andwhenthisislarge,wesaythatourmodels overfittothetrainingdata.\nIn extreme cases of overfitting, we might exactly fit the training data, even when the test\nerrorremainssignificant. Andintheclassicalview,theinterpretationisthatourmodelsare\ntoocomplex, requiringthatweeithershrinkthenumberoffeatures, thenumberofnonzero\nparameters learned, or the size of the parameters as quantified. Recall the plot of model\ncomplexity compared with loss ( Fig. 3.6.1 ) fromSection 3.6 .\nHowever deep learning complicates this picture in counterintuitive ways. First, for classifi-\ncation problems, our models are typically expressive enough to perfectly fit every training\nexample, even in datasets consisting of millions ( Zhanget al., 2021). In the classical pic-\nture, we might think that this setting lies on the far right extreme of the model complexity\naxis, and that any improvements in generalization error must come by way of regulariza-\ntion,eitherbyreducingthecomplexityofthemodelclass,orbyapplyingapenalty,severely\nconstraining the set of values that our parameters might take. But that is where things start\nto get weird.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7133b477-be42-46bc-a1cc-1c65119ff934": {"__data__": {"id_": "7133b477-be42-46bc-a1cc-1c65119ff934", "embedding": null, "metadata": {"page_label": "191", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d374bf29-cec2-436d-9cff-ea1d908c4bee", "node_type": "4", "metadata": {"page_label": "191", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "620b7530e71f3ee7713442c45ca9612695fccf2bc65c76203e50d16e4906ffdb", "class_name": "RelatedNodeInfo"}}, "text": "191 Generalization in Deep Learning\nStrangely, for many deep learning tasks (e.g., image recognition and text classification) we\nare typically choosing among model architectures, all of which can achieve arbitrarily low\ntraining loss (and zero training error). Because all models under consideration achieve\nzero training error, theonlyavenueforfurthergainsistoreduceoverfitting . Even stranger,\nit is often the case that despite fitting the training data perfectly, we can actually reduce\nthe generalization error further by making the model even more expressive , e.g., adding\nlayers, nodes, or training for a larger number of epochs. Stranger yet, the pattern relating\nthegeneralizationgaptothe complexity ofthemodel(ascaptured,forexample,inthedepth\nor width of the networks) can be non-monotonic, with greater complexity hurting at first\nbut subsequently helping in a so-called \u201cdouble-descent\u201d pattern ( Nakkiran et al., 2021).\nThus the deep learning practitioner possesses a bag of tricks, some of which seemingly\nrestrict the model in some fashion and others that seemingly make it even more expressive,\nand all of which, in some sense, are applied to mitigate overfitting.\nComplicating things even further, while the guarantees provided by classical learning the-\nory can be conservative even for classical models, they appear powerless to explain why\nit is that deep neural networks generalize in the first place. Because deep neural networks\nare capable of fitting arbitrary labels even for large datasets, and despite the use of famil-\niar methods such as \u21132regularization, traditional complexity-based generalization bounds,\ne.g., those based on the VC dimension or Rademacher complexity of a hypothesis class\ncannot explain why neural networks generalize.\n5.5.2InspirationfromNonparametrics\nApproaching deep learning for the first time, it is tempting to think of them as parametric\nmodels. Afterall,themodels dohavemillionsofparameters. Whenweupdatethemodels,\nwe update their parameters. When we save the models, we write their parameters to disk.\nHowever, mathematics and computer science are riddled with counterintuitive changes of\nperspective, and surprising isomorphisms between seemingly different problems. While\nneural networks clearly haveparameters, in some ways it can be more fruitful to think of\nthem as behaving like nonparametric models. So what precisely makes a model nonpara-\nmetric? While the name covers a diverse set of approaches, one common theme is that\nnonparametric methods tend to have a level of complexity that grows as the amount of\navailable data grows.\nPerhapsthesimplestexampleofanonparametricmodelisthe \ud835\udc58-nearestneighboralgorithm\n(we will cover more nonparametric models later, for example in Section 11.2 ). Here, at\ntraining time, the learner simply memorizes the dataset. Then, at prediction time, when\nconfronted with a new point x, the learner looks up the \ud835\udc58nearest neighbors (the \ud835\udc58points\nx0\n\ud835\udc56that minimize some distance \ud835\udc51\u00b9x,x0\n\ud835\udc56\u00ba). When\ud835\udc58=1, this algorithm is called 1-nearest\nneighbors, and the algorithm will always achieve a training error of zero. That however,\ndoes not mean that the algorithm will not generalize. In fact, it turns out that under some\nmild conditions, the 1-nearest neighbor algorithm is consistent (eventually converging to\nthe optimal predictor).\nNote that 1-nearest neighbor requires that we specify some distance function \ud835\udc51, or equiva-\nlently, that we specify some vector-valued basis function \ud835\udf19\u00b9x\u00bafor featurizing our data. For", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b205836b-b2d8-4f6f-91f9-83795f6eed20": {"__data__": {"id_": "b205836b-b2d8-4f6f-91f9-83795f6eed20", "embedding": null, "metadata": {"page_label": "192", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2c2a386-64fc-4042-bd59-0a7e627a16ab", "node_type": "4", "metadata": {"page_label": "192", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "de5ca051438a8c6cd1d1eb2ba0f070bc65f34645b9cff59047a63def4e072b02", "class_name": "RelatedNodeInfo"}}, "text": "192 Multilayer Perceptrons\nany choice of the distance metric, we will achieve zero training error and eventually reach\nan optimal predictor, but different distance metrics \ud835\udc51encode different inductive biases and\nwith a finite amount of available data will yield different predictors. Different choices of\nthe distance metric \ud835\udc51represent different assumptions about the underlying patterns and the\nperformanceofthedifferentpredictorswilldependonhowcompatibletheassumptionsare\nwith the observed data.\nInasense, becauseneuralnetworksareover-parametrized, possessingmanymoreparame-\ntersthanareneededtofitthetrainingdata, theytendto interpolate thetrainingdata(fitting\nit perfectly) and thus behave, in some ways, more like nonparametric models. More re-\ncent theoretical research has established deep connection between large neural networks\nand nonparametric methods, notably kernel methods. In particular, Jacot et al.(2018)\ndemonstrated that in the limit, as multilayer perceptrons with randomly initialized weights\ngrow infinitely wide, they become equivalent to (nonparametric) kernel methods for a spe-\ncific choice of the kernel function (essentially, a distance function), which they call the\nneural tangent kernel. While current neural tangent kernel models may not fully explain\nthe behavior of modern deep networks, their success as an analytical tool underscores the\nusefulnessofnonparametricmodelingforunderstandingthebehaviorofover-parametrized\ndeep networks.\n5.5.3EarlyStopping\nWhile deep neural networks are capable of fitting arbitrary labels, even when labels are\nassigned incorrectly or randomly ( Zhanget al., 2021), this capability only emerges over\nmany iterations of training. A new line of work ( Rolnicket al., 2017) has revealed that\nin the setting of label noise, neural networks tend to fit cleanly labeled data first and only\nsubsequently to interpolate the mislabeled data. Moreover, it has been established that this\nphenomenon translates directly into a guarantee on generalization: whenever a model has\nfitted the cleanly labeled data but not randomly labeled examples included in the training\nset, it has in fact generalized ( Gargetal., 2021).\nTogetherthesefindingshelptomotivate earlystopping ,aclassictechniqueforregularizing\ndeepneuralnetworks. Here,ratherthandirectlyconstrainingthevaluesoftheweights,one\nconstrains the number of epochs of training. The most common way to determine the\nstopping criterion is to monitor validation error throughout training (typically by checking\nonce after each epoch) and to cut off training when the validation error has not decreased\nby more than some small amount \ud835\udf16for some number of epochs. This is sometimes called a\npatiencecriterion . As well as the potential to lead to better generalization in the setting of\nnoisylabels,anotherbenefitofearlystoppingisthetimesaved. Oncethepatiencecriterion\nis met, one can terminate training. For large models that might require days of training\nsimultaneously across eight or more GPUs, well-tuned early stopping can save researchers\ndays of time and can save their employers many thousands of dollars.\nNotably, when there is no label noise and datasets are realizable (the classes are truly sep-\narable, e.g., distinguishing cats from dogs), early stopping tends not to lead to significant\nimprovements in generalization. On the other hand, when there is label noise, or intrinsic", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3393, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70bf1c15-5e06-418c-ba4c-568064428ffa": {"__data__": {"id_": "70bf1c15-5e06-418c-ba4c-568064428ffa", "embedding": null, "metadata": {"page_label": "193", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f4a210c9-ca68-4289-8ba6-61a4979a31a1", "node_type": "4", "metadata": {"page_label": "193", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c01f2f8a4bbcbf8a211c4f66aebf802e96229a66c316689cc02790921a0592a6", "class_name": "RelatedNodeInfo"}}, "text": "193 Generalization in Deep Learning\nvariabilityinthelabel(e.g., predictingmortalityamongpatients), earlystoppingiscrucial.\nTraining models until they interpolate noisy data is typically a bad idea.\n5.5.4Classical RegularizationMethodsforDeep Networks\nInChapter 3 , we described several classical regularization techniques for constraining the\ncomplexity of our models. In particular, Section 3.7 introduced a method called weight\ndecay,whichconsistsofaddingaregularizationtermtothelossfunctioninordertopenalize\nlarge values of the weights. Depending on which weight norm is penalized this technique\nis known either as ridge regularization (for \u21132penalty) or lasso regularization (for an \u21131\npenalty). In the classical analysis of these regularizers, they are considered as sufficiently\nrestrictiveonthevaluesthattheweightscantaketopreventthemodelfromfittingarbitrary\nlabels.\nIn deep learning implementations, weight decay remains a popular tool. However, re-\nsearchershavenotedthattypicalstrengthsof \u21132regularizationareinsufficienttopreventthe\nnetworksfrominterpolatingthedata( Zhangetal.,2021)andthusthebenefitsifinterpreted\nas regularization might only make sense in combination with the early stopping criterion.\nAbsent early stopping, it is possible that just like the number of layers or number of nodes\n(indeeplearning)orthedistancemetric(in1-nearestneighbor),thesemethodsmayleadto\nbetter generalization not because they meaningfully constrain the power of the neural net-\nwork but rather because they somehow encode inductive biases that are better compatible\nwith the patterns found in datasets of interests. Thus, classical regularizers remain popular\nin deep learning implementations, even if the theoretical rationale for their efficacy may be\nradically different.\nNotably, deep learning researchers have also built on techniques first popularized in classi-\ncalregularizationcontexts,suchasaddingnoisetomodelinputs. Inthenextsectionwewill\nintroduce the famous dropout technique (invented by Srivastava et al.(2014)), which has\nbecome a mainstay of deep learning, even as the theoretical basis for its efficacy remains\nsimilarly mysterious.\n5.5.5Summary\nUnlike classical linear models, which tend to have fewer parameters than examples, deep\nnetworks tend to be over-parametrized, and for most tasks are capable of perfectly fitting\nthetrainingset. This interpolationregime challengesmanyhardfast-heldintuitions. Func-\ntionally, neural networks look like parametric models. But thinking of them as nonpara-\nmetric models can sometimes be a more reliable source of intuition. Because it is often\nthe case that all deep networks under consideration are capable of fitting all of the train-\ning labels, nearly all gains must come by mitigating overfitting (closing the generalization\ngap). Paradoxically, the interventions that reduce the generalization gap sometimes appear\nto increase model complexity and at other times appear to decrease complexity. However,\nthese methods seldom decrease complexity sufficiently for classical theory to explain the\ngeneralization of deep networks, and why certain choices lead to improved generalization\nremains for the most part a massive open question despite the concerted efforts of many\nbrilliant researchers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3259, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4b497d6-8070-4b37-bbbe-e642b7f867e0": {"__data__": {"id_": "b4b497d6-8070-4b37-bbbe-e642b7f867e0", "embedding": null, "metadata": {"page_label": "194", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b2696fa-239a-48e2-b1f3-36374ca3ab48", "node_type": "4", "metadata": {"page_label": "194", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5f0ac042063cd908d07ac5084b71050930ced9c7c7e75ce9e8929fe5e07f6d9e", "class_name": "RelatedNodeInfo"}}, "text": "194 Multilayer Perceptrons\n1065.5.6Exercises\n1.Inwhatsensedotraditionalcomplexity-basedmeasuresfailtoaccountforgeneralization\nof deep neural networks?\n2.Why might earlystopping be considered a regularization technique?\n3.How do researchers typically determine the stopping criterion?\n4.What important factor seems to differentiate cases when early stopping leads to big\nimprovements in generalization?\n5.Beyond generalization, describe another benefit of early stopping.\nDiscussions106.\n5.6Dropout\nLet\u2019s think briefly about what we expect from a good predictive model. We want it to pe-\nform well on unseen data. Classical generalization theory suggests that to close the gap\nbetween train and test performance, we should aim for a simple model. Simplicity can\ncome in the form of a small number of dimensions. We explored this when discussing the\nmonomial basis functions of linear models in Section 3.6 . Additionally, as we saw when\ndiscussingweightdecay( \u21132regularization)in Section3.7 ,the(inverse)normoftheparam-\neters also represents a useful measure of simplicity. Another useful notion of simplicity\nis smoothness, i.e., that the function should not be sensitive to small changes to its inputs.\nFor instance, when we classify images, we would expect that adding some random noise to\nthe pixels should be mostly harmless.\nBishop( 1995)formalizedthisideawhenheprovedthattrainingwithinputnoiseisequiva-\nlent to Tikhonov regularization. This work drew a clear mathematical connection between\nthe requirement that a function be smooth (and thus simple), and the requirement that it be\nresilient to perturbations in the input.\nThen, Srivastava etal.(2014)developedacleverideaforhowtoapplyBishop\u2019sideatothe\ninternal layers of a network, too. Their idea, called dropout, involves injecting noise while\ncomputing each internal layer during forward propagation, and it has become a standard\ntechnique for training neural networks. The method is called dropout because we literally\ndrop out some neurons during training. Throughout training, on each iteration, standard\ndropout consists of zeroing out some fraction of the nodes in each layer before calculating\nthe subsequent layer.\nTo be clear, we are imposing our own narrative with the link to Bishop. The original pa-\nper on dropout offers intuition through a surprising analogy to sexual reproduction. The\nauthors argue that neural network overfitting is characterized by a state in which each layer", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2453, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab53b641-576b-4bd8-b5ad-b1aab0a4e94a": {"__data__": {"id_": "ab53b641-576b-4bd8-b5ad-b1aab0a4e94a", "embedding": null, "metadata": {"page_label": "195", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73bc6c90-2e8b-4c4b-9735-fadfe29b721e", "node_type": "4", "metadata": {"page_label": "195", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2072fac4a31fc7665ff31d10ff630cc51703d35a71e85f76987eff946a47e63f", "class_name": "RelatedNodeInfo"}}, "text": "195 Dropout\nrelies on a specific pattern of activations in the previous layer, calling this condition co-\nadaptation . Dropout, they claim, breaks up co-adaptation just as sexual reproduction is\nargued to break up co-adapted genes. While such an justification of this theory is cer-\ntainlyupfordebate, thedropouttechniqueitselfhasprovedenduring, andvariousformsof\ndropout are implemented in most deep learning libraries.\nThekeychallengeishowto injectthisnoise. Oneidea istoinjectitin an unbiased manner\nsothattheexpectedvalueofeachlayer\u2014whilefixingtheothers\u2014equalsthevalueitwould\nhavetakenabsentnoise. InBishop\u2019swork,headdedGaussiannoisetotheinputstoalinear\nmodel. At each training iteration, he added noise sampled from a distribution with mean\nzero\ud835\udf16\u0018N\u00b9 0,\ud835\udf0e2\u00bato the input x, yielding a perturbed point x0=x\u00b8\ud835\udf16. In expectation,\n\ud835\udc38\u00bbx0\u00bc=x.\nIn standard dropout regularization, one zeros out some fraction of the nodes in each layer\nandthendebiases eachlayerbynormalizingbythefractionofnodesthatwereretained(not\ndropped out). In other words, with dropoutprobability \ud835\udc5d, each intermediate activation \u210eis\nreplaced by a random variable \u210e0as follows:\n\u210e0=(\n0with probability \ud835\udc5d\n\u210e\n1\u0000\ud835\udc5dotherwise(5.6.1)\nBy design, the expectation remains unchanged, i.e., \ud835\udc38\u00bb\u210e0\u00bc=\u210e.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n5.6.1Dropoutin Practice\nRecall the MLP with a hidden layer and five hidden units from Fig. 5.1.1 . When we apply\ndropout to a hidden layer, zeroing out each hidden unit with probability \ud835\udc5d, the result can\nbe viewed as a network containing only a subset of the original neurons. In Fig. 5.6.1 ,\u210e2\nand\u210e5are removed. Consequently, the calculation of the outputs no longer depends on \u210e2\nor\u210e5andtheirrespectivegradientalsovanisheswhenperformingbackpropagation. Inthis\nway, the calculation of the output layer cannot be overly dependent on any one element of\n\u210e1,...,\u210e 5.\ntFig. 5.6.1 MLP before and after dropout.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ee8f710-08e9-44f2-8e0d-78a4df39ab1e": {"__data__": {"id_": "5ee8f710-08e9-44f2-8e0d-78a4df39ab1e", "embedding": null, "metadata": {"page_label": "196", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33646249-555b-4a63-88db-42b1698676d3", "node_type": "4", "metadata": {"page_label": "196", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "063426fef8ad3f460790d067e0201d418052c4a24dfacdc96cf1008e42789400", "class_name": "RelatedNodeInfo"}}, "text": "196 Multilayer Perceptrons\nTypically, we disable dropout at test time. Given a trained model and a new example,\nwe do not drop out any nodes and thus do not need to normalize. However, there are\nsomeexceptions: someresearchersusedropoutattesttimeasaheuristicforestimatingthe\nuncertainty of neural network predictions: if the predictions agree across many different\ndropout outputs, then we might say that the network is more confident.\n5.6.2Implementation fromScratch\nToimplementthedropoutfunctionforasinglelayer,wemustdrawasmanysamplesfroma\nBernoulli(binary)randomvariableasourlayerhasdimensions,wheretherandomvariable\ntakes value 1(keep) with probability 1\u0000\ud835\udc5dand0(drop) with probability \ud835\udc5d. One easy way\nto implement this is to first draw samples from the uniform distribution \ud835\udc48\u00bb0,1\u00bc. Then we\ncan keep those nodes for which the corresponding sample is greater than \ud835\udc5d, dropping the\nrest.\nInthefollowingcode,weimplementa dropout_layer functionthatdropsouttheelements\ninthetensorinput Xwithprobability dropout , rescalingtheremainderasdescribedabove:\ndividing the survivors by 1.0-dropout .\ndef dropout_layer (X, dropout):\nassert 0<=dropout <=1\nifdropout ==1:return torch .zeros_like(X)\nmask =(torch .rand(X .shape) >dropout) .float()\nreturn mask *X/(1.0 -dropout)\nWe can test out the dropout_layer function on a few examples. In the following lines of\ncode, we pass our input Xthrough the dropout operation, with probabilities 0, 0.5, and 1,\nrespectively.\nX=torch .arange( 16, dtype =torch .float32) .reshape(( 2,8))\nprint ('dropout_p = 0: ', dropout_layer(X, 0))\nprint ('dropout_p = 0.5: ', dropout_layer(X, 0.5))\nprint ('dropout_p = 1: ', dropout_layer(X, 1))\ndropout_p =0: tensor([[ 0.,1.,2.,3.,4.,5.,6.,7.],\n[8.,9.,10.,11.,12.,13.,14.,15.]])\ndropout_p =0.5: tensor([[ 0.,2.,0.,6.,8.,0.,0.,0.],\n[16.,18.,20.,22.,24.,26.,28.,30.]])\ndropout_p =1: tensor([[ 0.,0.,0.,0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.,0.,0.,0.]])\nDefiningthe Model\nThemodelbelowappliesdropouttotheoutputofeachhiddenlayer(followingtheactivation\nfunction). We can set dropout probabilities for each layer separately. A common choice is\nto set a lower dropout probability closer to the input layer. We ensure that dropout is only\nactive during training.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2213, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41b2d0ba-bb59-43f9-be98-fbb171e67658": {"__data__": {"id_": "41b2d0ba-bb59-43f9-be98-fbb171e67658", "embedding": null, "metadata": {"page_label": "197", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a519a03-f2c8-4903-9701-574a501fc006", "node_type": "4", "metadata": {"page_label": "197", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6e383e5ef3872ed07e0931884ce68738dc08d54ab2db93a0dfe53eff70120eb2", "class_name": "RelatedNodeInfo"}}, "text": "197 Dropout\nclass DropoutMLPScratch (d2l .Classifier):\ndef __init__ (self , num_outputs, num_hiddens_1, num_hiddens_2,\ndropout_1, dropout_2, lr):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .lin1 =nn.LazyLinear(num_hiddens_1)\nself .lin2 =nn.LazyLinear(num_hiddens_2)\nself .lin3 =nn.LazyLinear(num_outputs)\nself .relu =nn.ReLU()\ndef forward (self , X):\nH1=self .relu( self .lin1(X .reshape((X .shape[ 0],-1))))\nifself .training:\nH1=dropout_layer(H1, self .dropout_1)\nH2=self .relu( self .lin2(H1))\nifself .training:\nH2=dropout_layer(H2, self .dropout_2)\nreturn self .lin3(H2)\nTraining\nThe following is similar to the training of MLPs described previously.\nhparams ={'num_outputs ':10,'num_hiddens_1 ':256,'num_hiddens_2 ':256,\n'dropout_1 ':0.5,'dropout_2 ':0.5,'lr':0.1}\nmodel =DropoutMLPScratch( **hparams)\ndata =d2l.FashionMNIST(batch_size =256)\ntrainer =d2l.Trainer(max_epochs =10)\ntrainer .fit(model, data)\n5.6.3Concise Implementation\nWith high-level APIs, all we need to do is add a Dropout layer after each fully connected\nlayer, passing in the dropout probability as the only argument to its constructor. During\ntraining, the Dropout layer will randomly drop out outputs of the previous layer (or equiv-\nalently, the inputs to the subsequent layer) according to the specified dropout probability.\nWhen not in training mode, the Dropout layer simply passes the data through during test-\ning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1407, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a8611c5-04ed-4728-80cb-59b1adf3a17a": {"__data__": {"id_": "7a8611c5-04ed-4728-80cb-59b1adf3a17a", "embedding": null, "metadata": {"page_label": "198", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c6a1889b-6223-4696-bf0d-fb89b957e063", "node_type": "4", "metadata": {"page_label": "198", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b310af6c430a72c3d245979cbb0d9ff6cfebca5d7aada8c42272916bbca0706f", "class_name": "RelatedNodeInfo"}}, "text": "198 Multilayer Perceptrons\nclass DropoutMLP (d2l .Classifier):\ndef __init__ (self , num_outputs, num_hiddens_1, num_hiddens_2,\ndropout_1, dropout_2, lr):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(\nnn.Flatten(), nn .LazyLinear(num_hiddens_1), nn .ReLU(),\nnn.Dropout(dropout_1), nn .LazyLinear(num_hiddens_2), nn .ReLU(),\nnn.Dropout(dropout_2), nn .LazyLinear(num_outputs))\nNext, we train the model.\nmodel =DropoutMLP( **hparams)\ntrainer .fit(model, data)\n5.6.4Summary\nBeyond controlling the number of dimensions and the size of the weight vector, dropout is\nyet another tool for avoiding overfitting. Often tools are used jointly. Note that dropout is\nused only during training: it replaces an activation \u210ewith a random variable with expected\nvalue\u210e.\n5.6.5Exercises\n1.What happens if you change the dropout probabilities for the first and second layers?\nIn particular, what happens if you switch the ones for both layers? Design an experi-\nment to answer these questions, describe your results quantitatively, and summarize the\nqualitative takeaways.\n2.Increase the number of epochs and compare the results obtained when using dropout\nwith those when not using it.\n3.What is the variance of the activations in each hidden layer when dropout is and is not\napplied? Draw a plot to show how this quantity evolves over time for both models.\n4.Why is dropout not typically used at test time?\n5.Usingthemodelinthissectionasanexample, comparetheeffectsofusingdropoutand", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b134b31d-0896-478e-9f03-182501dfd95b": {"__data__": {"id_": "b134b31d-0896-478e-9f03-182501dfd95b", "embedding": null, "metadata": {"page_label": "199", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf477d16-c49a-414c-92b7-85d8f83e9791", "node_type": "4", "metadata": {"page_label": "199", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ca7ed030aca2717d8acf3134ea891b5c01eb339bca46ab3dcb69407b737d6648", "class_name": "RelatedNodeInfo"}}, "text": "199 Predicting House Prices on Kaggle\n107\n108weightdecay. Whathappenswhendropoutandweightdecayareusedatthesametime?\nAre the results additive? Are there diminished returns (or worse)? Do they cancel each\nother out?\n6.What happens if we apply dropout to the individual weights of the weight matrix rather\nthan the activations?\n7.Invent another technique for injecting random noise at each layer that is different from\nthestandarddropouttechnique. Canyoudevelopamethodthatoutperformsdropouton\nthe Fashion-MNIST dataset (for a fixed architecture)?\nDiscussions107.\n5.7PredictingHouse Prices on Kaggle\nNow that we have introduced some basic tools for building and training deep networks and\nregularizing them with techniques including weight decay and dropout, we are ready to\nput all this knowledge into practice by participating in a Kaggle competition. The house\nprice prediction competition is a great place to start. The data is fairly generic and do not\nexhibit exotic structure that might require specialized models (as audio or video might).\nThis dataset, collected by De Cock ( 2011), covers house prices in Ames, Iowa from the\nperiod 2006\u20132010. It is considerably larger than the famous Boston housing dataset108of\nHarrison and Rubinfeld (1978), boasting both more examples and more features.\nIn this section, we will walk you through details of data preprocessing, model design, and\nhyperparameter selection. We hope that through a hands-on approach, you will gain some\nintuitions that will guide you in your career as a data scientist.\n%matplotlib inline\nimport pandas aspd\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n5.7.1DownloadingData\nThroughout the book, we will train and test models on various downloaded datasets. Here,\nwe implement two utility functions for downloading and extracting zip or tar files. Again,\nwe skip implementation details of such utility functions.\ndef download (url, folder, sha1_hash =None ):\n\"\"\"Download a file to folder and return the local filepath.\"\"\"\ndef extract (filename, folder):\n\"\"\"Extract a zip/tar file into folder.\"\"\"", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dfa37cee-8dfb-4bc4-bde6-07d3e2b5bf80": {"__data__": {"id_": "dfa37cee-8dfb-4bc4-bde6-07d3e2b5bf80", "embedding": null, "metadata": {"page_label": "200", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c882f15-c630-4b3f-a944-5637186abcac", "node_type": "4", "metadata": {"page_label": "200", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3346a8d525f739dd5454375f245a17f4d87bfe6e2d15fe1e4f53b8f490ceb6be", "class_name": "RelatedNodeInfo"}}, "text": "200 Multilayer Perceptrons\n1095.7.2Kaggle\nKaggle109is a popular platform that hosts machine learning competitions. Each com-\npetition centers on a dataset and many are sponsored by stakeholders who offer prizes to\nthe winning solutions. The platform helps users to interact via forums and shared code,\nfostering both collaboration and competition. While leaderboard chasing often spirals out\nof control, with researchers focusing myopically on preprocessing steps rather than asking\nfundamental questions, there is also tremendous value in the objectivity of a platform that\nfacilitates direct quantitative comparisons among competing approaches as well as code\nsharing so that everyone can learn what did and did not work. If you want to participate in\na Kaggle competition, you will first need to register for an account (see Fig. 5.7.1 ).\ntFig. 5.7.1 The Kaggle website.\nOn the house price prediction competition page, as illustrated in Fig. 5.7.2 , you can find\nthe dataset (under the \u201cData\u201d tab), submit predictions, and see your ranking, The URL is\nright here:\nhttps://www.kaggle.com/c/house-prices-advanced-regression-techniques\ntFig. 5.7.2 The house price prediction competition page.\n5.7.3Accessingand Readingthe Dataset", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ec048b2-c652-46e0-aa89-5632d8fca54b": {"__data__": {"id_": "6ec048b2-c652-46e0-aa89-5632d8fca54b", "embedding": null, "metadata": {"page_label": "201", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0934978-9309-40ff-96f4-5834ca9c6eef", "node_type": "4", "metadata": {"page_label": "201", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "20ca4cb6914b5f408c7021c2f652d990c2bddb2b0363724c6cdbdeab77dac80a", "class_name": "RelatedNodeInfo"}}, "text": "201 Predicting House Prices on Kaggle\nNote that the competition data is separated into training and test sets. Each record includes\nthe property value of the house and attributes such as street type, year of construction, roof\ntype, basement condition, etc. The features consist of various data types. For example,\nthe year of construction is represented by an integer, the roof type by discrete categorical\nassignments, and other features by floating point numbers. And here is where reality com-\nplicates things: for some examples, some data is altogether missing with the missing value\nmarked simply as \u201cna\u201d. The price of each house is included for the training set only (it is\na competition after all). We will want to partition the training set to create a validation set,\nbut we only get to evaluate our models on the official test set after uploading predictions to\nKaggle. The \u201cData\u201d tab on the competition tab in Fig. 5.7.2 has links for downloading the\ndata.\nTo get started, we will read in and process the data using pandas, which we introduced\ninSection 2.2 . For convenience, we can download and cache the Kaggle housing dataset.\nIf a file corresponding to this dataset already exists in the cache directory and its SHA-1\nmatches sha1_hash , our code will use the cached file to avoid clogging up your Internet\nwith redundant downloads.\nclass KaggleHouse (d2l .DataModule):\ndef __init__ (self , batch_size, train =None , val =None ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nifself .train isNone :\nself .raw_train =pd.read_csv(d2l .download(\nd2l.DATA_URL +'kaggle_house_pred_train.csv ',self .root,\nsha1_hash ='585e9cc93e70b39160e7921475f9bcd7d31219ce '))\nself .raw_val =pd.read_csv(d2l .download(\nd2l.DATA_URL +'kaggle_house_pred_test.csv ',self .root,\nsha1_hash ='fa19780a7b011d9b009e8bff8e99922a8ee2eb90 '))\nThetrainingdatasetincludes1460examples,80features,andonelabel,whilethevalidation\ndata contains 1459 examples and 80 features.\ndata =KaggleHouse(batch_size =64)\nprint (data .raw_train .shape)\nprint (data .raw_val .shape)\nDownloading ../data /kaggle_house_pred_train .csv from http ://d2l-data .s3-\n\u21a9!accelerate .amazonaws .com/kaggle_house_pred_train .csv...\nDownloading ../data /kaggle_house_pred_test .csv from http ://d2l-data .s3-\n\u21a9!accelerate .amazonaws .com/kaggle_house_pred_test .csv...\n(1460 ,81)\n(1459 ,80)\n5.7.4Data Preprocessing\nLet\u2019s take a look at the first four and final two features as well as the label (SalePrice) from\nthe first four examples.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c118c3a9-01f7-4f93-b372-48f394797456": {"__data__": {"id_": "c118c3a9-01f7-4f93-b372-48f394797456", "embedding": null, "metadata": {"page_label": "202", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "93cb5ba1-df53-493d-91bf-add9213e7d5c", "node_type": "4", "metadata": {"page_label": "202", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "59a5adc882ebbf18dc6ce2205772e3c4c234c5f48eeffb47304e9ed83af5d9a8", "class_name": "RelatedNodeInfo"}}, "text": "202 Multilayer Perceptrons\nprint (data .raw_train .iloc[: 4, [0,1,2,3,-3,-2,-1]])\nId MSSubClass MSZoning LotFrontage SaleType SaleCondition SalePrice\n0 1 60 RL 65.0 WD Normal 208500\n1 2 20 RL 80.0 WD Normal 181500\n2 3 60 RL 68.0 WD Normal 223500\n3 4 70 RL 60.0 WD Abnorml 140000\nWe can see that in each example, the first feature is the identifier. This helps the model\ndetermineeachtrainingexample. Whilethisisconvenient,itdoesnotcarryanyinformation\nfor prediction purposes. Hence, we will remove it from the dataset before feeding the data\nintothemodel. Furthermore,givenawidevarietyofdatatypes,wewillneedtopreprocess\nthe data before we can start modeling.\nLet\u2019s start with the numerical features. First, we apply a heuristic, replacing all missing\nvalues by the corresponding feature\u2019s mean. Then, to put all features on a common scale,\nwestandardize the data by rescaling features to zero mean and unit variance:\n\ud835\udc65 \ud835\udc65\u0000\ud835\udf07\n\ud835\udf0e, (5.7.1)\nwhere\ud835\udf07and\ud835\udf0edenotemeanandstandarddeviation, respectively. Toverifythatthisindeed\ntransforms our feature (variable) such that it has zero mean and unit variance, note that\n\ud835\udc38\u00bb\ud835\udc65\u0000\ud835\udf07\n\ud835\udf0e\u00bc=\ud835\udf07\u0000\ud835\udf07\n\ud835\udf0e=0and that\ud835\udc38\u00bb\u00b9\ud835\udc65\u0000\ud835\udf07\u00ba2\u00bc=\u00b9\ud835\udf0e2\u00b8\ud835\udf072\u00ba\u00002\ud835\udf072\u00b8\ud835\udf072=\ud835\udf0e2. Intuitively, we\nstandardize the data for two reasons. First, it proves convenient for optimization. Second,\nbecausewedonotknow aprioriwhichfeatureswillberelevant,wedonotwanttopenalize\ncoefficients assigned to one feature more than any other.\nNextwedealwithdiscretevalues. Theseincludefeaturessuchas\u201cMSZoning\u201d. Wereplace\nthem by a one-hot encoding in the same way that we earlier transformed multiclass labels\ninto vectors (see Section 4.1.1 ). For instance, \u201cMSZoning\u201d assumes the values \u201cRL\u201d and\n\u201cRM\u201d. Dropping the \u201cMSZoning\u201d feature, two new indicator features \u201cMSZoning_RL\u201d\nand \u201cMSZoning_RM\u201d are created with values being either 0 or 1. According to one-hot\nencoding, if the original value of \u201cMSZoning\u201d is \u201cRL\u201d, then \u201cMSZoning_RL\u201d is 1 and\n\u201cMSZoning_RM\u201d is 0. The pandaspackage does this automatically for us.\n@d2l .add_to_class(KaggleHouse)\ndef preprocess (self ):\n# Remove the ID and label columns\nlabel ='SalePrice '\nfeatures =pd.concat(\n(self .raw_train .drop(columns =['Id', label]),\nself .raw_val .drop(columns =['Id'])))\n# Standardize numerical columns\nnumeric_features =features .dtypes[features .dtypes !='object '].index\nfeatures[numeric_features] =features[numeric_features] .apply(\nlambda x: (x -x.mean()) /(x.std()))\n# Replace NAN numerical features by 0\nfeatures[numeric_features] =features[numeric_features] .fillna( 0)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f05aee89-a1c5-43dd-a0d5-d362dcc094b7": {"__data__": {"id_": "f05aee89-a1c5-43dd-a0d5-d362dcc094b7", "embedding": null, "metadata": {"page_label": "203", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1ba9f54-2dd8-4428-b80d-9ec40e9e8ed6", "node_type": "4", "metadata": {"page_label": "203", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1a440c2f73d763abb17c5ce5dc58f59a591dd2047034faf62a71982d93c7c44a", "class_name": "RelatedNodeInfo"}}, "text": "203 Predicting House Prices on Kaggle\n(continued from previous page)\n# Replace discrete features by one-hot encoding\nfeatures =pd.get_dummies(features, dummy_na =True )\n# Save preprocessed features\nself .train =features[: self .raw_train .shape[ 0]].copy()\nself .train[label] =self .raw_train[label]\nself .val =features[ self .raw_train .shape[ 0]:].copy()\nYoucanseethatthisconversionincreasesthenumberoffeaturesfrom79to331(excluding\nID and label columns).\ndata .preprocess()\ndata .train .shape\n(1460 ,331)\n5.7.5ErrorMeasure\nTo get started we will train a linear model with squared loss. Not surprisingly, our linear\nmodelwillnotleadtoacompetition-winningsubmissionbutitdoesprovideasanitycheck\nto see whether there is meaningful information in the data. If we cannot do better than\nrandom guessing here, then there might be a good chance that we have a data processing\nbug. And if things work, the linear model will serve as a baseline giving us some intuition\nabout how close the simple model gets to the best reported models, giving us a sense of\nhow much gain we should expect from fancier models.\nWith house prices, as with stock prices, we care about relative quantities more than ab-\nsolute quantities. Thus we tend to care more about the relative error\ud835\udc66\u0000\u02c6\ud835\udc66\n\ud835\udc66than about the\nabsolute error \ud835\udc66\u0000\u02c6\ud835\udc66. For instance, if our prediction is off by $100,000 when estimating the\nprice of a house in rural Ohio, where the value of a typical house is $125,000, then we are\nprobably doing a horrible job. On the other hand, if we err by this amount in Los Altos\nHills, California, this might represent a stunningly accurate prediction (there, the median\nhouse price exceeds $4 million).\nOnewaytoaddressthisproblemistomeasurethediscrepancyinthelogarithmoftheprice\nestimates. In fact, this is also the official error measure used by the competition to evaluate\nthe quality of submissions. After all, a small value \ud835\udeffforjlog\ud835\udc66\u0000log \u02c6\ud835\udc66j\u0014\ud835\udefftranslates into\n\ud835\udc52\u0000\ud835\udeff\u0014\u02c6\ud835\udc66\n\ud835\udc66\u0014\ud835\udc52\ud835\udeff. Thisleadstothefollowingroot-mean-squared-errorbetweenthelogarithm\nof the predicted price and the logarithm of the label price:\nvt\n1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1\u00b9log\ud835\udc66\ud835\udc56\u0000log \u02c6\ud835\udc66\ud835\udc56\u00ba2. (5.7.2)\n@d2l .add_to_class(KaggleHouse)\ndef get_dataloader (self , train):\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2dfd7b78-c397-4997-93c5-13915867be0e": {"__data__": {"id_": "2dfd7b78-c397-4997-93c5-13915867be0e", "embedding": null, "metadata": {"page_label": "204", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07904c0b-1395-4ece-9f39-e95cb1463312", "node_type": "4", "metadata": {"page_label": "204", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b48fc1f4b3f2ff2cb749028e9283bf38e460f7fd41aa862dec3c380aa19f280b", "class_name": "RelatedNodeInfo"}}, "text": "204 Multilayer Perceptrons\n(continued from previous page)\nlabel ='SalePrice '\ndata =self .train iftrain else self .val\niflabel not indata: return\nget_tensor =lambda x: torch .tensor(x .values .astype( float ),\ndtype =torch .float32)\n# Logarithm of prices\ntensors =(get_tensor(data .drop(columns =[label])), # X\ntorch .log(get_tensor(data[label])) .reshape(( -1,1))) # Y\nreturn self .get_tensorloader(tensors, train)\n5.7.6\ud835\udc3e-FoldCross-Validation\nYou might recall that we introduced cross-validation in Section 3.6.3 , where we discussed\nhow to deal with model selection. We will put this to good use to select the model design\nand to adjust the hyperparameters. We first need a function that returns the \ud835\udc56thfold of the\ndata in a\ud835\udc3e-fold cross-validation procedure. It proceeds by slicing out the \ud835\udc56thsegment as\nvalidationdataandreturningtherestastrainingdata. Notethatthisisnotthemostefficient\nwayofhandlingdataandwewoulddefinitelydosomethingmuchsmarterifourdatasetwas\nconsiderably larger. But this added complexity might obfuscate our code unnecessarily so\nwe can safely omit it here owing to the simplicity of our problem.\ndef k_fold_data (data, k):\nrets =[]\nfold_size =data .train .shape[ 0]//k\nfor jinrange (k):\nidx =range (j*fold_size, (j +1)*fold_size)\nrets .append(KaggleHouse(data .batch_size, data .train .drop(index =idx),\ndata .train .loc[idx]))\nreturn rets\nTheaveragevalidationerrorisreturnedwhenwetrain \ud835\udc3etimesinthe\ud835\udc3e-foldcross-validation.\ndef k_fold (trainer, data, k, lr):\nval_loss, models =[], []\nfor i, data_fold inenumerate (k_fold_data(data, k)):\nmodel =d2l.LinearRegression(lr)\nmodel .board .yscale ='log'\nifi!=0: model .board .display =False\ntrainer .fit(model, data_fold)\nval_loss .append( float (model .board .data[ 'val_loss '][-1].y))\nmodels .append(model)\nprint (f'average validation log mse = {sum(val_loss) /len(val_loss) }')\nreturn models\n5.7.7Model Selection\nIn this example, we pick an untuned set of hyperparameters and leave it up to the reader to\nimprovethemodel. Findingagoodchoicecantaketime,dependingonhowmanyvariables\none optimizes over. With a large enough dataset, and the normal sorts of hyperparameters,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2138, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f62d6b9-6613-4a32-bde9-1697596a054a": {"__data__": {"id_": "3f62d6b9-6613-4a32-bde9-1697596a054a", "embedding": null, "metadata": {"page_label": "205", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61c47e37-f84a-49d8-9ca2-cce1a57b1e9b", "node_type": "4", "metadata": {"page_label": "205", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "495044425bf22919c69c1359b2fb92de53fb17f0b8f62c9ebbbfc0d4e61f8a70", "class_name": "RelatedNodeInfo"}}, "text": "205 Predicting House Prices on Kaggle\n\ud835\udc3e-fold cross-validation tends to be reasonably resilient against multiple testing. However,\nif we try an unreasonably large number of options we might find that our validation perfor-\nmance is no longer representative of the true error.\ntrainer =d2l.Trainer(max_epochs =10)\nmodels =k_fold(trainer, data, k =5, lr =0.01 )\naverage validation log mse =0.17325432986021042\nNoticethatsometimesthenumberoftrainingerrorsforasetofhyperparameterscanbevery\nlow, even as the number of errors on \ud835\udc3e-fold cross-validation grows considerably higher.\nThis indicates that we are overfitting. Throughout training you will want to monitor both\nnumbers. Less overfitting might indicate that our data can support a more powerful model.\nMassive overfitting might suggest that we can gain by incorporating regularization tech-\nniques.\n5.7.8SubmittingPredictionson Kaggle\nNow that we know what a good choice of hyperparameters should be, we might calculate\nthe average predictions on the test set by all the \ud835\udc3emodels. Saving the predictions in a csv\nfile will simplify uploading the results to Kaggle. The following code will generate a file\ncalled submission.csv .\npreds =[model(torch .tensor(data .val.values .astype( float ), dtype =torch .\n\u21a9!float32))\nfor model inmodels]\n# Taking exponentiation of predictions in the logarithm scale\nensemble_preds =torch .exp(torch .cat(preds, 1)).mean( 1)\nsubmission =pd.DataFrame({ 'Id':data .raw_val .Id,\n'SalePrice ':ensemble_preds .detach() .numpy()})\nsubmission .to_csv( 'submission.csv ', index =False )\nNext, as demonstrated in Fig. 5.7.3 , we can submit our predictions on Kaggle and see how\nthey compare with the actual house prices (labels) on the test set. The steps are quite\nsimple:\n\u000fLog in to the Kaggle website and visit the house price prediction competition page.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1d91af9-b184-4a8e-8a55-3d41cb14e79c": {"__data__": {"id_": "b1d91af9-b184-4a8e-8a55-3d41cb14e79c", "embedding": null, "metadata": {"page_label": "206", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9521e2d2-600c-45f3-a51e-17d2e0a66468", "node_type": "4", "metadata": {"page_label": "206", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "73dba350894726552f5eea5b371eb282583ff64338cf2c640e0a1451db6e9e4b", "class_name": "RelatedNodeInfo"}}, "text": "206 Multilayer Perceptrons\n110\u000fClick the \u201cSubmit Predictions\u201d or \u201cLate Submission\u201d button.\n\u000fClick the \u201cUpload Submission File\u201d button in the dashed box at the bottom of the page\nand select the prediction file you wish to upload.\n\u000fClick the \u201cMake Submission\u201d button at the bottom of the page to view your results.\ntFig. 5.7.3 Submitting data to Kaggle\n5.7.9Summaryand Discussion\nRealdataoftencontainsamixofdifferentdatatypesandneedstobepreprocessed. Rescal-\ning real-valued data to zero mean and unit variance is a good default. So is replacing miss-\ning values with their mean. Furthermore, transforming categorical features into indicator\nfeatures allows us to treat them like one-hot vectors. When we tend to care more about the\nrelative error than about the absolute error, we can measure the discrepancy in the loga-\nrithm of the prediction. To select the model and adjust the hyperparameters, we can use\n\ud835\udc3e-fold cross-validation .\n5.7.10Exercises\n1.Submit your predictions for this section to Kaggle. How good are they?\n2.Is it always a good idea to replace missing values by a mean? Hint: can you construct a\nsituation where the values are not missing at random?\n3.Improve the score by tuning the hyperparameters through \ud835\udc3e-fold cross-validation.\n4.Improve the score by improving the model (e.g., layers, weight decay, and dropout).\n5.What happens if we do not standardize the continuous numerical features as we have\ndone in this section?\nDiscussions110.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1459, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fce3b756-3e14-4188-8246-553d8fd0bfab": {"__data__": {"id_": "fce3b756-3e14-4188-8246-553d8fd0bfab", "embedding": null, "metadata": {"page_label": "207", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a01a01d-316a-4056-abbe-5c923357eb64", "node_type": "4", "metadata": {"page_label": "207", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "023e16f135c4a8ffca80605d94a45c9a7f065c4c1373d372ade6c925acb22e6c", "class_name": "RelatedNodeInfo"}}, "text": "6 Builders\u2019 Guide\nAlongside giant datasets and powerful hardware, great software tools have played an in-\ndispensable role in the rapid progress of deep learning. Starting with the pathbreaking\nTheano library released in 2007, flexible open-source tools have enabled researchers to\nrapidly prototype models, avoiding repetitive work when recycling standard components\nwhile still maintaining the ability to make low-level modifications. Over time, deep learn-\ning\u2019slibrarieshaveevolvedtoofferincreasinglycoarseabstractions. Justassemiconductor\ndesigners went from specifying transistors to logical circuits to writing code, neural net-\nworksresearchershavemovedfromthinkingaboutthebehaviorofindividualartificialneu-\nronstoconceivingofnetworksintermsofwholelayers,andnowoftendesignarchitectures\nwith far coarser blocksin mind.\nSo far, we have introduced some basic machine learning concepts, ramping up to fully-\nfunctional deep learning models. In the last chapter, we implemented each component of\nan MLP from scratch and even showed how to leverage high-level APIs to roll out the\nsame models effortlessly. To get you that far that fast, we called upon the libraries, but\nskipped over more advanced details about how they work . In this chapter, we will peel\nback the curtain, digging deeper into the key components of deep learning computation,\nnamely model construction, parameter access and initialization, designing custom layers\nand blocks, reading and writing models to disk, and leveraging GPUs to achieve dramatic\nspeedups. These insights will move you from enduser topoweruser , giving you the tools\nneededtoreapthebenefitsofamaturedeeplearninglibrarywhileretainingtheflexibilityto\nimplement more complex models, including those you invent yourself! While this chapter\ndoesnotintroduceanynewmodelsordatasets,theadvancedmodelingchaptersthatfollow\nrely heavily on these techniques.\n6.1Layersand Modules\nWhen we first introduced neural networks, we focused on linear models with a single out-\nput. Here, the entire model consists of just a single neuron. Note that a single neuron (i)\ntakes some set of inputs; (ii) generates a corresponding scalar output; and (iii) has a set of\nassociated parameters that can be updated to optimize some objective function of interest.\nThen, once we started thinking about networks with multiple outputs, we leveraged vec-\ntorized arithmetic to characterize an entire layer of neurons. Just like individual neurons,\n207", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88f04a66-54bf-4987-9a85-aef746ce1545": {"__data__": {"id_": "88f04a66-54bf-4987-9a85-aef746ce1545", "embedding": null, "metadata": {"page_label": "208", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec9af7d3-c0ff-4bad-89cd-db63a60b8a37", "node_type": "4", "metadata": {"page_label": "208", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9bac179451dbb7d9c60c6ff0addfb3285ac2305aec95da08d64c38aebd3f8953", "class_name": "RelatedNodeInfo"}}, "text": "208 Builders\u2019 Guide\nlayers (i) take a set of inputs, (ii) generate corresponding outputs, and (iii) are described by\na set of tunable parameters. When we worked through softmax regression, a single layer\nwas itself the model. However, even when we subsequently introduced MLPs, we could\nstill think of the model as retaining this same basic structure.\nInterestingly,forMLPs,boththeentiremodelanditsconstituentlayerssharethisstructure.\nTheentiremodeltakesinrawinputs(thefeatures), generatesoutputs(thepredictions), and\npossesses parameters (the combined parameters from all constituent layers). Likewise,\neach individual layer ingests inputs (supplied by the previous layer) generates outputs (the\ninputs to the subsequent layer), and possesses a set of tunable parameters that are updated\naccording to the signal that flows backwards from the subsequent layer.\nWhile you might think that neurons, layers, and models give us enough abstractions to go\nabout our business, it turns out that we often find it convenient to speak about components\nthat are larger than an individual layer but smaller than the entire model. For example, the\nResNet-152architecture,whichiswildlypopularincomputervision,possesseshundredsof\nlayers. These layers consist of repeating patterns of groups of layers . Implementing such\na network one layer at a time can grow tedious. This concern is not just hypothetical\u2014\nsuch design patterns are common in practice. The ResNet architecture mentioned above\nwonthe2015ImageNetandCOCOcomputervisioncompetitionsforbothrecognitionand\ndetection ( Heetal., 2016) and remains a go-to architecture for many vision tasks. Similar\narchitectures in which layers are arranged in various repeating patterns are now ubiquitous\nin other domains, including natural language processing and speech.\nTo implement these complex networks, we introduce the concept of a neural network mod-\nule. A module could describe a single layer, a component consisting of multiple layers,\nor the entire model itself! One benefit of working with the module abstraction is that they\ncan be combined into larger artifacts, often recursively. This is illustrated in Fig. 6.1.1 .\nBy defining code to generate modules of arbitrary complexity on demand, we can write\nsurprisingly compact code and still implement complex neural networks.\ntFig. 6.1.1 Multiple layers are combined into modules, forming repeating patterns of larger models.\nFrom a programming standpoint, a module is represented by a class. Any subclass of it\nmust define a forward propagation method that transforms its input into output and must\nstore any necessary parameters. Note that some modules do not require any parameters at", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7d2e8d4-15a4-4a8a-97fd-83642ff5fcce": {"__data__": {"id_": "a7d2e8d4-15a4-4a8a-97fd-83642ff5fcce", "embedding": null, "metadata": {"page_label": "209", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "63251683-4b8a-408b-9bb8-2c2476718a2a", "node_type": "4", "metadata": {"page_label": "209", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3e939303721b9469fc6c1112779f25bc3112f3dac73eede29611b9320e46d96d", "class_name": "RelatedNodeInfo"}}, "text": "209 Layers and Modules\nall. Finally a module must possess a backpropagation method, for purposes of calculating\ngradients. Fortunately,duetosomebehind-the-scenesmagicsuppliedbytheautodifferen-\ntiation (introduced in Section 2.5 ) when defining our own module, we only need to worry\nabout parameters and the forward propagation method.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nTo begin, we revisit the code that we used to implement MLPs ( Section 5.1 ). The follow-\ning code generates a network with one fully connected hidden layer with 256 units and\nReLU activation, followed by a fully connected output layer with ten units (no activation\nfunction).\nnet =nn.Sequential(nn .LazyLinear( 256), nn .ReLU(), nn .LazyLinear( 10))\nX=torch .rand( 2,20)\nnet(X) .shape\ntorch .Size([ 2,10])\nIn this example, we constructed our model by instantiating an nn.Sequential , with layers\nin the order that they should be executed passed as arguments. In short, nn.Sequential\ndefines a special kind of Module, the class that presents a module in PyTorch. It maintains\nanorderedlistofconstituent Modules. Notethateachofthetwofullyconnectedlayersisan\ninstanceofthe Linearclasswhichis itselfa subclassof Module. The forwardpropagation\n(forward ) method is also remarkably simple: it chains each module in the list together,\npassing the output of each as input to the next. Note that until now, we have been invok-\ning our models via the construction net(X)to obtain their outputs. This is actually just\nshorthand for net.__call__(X) .\n6.1.1A Custom Module\nPerhaps the easiest way to develop intuition about how a module works is to implement\none ourselves. Before we do that, we briefly summarize the basic functionality that each\nmodule must provide:\n1.Ingest input data as arguments to its forward propagation method.\n2.Generate an output by having the forward propagation method return a value. Note\nthat the output may have a different shape from the input. For example, the first fully\nconnected layer in our model above ingests an input of arbitrary dimension but returns\nan output of dimension 256.\n3.Calculate the gradient of its output with respect to its input, which can be accessed via\nits backpropagation method. Typically this happens automatically.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d7eabca-baf7-449a-b954-5ffbd17905f3": {"__data__": {"id_": "1d7eabca-baf7-449a-b954-5ffbd17905f3", "embedding": null, "metadata": {"page_label": "210", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91f81465-71ad-4397-8fe4-b85beac190d2", "node_type": "4", "metadata": {"page_label": "210", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a5d39921cf70cdf68368518f742b50b39ec7965c87bca131f4339432e0bfd363", "class_name": "RelatedNodeInfo"}}, "text": "210 Builders\u2019 Guide\n4.Store and provide access to those parameters necessary for executing the forward prop-\nagation computation.\n5.Initialize model parameters as needed.\nIn the following snippet, we code up a module from scratch corresponding to an MLP\nwith one hidden layer with 256 hidden units, and a 10-dimensional output layer. Note that\ntheMLPclass below inherits the class that represents a module. We will heavily rely on\nthe parent class\u2019s methods, supplying only our own constructor (the __init__ method in\nPython) and the forward propagation method.\nclass MLP(nn.Module):\ndef __init__ (self ):\n# Call the constructor of the parent class nn.Module to perform\n# the necessary initialization\nsuper ().__init__ ()\nself .hidden =nn.LazyLinear( 256)\nself .out =nn.LazyLinear( 10)\n# Define the forward propagation of the model, that is, how to return the\n# required model output based on the input X\ndef forward (self , X):\nreturn self .out(F .relu( self .hidden(X)))\nLet\u2019s first focus on the forward propagation method. Note that it takes Xas input, calcu-\nlates the hidden representation with the activation function applied, and outputs its logits.\nIn this MLPimplementation, both layers are instance variables. To see why this is reason-\nable, imagine instantiating two MLPs, net1andnet2, and training them on different data.\nNaturally, we would expect them to represent two different learned models.\nWe instantiate the MLP\u2019s layers in the constructor and subsequently invoke these layers on\neach call to the forward propagation method. Note a few key details. First, our customized\n__init__ method invokes the parent class\u2019s __init__ method via super().__init__()\nsparing us the pain of restating boilerplate code applicable to most modules. We then\ninstantiate our two fully connected layers, assigning them to self.hidden andself.out .\nNote that unless we implement a new layer, we need not worry about the backpropagation\nmethod or parameter initialization. The system will generate these methods automatically.\nLet\u2019s try this out.\nnet =MLP()\nnet(X) .shape\ntorch .Size([ 2,10])\nAkeyvirtueofthemoduleabstractionisitsversatility. Wecansubclassamoduletocreate\nlayers(suchasthefullyconnectedlayerclass), entiremodels(suchasthe MLPclassabove),\nor various components of intermediate complexity. We exploit this versatility throughout\nthe coming chapters, such as when addressing convolutional neural networks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbeb9871-7723-4e10-84cb-da3dc14b2f2e": {"__data__": {"id_": "bbeb9871-7723-4e10-84cb-da3dc14b2f2e", "embedding": null, "metadata": {"page_label": "211", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e25104d-afc3-4ddb-b038-99e72e084c99", "node_type": "4", "metadata": {"page_label": "211", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0f9206cf0d1f199b9ead7e71a6c0f1151e30045d2c73677c21d32adf2fc63863", "class_name": "RelatedNodeInfo"}}, "text": "211 Layers and Modules\n6.1.2The SequentialModule\nWe can now take a closer look at how the Sequential class works. Recall that Sequen-\ntialwas designed to daisy-chain other modules together. To build our own simplified\nMySequential , we just need to define two key methods:\n1.A method for appending modules one by one to a list.\n2.Aforwardpropagationmethodforpassinganinputthroughthechainofmodules,inthe\nsame order as they were appended.\nThe following MySequential class delivers the same functionality of the default Sequen-\ntialclass.\nclass MySequential (nn.Module):\ndef __init__ (self ,*args):\nsuper ().__init__ ()\nfor idx, module inenumerate (args):\nself .add_module( str(idx), module)\ndef forward (self , X):\nfor module inself .children():\nX=module(X)\nreturn X\nInthe __init__ method,weaddeverymodulebycallingthe add_modules method. These\nmodules can be accessed by the children method at a later date. In this way the system\nknows the added modules, and it will properly initialize each module\u2019s parameters.\nWhen our MySequential \u2019s forward propagation method is invoked, each added module is\nexecuted in the order in which they were added. We can now reimplement an MLP using\nourMySequential class.\nnet =MySequential(nn .LazyLinear( 256), nn .ReLU(), nn .LazyLinear( 10))\nnet(X) .shape\ntorch .Size([ 2,10])\nNote that this use of MySequential is identical to the code we previously wrote for the\nSequential class (as described in Section 5.1 ).\n6.1.3ExecutingCode in the ForwardPropagationMethod\nTheSequential class makes model construction easy, allowing us to assemble new archi-\ntectures without having to define our own class. However, not all architectures are simple\ndaisy chains. When greater flexibility is required, we will want to define our own blocks.\nFor example, we might want to execute Python\u2019s control flow within the forward propaga-\ntion method. Moreover, we might want to perform arbitrary mathematical operations, not\nsimply relying on predefined neural network layers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91df546e-9d35-4166-b260-2f65c0063dca": {"__data__": {"id_": "91df546e-9d35-4166-b260-2f65c0063dca", "embedding": null, "metadata": {"page_label": "212", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "caef289a-0146-450e-8579-d593be74aa61", "node_type": "4", "metadata": {"page_label": "212", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "701ace2d3867f752ecbf30b95e19a8f617b85bf5e9c0d25e983bf92b4998604d", "class_name": "RelatedNodeInfo"}}, "text": "212 Builders\u2019 Guide\nYou may have noticed that until now, all of the operations in our networks have acted upon\nour network\u2019s activations and its parameters. Sometimes, however, we might want to in-\ncorporate terms that are neither the result of previous layers nor updatable parameters. We\ncall these constant parameters . Say for example that we want a layer that calculates the\nfunction\ud835\udc53\u00b9x,w\u00ba=\ud835\udc50\u0001w>x, where xis the input, wis our parameter, and \ud835\udc50is some speci-\nfiedconstantthatisnotupdatedduringoptimization. Soweimplementa FixedHiddenMLP\nclass as follows.\nclass FixedHiddenMLP (nn.Module):\ndef __init__ (self ):\nsuper ().__init__ ()\n# Random weight parameters that will not compute gradients and\n# therefore keep constant during training\nself .rand_weight =torch .rand(( 20,20))\nself .linear =nn.LazyLinear( 20)\ndef forward (self , X):\nX=self .linear(X)\nX=F.relu(X @self .rand_weight +1)\n# Reuse the fully connected layer. This is equivalent to sharing\n# parameters with two fully connected layers\nX=self .linear(X)\n# Control flow\nwhile X.abs() .sum() >1:\nX/=2\nreturn X.sum()\nIn this model, we implement a hidden layer whose weights ( self.rand_weight ) are ini-\ntialized randomly at instantiation and are thereafter constant. This weight is not a model\nparameter and thus it is never updated by backpropagation. The network then passes the\noutput of this \u201cfixed\u201d layer through a fully connected layer.\nNote that before returning the output, our model did something unusual. We ran a while-\nloop, testing on the condition its \u21131norm is larger than 1, and dividing our output vector\nby2until it satisfied the condition. Finally, we returned the sum of the entries in X. To our\nknowledge, no standard neural network performs this operation. Note that this particular\noperation may not be useful in any real-world task. Our point is only to show you how to\nintegrate arbitrary code into the flow of your neural network computations.\nnet =FixedHiddenMLP()\nnet(X)\ntensor( -0.3836 , grad_fn =<SumBackward0 >)\nWe can mix and match various ways of assembling modules together. In the following\nexample, we nest modules in some creative ways.\nclass NestMLP (nn.Module):\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40537af6-6198-4407-b8af-86f3dcc0c020": {"__data__": {"id_": "40537af6-6198-4407-b8af-86f3dcc0c020", "embedding": null, "metadata": {"page_label": "213", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96c46d1c-f132-422c-ba42-ec1cc9d4a37d", "node_type": "4", "metadata": {"page_label": "213", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e19952c8d59fc66a7b88f5005fd082c338c80441bf93ee91b7d9c54c3723fc3a", "class_name": "RelatedNodeInfo"}}, "text": "213 Parameter Management\n111(continued from previous page)\ndef __init__ (self ):\nsuper ().__init__ ()\nself .net =nn.Sequential(nn .LazyLinear( 64), nn .ReLU(),\nnn.LazyLinear( 32), nn .ReLU())\nself .linear =nn.LazyLinear( 16)\ndef forward (self , X):\nreturn self .linear( self .net(X))\nchimera =nn.Sequential(NestMLP(), nn .LazyLinear( 20), FixedHiddenMLP())\nchimera(X)\ntensor( 0.0679 , grad_fn =<SumBackward0 >)\n6.1.4Summary\nIndividuallayerscanbemodules. Manylayerscancompriseamodule. Manymodulescan\ncomprise a module.\nA module can contain code. Modules take care of lots of housekeeping, including param-\neter initialization and backpropagation. Sequential concatenations of layers and modules\nare handled by the Sequential module.\n6.1.5Exercises\n1.What kinds of problems will occur if you change MySequential to store modules in a\nPython list?\n2.Implement a module that takes two modules as an argument, say net1andnet2and\nreturns the concatenated output of both networks in the forward propagation. This is\nalso called a parallelmodule .\n3.Assume that you want to concatenate multiple instances of the same network. Imple-\nment a factory function that generates multiple instances of the same module and build\na larger network from it.\nDiscussions111.\n6.2ParameterManagement\nOnce we have chosen an architecture and set our hyperparameters, we proceed to the train-\ning loop, where our goal is to find parameter values that minimize our loss function. After\ntraining, we will need these parameters in order to make future predictions. Additionally,\nwe will sometimes wish to extract the parameters perhaps to reuse them in some other", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b45b8b8-c0d3-4b55-a3cf-4ce4916c34be": {"__data__": {"id_": "6b45b8b8-c0d3-4b55-a3cf-4ce4916c34be", "embedding": null, "metadata": {"page_label": "214", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "254f834a-a45a-4c20-b76a-8dbd1bc5b0e3", "node_type": "4", "metadata": {"page_label": "214", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cc3f813d9603980095020937eae54414d0cecf1cd4efb0b13852a80827b5304b", "class_name": "RelatedNodeInfo"}}, "text": "214 Builders\u2019 Guide\ncontext, to save our model to disk so that it may be executed in other software, or for ex-\namination in the hope of gaining scientific understanding.\nMost of the time, we will be able to ignore the nitty-gritty details of how parameters are\ndeclared and manipulated, relying on deep learning frameworks to do the heavy lifting.\nHowever, when we move away from stacked architectures with standard layers, we will\nsometimes need to get into the weeds of declaring and manipulating parameters. In this\nsection, we cover the following:\n\u000fAccessing parameters for debugging, diagnostics, and visualizations.\n\u000fSharing parameters across different model components.\nimport torch\nfrom torch import nn\nWe start by focusing on an MLP with one hidden layer.\nnet =nn.Sequential(nn .LazyLinear( 8),\nnn.ReLU(),\nnn.LazyLinear( 1))\nX=torch .rand(size =(2,4))\nnet(X) .shape\ntorch .Size([ 2,1])\n6.2.1ParameterAccess\nLet\u2019s start with how to access parameters from the models that you already know.\nWhenamodelisdefinedviathe Sequential class,wecanfirstaccessanylayerbyindexing\ninto the model as though it were a list. Each layer\u2019s parameters are conveniently located in\nits attribute.\nWe can inspect the parameters of the second fully connected layer as follows.\nnet[ 2].state_dict()\nOrderedDict([( 'weight ',\ntensor([[ -0.1649 ,0.0605 ,0.1694 ,-0.2524 ,0.3526 ,-0.3414 ,-\n\u21a9!0.2322 ,0.0822 ]])),\n('bias ', tensor([ 0.0709 ]))])\nWe can see that this fully connected layer contains two parameters, corresponding to that\nlayer\u2019s weights and biases, respectively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c314ddf-0317-471e-b4fc-7e8b5bd48497": {"__data__": {"id_": "4c314ddf-0317-471e-b4fc-7e8b5bd48497", "embedding": null, "metadata": {"page_label": "215", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0ec40fb-9a7d-4fd2-924a-7c6b40e7a484", "node_type": "4", "metadata": {"page_label": "215", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "52bc8b92d347070f9f77d8456d9f5274de601bc180ddf05080af67789b42e387", "class_name": "RelatedNodeInfo"}}, "text": "215 Parameter Management\nTargetedParameters\nNote that each parameter is represented as an instance of the parameter class. To do any-\nthing useful with the parameters, we first need to access the underlying numerical values.\nThere are several ways to do this. Some are simpler while others are more general. The\nfollowing code extracts the bias from the second neural network layer, which returns a\nparameter class instance, and further accesses that parameter\u2019s value.\ntype (net[ 2].bias), net[ 2].bias .data\n(torch .nn.parameter .Parameter, tensor([ 0.0709 ]))\nParameters are complex objects, containing values, gradients, and additional information.\nThat is why we need to request the value explicitly.\nIn addition to the value, each parameter also allows us to access the gradient. Because we\nhave not invoked backpropagation for this network yet, it is in its initial state.\nnet[ 2].weight .grad ==None\nTrue\nAllParametersat Once\nWhen we need to perform operations on all parameters, accessing them one-by-one can\ngrow tedious. The situation can grow especially unwieldy when we work with more com-\nplex,e.g.,nested,modules,sincewewouldneedtorecursethroughtheentiretreetoextract\neach sub-module\u2019s parameters. Below we demonstrate accessing the parameters of all lay-\ners.\n[(name, param .shape) for name, param innet.named_parameters()]\n[('0.weight ', torch .Size([ 8,4])),\n('0.bias ', torch .Size([ 8])),\n('2.weight ', torch .Size([ 1,8])),\n('2.bias ', torch .Size([ 1]))]\n6.2.2TiedParameters\nOften,wewanttoshareparametersacrossmultiplelayers. Let\u2019sseehowtodothiselegantly.\nIn the following we allocate a fully connected layer and then use its parameters specifically\nto set those of another layer. Here we need to run the forward propagation net(X)before\naccessing the parameters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1785, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1594afa7-6d8d-4451-abb9-1ae1c0e200a8": {"__data__": {"id_": "1594afa7-6d8d-4451-abb9-1ae1c0e200a8", "embedding": null, "metadata": {"page_label": "216", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f38b593-f275-492b-9c6e-13c767e4a895", "node_type": "4", "metadata": {"page_label": "216", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2992916c6036a0ede557b3c7a4435fb7937de28e7f1da05584048ada050c878b", "class_name": "RelatedNodeInfo"}}, "text": "216 Builders\u2019 Guide\n112# We need to give the shared layer a name so that we can refer to its\n# parameters\nshared =nn.LazyLinear( 8)\nnet =nn.Sequential(nn .LazyLinear( 8), nn .ReLU(),\nshared, nn .ReLU(),\nshared, nn .ReLU(),\nnn.LazyLinear( 1))\nnet(X)\n# Check whether the parameters are the same\nprint (net[ 2].weight .data[ 0]==net[ 4].weight .data[ 0])\nnet[ 2].weight .data[ 0,0]=100\n# Make sure that they are actually the same object rather than just having the\n# same value\nprint (net[ 2].weight .data[ 0]==net[ 4].weight .data[ 0])\ntensor([ True ,True ,True ,True ,True ,True ,True ,True ])\ntensor([ True ,True ,True ,True ,True ,True ,True ,True ])\nThis example shows that the parameters of the second and third layer are tied. They are\nnot just equal, they are represented by the same exact tensor. Thus, if we change one of the\nparameters, the other one changes, too.\nYou might wonder, when parameters are tied what happens to the gradients? Since the\nmodel parameters contain gradients, the gradients of the second hidden layer and the third\nhidden layer are added together during backpropagation.\n6.2.3Summary\nWe have several ways of accessing and tying model parameters.\n6.2.4Exercises\n1.Use the NestMLP model defined in Section 6.1 and access the parameters of the various\nlayers.\n2.Construct an MLP containing a shared parameter layer and train it. During the training\nprocess, observe the model parameters and gradients of each layer.\n3.Why is sharing parameters a good idea?\nDiscussions112.\n6.3ParameterInitialization\nNow that we know how to access the parameters, let\u2019s look at how to initialize them prop-\nerly. We discussed the need for proper initialization in Section 5.4 . The deep learning", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3a331c9-35bc-43bd-97e8-c20deb8c9623": {"__data__": {"id_": "b3a331c9-35bc-43bd-97e8-c20deb8c9623", "embedding": null, "metadata": {"page_label": "217", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcd94cf8-f137-4f97-8fd9-7477d7a91e15", "node_type": "4", "metadata": {"page_label": "217", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cd343975793d4ee8d1a091ad403213ef344a51d293b2bd2a26b0f6d57101974b", "class_name": "RelatedNodeInfo"}}, "text": "217 Parameter Initialization\nframework provides default random initializations to its layers. However, we often want to\ninitialize our weights according to various other protocols. The framework provides most\ncommonly used protocols, and also allows to create a custom initializer.\nimport torch\nfrom torch import nn\nBydefault,PyTorchinitializesweightandbiasmatricesuniformlybydrawingfromarange\nthat is computed according to the input and output dimension. PyTorch\u2019s nn.init module\nprovides a variety of preset initialization methods.\nnet =nn.Sequential(nn .LazyLinear( 8), nn .ReLU(), nn .LazyLinear( 1))\nX=torch .rand(size =(2,4))\nnet(X) .shape\ntorch .Size([ 2,1])\n6.3.1Built-in Initialization\nLet\u2019sbeginbycallingonbuilt-ininitializers. Thecodebelowinitializesallweightparame-\nters as Gaussian random variables with standard deviation 0.01, while bias parameters are\ncleared to zero.\ndef init_normal (module):\niftype (module) ==nn.Linear:\nnn.init .normal_(module .weight, mean =0, std =0.01 )\nnn.init .zeros_(module .bias)\nnet.apply(init_normal)\nnet[ 0].weight .data[ 0], net[ 0].bias .data[ 0]\n(tensor([ -0.0129 ,-0.0007 ,-0.0033 ,0.0276 ]), tensor( 0.))\nWe can also initialize all the parameters to a given constant value (say, 1).\ndef init_constant (module):\niftype (module) ==nn.Linear:\nnn.init .constant_(module .weight, 1)\nnn.init .zeros_(module .bias)\nnet.apply(init_constant)\nnet[ 0].weight .data[ 0], net[ 0].bias .data[ 0]\n(tensor([ 1.,1.,1.,1.]), tensor( 0.))\nWe can also apply different initializers for certain blocks. For example, below we initialize", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42735e49-ae12-4963-9434-79e9e24ece53": {"__data__": {"id_": "42735e49-ae12-4963-9434-79e9e24ece53", "embedding": null, "metadata": {"page_label": "218", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68b9aa22-13f6-494e-b3a8-6335e09829eb", "node_type": "4", "metadata": {"page_label": "218", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2f1116be3d5707d46d76d029aa3f408782eba0513f4b3fada0aa19a8e635c2d8", "class_name": "RelatedNodeInfo"}}, "text": "218 Builders\u2019 Guide\nthe first layer with the Xavier initializer and initialize the second layer to a constant value\nof 42.\ndef init_xavier (module):\niftype (module) ==nn.Linear:\nnn.init .xavier_uniform_(module .weight)\ndef init_42 (module):\niftype (module) ==nn.Linear:\nnn.init .constant_(module .weight, 42)\nnet[ 0].apply(init_xavier)\nnet[ 2].apply(init_42)\nprint (net[ 0].weight .data[ 0])\nprint (net[ 2].weight .data)\ntensor([ -0.0974 ,0.1707 ,0.5840 ,-0.5032 ])\ntensor([[ 42.,42.,42.,42.,42.,42.,42.,42.]])\nCustomInitialization\nSometimes,theinitializationmethodsweneedarenotprovidedbythedeeplearningframe-\nwork. In the example below, we define an initializer for any weight parameter \ud835\udc64using the\nfollowing strange distribution:\n\ud835\udc64\u00188>>> <\n>>>:\ud835\udc48\u00b95,10\u00bawith probability1\n4\n0 with probability1\n2\n\ud835\udc48\u00b9\u000010,\u00005\u00bawith probability1\n4(6.3.1)\nAgain, we implement a my_init function to apply to net.\ndef my_init (module):\niftype (module) ==nn.Linear:\nprint (\"Init \",*[(name, param .shape)\nfor name, param inmodule .named_parameters()][ 0])\nnn.init .uniform_(module .weight, -10,10)\nmodule .weight .data *=module .weight .data .abs() >=5\nnet.apply(my_init)\nnet[ 0].weight[: 2]\nInit weight torch .Size([ 8,4])\nInit weight torch .Size([ 1,8])\ntensor([[ 0.0000 ,-7.6364 ,-0.0000 ,-6.1206 ],\n[9.3516 ,-0.0000 ,5.1208 ,-8.4003 ]], grad_fn =<SliceBackward0 >)\nNote that we always have the option of setting parameters directly.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "968c399e-0c32-4318-a444-4df914f18a78": {"__data__": {"id_": "968c399e-0c32-4318-a444-4df914f18a78", "embedding": null, "metadata": {"page_label": "219", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c85331b-8854-4dab-bdf8-3280c407e252", "node_type": "4", "metadata": {"page_label": "219", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e13517f692eab5fd57b9a312085fa020cbd0c145afd86a398b576c045c09681d", "class_name": "RelatedNodeInfo"}}, "text": "219 Lazy Initialization\n113net[ 0].weight .data[:] +=1\nnet[ 0].weight .data[ 0,0]=42\nnet[ 0].weight .data[ 0]\ntensor([ 42.0000 ,-6.6364 ,1.0000 ,-5.1206 ])\n6.3.2Summary\nWe can initialize parameters using built-in and custom initializers.\n6.3.3Exercises\nLook up the online documentation for more built-in initializers.\nDiscussions113.\n6.4Lazy Initialization\nSofar,itmightseemthatwegotawaywithbeingsloppyinsettingupournetworks. Specif-\nically, we did the following unintuitive things, which might not seem like they should\nwork:\n\u000fWe defined the network architectures without specifying the input dimensionality.\n\u000fWe added layers without specifying the output dimension of the previous layer.\n\u000fWe even \u201cinitialized\u201d these parameters before providing enough information to deter-\nmine how many parameters our models should contain.\nYoumightbesurprisedthatourcoderunsatall. Afterall,thereisnowaythedeeplearning\nframework could tell what the input dimensionality of a network would be. The trick here\nis that the framework defersinitialization , waiting until the first time we pass data through\nthe model, to infer the sizes of each layer on the fly.\nLater on, when working with convolutional neural networks, this technique will become\neven more convenient since the input dimensionality (e.g., the resolution of an image) will\naffect the dimensionality of each subsequent layer. Hence the ability to set parameters\nwithout the need to know, at the time of writing the code, the value of the dimension can\ngreatly simplify the task of specifying and subsequently modifying our models. Next, we\ngo deeper into the mechanics of initialization.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d45e234-135b-4694-aaeb-b61782370124": {"__data__": {"id_": "2d45e234-135b-4694-aaeb-b61782370124", "embedding": null, "metadata": {"page_label": "220", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab439d72-3acb-4a1f-9886-3b8916fa44b3", "node_type": "4", "metadata": {"page_label": "220", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ce4a4f09489569c353541f493451ccdff3f826992776efcb36a23272080e09f6", "class_name": "RelatedNodeInfo"}}, "text": "220 Builders\u2019 Guide\nTo begin, let\u2019s instantiate an MLP.\nnet =nn.Sequential(nn .LazyLinear( 256), nn .ReLU(), nn .LazyLinear( 10))\nAtthispoint,thenetworkcannotpossiblyknowthedimensionsoftheinputlayer\u2019sweights\nbecause the input dimension remains unknown.\nConsequently the framework has not yet initialized any parameters. We confirm by at-\ntempting to access the parameters below.\nnet[ 0].weight\n<UninitializedParameter >\nNext let\u2019s pass data through the network to make the framework finally initialize parame-\nters.\nX=torch .rand( 2,20)\nnet(X)\nnet[ 0].weight .shape\ntorch .Size([ 256,20])\nAs soon as we know the input dimensionality, 20, the framework can identify the shape of\nthe first layer\u2019s weight matrix by plugging in the value of 20. Having recognized the first\nlayer\u2019s shape, the framework proceeds to the second layer, and so on through the computa-\ntional graph until all shapes are known. Note that in this case, only the first layer requires\nlazy initialization, but the framework initializes sequentially. Once all parameter shapes\nare known, the framework can finally initialize the parameters.\nThe following method passes in dummy inputs through the network for a dry run to infer\nallparametershapesandsubsequentlyinitializestheparameters. Itwillbeusedlaterwhen\ndefault random initializations are not desired.\n@d2l .add_to_class(d2l .Module) #@save\ndef apply_init (self , inputs, init =None ):\nself .forward( *inputs)\nifinit isnot None :\nself .net.apply(init)\n6.4.1Summary\nLazy initialization can be convenient, allowing the framework to infer parameter shapes\nautomatically, making it easy to modify architectures and eliminating one common source\nof errors. We can pass data through the model to make the framework finally initialize\nparameters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5cbc5313-5de9-42f9-b17a-59fa71e8f6ff": {"__data__": {"id_": "5cbc5313-5de9-42f9-b17a-59fa71e8f6ff", "embedding": null, "metadata": {"page_label": "221", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f789e0a7-0e28-47ae-ab59-3d0d9f179465", "node_type": "4", "metadata": {"page_label": "221", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1b48eb1a5f4974a39a267e1a0c1dbb044e003096db6a6f3d2eb56d00291dc469", "class_name": "RelatedNodeInfo"}}, "text": "221 Custom Layers\n1146.4.2Exercises\n1.Whathappensifyouspecifytheinputdimensionstothefirstlayerbutnottosubsequent\nlayers? Do you get immediate initialization?\n2.What happens if you specify mismatching dimensions?\n3.What would you need to do if you have input of varying dimensionality? Hint: look at\nthe parameter tying.\nDiscussions114.\n6.5Custom Layers\nOne factor behind deep learning\u2019s success is the availability of a wide range of layers that\ncan be composed in creative ways to design architectures suitable for a wide variety of\ntasks. For instance, researchers have invented layers specifically for handling images, text,\nlooping over sequential data, and performing dynamic programming. Sooner or later, you\nwill need a layer that does not exist yet in the deep learning framework. In these cases, you\nmust build a custom layer. In this section, we show you how.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n6.5.1Layerswithout Parameters\nTo start, we construct a custom layer that does not have any parameters of its own. This\nshouldlookfamiliarifyourecallourintroductiontomodulesin Section6.1 . Thefollowing\nCenteredLayer classsimplysubtractsthemeanfromitsinput. Tobuildit,wesimplyneed\ntoinheritfromthebaselayerclassandimplementtheforwardpropagationfunction.\nclass CenteredLayer (nn.Module):\ndef __init__ (self ):\nsuper ().__init__ ()\ndef forward (self , X):\nreturn X-X.mean()\nLet\u2019s verify that our layer works as intended by feeding some data through it.\nlayer =CenteredLayer()\nlayer(torch .tensor([ 1.0,2,3,4,5]))", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51c2d5f4-7088-440e-87d2-a4b4fc1c19e0": {"__data__": {"id_": "51c2d5f4-7088-440e-87d2-a4b4fc1c19e0", "embedding": null, "metadata": {"page_label": "222", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1f46888-af6a-478a-9d73-a6bf77e9220b", "node_type": "4", "metadata": {"page_label": "222", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b925996b4cc2810c719ff7ebe87b1227ef359a8b8814651c9425546b0f8f5db0", "class_name": "RelatedNodeInfo"}}, "text": "222 Builders\u2019 Guide\ntensor([ -2.,-1.,0.,1.,2.])\nWe can now incorporate our layer as a component in constructing more complex mod-\nels.\nnet =nn.Sequential(nn .LazyLinear( 128), CenteredLayer())\nAs an extra sanity check, we can send random data through the network and check that the\nmean is in fact 0. Because we are dealing with floating point numbers, we may still see a\nvery small nonzero number due to quantization.\nY=net(torch .rand( 4,8))\nY.mean()\ntensor( -6.5193e-09 , grad_fn =<MeanBackward0 >)\n6.5.2Layerswith Parameters\nNow that we know how to define simple layers, let\u2019s move on to defining layers with pa-\nrameters that can be adjusted through training. We can use built-in functions to create\nparameters, which provide some basic housekeeping functionality. In particular, they gov-\nern access, initialization, sharing, saving, and loading model parameters. This way, among\nother benefits, we will not need to write custom serialization routines for every custom\nlayer.\nNow let\u2019s implement our own version of the fully connected layer. Recall that this layer\nrequires two parameters, one to represent the weight and the other for the bias. In this im-\nplementation, we bake in the ReLU activation as a default. This layer requires two input\narguments: in_units andunits, which denote the number of inputs and outputs, respec-\ntively.\nclass MyLinear (nn.Module):\ndef __init__ (self , in_units, units):\nsuper ().__init__ ()\nself .weight =nn.Parameter(torch .randn(in_units, units))\nself .bias =nn.Parameter(torch .randn(units,))\ndef forward (self , X):\nlinear =torch .matmul(X, self .weight .data) +self .bias .data\nreturn F.relu(linear)\nNext, we instantiate the MyLinear class and access its model parameters.\nlinear =MyLinear( 5,3)\nlinear .weight", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e4b19dc-ba5b-4021-a3f8-b2746542b558": {"__data__": {"id_": "2e4b19dc-ba5b-4021-a3f8-b2746542b558", "embedding": null, "metadata": {"page_label": "223", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "450ca569-6d9f-47c5-9a23-8f041807a11c", "node_type": "4", "metadata": {"page_label": "223", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a945d6cfb5184b7ebe2a7bd400885f96e2625ce9ecd0c35185c0a2ad91df9382", "class_name": "RelatedNodeInfo"}}, "text": "223 File I/O\n115Parameter containing:\ntensor([[ 0.4783 ,0.4284 ,-0.0899 ],\n[-0.6347 ,0.2913 ,-0.0822 ],\n[-0.4325 ,-0.1645 ,-0.3274 ],\n[1.1898 ,0.6482 ,-1.2384 ],\n[-0.1479 ,0.0264 ,-0.9597 ]], requires_grad =True )\nWe can directly carry out forward propagation calculations using custom layers.\nlinear(torch .rand( 2,5))\ntensor([[ 0.0000 ,0.9316 ,0.0000 ],\n[0.1808 ,1.4208 ,0.0000 ]])\nWe can also construct models using custom layers. Once we have that we can use it just\nlike the built-in fully connected layer.\nnet =nn.Sequential(MyLinear( 64,8), MyLinear( 8,1))\nnet(torch .rand( 2,64))\ntensor([[ 0.0000 ],\n[13.0800 ]])\n6.5.3Summary\nWe can design custom layers via the basic layer class. This allows us to define flexible\nnew layers that behave differently from any existing layers in the library. Once defined,\ncustom layers can be invoked in arbitrary contexts and architectures. Layers can have local\nparameters, which can be created through built-in functions.\n6.5.4Exercises\n1.Design a layer that takes an input and computes a tensor reduction, i.e., it returns \ud835\udc66\ud835\udc58=\u00cd\n\ud835\udc56,\ud835\udc57\ud835\udc4a\ud835\udc56\ud835\udc57\ud835\udc58\ud835\udc65\ud835\udc56\ud835\udc65\ud835\udc57.\n2.Design a layer that returns the leading half of the Fourier coefficients of the data.\nDiscussions115.\n6.6File I/O\nSo far we have discussed how to process data and how to build, train, and test deep learn-\ning models. However, at some point we will hopefully be happy enough with the learned", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1378, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a15aa4ae-db5d-43be-83e9-2a94eaa4346e": {"__data__": {"id_": "a15aa4ae-db5d-43be-83e9-2a94eaa4346e", "embedding": null, "metadata": {"page_label": "224", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b6a2885-1200-4887-83db-4313e1e406c1", "node_type": "4", "metadata": {"page_label": "224", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4d2272d0d9bbf0a7ab0a3e7978e5742b4c4a3163b7d2ac39972ffb85bd58e50e", "class_name": "RelatedNodeInfo"}}, "text": "224 Builders\u2019 Guide\nmodels that we will want to save the results for later use in various contexts (perhaps even\nto make predictions in deployment). Additionally, when running a long training process,\nthe best practice is to periodically save intermediate results (checkpointing) to ensure that\nwe do not lose several days\u2019 worth of computation if we trip over the power cord of our\nserver. Thus it is time to learn how to load and store both individual weight vectors and\nentire models. This section addresses both issues.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\n6.6.1Loading and SavingTensors\nFor individual tensors, we can directly invoke the loadandsavefunctions to read and\nwrite them respectively. Both functions require that we supply a name, and saverequires\nas input the variable to be saved.\nx=torch .arange( 4)\ntorch .save(x, 'x-file ')\nWe can now read the data from the stored file back into memory.\nx2=torch .load( 'x-file ')\nx2\ntensor([ 0,1,2,3])\nWe can store a list of tensors and read them back into memory.\ny=torch .zeros( 4)\ntorch .save([x, y], 'x-files ')\nx2, y2 =torch .load( 'x-files ')\n(x2, y2)\n(tensor([ 0,1,2,3]), tensor([ 0.,0.,0.,0.]))\nWe can even write and read a dictionary that maps from strings to tensors. This is conve-\nnient when we want to read or write all the weights in a model.\nmydict ={'x': x, 'y': y}\ntorch .save(mydict, 'mydict ')\nmydict2 =torch .load( 'mydict ')\nmydict2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "724fd726-c38f-4352-bc36-99456f123971": {"__data__": {"id_": "724fd726-c38f-4352-bc36-99456f123971", "embedding": null, "metadata": {"page_label": "225", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc955964-ce62-4dbc-b97c-8ffe8153c8ea", "node_type": "4", "metadata": {"page_label": "225", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2dfb3e53ddc3515ea241428aec761f9c26ff04189f33b0f4e50d1ce56c4328e7", "class_name": "RelatedNodeInfo"}}, "text": "225 File I/O\n{'x': tensor([ 0,1,2,3]), 'y': tensor([ 0.,0.,0.,0.])}\n6.6.2Loading and SavingModel Parameters\nSaving individual weight vectors (or other tensors) is useful, but it gets very tedious if we\nwant to save (and later load) an entire model. After all, we might have hundreds of param-\neter groups sprinkled throughout. For this reason the deep learning framework provides\nbuilt-in functionalities to load and save entire networks. An important detail to note is that\nthis saves model parameters and not the entire model. For example, if we have a 3-layer\nMLP, we need to specify the architecture separately. The reason for this is that the models\nthemselves can contain arbitrary code, hence they cannot be serialized as naturally. Thus,\nin order to reinstate a model, we need to generate the architecture in code and then load the\nparameters from disk. Let\u2019s start with our familiar MLP.\nclass MLP(nn.Module):\ndef __init__ (self ):\nsuper ().__init__ ()\nself .hidden =nn.LazyLinear( 256)\nself .output =nn.LazyLinear( 10)\ndef forward (self , x):\nreturn self .output(F .relu( self .hidden(x)))\nnet =MLP()\nX=torch .randn(size =(2,20))\nY=net(X)\nNext, we store the parameters of the model as a file with the name \u201cmlp.params\u201d.\ntorch .save(net .state_dict(), 'mlp.params ')\nTo recover the model, we instantiate a clone of the original MLP model. Instead of ran-\ndomlyinitializingthemodelparameters,wereadtheparametersstoredinthefiledirectly.\nclone =MLP()\nclone .load_state_dict(torch .load( 'mlp.params '))\nclone .eval()\nMLP(\n(hidden): LazyLinear(in_features =0, out_features =256, bias =True )\n(output): LazyLinear(in_features =0, out_features =10, bias =True )\n)\nSincebothinstanceshavethesamemodelparameters,thecomputationalresultofthesame\ninput Xshould be the same. Let\u2019s verify this.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83328087-dd27-4a99-9547-6095b094bcca": {"__data__": {"id_": "83328087-dd27-4a99-9547-6095b094bcca", "embedding": null, "metadata": {"page_label": "226", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f924cb6-2415-4d94-89ca-55397bc16c6a", "node_type": "4", "metadata": {"page_label": "226", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f1684704a6151b5f0943ab359c242dc9cc1e9dbb48c8691d0af4ebfeac607159", "class_name": "RelatedNodeInfo"}}, "text": "226 Builders\u2019 Guide\n116\n117Y_clone =clone(X)\nY_clone ==Y\ntensor([[ True ,True ,True ,True ,True ,True ,True ,True ,True ,True ],\n[True ,True ,True ,True ,True ,True ,True ,True ,True ,True ]])\n6.6.3Summary\nThesaveandloadfunctions can be used to perform file I/O for tensor objects. We can\nsaveandloadtheentiresetsofparametersforanetworkviaaparameterdictionary. Saving\nthe architecture has to be done in code rather than in parameters.\n6.6.4Exercises\n1.Even if there is no need to deploy trained models to a different device, what are the\npractical benefits of storing model parameters?\n2.Assume that we want to reuse only parts of a network to be incorporated into a network\nhaving a different architecture. How would you go about using, say the first two layers\nfrom a previous network in a new network?\n3.Howwouldyougoaboutsavingthenetworkarchitectureandparameters? Whatrestric-\ntions would you impose on the architecture?\nDiscussions116.\n6.7GPUs\nIntab_intro_decade , we illustrated the rapid growth of computation over the past two\ndecades. In a nutshell, GPU performance has increased by a factor of 1000 every decade\nsince 2000. This offers great opportunities but it also suggests that there was significant\ndemand for such performance.\nInthissection,webegintodiscusshowtoharnessthiscomputationalperformanceforyour\nresearch. First by using a single GPU and at a later point, how to use multiple GPUs and\nmultiple servers (with multiple GPUs).\nSpecifically, we will discuss how to use a single NVIDIA GPU for calculations. First,\nmake sure you have at least one NVIDIA GPU installed. Then, download the NVIDIA\ndriverandCUDA117andfollowthepromptstosettheappropriatepath. Oncetheseprepa-\nrations are complete, the nvidia-smi command can be used to view the graphics card\ninformation.\nIn PyTorch, every array has a device; we often refer it as a context. So far, by default, all", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39fc51ca-2f16-4bc3-a15e-06f1bba31104": {"__data__": {"id_": "39fc51ca-2f16-4bc3-a15e-06f1bba31104", "embedding": null, "metadata": {"page_label": "227", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "94b14313-2a41-4826-adab-0e98b7b93609", "node_type": "4", "metadata": {"page_label": "227", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6f9a3e4f40609fbf54aab6d9eb66c2b3fa91505e19172cd7172edef26d01ccf1", "class_name": "RelatedNodeInfo"}}, "text": "227 GPUs\nvariables and associated computation have been assigned to the CPU. Typically, other con-\ntextsmightbevariousGPUs. Thingscangetevenhairierwhenwedeployjobsacrossmul-\ntiple servers. By assigning arrays to contexts intelligently, we can minimize the time spent\ntransferring data between devices. For example, when training neural networks on a server\nwith a GPU, we typically prefer for the model\u2019s parameters to live on the GPU.\nTo run the programs in this section, you need at least two GPUs. Note that this might\nbe extravagant for most desktop computers but it is easily available in the cloud, e.g., by\nusingtheAWSEC2multi-GPUinstances. Almostallothersectionsdo notrequiremultiple\nGPUs, but here we simply wish to illustrate data flow between different devices.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n6.7.1ComputingDevices\nWe can specify devices, such as CPUs and GPUs, for storage and calculation. By default,\ntensors are created in the main memory and then the CPU is used for calculations.\nIn PyTorch, the CPU and GPU can be indicated by torch.device('cpu') andtorch.\ndevice('cuda') . It should be noted that the cpudevice means all physical CPUs and\nmemory. This means that PyTorch\u2019s calculations will try to use all CPU cores. However, a\ngpudevice only represents one card and the corresponding memory. If there are multiple\nGPUs, we use torch.device(f'cuda:{i}') to represent the \ud835\udc56thGPU (\ud835\udc56starts at 0). Also,\ngpu:0andgpuare equivalent.\ndef cpu(): #@save\n\"\"\"Get the CPU device.\"\"\"\nreturn torch .device( 'cpu')\ndef gpu(i=0): #@save\n\"\"\"Get a GPU device.\"\"\"\nreturn torch .device( f'cuda: {i}')\ncpu(), gpu(), gpu( 1)\n(device( type ='cpu'),\ndevice( type ='cuda ', index =0),\ndevice( type ='cuda ', index =1))\nWe can query the number of available GPUs.\ndef num_gpus (): #@save\n\"\"\"Get the number of available GPUs.\"\"\"\nreturn torch .cuda .device_count()\nnum_gpus()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1895, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43fe4339-f785-4a70-9c9b-721a95717416": {"__data__": {"id_": "43fe4339-f785-4a70-9c9b-721a95717416", "embedding": null, "metadata": {"page_label": "228", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cba11a72-fbbc-4353-a8e7-5295c2e84400", "node_type": "4", "metadata": {"page_label": "228", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "229c330f6a177d43735bf7d8f426b820d5b4d67a7fce4cb76285c156836d5ea5", "class_name": "RelatedNodeInfo"}}, "text": "228 Builders\u2019 Guide\n2\nNow we define two convenient functions that allow us to run code even if the requested\nGPUs do not exist.\ndef try_gpu (i=0): #@save\n\"\"\"Return gpu(i) if exists, otherwise return cpu().\"\"\"\nifnum_gpus() >=i+1:\nreturn gpu(i)\nreturn cpu()\ndef try_all_gpus (): #@save\n\"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\nreturn [gpu(i) for iinrange (num_gpus())]\ntry_gpu(), try_gpu( 10), try_all_gpus()\n(device( type ='cuda ', index =0),\ndevice( type ='cpu'),\n[device( type ='cuda ', index =0), device( type ='cuda ', index =1)])\n6.7.2Tensorsand GPUs\nBy default, tensors are created on the CPU. We can query the device where the tensor is\nlocated.\nx=torch .tensor([ 1,2,3])\nx.device\ndevice( type ='cpu')\nIt is important to note that whenever we want to operate on multiple terms, they need to be\non the same device. For instance, if we sum two tensors, we need to make sure that both\narguments live on the same device\u2014otherwise the framework would not know where to\nstore the result or even how to decide where to perform the computation.\nStorageon the GPU\nThere are several ways to store a tensor on the GPU. For example, we can specify a stor-\nage device when creating a tensor. Next, we create the tensor variable Xon the first gpu.\nThe tensor created on a GPU only consumes the memory of this GPU. We can use the\nnvidia-smi command to view GPU memory usage. In general, we need to make sure that\nwe do not create data that exceeds the GPU memory limit.\nX=torch .ones( 2,3, device =try_gpu())\nX", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1523, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4de1a22-e54e-433b-86cf-deb3f493fc59": {"__data__": {"id_": "a4de1a22-e54e-433b-86cf-deb3f493fc59", "embedding": null, "metadata": {"page_label": "229", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e83c011-99f1-4220-ba0a-f14481afd823", "node_type": "4", "metadata": {"page_label": "229", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "33a177f2f69fda97c92d59978341423c620ea1665517f19bc1fb7f72c8f64d27", "class_name": "RelatedNodeInfo"}}, "text": "229 GPUs\ntensor([[ 1.,1.,1.],\n[1.,1.,1.]], device ='cuda:0 ')\nAssuming that you have at least two GPUs, the following code will create a random tensor,\nY, on the second GPU.\nY=torch .rand( 2,3, device =try_gpu( 1))\nY\ntensor([[ 0.0022 ,0.5723 ,0.2890 ],\n[0.1456 ,0.3537 ,0.7359 ]], device ='cuda:1 ')\nCopying\nIf we want to compute X + Y, we need to decide where to perform this operation. For\ninstance, as shown in Fig. 6.7.1 , we can transfer Xto the second GPU and perform the\noperation there. Do notsimply add XandY, since this will result in an exception. The\nruntime engine would not know what to do: it cannot find data on the same device and it\nfails. Since Ylives on the second GPU, we need to move Xthere before we can add the\ntwo.\ntFig. 6.7.1 Copy data to perform an operation on the same device.\nZ=X.cuda( 1)\nprint (X)\nprint (Z)\ntensor([[ 1.,1.,1.],\n[1.,1.,1.]], device ='cuda:0 ')\ntensor([[ 1.,1.,1.],\n[1.,1.,1.]], device ='cuda:1 ')\nNow that the data (both ZandY) are on the same GPU), we can add them up.\nY+Z\ntensor([[ 1.0022 ,1.5723 ,1.2890 ],\n[1.1456 ,1.3537 ,1.7359 ]], device ='cuda:1 ')", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ea0b164-6607-4970-9c5a-18e0df76e625": {"__data__": {"id_": "6ea0b164-6607-4970-9c5a-18e0df76e625", "embedding": null, "metadata": {"page_label": "230", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e657479d-ed7e-4588-9e02-315b87303f97", "node_type": "4", "metadata": {"page_label": "230", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "585d3e6d6877518f9aa01e00df210faaff828e2eb2937e63b3939a6218c9eebd", "class_name": "RelatedNodeInfo"}}, "text": "230 Builders\u2019 Guide\nButwhatifyourvariable ZalreadylivedonyoursecondGPU?Whathappensifwestillcall\nZ.cuda(1) ? It will return Zinstead of making a copy and allocating new memory.\nZ.cuda( 1)isZ\nTrue\nSide Notes\nPeople use GPUs to do machine learning because they expect them to be fast. But trans-\nferring variables between devices is slow: much slower than computation. So we want you\nto be 100% certain that you want to do something slow before we let you do it. If the deep\nlearning framework just did the copy automatically without crashing then you might not\nrealize that you had written some slow code.\nTransferring data is not only slow, it also makes parallelization a lot more difficult, since\nwe have to wait for data to be sent (or rather to be received) before we can proceed with\nmore operations. This is why copy operations should be taken with great care. As a rule of\nthumb, many small operations are much worse than one big operation. Moreover, several\noperations at a time are much better than many single operations interspersed in the code\nunless you know what you are doing. This is the case since such operations can block if\none device has to wait for the other before it can do something else. It is a bit like ordering\nyour coffee in a queue rather than pre-ordering it by phone and finding out that it is ready\nwhen you are.\nLast, whenweprinttensorsorconverttensorstotheNumPyformat, ifthedataisnotinthe\nmain memory, the framework will copy it to the main memory first, resulting in additional\ntransmission overhead. Even worse, it is now subject to the dreaded global interpreter lock\nthat makes everything wait for Python to complete.\n6.7.3NeuralNetworksand GPUs\nSimilarly, a neural network model can specify devices. The following code puts the model\nparameters on the GPU.\nnet =nn.Sequential(nn .LazyLinear( 1))\nnet =net.to(device =try_gpu())\nWewillseemanymoreexamplesofhowtorunmodelsonGPUsinthefollowingchapters,\nsimplybecausethemodelswillbecomesomewhatmorecomputationallyintensive.\nFor example, when the input is a tensor on the GPU, the model will calculate the result on\nthe same GPU.\nnet(X)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0cc9fbe-d9f1-4e0a-bf00-d0c9fdd87543": {"__data__": {"id_": "c0cc9fbe-d9f1-4e0a-bf00-d0c9fdd87543", "embedding": null, "metadata": {"page_label": "231", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88cf75b4-4341-4b39-add8-71c8b8d85465", "node_type": "4", "metadata": {"page_label": "231", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cb1787ff8e61d404e9d9b1844b2635ca96b0fa137d0f78adc35b6faa25837f67", "class_name": "RelatedNodeInfo"}}, "text": "231 GPUs\ntensor([[ 0.7802 ],\n[0.7802 ]], device ='cuda:0 ', grad_fn =<AddmmBackward0 >)\nLet\u2019s confirm that the model parameters are stored on the same GPU.\nnet[ 0].weight .data .device\ndevice( type ='cuda ', index =0)\nLet the trainer support GPU.\n@d2l .add_to_class(d2l .Trainer) #@save\ndef __init__ (self , max_epochs, num_gpus =0, gradient_clip_val =0):\nself .save_hyperparameters()\nself .gpus =[d2l .gpu(i) for iinrange (min(num_gpus, d2l .num_gpus()))]\n@d2l .add_to_class(d2l .Trainer) #@save\ndef prepare_batch (self , batch):\nifself .gpus:\nbatch =[a.to(self .gpus[ 0])for ainbatch]\nreturn batch\n@d2l .add_to_class(d2l .Trainer) #@save\ndef prepare_model (self , model):\nmodel .trainer =self\nmodel .board .xlim =[0,self .max_epochs]\nifself .gpus:\nmodel .to(self .gpus[ 0])\nself .model =model\nIn short, as long as all data and parameters are on the same device, we can learn models\nefficiently. In the following chapters we will see several such examples.\n6.7.4Summary\nWe can specify devices for storage and calculation, such as the CPU or GPU. By default,\ndata is created in the main memory and then uses the CPU for calculations. The deep\nlearning framework requires all input data for calculation to be on the same device, be it\nCPU or the same GPU. You can lose significant performance by moving data without care.\nA typical mistake is as follows: computing the loss for every minibatch on the GPU and\nreporting it back to the user on the command line (or logging it in a NumPy ndarray ) will\ntriggeraglobalinterpreterlockwhichstallsallGPUs. Itismuchbettertoallocatememory\nfor logging inside the GPU and only move larger logs.\n6.7.5Exercises\n1.Try a larger computation task, such as the multiplication of large matrices, and see the\ndifference in speed between the CPU and GPU. What about a task with a small number\nof calculations?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1838, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c14ea4b0-5afd-488d-b81a-a748eb84949f": {"__data__": {"id_": "c14ea4b0-5afd-488d-b81a-a748eb84949f", "embedding": null, "metadata": {"page_label": "232", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18da970b-7bec-43c3-ae49-20589d503edf", "node_type": "4", "metadata": {"page_label": "232", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9af4d0af3eef045cc039a3415d7745d314ffd871beaee6604d65517cd762206e", "class_name": "RelatedNodeInfo"}}, "text": "232 Builders\u2019 Guide\n1182.How should we read and write model parameters on the GPU?\n3.Measure the time it takes to compute 1000 matrix\u2013matrix multiplications of 100\u0002100\nmatricesandlogtheFrobeniusnormoftheoutputmatrixoneresultatatime. Compare\nit with keeping a log on the GPU and transferring only the final result.\n4.Measure how much time it takes to perform two matrix\u2013matrix multiplications on two\nGPUs at the same time. Compare it with computing in in sequence on one GPU. Hint:\nyou should see almost linear scaling.\nDiscussions118.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 534, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7227e4c-0a55-4c37-aa59-3592b87c2d75": {"__data__": {"id_": "d7227e4c-0a55-4c37-aa59-3592b87c2d75", "embedding": null, "metadata": {"page_label": "233", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2481236-722f-4a2f-986d-8f9df3fcf96c", "node_type": "4", "metadata": {"page_label": "233", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "de376cf61e02ac46eebd9ca16e903a8c9242810c0b087241e008cc5e29f380ac", "class_name": "RelatedNodeInfo"}}, "text": "7 Convolutional Neural Networks\nImage data is represented as a two-dimensional grid of pixels, be the image monochro-\nmatic or in color. Accordingly each pixel corresponds to one or multiple numerical values\nrespectively. So far we have ignored this rich structure and treated images as vectors of\nnumbersby flattening them, irrespectiveofthe spatial relation betweenpixels. This deeply\nunsatisfyingapproachwasnecessaryinordertofeedtheresultingone-dimensionalvectors\nthrough a fully connected MLP.\nBecause these networks are invariant to the order of the features, we could get similar\nresults regardless of whether we preserve an order corresponding to the spatial structure\nof the pixels or if we permute the columns of our design matrix before fitting the MLP\u2019s\nparameters. Ideally,wewouldleverageourpriorknowledgethatnearbypixelsaretypically\nrelated to each other, to build efficient models for learning from image data.\nThis chapter introduces convolutional neural networks (CNNs) ( LeCunet al., 1995), a\npowerful family of neural networks that are designed for precisely this purpose. CNN-\nbased architectures are now ubiquitous in the field of computer vision. For instance, on the\nImagnetcollection( Dengetal.,2009)itwasonlytheuseofconvolutionalneuralnetworks,\nin short Convnets, that provided significant performance improvements ( Krizhevsky etal.,\n2012).\nModern CNNs, as they are called colloquially, owe their design to inspirations from biol-\nogy,grouptheory,andahealthydoseofexperimentaltinkering. Inadditiontotheirsample\nefficiency in achieving accurate models, CNNs tend to be computationally efficient, both\nbecause they require fewer parameters than fully connected architectures and because con-\nvolutions are easy to parallelize across GPU cores ( Chetluret al., 2014). Consequently,\npractitioners often apply CNNs whenever possible, and increasingly they have emerged\nas credible competitors even on tasks with a one-dimensional sequence structure, such as\naudio (Abdel-Hamid et al., 2014), text (Kalchbrenner et al., 2014), and time series analy-\nsis (LeCunet al., 1995), where recurrent neural networks are conventionally used. Some\nclever adaptations of CNNs have also brought them to bear on graph-structured data ( Kipf\nand Welling, 2016 ) and in recommender systems.\nFirst, wewilldivemoredeeplyintothemotivationforconvolutionalneuralnetworks. This\nis followed by a walk through the basic operations that comprise the backbone of all con-\nvolutional networks. These include the convolutional layersthemselves, nitty-gritty details\nincluding padding and stride, the pooling layers used to aggregate information across ad-\njacent spatial regions, the use of multiple channels at each layer, and a careful discussion\n233", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bdb2bfc-9dc4-456f-b591-f588cf284bf2": {"__data__": {"id_": "6bdb2bfc-9dc4-456f-b591-f588cf284bf2", "embedding": null, "metadata": {"page_label": "234", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1af1e19a-d55f-40a9-b23f-73e4f9fbff71", "node_type": "4", "metadata": {"page_label": "234", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a92463e0533bcf933f55d938e026eee379f252066aaacd340e1a6c989ccc4254", "class_name": "RelatedNodeInfo"}}, "text": "234 Convolutional Neural Networks\nof the structure of modern architectures. We will conclude the chapter with a full working\nexample of LeNet, the first convolutional network successfully deployed, long before the\nriseofmoderndeeplearning. Inthenextchapter, wewilldiveintofullimplementationsof\nsome popular and comparatively recent CNN architectures whose designs represent most\nof the techniques commonly used by modern practitioners.\n7.1FromFullyConnected Layersto Convolutions\nTo this day, the models that we have discussed so far remain appropriate options when we\nare dealing with tabular data. By tabular, we mean that the data consist of rows corre-\nsponding to examples and columns corresponding to features. With tabular data, we might\nanticipate that the patterns we seek could involve interactions among the features, but we\ndo not assume any structure a priori concerning how the features interact.\nSometimes, we truly lack the knowledge to be able to guide the construction of fancier\narchitectures. In these cases, an MLP may be the best that we can do. However, for high-\ndimensional perceptual data, such structureless networks can grow unwieldy.\nFor instance, let\u2019s return to our running example of distinguishing cats from dogs. Say that\nwe do a thorough job in data collection, collecting an annotated dataset of one-megapixel\nphotographs. This means that each input to the network has one million dimensions. Even\nanaggressivereductiontoonethousandhiddendimensionswouldrequireafullyconnected\nlayercharacterizedby 106\u0002103=109parameters. UnlesswehavelotsofGPUs,atalentfor\ndistributed optimization, and an extraordinary amount of patience, learning the parameters\nof this network may turn out to be infeasible.\nA careful reader might object to this argument on the basis that one megapixel resolution\nmay not be necessary. However, while we might be able to get away with one hundred\nthousand pixels, our hidden layer of size 1000 grossly underestimates the number of hid-\nden units that it takes to learn good representations of images, so a practical system will\nstill require billions of parameters. Moreover, learning a classifier by fitting so many pa-\nrameters might require collecting an enormous dataset. And yet today both humans and\ncomputers are able to distinguish cats from dogs quite well, seemingly contradicting these\nintuitions. That is because images exhibit rich structure that can be exploited by humans\nand machine learning models alike. Convolutional neural networks (CNNs) are one cre-\native way that machine learning has embraced for exploiting some of the known structure\nin natural images.\n7.1.1Invariance\nImagine that we want to detect an object in an image. It seems reasonable that whatever\nmethodweusetorecognizeobjectsshouldnotbeoverlyconcernedwiththepreciselocation\nof the object in the image. Ideally, our system should exploit this knowledge. Pigs usually\ndo not fly and planes usually do not swim. Nonetheless, we should still recognize a pig", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2994, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40dd4549-8e59-4be3-b957-6d41371f357e": {"__data__": {"id_": "40dd4549-8e59-4be3-b957-6d41371f357e", "embedding": null, "metadata": {"page_label": "235", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee2c0684-bb02-4c30-ae8b-20050ba6dd1d", "node_type": "4", "metadata": {"page_label": "235", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "68a27f03ae0fe62fdb686e3b84c8f14e57c53c3d574038ea018deb5a7cae0eed", "class_name": "RelatedNodeInfo"}}, "text": "235 From Fully Connected Layers to Convolutions\nwere one to appear at the top of the image. We can draw some inspiration here from the\nchildren\u2019sgame\u201cWhere\u2019sWaldo\u201d(whichitselfhasinspiredmanyreal-lifeimitations,such\nas that depicted in Fig. 7.1.1 ). The game consists of a number of chaotic scenes bursting\nwith activities. Waldo shows up somewhere in each, typically lurking in some unlikely\nlocation. The reader\u2019s goal is to locate him. Despite his characteristic outfit, this can be\nsurprisingly difficult, due to the large number of distractions. However, what Waldo looks\nlikedoesnotdependupon whereWaldoislocated . WecouldsweeptheimagewithaWaldo\ndetector that could assign a score to each patch, indicating the likelihood that the patch\ncontains Waldo. In fact, many object detection and segmentation algorithms are based\non this approach ( Longet al., 2015). CNNs systematize this idea of spatial invariance ,\nexploiting it to learn useful representations with fewer parameters.\ntFig. 7.1.1 Can you \ufb01nd Waldo (image courtesy of William Murphy (Infomatique))?\nWe can now make these intuitions more concrete by enumerating a few desiderata to guide\nour design of a neural network architecture suitable for computer vision:\n1.Intheearliestlayers,ournetworkshouldrespondsimilarlytothesamepatch,regardless\nof where it appears in the image. This principle is called translation invariance (or\ntranslationequivariance ).\n2.The earliest layers of the network should focus on local regions, without regard for the\ncontents of the image in distant regions. This is the localityprinciple. Eventually, these\nlocal representations can be aggregated to make predictions at the whole image level.\n3.As we proceed, deeper layers should be able to capture longer-range features of the\nimage, in a way similar to higher level vision in nature.\nLet\u2019s see how this translates into mathematics.\n7.1.2Constraining the MLP\nTostartoff,wecanconsideranMLPwithtwo-dimensionalimages Xasinputsandtheirim-\nmediatehiddenrepresentations Hsimilarlyrepresentedasmatrices(theyaretwo-dimensional", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24a75ce5-5b5a-466c-a1e9-29c0c1483f4e": {"__data__": {"id_": "24a75ce5-5b5a-466c-a1e9-29c0c1483f4e", "embedding": null, "metadata": {"page_label": "236", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d9dee5a-cb80-437e-800c-51aeedbbee79", "node_type": "4", "metadata": {"page_label": "236", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c549d61cf0a14cc9e7f8fff5dfb43c2e56d954b36be996e73eafc0c399695f5d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0f12eff9-ff9f-4b16-94ea-fa40c478e339", "node_type": "1", "metadata": {}, "hash": "316995f3b7a9bc6e524c2a8f6de46b1cd054374449bf2ae1d767022a2940ea2a", "class_name": "RelatedNodeInfo"}}, "text": "236 Convolutional Neural Networks\ntensors in code), where both XandHhave the same shape. Let that sink in. We now\nimagine that not only the inputs but also the hidden representations possess spatial struc-\nture.\nLet\u00bbX\u00bc\ud835\udc56,\ud835\udc57and\u00bbH\u00bc\ud835\udc56,\ud835\udc57denote the pixel at location \u00b9\ud835\udc56,\ud835\udc57\u00bain the input image and hidden rep-\nresentation,respectively. Consequently,tohaveeachofthehiddenunitsreceiveinputfrom\neachoftheinputpixels,wewouldswitchfromusingweightmatrices(aswedidpreviously\nin MLPs) to representing our parameters as fourth-order weight tensors W. Suppose that\nUcontains biases, we could formally express the fully connected layer as\n\u00bbH\u00bc\ud835\udc56,\ud835\udc57=\u00bbU\u00bc\ud835\udc56,\ud835\udc57\u00b8\u00d5\n\ud835\udc58\u00d5\n\ud835\udc59\u00bbW\u00bc\ud835\udc56,\ud835\udc57,\ud835\udc58,\ud835\udc59\u00bbX\u00bc\ud835\udc58,\ud835\udc59\n=\u00bbU\u00bc\ud835\udc56,\ud835\udc57\u00b8\u00d5\n\ud835\udc4e\u00d5\n\ud835\udc4f\u00bbV\u00bc\ud835\udc56,\ud835\udc57,\ud835\udc4e,\ud835\udc4f\u00bbX\u00bc\ud835\udc56\u00b8\ud835\udc4e,\ud835\udc57\u00b8\ud835\udc4f.(7.1.1)\nTheswitchfrom WtoVisentirelycosmeticfornowsincethereisaone-to-onecorrespon-\ndence between coefficients in both fourth-order tensors. We simply re-index the subscripts\n\u00b9\ud835\udc58,\ud835\udc59\u00basuch that\ud835\udc58=\ud835\udc56\u00b8\ud835\udc4eand\ud835\udc59=\ud835\udc57\u00b8\ud835\udc4f. In other words, we set \u00bbV\u00bc\ud835\udc56,\ud835\udc57,\ud835\udc4e,\ud835\udc4f =\u00bbW\u00bc\ud835\udc56,\ud835\udc57,\ud835\udc56\u00b8\ud835\udc4e,\ud835\udc57\u00b8\ud835\udc4f.\nThe indices\ud835\udc4eand\ud835\udc4frun over both positive and negative offsets, covering the entire image.\nFor any given location ( \ud835\udc56,\ud835\udc57) in the hidden representation \u00bbH\u00bc\ud835\udc56,\ud835\udc57, we compute its value by\nsumming over pixels in \ud835\udc65, centered around\u00b9\ud835\udc56,\ud835\udc57\u00baand weighted by\u00bbV\u00bc\ud835\udc56,\ud835\udc57,\ud835\udc4e,\ud835\udc4f. Before we\ncarry on, let\u2019s consider the total number of parameters required for a singlelayer in this\nparametrization: a 1000\u00021000image (1 megapixel) is mapped to a 1000\u00021000hidden\nrepresentation. This requires 1012parameters, far beyond what computers currently can\nhandle.\nTranslationInvariance\nNow let\u2019s invoke the first principle established above: translation invariance ( Zhanget al.,\n1988). This implies that a shift in the input Xshould simply lead to a shift in the hidden\nrepresentation H. This is only possible if VandUdo not actually depend on \u00b9\ud835\udc56,\ud835\udc57\u00ba. As\nsuch, we have\u00bbV\u00bc\ud835\udc56,\ud835\udc57,\ud835\udc4e,\ud835\udc4f =\u00bbV\u00bc\ud835\udc4e,\ud835\udc4fandUis a constant, say \ud835\udc62. As a result, we can simplify\nthe definition for H:\n\u00bbH\u00bc\ud835\udc56,\ud835\udc57=\ud835\udc62\u00b8\u00d5\n\ud835\udc4e\u00d5\n\ud835\udc4f\u00bbV\u00bc\ud835\udc4e,\ud835\udc4f\u00bbX\u00bc\ud835\udc56\u00b8\ud835\udc4e,\ud835\udc57\u00b8\ud835\udc4f.(7.1.2)\nThis is aconvolution ! We are effectively weighting pixels at \u00b9\ud835\udc56\u00b8\ud835\udc4e,\ud835\udc57\u00b8\ud835\udc4f\u00bain the vicinity of\nlocation\u00b9\ud835\udc56,\ud835\udc57\u00bawith coefficients\u00bbV\u00bc\ud835\udc4e,\ud835\udc4fto obtain the value\u00bbH\u00bc\ud835\udc56,\ud835\udc57. Note that\u00bbV\u00bc\ud835\udc4e,\ud835\udc4fneeds\nmany fewer coefficients than \u00bbV\u00bc\ud835\udc56,\ud835\udc57,\ud835\udc4e,\ud835\udc4fsince it no longer depends on the location within\nthe image. Consequently, the number of parameters required is no longer 1012but a much\nmore reasonable 4\u0002106: we still have the dependency on \ud835\udc4e,\ud835\udc4f2\u00b9\u0000 1000,1000\u00ba. In short,\nwe have made significant progress.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2377, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f12eff9-ff9f-4b16-94ea-fa40c478e339": {"__data__": {"id_": "0f12eff9-ff9f-4b16-94ea-fa40c478e339", "embedding": null, "metadata": {"page_label": "236", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d9dee5a-cb80-437e-800c-51aeedbbee79", "node_type": "4", "metadata": {"page_label": "236", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c549d61cf0a14cc9e7f8fff5dfb43c2e56d954b36be996e73eafc0c399695f5d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "24a75ce5-5b5a-466c-a1e9-29c0c1483f4e", "node_type": "1", "metadata": {"page_label": "236", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2c8271a4720bc2db600a7973e142be7c31adf0e7aa8d39234de6a588b56f4dc1", "class_name": "RelatedNodeInfo"}}, "text": "(7.1.2)\nThis is aconvolution ! We are effectively weighting pixels at \u00b9\ud835\udc56\u00b8\ud835\udc4e,\ud835\udc57\u00b8\ud835\udc4f\u00bain the vicinity of\nlocation\u00b9\ud835\udc56,\ud835\udc57\u00bawith coefficients\u00bbV\u00bc\ud835\udc4e,\ud835\udc4fto obtain the value\u00bbH\u00bc\ud835\udc56,\ud835\udc57. Note that\u00bbV\u00bc\ud835\udc4e,\ud835\udc4fneeds\nmany fewer coefficients than \u00bbV\u00bc\ud835\udc56,\ud835\udc57,\ud835\udc4e,\ud835\udc4fsince it no longer depends on the location within\nthe image. Consequently, the number of parameters required is no longer 1012but a much\nmore reasonable 4\u0002106: we still have the dependency on \ud835\udc4e,\ud835\udc4f2\u00b9\u0000 1000,1000\u00ba. In short,\nwe have made significant progress. Time-delay neural networks (TDNNs) are some of the\nfirst examples to exploit this idea ( Waibeletal., 1989).\nLocality\nNow let\u2019s invoke the second principle: locality. As motivated above, we believe that we\nshould not have to look very far away from location \u00b9\ud835\udc56,\ud835\udc57\u00bain order to glean relevant infor-", "mimetype": "text/plain", "start_char_idx": 1901, "end_char_idx": 2674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ade65fc-7423-4f68-bc86-5fa719f1bb42": {"__data__": {"id_": "1ade65fc-7423-4f68-bc86-5fa719f1bb42", "embedding": null, "metadata": {"page_label": "237", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "10274fe2-f588-4446-9cc7-85f785c0bb2c", "node_type": "4", "metadata": {"page_label": "237", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "448d5db7ad24f572e57fe77e5205a8e35251c7b462d47dd365323388d2de4727", "class_name": "RelatedNodeInfo"}}, "text": "237 From Fully Connected Layers to Convolutions\nmation to assess what is going on at \u00bbH\u00bc\ud835\udc56,\ud835\udc57. This means that outside some range j\ud835\udc4ej>\u0394\norj\ud835\udc4fj>\u0394, we should set\u00bbV\u00bc\ud835\udc4e,\ud835\udc4f=0. Equivalently, we can rewrite \u00bbH\u00bc\ud835\udc56,\ud835\udc57as\n\u00bbH\u00bc\ud835\udc56,\ud835\udc57=\ud835\udc62\u00b8\u0394\u00d5\n\ud835\udc4e=\u0000\u0394\u0394\u00d5\n\ud835\udc4f=\u0000\u0394\u00bbV\u00bc\ud835\udc4e,\ud835\udc4f\u00bbX\u00bc\ud835\udc56\u00b8\ud835\udc4e,\ud835\udc57\u00b8\ud835\udc4f. (7.1.3)\nThisreducesthenumberofparametersfrom 4\u0002106to4\u03942,where \u0394istypicallysmallerthan\n10. As such, we reduced the number of parameters by another four orders of magnitude.\nNote that (7.1.3 ), is what is called, in a nutshell, a convolutional layer .Convolutional\nneuralnetworks (CNNs) are a special family of neural networks that contain convolutional\nlayers. In the deep learning research community, Vis referred to as a convolution kernel ,\nafilter, or simply the layer\u2019s weightsthat are learnable parameters.\nWhile previously, we might have required billions of parameters to represent just a single\nlayer in an image-processing network, we now typically need just a few hundred, without\naltering the dimensionality of either the inputs or the hidden representations. The price\npaidforthisdrasticreductioninparametersisthatourfeaturesarenowtranslationinvariant\nand that our layer can only incorporate local information, when determining the value of\neach hidden activation. All learning depends on imposing inductive bias. When that bias\nagrees with reality, we get sample-efficient models that generalize well to unseen data. But\nof course, if those biases do not agree with reality, e.g., if images turned out not to be\ntranslation invariant, our models might struggle even to fit our training data.\nThisdramaticreductioninparametersbringsustoourlastdesideratum,namelythatdeeper\nlayersshouldrepresentlargerandmorecomplexaspectsofanimage. Thiscanbeachieved\nby interleaving nonlinearities and convolutional layers repeatedly.\n7.1.3Convolutions\nLet\u2019s briefly review why (7.1.3 )is called a convolution. In mathematics, the convolution\nbetween two functions ( Rudin, 1973 ), say\ud835\udc53,\ud835\udc54:R\ud835\udc51!Ris defined as\n\u00b9\ud835\udc53\u0003\ud835\udc54\u00ba\u00b9x\u00ba=\u00b9\n\ud835\udc53\u00b9z\u00ba\ud835\udc54\u00b9x\u0000z\u00ba\ud835\udc51z. (7.1.4)\nThatis,wemeasuretheoverlapbetween \ud835\udc53and\ud835\udc54whenonefunctionis\u201cflipped\u201dandshifted\nbyx. Whenever we have discrete objects, the integral turns into a sum. For instance, for\nvectors from the set of square-summable infinite-dimensional vectors with index running\noverZwe obtain the following definition:\n\u00b9\ud835\udc53\u0003\ud835\udc54\u00ba\u00b9\ud835\udc56\u00ba=\u00d5\n\ud835\udc4e\ud835\udc53\u00b9\ud835\udc4e\u00ba\ud835\udc54\u00b9\ud835\udc56\u0000\ud835\udc4e\u00ba.(7.1.5)\nFor two-dimensional tensors, we have a corresponding sum with indices \u00b9\ud835\udc4e,\ud835\udc4f\u00bafor\ud835\udc53and\n\u00b9\ud835\udc56\u0000\ud835\udc4e,\ud835\udc57\u0000\ud835\udc4f\u00bafor\ud835\udc54, respectively:\n\u00b9\ud835\udc53\u0003\ud835\udc54\u00ba\u00b9\ud835\udc56,\ud835\udc57\u00ba=\u00d5\n\ud835\udc4e\u00d5\n\ud835\udc4f\ud835\udc53\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\ud835\udc54\u00b9\ud835\udc56\u0000\ud835\udc4e,\ud835\udc57\u0000\ud835\udc4f\u00ba.(7.1.6)\nThis looks similar to (7.1.3 ), with one major difference. Rather than using \u00b9\ud835\udc56\u00b8\ud835\udc4e,\ud835\udc57\u00b8\ud835\udc4f\u00ba,\nwe are using the difference instead. Note, though, that this distinction is mostly cosmetic", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2637, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4864571-2b3f-4cb6-97d9-f12a5f4eb5cb": {"__data__": {"id_": "b4864571-2b3f-4cb6-97d9-f12a5f4eb5cb", "embedding": null, "metadata": {"page_label": "238", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4699e30-18f2-48c7-a579-769ec7cae559", "node_type": "4", "metadata": {"page_label": "238", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c4eec2dc41090c0e392362a25a0b8b925443a35b740b6d248fca8d8009711ea6", "class_name": "RelatedNodeInfo"}}, "text": "238 Convolutional Neural Networks\nsincewecanalwaysmatchthenotationbetween (7.1.3 )and(7.1.6 ). Ouroriginaldefinition\nin(7.1.3 )more properly describes a cross-correlation . We will come back to this in the\nfollowing section.\n7.1.4Channels\nReturningtoourWaldodetector,let\u2019sseewhatthislookslike. Theconvolutionallayerpicks\nwindows of a given size and weighs intensities according to the filter V, as demonstrated\ninFig. 7.1.2 . We might aim to learn a model so that wherever the \u201cwaldoness\u201d is highest,\nwe should find a peak in the hidden layer representations.\ntFig. 7.1.2 Detect Waldo (image courtesy of William Murphy (Infomatique)).\nThere is just one problem with this approach. So far, we blissfully ignored that images\nconsist of three channels: red, green, and blue. In sum, images are not two-dimensional\nobjects but rather third-order tensors, characterized by a height, width, and channel, e.g.,\nwithshape 1024\u00021024\u00023pixels. Whilethefirsttwooftheseaxesconcernspatialrelation-\nships,thethirdcanberegardedasassigningamultidimensionalrepresentationtoeachpixel\nlocation. We thus index Xas\u00bbX\u00bc\ud835\udc56,\ud835\udc57,\ud835\udc58. The convolutional filter has to adapt accordingly.\nInstead of\u00bbV\u00bc\ud835\udc4e,\ud835\udc4f, we now have\u00bbV\u00bc\ud835\udc4e,\ud835\udc4f,\ud835\udc50.\nMoreover, just as our input consists of a third-order tensor, it turns out to be a good idea\nto similarly formulate our hidden representations as third-order tensors H. In other words,\nratherthanjusthavingasinglehiddenrepresentationcorrespondingtoeachspatiallocation,\nwe want an entire vector of hidden representations corresponding to each spatial location.\nWe could think of the hidden representations as comprising a number of two-dimensional\ngrids stacked on top of each other. As in the inputs, these are sometimes called channels .\nThey are also sometimes called feature maps , as each provides a spatialized set of learned\nfeaturesforthesubsequentlayer. Intuitively,youmightimaginethatatlowerlayersthatare\ncloser to inputs, some channels could become specialized to recognize edges while others\ncould recognize textures.\nTosupportmultiplechannelsinbothinputs( X)andhiddenrepresentations( H),wecanadd", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2098, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d8224aa-0b3c-462b-9c66-8d7cd3342bfc": {"__data__": {"id_": "0d8224aa-0b3c-462b-9c66-8d7cd3342bfc", "embedding": null, "metadata": {"page_label": "239", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e4faa9c-a637-4b60-af72-fc6e78cff3d9", "node_type": "4", "metadata": {"page_label": "239", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "48cd989e73e0e6009dd91393f854f7077c061894b4c78e5d3ef7e017dac1ca64", "class_name": "RelatedNodeInfo"}}, "text": "239 From Fully Connected Layers to Convolutions\na fourth coordinate to V:\u00bbV\u00bc\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51. Putting everything together we have:\n\u00bbH\u00bc\ud835\udc56,\ud835\udc57,\ud835\udc51=\u0394\u00d5\n\ud835\udc4e=\u0000\u0394\u0394\u00d5\n\ud835\udc4f=\u0000\u0394\u00d5\n\ud835\udc50\u00bbV\u00bc\ud835\udc4e,\ud835\udc4f,\ud835\udc50,\ud835\udc51\u00bbX\u00bc\ud835\udc56\u00b8\ud835\udc4e,\ud835\udc57\u00b8\ud835\udc4f,\ud835\udc50, (7.1.7)\nwhere\ud835\udc51indexes the output channels in the hidden representations H. The subsequent con-\nvolutionallayerwillgoontotakeathird-ordertensor, H,asinput. Wetake (7.1.7 ),because\nof its generality, as the definition of a convolutional layer for multiple channels, where V\nis a kernel or filter of the layer.\nThereare still manyoperations that weneed to address. Forinstance, weneed to figure out\nhow to combine all the hidden representations to a single output, e.g., whether there is a\nWaldoanywhere in the image. We also need to decide how to compute things efficiently,\nhowtocombinemultiplelayers, appropriateactivationfunctions, andhowtomakereason-\nable design choices to yield networks that are effective in practice. We turn to these issues\nin the remainder of the chapter.\n7.1.5Summaryand Discussion\nIn this section we derived the structure of convolutional neural networks from first prin-\nciples. While it is unclear whether this was the route taken to the invention of CNNs, it\nis satisfying to know that they are the rightchoice when applying reasonable principles\nto how image processing and computer vision algorithms should operate, at least at lower\nlevels. In particular, translation invariance in images implies that all patches of an image\nwill be treated in the same manner. Locality means that only a small neighborhood of pix-\nels will be used to compute the corresponding hidden representations. Some of the earliest\nreferences to CNNs are in the form of the Neocognitron ( Fukushima, 1982 ).\nA second principle that we encountered in our reasoning is how to reduce the number of\nparameters in a function class without limiting its expressive power, at least, whenever\ncertain assumptions on the model hold. We saw a dramatic reduction of complexity as a\nresult of this restriction, turning computationally and statistically infeasible problems into\ntractable models.\nAddingchannelsallowedustobringbacksomeofthecomplexitythatwaslostduetothere-\nstrictions imposed on the convolutional kernel by locality and translation invariance. Note\nthat it is quite natural to add channels other than just red, green, and blue. Many satellite\nimages, in particular for agriculture and meteorology, have tens to hundreds of channels,\ngenerating hyperspectral images instead. They report data on many different wavelengths.\nIn the following we will see how to use convolutions effectively to manipulate the dimen-\nsionalityoftheimagestheyoperateon, howtomovefromlocation-basedtochannel-based\nrepresentations, and how to deal with large numbers of categories efficiently.\n7.1.6Exercises\n1.Assume that the size of the convolution kernel is \u0394 = 0. Show that in this case the\nconvolution kernel implements an MLP independently for each set of channels. This\nleads to the Network in Network architectures ( Linetal., 2013).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44fa7bf1-29f4-45bc-94d4-18ceea75c946": {"__data__": {"id_": "44fa7bf1-29f4-45bc-94d4-18ceea75c946", "embedding": null, "metadata": {"page_label": "240", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d56510d5-914f-41ac-a9b9-2801aef9772f", "node_type": "4", "metadata": {"page_label": "240", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0ab1f8018f19d136a6bb2764886c0e85a7e009d062e6d5746aec33d7876549e6", "class_name": "RelatedNodeInfo"}}, "text": "240 Convolutional Neural Networks\n1192.Audio data is often represented as a one-dimensional sequence.\n1.When might you want to impose locality and translation invariance for audio?\n2.Derive the convolution operations for audio.\n3.Can you treat audio using the same tools as computer vision? Hint: use the spectro-\ngram.\n3.Why might translation invariance not be a good idea after all? Give an example.\n4.Do you think that convolutional layers might also be applicable for text data? Which\nproblems might you encounter with language?\n5.What happens with convolutions when an object is at the boundary of an image?\n6.Prove that the convolution is symmetric, i.e., \ud835\udc53\u0003\ud835\udc54=\ud835\udc54\u0003\ud835\udc53.\nDiscussions119.\n7.2ConvolutionsforImages\nNow that we understand how convolutional layers work in theory, we are ready to see how\nthey work in practice. Building on our motivation of convolutional neural networks as\nefficient architectures for exploring structure in image data, we stick with images as our\nrunning example.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n7.2.1The Cross-CorrelationOperation\nRecallthatstrictlyspeaking,convolutionallayersareamisnomer,sincetheoperationsthey\nexpress are more accurately described as cross-correlations. Based on our descriptions of\nconvolutional layers in Section 7.1 , in such a layer, an input tensor and a kernel tensor are\ncombined to produce an output tensor through a cross-correlation operation.\nLet\u2019signorechannelsfornowandseehowthisworkswithtwo-dimensionaldataandhidden\nrepresentations. In Fig. 7.2.1 , the input is a two-dimensional tensor with a height of 3 and\nwidth of 3. We mark the shape of the tensor as 3\u00023or (3,3). The height and width of the\nkernel are both 2. The shape of the kernelwindow (orconvolutionwindow ) is given by the\nheight and width of the kernel (here it is 2\u00022).\nInthetwo-dimensionalcross-correlationoperation,webeginwiththeconvolutionwindow\npositioned at the upper-left corner of the input tensor and slide it across the input tensor,\nboth from left to right and top to bottom. When the convolution window slides to a certain", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "483f6f18-68bc-4e9f-8959-dc8553ae8a4b": {"__data__": {"id_": "483f6f18-68bc-4e9f-8959-dc8553ae8a4b", "embedding": null, "metadata": {"page_label": "241", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8d32618-dd77-46de-88d9-0b268f7f5b89", "node_type": "4", "metadata": {"page_label": "241", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "06302bfafb3fcee27b7461d6fdb91a1f08fad03503683792e86d516f3eaa4abc", "class_name": "RelatedNodeInfo"}}, "text": "241 Convolutions for Images\ntFig. 7.2.1 Two-dimensional cross-correlation operation. The shaded portions are the \ufb01rst output\nelement as well as the input and kernel tensor elements used for the output computation:\n0\u00020\u00b81\u00021\u00b83\u00022\u00b84\u00023=19.\nposition, the input subtensor contained in that window and the kernel tensor are multiplied\nelementwise and the resulting tensor is summed up yielding a single scalar value. This\nresult gives the value of the output tensor at the corresponding location. Here, the output\ntensor has a height of 2 and width of 2 and the four elements are derived from the two-\ndimensional cross-correlation operation:\n0\u00020\u00b81\u00021\u00b83\u00022\u00b84\u00023=19,\n1\u00020\u00b82\u00021\u00b84\u00022\u00b85\u00023=25,\n3\u00020\u00b84\u00021\u00b86\u00022\u00b87\u00023=37,\n4\u00020\u00b85\u00021\u00b87\u00022\u00b88\u00023=43.(7.2.1)\nNote that along each axis, the output size is slightly smaller than the input size. Because\nthe kernel has width and height greater than 1, we can only properly compute the cross-\ncorrelation for locations where the kernel fits wholly within the image, the output size is\ngiven by the input size \ud835\udc5bh\u0002\ud835\udc5bwminus the size of the convolution kernel \ud835\udc58h\u0002\ud835\udc58wvia\n\u00b9\ud835\udc5bh\u0000\ud835\udc58h\u00b81\u00ba\u0002\u00b9\ud835\udc5bw\u0000\ud835\udc58w\u00b81\u00ba. (7.2.2)\nThis is the case since we need enough space to \u201cshift\u201d the convolution kernel across the\nimage. Later we will see how to keep the size unchanged by padding the image with zeros\naround its boundary so that there is enough space to shift the kernel. Next, we implement\nthis process in the corr2dfunction, which accepts an input tensor Xand a kernel tensor K\nand returns an output tensor Y.\ndef corr2d (X, K): #@save\n\"\"\"Compute 2D cross-correlation.\"\"\"\nh, w =K.shape\nY=torch .zeros((X .shape[ 0]-h+1, X.shape[ 1]-w+1))\nfor iinrange (Y.shape[ 0]):\nfor jinrange (Y.shape[ 1]):\nY[i, j] =(X[i:i +h, j:j +w]*K).sum()\nreturn Y\nWe can construct the input tensor Xand the kernel tensor KfromFig. 7.2.1 to validate\nthe output of the above implementation of the two-dimensional cross-correlation opera-\ntion.\nX=torch .tensor([[ 0.0,1.0,2.0], [ 3.0,4.0,5.0], [ 6.0,7.0,8.0]])\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a71e983-769c-44d2-bc0c-e2e46829a5c6": {"__data__": {"id_": "5a71e983-769c-44d2-bc0c-e2e46829a5c6", "embedding": null, "metadata": {"page_label": "242", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b07e079-d31d-4270-882e-316e48ee7913", "node_type": "4", "metadata": {"page_label": "242", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6e308fd6aa4155779eb88aa700280ebe0eac5f76be0c1691ab0c1398da041961", "class_name": "RelatedNodeInfo"}}, "text": "242 Convolutional Neural Networks\n(continued from previous page)\nK=torch .tensor([[ 0.0,1.0], [ 2.0,3.0]])\ncorr2d(X, K)\ntensor([[ 19.,25.],\n[37.,43.]])\n7.2.2ConvolutionalLayers\nAconvolutionallayercross-correlatestheinputandkernelandaddsascalarbiastoproduce\nan output. The two parameters of a convolutional layer are the kernel and the scalar bias.\nWhen training models based on convolutional layers, we typically initialize the kernels\nrandomly, just as we would with a fully connected layer.\nWearenowreadytoimplementatwo-dimensionalconvolutionallayerbasedonthe corr2d\nfunction defined above. In the __init__ constructor method, wedeclare weightandbias\nas the two model parameters. The forward propagation method calls the corr2dfunction\nand adds the bias.\nclass Conv2D (nn.Module):\ndef __init__ (self , kernel_size):\nsuper ().__init__ ()\nself .weight =nn.Parameter(torch .rand(kernel_size))\nself .bias =nn.Parameter(torch .zeros( 1))\ndef forward (self , x):\nreturn corr2d(x, self .weight) +self .bias\nIn\u210e\u0002\ud835\udc64convolutionoran \u210e\u0002\ud835\udc64convolutionkernel,theheightandwidthoftheconvolution\nkernel are\u210eand\ud835\udc64, respectively. We also refer to a convolutional layer with an \u210e\u0002\ud835\udc64\nconvolution kernel simply as an \u210e\u0002\ud835\udc64convolutional layer.\n7.2.3Object EdgeDetectionin Images\nLet\u2019s take a moment to parse a simple application of a convolutional layer: detecting the\nedgeofanobjectinanimagebyfindingthelocationofthepixelchange. First,weconstruct\nan \u201cimage\u201d of 6\u00028pixels. The middle four columns are black ( 0) and the rest are white\n(1).\nX=torch .ones(( 6,8))\nX[:, 2:6]=0\nX\ntensor([[ 1.,1.,0.,0.,0.,0.,1.,1.],\n[1.,1.,0.,0.,0.,0.,1.,1.],\n[1.,1.,0.,0.,0.,0.,1.,1.],\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8638dc00-f234-413b-a28e-33c7fdbef708": {"__data__": {"id_": "8638dc00-f234-413b-a28e-33c7fdbef708", "embedding": null, "metadata": {"page_label": "243", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9008f41e-992e-42e7-ac6d-7e25b65e5f43", "node_type": "4", "metadata": {"page_label": "243", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "747d5f746c4b7f040bfff46b17ff7d82c92169db0e69e2acd38107edd1ad2a7f", "class_name": "RelatedNodeInfo"}}, "text": "243 Convolutions for Images\n(continued from previous page)\n[1.,1.,0.,0.,0.,0.,1.,1.],\n[1.,1.,0.,0.,0.,0.,1.,1.],\n[1.,1.,0.,0.,0.,0.,1.,1.]])\nNext, we construct a kernel Kwith a height of 1 and a width of 2. When we perform\nthe cross-correlation operation with the input, if the horizontally adjacent elements are\nthe same, the output is 0. Otherwise, the output is nonzero. Note that this kernel is a\nspecial case of a finite difference operator. At location \u00b9\ud835\udc56,\ud835\udc57\u00bait computes \ud835\udc65\ud835\udc56,\ud835\udc57\u0000\ud835\udc65\u00b9\ud835\udc56\u00b81\u00ba,\ud835\udc57,\ni.e., it computes the difference between the values of horizontally adjacent pixels. This is\na discrete approximation of the first derivative in the horizontal direction. After all, for\na function\ud835\udc53\u00b9\ud835\udc56,\ud835\udc57\u00baits derivative\u0000\ud835\udf15\ud835\udc56\ud835\udc53\u00b9\ud835\udc56,\ud835\udc57\u00ba=lim\ud835\udf16!0\ud835\udc53\u00b9\ud835\udc56,\ud835\udc57\u00ba\u0000\ud835\udc53\u00b9\ud835\udc56\u00b8\ud835\udf16,\ud835\udc57\u00ba\n\ud835\udf16. Let\u2019s see how this\nworks in practice.\nK=torch .tensor([[ 1.0,-1.0]])\nWe are ready to perform the cross-correlation operation with arguments X(our input) and\nK(our kernel). As you can see, we detect 1for the edge from white to black and \u00001for the\nedge from black to white. All other outputs take value 0.\nY=corr2d(X, K)\nY\ntensor([[ 0.,1.,0.,0.,0.,-1.,0.],\n[0.,1.,0.,0.,0.,-1.,0.],\n[0.,1.,0.,0.,0.,-1.,0.],\n[0.,1.,0.,0.,0.,-1.,0.],\n[0.,1.,0.,0.,0.,-1.,0.],\n[0.,1.,0.,0.,0.,-1.,0.]])\nWecannowapplythekerneltothetransposedimage. Asexpected, itvanishes. Thekernel\nKonly detects vertical edges.\ncorr2d(X .t(), K)\ntensor([[ 0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.],\n[0.,0.,0.,0.,0.]])\n7.2.4Learning a Kernel", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2ed8b6b-f5f9-4f1e-8f24-ae3b2a5592c3": {"__data__": {"id_": "f2ed8b6b-f5f9-4f1e-8f24-ae3b2a5592c3", "embedding": null, "metadata": {"page_label": "244", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bf7cb86-07a3-4796-840a-ce9ae4ef2cda", "node_type": "4", "metadata": {"page_label": "244", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a61aac690443733b1eb315bd3c8075c8c863c2e2e59855bdfd036d73244040b0", "class_name": "RelatedNodeInfo"}}, "text": "244 Convolutional Neural Networks\nDesigninganedgedetectorbyfinitedifferences [1, -1] isneatifweknowthisisprecisely\nwhat we are looking for. However, as we look at larger kernels, and consider successive\nlayers of convolutions, it might be impossible to specify precisely what each filter should\nbe doing manually.\nNow let\u2019s see whether we can learn the kernel that generated Yfrom Xby looking at the\ninput\u2013output pairs only. We first construct a convolutional layer and initialize its kernel as\narandomtensor. Next,ineachiteration,wewillusethesquarederrortocompare Ywiththe\noutput of the convolutional layer. We can then calculate the gradient to update the kernel.\nFor the sake of simplicity, in the following we use the built-in class for two-dimensional\nconvolutional layers and ignore the bias.\n# Construct a two-dimensional convolutional layer with 1 output channel and a\n# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\nconv2d =nn.LazyConv2d( 1, kernel_size =(1,2), bias =False )\n# The two-dimensional convolutional layer uses four-dimensional input and\n# output in the format of (example, channel, height, width), where the batch\n# size (number of examples in the batch) and the number of channels are both 1\nX=X.reshape(( 1,1,6,8))\nY=Y.reshape(( 1,1,6,7))\nlr=3e-2 # Learning rate\nfor iinrange (10):\nY_hat =conv2d(X)\nl=(Y_hat -Y)**2\nconv2d .zero_grad()\nl.sum() .backward()\n# Update the kernel\nconv2d .weight .data[:] -=lr*conv2d .weight .grad\nif(i+1)%2==0:\nprint (f'epoch {i+1}, loss {l.sum() :.3f}')\nepoch 2, loss 16.481\nepoch 4, loss 5.069\nepoch 6, loss 1.794\nepoch 8, loss 0.688\nepoch 10, loss 0.274\nNotethattheerrorhasdroppedtoasmallvalueafter10iterations. Nowwewilltakealook\nat the kernel tensor we learned.\nconv2d .weight .data .reshape(( 1,2))\ntensor([[ 1.0398 ,-0.9328 ]])\nIndeed, the learned kernel tensor is remarkably close to the kernel tensor Kwe defined\nearlier.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab7bc87b-2e7b-4b0d-af8f-e24f28bcc636": {"__data__": {"id_": "ab7bc87b-2e7b-4b0d-af8f-e24f28bcc636", "embedding": null, "metadata": {"page_label": "245", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be66d327-b470-4998-8cda-08a4400075a8", "node_type": "4", "metadata": {"page_label": "245", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ebece465b7ad97eea93b1b9afd55256f88babe8afe13897836c4de824831798e", "class_name": "RelatedNodeInfo"}}, "text": "245 Convolutions for Images\n7.2.5Cross-Correlationand Convolution\nRecallourobservationfrom Section7.1 ofthecorrespondencebetweenthecross-correlation\nandconvolutionoperations. Herelet\u2019scontinuetoconsidertwo-dimensionalconvolutional\nlayers. What if such layers perform strict convolution operations as defined in (7.1.6 )in-\nsteadofcross-correlations? Inordertoobtaintheoutputofthestrict convolution operation,\nweonlyneedtoflipthetwo-dimensionalkerneltensorbothhorizontallyandvertically,and\nthen perform the cross-correlation operation with the input tensor.\nIt is noteworthy that since kernels are learned from data in deep learning, the outputs of\nconvolutional layers remain unaffected no matter such layers perform either the strict con-\nvolution operations or the cross-correlation operations.\nTo illustrate this, suppose that a convolutional layer performs cross-correlation and learns\nthe kernel in Fig. 7.2.1 , which is here denoted as the matrix K. Assuming that other con-\nditions remain unchanged, when this layer instead performs strict convolution , the learned\nkernel K0will be the same as KafterK0is flipped both horizontally and vertically. That\nis to say, when the convolutional layer performs strict convolution for the input in Fig.\n7.2.1andK0, the same output in Fig. 7.2.1 (cross-correlation of the input and K) will be\nobtained.\nInkeepingwithstandardterminologyindeeplearningliterature,wewillcontinuetoreferto\nthecross-correlationoperationasaconvolutioneventhough,strictly-speaking,itisslightly\ndifferent. Furthermore, we use the term element to refer to an entry (or component) of any\ntensor representing a layer representation or a convolution kernel.\n7.2.6FeatureMap and ReceptiveField\nAs described in Section 7.1.4 , the convolutional layer output in Fig. 7.2.1 is sometimes\ncalled afeature map , as it can be regarded as the learned representations (features) in the\nspatial dimensions (e.g., width and height) to the subsequent layer. In CNNs, for any el-\nement\ud835\udc65of some layer, its receptive field refers to all the elements (from all the previous\nlayers) that may affect the calculation of \ud835\udc65during the forward propagation. Note that the\nreceptive field may be larger than the actual size of the input.\nLet\u2019s continue to use Fig. 7.2.1 to explain the receptive field. Given the 2\u00022convolution\nkernel, the receptive field of the shaded output element (of value 19) is the four elements\nin the shaded portion of the input. Now let\u2019s denote the 2\u00022output as Yand consider a\ndeeperCNNwithanadditional 2\u00022convolutionallayerthattakes Yasitsinput,outputting\na single element \ud835\udc67. In this case, the receptive field of \ud835\udc67onYincludes all the four elements\nofY,whilethereceptivefieldontheinputincludesallthenineinputelements. Thus,when\nany element in a feature map needs a larger receptive field to detect input features over a\nbroader area, we can build a deeper network.\nReceptive fields derive their name from neurophysiology. A series of experiments on a\nrangeofanimalsusingdifferentstimuli( HubelandWiesel, 1959 ,HubelandWiesel, 1962 ,\nHubel and Wiesel, 1968 ) explored the response of what is called the visual cortex on said\nstimuli. By and large they found that lower levels respond to edges and related shapes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3230, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f997f261-a64e-40fb-a857-8df56b419cb8": {"__data__": {"id_": "f997f261-a64e-40fb-a857-8df56b419cb8", "embedding": null, "metadata": {"page_label": "246", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5d2b179-533c-412a-9b70-ee3360290f75", "node_type": "4", "metadata": {"page_label": "246", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f72d8ccf5714bdd35bf2adc97ab2502d2795aa86c6ced08a5fa7e957788a65bc", "class_name": "RelatedNodeInfo"}}, "text": "246 Convolutional Neural Networks\nLateron,Field( 1987)illustratedthiseffectonnaturalimageswith,whatcanonlybecalled,\nconvolutional kernels. We reprint a key figure in Fig. 7.2.2 to illustrate the striking simi-\nlarities.\ntFig. 7.2.2 Figure and caption taken from Field ( 1987 ): An example of coding with six different\nchannels. (Left) Examples of the six types of sensor associated with each channel. (Right)\nConvolution of the image in (Middle) with the six sensors shown in (Left). The response\nof the individual sensors is determined by sampling these \ufb01ltered images at a distance\nproportional to the size of the sensor (shown with dots). This diagram shows the response\nof only the even symmetric sensors.\nAs it turns out, this relation even holds for the features computed by deeper layers of net-\nworks trained on image classification tasks, as demonstrated in, for example, Kuzovkin et\nal.(2018). Suffice it to say, convolutions have proven to be an incredibly powerful tool for\ncomputer vision, both in biology and in code. As such, it is not surprising (in hindsight)\nthat they heralded the recent success in deep learning.\n7.2.7Summary\nThe core computation required for a convolutional layer is a cross-correlation operation.\nWe saw that a simple nested for-loop is all that is required to compute its value. If we\nhave multiple input and multiple output channels, we are performing a matrix\u2013matrix op-\neration between channels. As can be seen, the computation is straightforward and, most\nimportantly, highly local. This affords significant hardware optimization and many recent\nresults in computer vision are only possible because of that. After all, it means that chip\ndesigners can invest in fast computation rather than memory when it comes to optimizing\nfor convolutions. While this may not lead to optimal designs for other applications, it does\nopen the door to ubiquitous and affordable computer vision.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02cebaba-a770-4ab2-8a23-2914fa239df3": {"__data__": {"id_": "02cebaba-a770-4ab2-8a23-2914fa239df3", "embedding": null, "metadata": {"page_label": "247", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6578f30c-79e5-4de7-a6fd-cfaeda163f10", "node_type": "4", "metadata": {"page_label": "247", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3551ce9eb6023691f8d560a29ee3fee7061fae9dc79523c78dddeb47dde4c63c", "class_name": "RelatedNodeInfo"}}, "text": "247 Padding and Stride\n120In terms of convolutions themselves, they can be used for many purposes, for example\ndetecting edges and lines, blurring images, or sharpening them. Most importantly, it is\nnot necessary that the statistician (or engineer) invents suitable filters. Instead, we can\nsimplylearnthem from data. This replaces feature engineering heuristics by evidence-\nbased statistics. Lastly, and quite delightfully, these filters are not just advantageous for\nbuilding deep networks but they also correspond to receptive fields and feature maps in the\nbrain. This gives us confidence that we are on the right track.\n7.2.8Exercises\n1.Construct an image Xwith diagonal edges.\n1.What happens if you apply the kernel Kin this section to it?\n2.What happens if you transpose X?\n3.What happens if you transpose K?\n2.Design some kernels manually.\n1.Given a directional vector v=\u00b9\ud835\udc631,\ud835\udc632\u00ba, derive an edge-detection kernel that detects\nedges orthogonal to v, i.e., edges in the direction \u00b9\ud835\udc632,\u0000\ud835\udc631\u00ba.\n2.Derive a finite difference operator for the second derivative. What is the minimum\nsize of the convolutional kernel associated with it? Which structures in images re-\nspond most strongly to it?\n3.How would you design a blur kernel? Why might you want to use such a kernel?\n4.What is the minimum size of a kernel to obtain a derivative of order \ud835\udc51?\n3.When you try to automatically find the gradient for the Conv2Dclass we created, what\nkind of error message do you see?\n4.Howdoyourepresentacross-correlationoperationasamatrixmultiplicationbychang-\ning the input and kernel tensors?\nDiscussions120.\n7.3Paddingand Stride\nRecall the example of a convolution in Fig. 7.2.1 . The input had both a height and width of\n3and the convolutionkernel hadboth a height and width of2, yielding anoutput represen-\ntation with dimension 2\u00022. Assuming that the input shape is \ud835\udc5bh\u0002\ud835\udc5bwand the convolution\nkernel shape is \ud835\udc58h\u0002\ud835\udc58w, the output shape will be \u00b9\ud835\udc5bh\u0000\ud835\udc58h\u00b81\u00ba\u0002\u00b9\ud835\udc5bw\u0000\ud835\udc58w\u00b81\u00ba: we can\nonly shift the convolution kernel so far until it runs out of pixels to apply the convolution\nto.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c44f7d26-c882-40dc-b0ce-bbbcfb24a4f1": {"__data__": {"id_": "c44f7d26-c882-40dc-b0ce-bbbcfb24a4f1", "embedding": null, "metadata": {"page_label": "248", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e9595d2-b7e6-4fac-89ce-23ae16b31517", "node_type": "4", "metadata": {"page_label": "248", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fa5a5fe45c4890b4c124035b99f9829afa10bab917cb30243c15fd265dd5d2f0", "class_name": "RelatedNodeInfo"}}, "text": "248 Convolutional Neural Networks\nIn the following we will explore a number of techniques, including padding and strided\nconvolutions, that offer more control over the size of the output. As motivation, note that\nsincekernelsgenerallyhavewidthandheightgreaterthan 1,afterapplyingmanysuccessive\nconvolutions, wetendtowindupwithoutputsthatareconsiderablysmallerthanourinput.\nIf we start with a 240\u0002240pixel image, ten layers of 5\u00025convolutions reduce the image\nto200\u0002200pixels, slicing off 30%of the image and with it obliterating any interesting\ninformation on the boundaries of the original image. Padding is the most popular tool for\nhandling this issue. In other cases, we may want to reduce the dimensionality drastically,\ne.g., if we find the original input resolution to be unwieldy. Strided convolutions are a\npopular technique that can help in these instances.\nimport torch\nfrom torch import nn\n7.3.1Padding\nAs described above, one tricky issue when applying convolutional layers is that we tend\nto lose pixels on the perimeter of our image. Consider Fig. 7.3.1 that depicts the pixel\nutilization as a function of the convolution kernel size and the position within the image.\nThe pixels in the corners are hardly used at all.\ntFig. 7.3.1 Pixel utilization for convolutions of size 1 \u00021, 2\u00022, and 3\u00023 respectively.\nSince we typically use small kernels, for any given convolution we might only lose a few\npixels but this can add up as we apply many successive convolutional layers. One straight-\nforward solution to this problem is to add extra pixels of filler around the boundary of our\ninput image, thus increasing the effective size of the image. Typically, we set the values of\nthe extra pixels to zero. In Fig. 7.3.2 , we pad a 3\u00023input, increasing its size to 5\u00025. The\ncorrespondingoutputthenincreasestoa 4\u00024matrix. Theshadedportionsarethefirstout-\nputelementaswellastheinputandkerneltensorelementsusedfortheoutputcomputation:\n0\u00020\u00b80\u00021\u00b80\u00022\u00b80\u00023=0.\ntFig. 7.3.2 Two-dimensional cross-correlation with padding.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a0f5c50-c3de-46b1-a256-3c7556c48010": {"__data__": {"id_": "4a0f5c50-c3de-46b1-a256-3c7556c48010", "embedding": null, "metadata": {"page_label": "249", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fa136e7-1ad9-4a97-8732-fdba735eca2a", "node_type": "4", "metadata": {"page_label": "249", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dc14a57a6da7a62c0d9c47b1b1f3156ac0d596ff79cd65205d3d8930d86ed3cf", "class_name": "RelatedNodeInfo"}}, "text": "249 Padding and Stride\nIn general, if we add a total of \ud835\udc5dhrows of padding (roughly half on top and half on bottom)\nand a total of \ud835\udc5dwcolumns of padding (roughly half on the left and half on the right), the\noutput shape will be\n\u00b9\ud835\udc5bh\u0000\ud835\udc58h\u00b8\ud835\udc5dh\u00b81\u00ba\u0002\u00b9\ud835\udc5bw\u0000\ud835\udc58w\u00b8\ud835\udc5dw\u00b81\u00ba. (7.3.1)\nThis means that the height and width of the output will increase by \ud835\udc5dhand\ud835\udc5dw, respec-\ntively.\nIn many cases, we will want to set \ud835\udc5dh=\ud835\udc58h\u00001and\ud835\udc5dw=\ud835\udc58w\u00001to give the input and\noutput the same height and width. This will make it easier to predict the output shape of\neach layer when constructing the network. Assuming that \ud835\udc58his odd here, we will pad \ud835\udc5dh\u009d2\nrows on both sides of the height. If \ud835\udc58his even, one possibility is to pad d\ud835\udc5dh\u009d2erows on the\ntop of the input and b\ud835\udc5dh\u009d2crows on the bottom. We will pad both sides of the width in the\nsame way.\nCNNs commonly use convolution kernels with odd height and width values, such as 1, 3,\n5, or 7. Choosing odd kernel sizes has the benefit that we can preserve the dimensionality\nwhile padding with the same number of rows on top and bottom, and the same number of\ncolumns on left and right.\nMoreover, this practice of using odd kernels and padding to precisely preserve dimension-\nality offers a clerical benefit. For any two-dimensional tensor X, when the kernel\u2019s size is\nodd and the number of padding rows and columns on all sides are the same, thereby pro-\nducinganoutputwiththesameheightandwidthastheinput,weknowthattheoutput Y[i,\nj]is calculated by cross-correlation of the input and convolution kernel with the window\ncentered on X[i, j] .\nIn the following example, we create a two-dimensional convolutional layer with a height\nand width of 3 and apply 1 pixel of padding on all sides. Given an input with a height and\nwidth of 8, we find that the height and width of the output is also 8.\n# We define a helper function to calculate convolutions. It initializes the\n# convolutional layer weights and performs corresponding dimensionality\n# elevations and reductions on the input and output\ndef comp_conv2d (conv2d, X):\n# (1, 1) indicates that batch size and the number of channels are both 1\nX=X.reshape(( 1,1)+X.shape)\nY=conv2d(X)\n# Strip the first two dimensions: examples and channels\nreturn Y.reshape(Y .shape[ 2:])\n# 1 row and column is padded on either side, so a total of 2 rows or columns\n# are added\nconv2d =nn.LazyConv2d( 1, kernel_size =3, padding =1)\nX=torch .rand(size =(8,8))\ncomp_conv2d(conv2d, X) .shape\ntorch .Size([ 8,8])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2442, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b4521ee-3459-4c07-91e1-a55bdc18eb18": {"__data__": {"id_": "3b4521ee-3459-4c07-91e1-a55bdc18eb18", "embedding": null, "metadata": {"page_label": "250", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "219bc4cb-e111-4b42-90d4-bb088057caff", "node_type": "4", "metadata": {"page_label": "250", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7e84b8b2ac30b8535a8fc4d613773020cbe265a0931c660c8471d0e33f23ebc0", "class_name": "RelatedNodeInfo"}}, "text": "250 Convolutional Neural Networks\nWhen the height and width of the convolution kernel are different, we can make the output\nand input have the same height and width by setting different padding numbers for height\nand width.\n# We use a convolution kernel with height 5 and width 3. The padding on either\n# side of the height and width are 2 and 1, respectively\nconv2d =nn.LazyConv2d( 1, kernel_size =(5,3), padding =(2,1))\ncomp_conv2d(conv2d, X) .shape\ntorch .Size([ 8,8])\n7.3.2Stride\nWhen computing the cross-correlation, we start with the convolution window at the upper-\nleft corner of the input tensor, and then slide it over all locations both down and to the\nright. In the previous examples, we defaulted to sliding one element at a time. However,\nsometimes, either for computational efficiency or because we wish to downsample, we\nmove our window more than one element at a time, skipping the intermediate locations.\nThis is particularly useful if the convolution kernel is large since it captures a large area of\nthe underlying image.\nWe refer to the number of rows and columns traversed per slide as stride. So far, we have\nusedstridesof1,bothforheightandwidth. Sometimes,wemaywanttousealargerstride.\nFig. 7.3.3 shows a two-dimensional cross-correlation operation with a stride of 3 vertically\nand 2 horizontally. The shaded portions are the output elements as well as the input and\nkernel tensor elements used for the output computation: 0\u00020\u00b80\u00021\u00b81\u00022\u00b82\u00023=8,\n0\u00020\u00b86\u00021\u00b80\u00022\u00b80\u00023=6. Wecanseethatwhenthesecondelementofthefirstcolumnis\ngenerated,theconvolutionwindowslidesdownthreerows. Theconvolutionwindowslides\ntwo columns to the right when the second element of the first row is generated. When the\nconvolution window continues to slide two columns to the right on the input, there is no\noutput because the input element cannot fill the window (unless we add another column of\npadding).\ntFig. 7.3.3 Cross-correlation with strides of 3 and 2 for height and width, respectively.\nIn general, when the stride for the height is \ud835\udc60hand the stride for the width is \ud835\udc60w, the output\nshape is\nb\u00b9\ud835\udc5bh\u0000\ud835\udc58h\u00b8\ud835\udc5dh\u00b8\ud835\udc60h\u00ba\u009d\ud835\udc60hc\u0002b\u00b9\ud835\udc5bw\u0000\ud835\udc58w\u00b8\ud835\udc5dw\u00b8\ud835\udc60w\u00ba\u009d\ud835\udc60wc. (7.3.2)\nIf we set\ud835\udc5dh=\ud835\udc58h\u00001and\ud835\udc5dw=\ud835\udc58w\u00001, then the output shape can be simplified to b\u00b9\ud835\udc5bh\u00b8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2208, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f46cbe0-1b79-4c62-a805-07b054d1f586": {"__data__": {"id_": "3f46cbe0-1b79-4c62-a805-07b054d1f586", "embedding": null, "metadata": {"page_label": "251", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e70b6cc-9731-41dd-a673-82eb52d6204a", "node_type": "4", "metadata": {"page_label": "251", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "68f3b436276758cc494d4a32fc1ec51199a2c9930293af29d0b05f1104b124fc", "class_name": "RelatedNodeInfo"}}, "text": "251 Padding and Stride\n\ud835\udc60h\u00001\u00ba\u009d\ud835\udc60hc\u0002b\u00b9\ud835\udc5bw\u00b8\ud835\udc60w\u00001\u00ba\u009d\ud835\udc60wc. Going a step further, if the input height and width are\ndivisible by the strides on the height and width, then the output shape will be \u00b9\ud835\udc5bh\u009d\ud835\udc60h\u00ba\u0002\n\u00b9\ud835\udc5bw\u009d\ud835\udc60w\u00ba.\nBelow, we set the strides on both the height and width to 2, thus halving the input height\nand width.\nconv2d =nn.LazyConv2d( 1, kernel_size =3, padding =1, stride =2)\ncomp_conv2d(conv2d, X) .shape\ntorch .Size([ 4,4])\nLet\u2019s look at a slightly more complicated example.\nconv2d =nn.LazyConv2d( 1, kernel_size =(3,5), padding =(0,1), stride =(3,4))\ncomp_conv2d(conv2d, X) .shape\ntorch .Size([ 2,2])\n7.3.3Summaryand Discussion\nPadding can increase the height and width of the output. This is often used to give the\noutput the same height and width as the input to avoid undesirable shrinkage of the output.\nMoreover,itensuresthatallpixelsareusedequallyfrequently. Typicallywepicksymmetric\npadding on both sides of the input height and width. In this case we refer to \u00b9\ud835\udc5dh,\ud835\udc5dw\u00ba\npadding. Most commonly we set \ud835\udc5dh=\ud835\udc5dw, in which case we simply state that we choose\npadding\ud835\udc5d.\nA similar convention applies to strides. When horizontal stride \ud835\udc60hand vertical stride \ud835\udc60w\nmatch, we simply talk about stride \ud835\udc60. The stride can reduce the resolution of the output, for\nexample reducing the height and width of the output to only 1\u009d\ud835\udc5bof the height and width of\nthe input for \ud835\udc5b> 1. By default, the padding is 0 and the stride is 1.\nSo far all padding that we discussed simply extended images with zeros. This has signif-\nicant computational benefit since it is trivial to accomplish. Moreover, operators can be\nengineered to take advantage of this padding implicitly without the need to allocate addi-\ntional memory. At the same time, it allows CNNs to encode implicit position information\nwithinanimage,simplybylearningwherethe\u201cwhitespace\u201dis. Therearemanyalternatives\nto zero-padding. Alsallakh et al.(2020) provided an extensive overview of those (albeit\nwithout a clear case for when to use nonzero paddings unless artifacts occur).\n7.3.4Exercises\n1.Given the final code example in this section with kernel size \u00b93,5\u00ba, padding\u00b90,1\u00ba, and\nstride\u00b93,4\u00ba, calculate the output shape to check if it is consistent with the experimental\nresult.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1f4096a-2639-4b71-b403-6a3bab5e4960": {"__data__": {"id_": "b1f4096a-2639-4b71-b403-6a3bab5e4960", "embedding": null, "metadata": {"page_label": "252", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d115533-87a8-4a49-bcde-d0698f01f95a", "node_type": "4", "metadata": {"page_label": "252", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9003d0cd9c1f855bceec1013b0ca6acbbe379a9cb20688beda0c2ab989944998", "class_name": "RelatedNodeInfo"}}, "text": "252 Convolutional Neural Networks\n1212.For audio signals, what does a stride of 2 correspond to?\n3.Implement mirror padding, i.e., padding where the border values are simply mirrored\nto extend tensors.\n4.What are the computational benefits of a stride larger than 1?\n5.What might be statistical benefits of a stride larger than 1?\n6.Howwouldyouimplementastrideof1\n2? Whatdoesitcorrespondto? Whenwouldthis\nbe useful?\nDiscussions121.\n7.4MultipleInputand Multiple Output Channels\nWhile we described the multiple channels that comprise each image (e.g., color images\nhave the standard RGB channels to indicate the amount of red, green and blue) and con-\nvolutional layers for multiple channels in Section 7.1.4 , until now, we simplified all of our\nnumerical examples by working with just a single input and a single output channel. This\nallowedustothinkofourinputs,convolutionkernels,andoutputseachastwo-dimensional\ntensors.\nWhen we add channels into the mix, our inputs and hidden representations both become\nthree-dimensional tensors. For example, each RGB input image has shape 3\u0002\u210e\u0002\ud835\udc64. We\nrefer to this axis, with a size of 3, as the channel dimension. The notion of channels is\nas old as CNNs themselves: for instance LeNet-5 ( LeCunet al., 1995) uses them. In this\nsection, we will take a deeper look at convolution kernels with multiple input and multiple\noutput channels.\nimport torch\nfrom d2l import torch asd2l\n7.4.1Multiple InputChannels\nWhen the input data contains multiple channels, we need to construct a convolution kernel\nwith the same number of input channels as the input data, so that it can perform cross-\ncorrelation with the input data. Assuming that the number of channels for the input data\nis\ud835\udc50i, the number of input channels of the convolution kernel also needs to be \ud835\udc50i. If our\nconvolution kernel\u2019s window shape is \ud835\udc58h\u0002\ud835\udc58w, then, when \ud835\udc50i=1, we can think of our\nconvolution kernel as just a two-dimensional tensor of shape \ud835\udc58h\u0002\ud835\udc58w.\nHowever, when \ud835\udc50i>1, we need a kernel that contains a tensor of shape \ud835\udc58h\u0002\ud835\udc58wforev-\neryinput channel. Concatenating these \ud835\udc50itensors together yields a convolution kernel of\nshape\ud835\udc50i\u0002\ud835\udc58h\u0002\ud835\udc58w. Since the input and convolution kernel each have \ud835\udc50ichannels, we can", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50d74fcd-e380-4383-82bf-7905581bd8e4": {"__data__": {"id_": "50d74fcd-e380-4383-82bf-7905581bd8e4", "embedding": null, "metadata": {"page_label": "253", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4742d777-1323-4c6c-8a39-d544997464ff", "node_type": "4", "metadata": {"page_label": "253", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e58993576cc5ebbefc988e977780929c29b03e45072df323b726bc99de288185", "class_name": "RelatedNodeInfo"}}, "text": "253 Multiple Input and Multiple Output Channels\nperform a cross-correlation operation on the two-dimensional tensor of the input and the\ntwo-dimensional tensor of the convolution kernel for each channel, adding the \ud835\udc50iresults\ntogether (summing over the channels) to yield a two-dimensional tensor. This is the result\nof a two-dimensional cross-correlation between a multi-channel input and a multi-input-\nchannel convolution kernel.\nFig.7.4.1 providesanexampleofatwo-dimensionalcross-correlationwithtwoinputchan-\nnels. The shaded portions are the first output element as well as the input and kernel tensor\nelements used for the output computation: \u00b91\u00021\u00b82\u00022\u00b84\u00023\u00b85\u00024\u00ba\u00b8\u00b9 0\u00020\u00b81\u00021\u00b8\n3\u00022\u00b84\u00023\u00ba=56.\ntFig. 7.4.1 Cross-correlation computation with two input channels.\nTomakesurewereallyunderstandwhatisgoingonhere,wecanimplementcross-correlation\noperations with multiple input channels ourselves. Notice that all we are doing is perform-\ning a cross-correlation operation per channel and then adding up the results.\ndef corr2d_multi_in (X, K):\n# Iterate through the 0th dimension (channel) of K first, then add them up\nreturn sum(d2l .corr2d(x, k) for x, k inzip(X, K))\nWe can construct the input tensor Xand the kernel tensor Kcorresponding to the values in\nFig. 7.4.1 to validate the output of the cross-correlation operation.\nX=torch .tensor([[[ 0.0,1.0,2.0], [ 3.0,4.0,5.0], [ 6.0,7.0,8.0]],\n[[1.0,2.0,3.0], [ 4.0,5.0,6.0], [ 7.0,8.0,9.0]]])\nK=torch .tensor([[[ 0.0,1.0], [ 2.0,3.0]], [[ 1.0,2.0], [ 3.0,4.0]]])\ncorr2d_multi_in(X, K)\ntensor([[ 56.,72.],\n[104. ,120. ]])\n7.4.2MultipleOutput Channels\nRegardless of the number of input channels, so far we always ended up with one output\nchannel. However, as we discussed in Section 7.1.4 , it turns out to be essential to have\nmultiple channels at each layer. In the most popular neural network architectures, we actu-\nallyincreasethechanneldimensionaswegodeeperintheneuralnetwork, typicallydown-\nsampling to trade off spatial resolution for greater channel depth . Intuitively, you could", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1b14bcc-7f65-4ba5-820b-de94d15bb10c": {"__data__": {"id_": "a1b14bcc-7f65-4ba5-820b-de94d15bb10c", "embedding": null, "metadata": {"page_label": "254", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0eede7f1-dbdd-4084-a6ae-3edec85ca30c", "node_type": "4", "metadata": {"page_label": "254", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d53d0d2aa2b8ca2cce09d3e42118b858af234ddde1f6817a89ea1ff75a99deb3", "class_name": "RelatedNodeInfo"}}, "text": "254 Convolutional Neural Networks\nthink of each channel as responding to a different set of features. The reality is a bit more\ncomplicatedthanthis. Anaiveinterpretationwouldsuggestthatrepresentationsarelearned\nindependentlyperpixelorperchannel. Instead,channelsareoptimizedtobejointlyuseful.\nThis means that rather than mapping a single channel to an edge detector, it may simply\nmean that some direction in channel space corresponds to detecting edges.\nDenoteby\ud835\udc50iand\ud835\udc50othenumberofinputandoutputchannels,respectively,andby \ud835\udc58hand\ud835\udc58w\nthe height and width of the kernel. To get an output with multiple channels, we can create\na kernel tensor of shape \ud835\udc50i\u0002\ud835\udc58h\u0002\ud835\udc58wforeveryoutput channel. We concatenate them on the\noutput channel dimension, so that the shape of the convolution kernel is \ud835\udc50o\u0002\ud835\udc50i\u0002\ud835\udc58h\u0002\ud835\udc58w.\nIn cross-correlation operations, the result on each output channel is calculated from the\nconvolution kernel corresponding to that output channel and takes input from all channels\nin the input tensor.\nWe implement a cross-correlation function to calculate the output of multiple channels as\nshown below.\ndef corr2d_multi_in_out (X, K):\n# Iterate through the 0th dimension of K, and each time, perform\n# cross-correlation operations with input X. All of the results are\n# stacked together\nreturn torch .stack([corr2d_multi_in(X, k) for kinK], 0)\nWe construct a trivial convolution kernel with three output channels by concatenating the\nkernel tensor for Kwith K+1andK+2.\nK=torch .stack((K, K +1, K +2),0)\nK.shape\ntorch .Size([ 3,2,2,2])\nBelow,weperformcross-correlationoperationsontheinputtensor Xwiththekerneltensor\nK. Now the output contains three channels. The result of the first channel is consistent with\nthe result of the previous input tensor Xand the multi-input channel, single-output channel\nkernel.\ncorr2d_multi_in_out(X, K)\ntensor([[[ 56.,72.],\n[104. ,120. ]],\n[[76.,100. ],\n[148. ,172. ]],\n[[96.,128. ],\n[192. ,224. ]]])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3c7b034-046c-4505-bd28-358180dce4eb": {"__data__": {"id_": "a3c7b034-046c-4505-bd28-358180dce4eb", "embedding": null, "metadata": {"page_label": "255", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d29ebef-9e59-4f32-83e3-952018e16349", "node_type": "4", "metadata": {"page_label": "255", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0989ca6e9fbb2fec97067f9af2bdd5ce11a94a5b8140d0cfa1a4b008a85a8fe4", "class_name": "RelatedNodeInfo"}}, "text": "255 Multiple Input and Multiple Output Channels\n7.4.3 1\u00021ConvolutionalLayer\nAt first, a 1\u00021convolution, i.e., \ud835\udc58h=\ud835\udc58w=1, does not seem to make much sense.\nAfter all, a convolution correlates adjacent pixels. A 1\u00021convolution obviously does\nnot. Nonetheless, they are popular operations that are sometimes included in the designs\nof complex deep networks ( Linetal., 2013,Szegedyetal., 2017). Let\u2019s see in some detail\nwhat it actually does.\nBecausetheminimumwindowisused, the 1\u00021convolutionlosestheabilityoflargercon-\nvolutional layers to recognize patterns consisting of interactions among adjacent elements\nin the height and width dimensions. The only computation of the 1\u00021convolution occurs\non the channel dimension.\nFig.7.4.2 showsthecross-correlationcomputationusingthe 1\u00021convolutionkernelwith3\ninputchannelsand2outputchannels. Notethattheinputsandoutputshavethesameheight\nand width. Each element in the output is derived from a linear combination of elements at\nthe same position in the input image. You could think of the 1\u00021convolutional layer as\nconstituting a fully connected layer applied at every single pixel location to transform the\n\ud835\udc50icorresponding input values into \ud835\udc50ooutput values. Because this is still a convolutional\nlayer, the weights are tied across pixel location. Thus the 1\u00021convolutional layer requires\n\ud835\udc50o\u0002\ud835\udc50iweights (plus the bias). Also note that convolutional layers are typically followed\nby nonlinearities. This ensures that 1\u00021convolutions cannot simply be folded into other\nconvolutions.\ntFig. 7.4.2 The cross-correlation computation uses the 1 \u00021 convolution kernel with three input\nchannels and two output channels. The input and output have the same height and width.\nLet\u2019s check whether this works in practice: we implement a 1\u00021convolution using a fully\nconnectedlayer. Theonlythingisthatweneedtomakesomeadjustmentstothedatashape\nbefore and after the matrix multiplication.\ndef corr2d_multi_in_out_1x1 (X, K):\nc_i, h, w =X.shape\nc_o =K.shape[ 0]\nX=X.reshape((c_i, h *w))\nK=K.reshape((c_o, c_i))\n# Matrix multiplication in the fully connected layer\nY=torch .matmul(K, X)\nreturn Y.reshape((c_o, h, w))\nWhenperforming 1\u00021convolutions,theabovefunctionisequivalenttothepreviouslyim-\nplemented cross-correlation function corr2d_multi_in_out . Let\u2019s check this with some\nsample data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2312, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43ca4f5b-6672-4e88-9a19-391df0ee778b": {"__data__": {"id_": "43ca4f5b-6672-4e88-9a19-391df0ee778b", "embedding": null, "metadata": {"page_label": "256", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d9e1f79-327b-4349-98ae-0909f1d20eaa", "node_type": "4", "metadata": {"page_label": "256", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ac98c175c6eeac291d3fe497bd9fe4ec454d75382052eed36669e7f29eaccca5", "class_name": "RelatedNodeInfo"}}, "text": "256 Convolutional Neural Networks\nX=torch .normal( 0,1, (3,3,3))\nK=torch .normal( 0,1, (2,3,1,1))\nY1=corr2d_multi_in_out_1x1(X, K)\nY2=corr2d_multi_in_out(X, K)\nassert float (torch .abs(Y1 -Y2).sum()) <1e-6\n7.4.4Discussion\nChannels allow us to combine the best of both worlds: MLPs that allow for significant\nnonlinearities and convolutions that allow for localized analysis of features. In particular,\nchannels allow the CNN to reason with multiple features, such as edge and shape detec-\ntors at the same time. They also offer a practical trade-off between the drastic parameter\nreduction arising from translation invariance and locality, and the need for expressive and\ndiverse models in computer vision.\nNote, though, that thisflexibilitycomes ata price. Givenanimageofsize \u00b9\u210e\u0002\ud835\udc64\u00ba, thecost\nfor computing a \ud835\udc58\u0002\ud835\udc58convolution isO\u00b9\u210e\u0001\ud835\udc64\u0001\ud835\udc582\u00ba. For\ud835\udc50iand\ud835\udc50oinput and output channels\nrespectively this increases to O\u00b9\u210e\u0001\ud835\udc64\u0001\ud835\udc582\u0001\ud835\udc50i\u0001\ud835\udc50o\u00ba. For a 256\u0002256pixel image with a\n5\u00025kernel and 128input and output channels respectively this amounts to over 53 billion\noperations (we count multiplications and additions separately). Later on we will encounter\neffective strategies to cut down on the cost, e.g., by requiring the channel-wise operations\nto be block-diagonal, leading to architectures such as ResNeXt ( Xieetal., 2017).\n7.4.5Exercises\n1.Assume that we have two convolution kernels of size \ud835\udc581and\ud835\udc582, respectively (with no\nnonlinearity in between).\n1.Prove that the result of the operation can be expressed by a single convolution.\n2.What is the dimensionality of the equivalent single convolution?\n3.Is the converse true, i.e., can you always decompose a convolution into two smaller\nones?\n2.Assumeaninputofshape \ud835\udc50i\u0002\u210e\u0002\ud835\udc64andaconvolutionkernelofshape \ud835\udc50o\u0002\ud835\udc50i\u0002\ud835\udc58h\u0002\ud835\udc58w,\npadding of\u00b9\ud835\udc5dh,\ud835\udc5dw\u00ba, and stride of\u00b9\ud835\udc60h,\ud835\udc60w\u00ba.\n1.What is the computational cost (multiplications and additions) for the forward prop-\nagation?\n2.What is the memory footprint?\n3.What is the memory footprint for the backward computation?\n4.What is the computational cost for the backpropagation?\n3.By what factor does the number of calculations increase if we double both the number\nof input channels \ud835\udc50iand the number of output channels \ud835\udc50o? What happens if we double\nthe padding?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dde949c0-78a0-40da-8ad9-2a3ecb602534": {"__data__": {"id_": "dde949c0-78a0-40da-8ad9-2a3ecb602534", "embedding": null, "metadata": {"page_label": "257", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1c1a9b5-7673-4e63-9261-279a897f5773", "node_type": "4", "metadata": {"page_label": "257", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0c45e8c7a8d8c780fa9165b14b92aad651e990e59d7f7a294f019ff6cd1bc14f", "class_name": "RelatedNodeInfo"}}, "text": "257 Pooling\n1224.Are the variables Y1andY2in the final example of this section exactly the same? Why?\n5.Express convolutions as a matrix multiplication, even when the convolution window is\nnot1\u00021.\n6.Your task is to implement fast convolutions with a \ud835\udc58\u0002\ud835\udc58kernel. One of the algorithm\ncandidates is to scan horizontally across the source, reading a \ud835\udc58-wide strip and comput-\ning the 1-wide output strip one value at a time. The alternative is to read a \ud835\udc58\u00b8\u0394wide\nstrip and compute a \u0394-wide output strip. Why is the latter preferable? Is there a limit to\nhow large you should choose \u0394?\n7.Assume that we have a \ud835\udc50\u0002\ud835\udc50matrix.\n1.Howmuchfasterisittomultiplywithablock-diagonalmatrixifthematrixisbroken\nup into\ud835\udc4fblocks?\n2.What is the downside of having \ud835\udc4fblocks? How could you fix it, at least partly?\nDiscussions122.\n7.5Pooling\nIn many cases our ultimate task asks some global question about the image, e.g., does it\ncontain a cat? Consequently, the units of our final layer should be sensitive to the entire\ninput. By gradually aggregating information, yielding coarser and coarser maps, we ac-\ncomplish this goal of ultimately learning a global representation, while keeping all of the\nadvantages of convolutional layers at the intermediate layers of processing. The deeper\nwe go in the network, the larger the receptive field (relative to the input) to which each\nhidden node is sensitive. Reducing spatial resolution accelerates this process, since the\nconvolution kernels cover a larger effective area.\nMoreover, when detecting lower-level features, such as edges (as discussed in Section 7.2 ),\nwe often want our representations to be somewhat invariant to translation. For instance,\nif we take the image Xwith a sharp delineation between black and white and shift the\nwhole image by one pixel to the right, i.e., Z[i, j] = X[i, j + 1] , then the output\nfor the new image Zmight be vastly different. The edge will have shifted by one pixel. In\nreality, objects hardly ever occur exactly at the same place. In fact, even with a tripod and\na stationary object, vibration of the camera due to the movement of the shutter might shift\neverything by a pixel or so (high-end cameras are loaded with special features to address\nthis problem).\nThis section introduces pooling layers , which serve the dual purposes of mitigating the\nsensitivity of convolutional layers to location and of spatially downsampling representa-\ntions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35ce7004-3c39-4604-90c9-30fb268bd43f": {"__data__": {"id_": "35ce7004-3c39-4604-90c9-30fb268bd43f", "embedding": null, "metadata": {"page_label": "258", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f7132beb-df11-4b51-8a84-b39cc2c557e3", "node_type": "4", "metadata": {"page_label": "258", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "421affbfde9f3e39a4022a93db8c31e29ede8d6b3b8a63448a86d98b70a3ece8", "class_name": "RelatedNodeInfo"}}, "text": "258 Convolutional Neural Networks\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n7.5.1Maximum Poolingand AveragePooling\nLike convolutional layers, pooling operators consist of a fixed-shape window that is slid\nover all regions in the input according to its stride, computing a single output for each lo-\ncation traversed by the fixed-shape window (sometimes known as the pooling window ).\nHowever, unlike the cross-correlation computation of the inputs and kernels in the con-\nvolutional layer, the pooling layer contains no parameters (there is no kernel). Instead,\npooling operators are deterministic, typically calculating either the maximum or the aver-\nage value of the elements in the pooling window. These operations are called maximum\npooling(max-pooling for short) and averagepooling , respectively.\nAveragepooling isessentiallyasoldasCNNs. Theideaisakintodownsamplinganimage.\nRather than just taking the value of every second (or third) pixel for the lower resolution\nimage, we can average over adjacent pixels to obtain an image with better signal-to-noise\nratio since we are combining the information from multiple adjacent pixels. Max-pooling\nwas introduced in Riesenhuber and Poggio ( 1999) in the context of cognitive neuroscience\ntodescribehowinformationaggregationmightbeaggregatedhierarchicallyforthepurpose\nofobjectrecognition;therealreadywasanearlierversioninspeechrecognition( Yamaguchi\net al., 1990). In almost all cases, max-pooling, as it is also referred to, is preferable to\naverage pooling.\nIn both cases, as with the cross-correlation operator, we can think of the pooling window\nas starting from the upper-left of the input tensor and sliding across it from left to right and\ntop to bottom. At each location that the pooling window hits, it computes the maximum or\naverage value of the input subtensor in the window, depending on whether max or average\npooling is employed.\ntFig. 7.5.1 Max-pooling with a pooling window shape of 2 \u00022. The shaded portions are the \ufb01rst\noutput element as well as the input tensor elements used for the output computation:\nmax\u00b90,1,3,4\u00ba=4.\nThe output tensor in Fig. 7.5.1 has a height of 2 and a width of 2. The four elements are", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24f0e8eb-c3d8-4f20-a168-f47585aa0345": {"__data__": {"id_": "24f0e8eb-c3d8-4f20-a168-f47585aa0345", "embedding": null, "metadata": {"page_label": "259", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe8ebbc1-5be8-46af-b5c8-d9ba47012550", "node_type": "4", "metadata": {"page_label": "259", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b7949bceaf9df0f1dc9b064d24da1d72bbf7d453295ad3e0b1faeffb62d5c816", "class_name": "RelatedNodeInfo"}}, "text": "259 Pooling\nderived from the maximum value in each pooling window:\nmax\u00b90,1,3,4\u00ba=4,\nmax\u00b91,2,4,5\u00ba=5,\nmax\u00b93,4,6,7\u00ba=7,\nmax\u00b94,5,7,8\u00ba=8.(7.5.1)\nMore generally, we can define a \ud835\udc5d\u0002\ud835\udc5epooling layer by aggregating over a region of said\nsize. Returning to the problem of edge detection, we use the output of the convolutional\nlayer as input for 2\u00022max-pooling. Denote by Xthe input of the convolutional layer input\nandYthe pooling layer output. Regardless of whether or not the values of X[i, j] ,X[i,\nj + 1],X[i+1, j] andX[i+1, j + 1] are different, the pooling layer always outputs\nY[i, j] = 1 . That is to say, using the 2\u00022max-pooling layer, we can still detect if the\npattern recognized by the convolutional layer movesno more than one element in height or\nwidth.\nInthecodebelow,weimplementtheforwardpropagationofthepoolinglayerinthe pool2d\nfunction. Thisfunctionissimilartothe corr2dfunctionin Section7.2 . However,nokernel\nisneeded, computingtheoutputaseitherthemaximumortheaverageofeachregioninthe\ninput.\ndef pool2d (X, pool_size, mode ='max'):\np_h, p_w =pool_size\nY=torch .zeros((X .shape[ 0]-p_h +1, X.shape[ 1]-p_w +1))\nfor iinrange (Y.shape[ 0]):\nfor jinrange (Y.shape[ 1]):\nifmode =='max':\nY[i, j] =X[i: i +p_h, j: j +p_w] .max()\nelif mode =='avg':\nY[i, j] =X[i: i +p_h, j: j +p_w] .mean()\nreturn Y\nWecanconstructtheinputtensor XinFig.7.5.1 tovalidatetheoutputofthetwo-dimensional\nmax-pooling layer.\nX=torch .tensor([[ 0.0,1.0,2.0], [ 3.0,4.0,5.0], [ 6.0,7.0,8.0]])\npool2d(X, ( 2,2))\ntensor([[ 4.,5.],\n[7.,8.]])\nAlso, we can experiment with the average pooling layer.\npool2d(X, ( 2,2),'avg')\ntensor([[ 2.,3.],\n[5.,6.]])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "998d537a-a824-425d-b275-0f3a77767c73": {"__data__": {"id_": "998d537a-a824-425d-b275-0f3a77767c73", "embedding": null, "metadata": {"page_label": "260", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20335c3f-0b91-447b-939e-af2e36e22355", "node_type": "4", "metadata": {"page_label": "260", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f9498d614c927e77c63c2a3d5a5bb2149cd0e1734e2b26eeb6662e07cd8bdf38", "class_name": "RelatedNodeInfo"}}, "text": "260 Convolutional Neural Networks\n7.5.2Paddingand Stride\nAswithconvolutionallayers,poolinglayerschangetheoutputshape. Andasbefore,wecan\nadjusttheoperationtoachieveadesiredoutputshapebypaddingtheinputandadjustingthe\nstride. We can demonstrate the use of padding and strides in pooling layers via the built-in\ntwo-dimensional max-pooling layer from the deep learning framework. We first construct\nan input tensor Xwhose shape has four dimensions, where the number of examples (batch\nsize) and number of channels are both 1.\nX=torch .arange( 16, dtype =torch .float32) .reshape(( 1,1,4,4))\nX\ntensor([[[[ 0.,1.,2.,3.],\n[4.,5.,6.,7.],\n[8.,9.,10.,11.],\n[12.,13.,14.,15.]]]])\nSince pooling aggregates information from an area, deep learning frameworks default to\nmatching pooling window sizes and stride. For instance, if we use a pooling window of\nshape (3, 3)we get a stride shape of (3, 3)by default.\npool2d =nn.MaxPool2d( 3)\n# Pooling has no model parameters, hence it needs no initialization\npool2d(X)\ntensor([[[[ 10.]]]])\nNeedless to say, the stride and padding can be manually specified to override framework\ndefaults if required.\npool2d =nn.MaxPool2d( 3, padding =1, stride =2)\npool2d(X)\ntensor([[[[ 5.,7.],\n[13.,15.]]]])\nOf course, we can specify an arbitrary rectangular pooling window with arbitrary height\nand width respectively, as the example below shows.\npool2d =nn.MaxPool2d(( 2,3), stride =(2,3), padding =(0,1))\npool2d(X)\ntensor([[[[ 5.,7.],\n[13.,15.]]]])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1466, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "567cb8f3-cd58-4708-8595-471f3fef1d32": {"__data__": {"id_": "567cb8f3-cd58-4708-8595-471f3fef1d32", "embedding": null, "metadata": {"page_label": "261", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fb3cca11-1b34-438b-a98d-8f55202472af", "node_type": "4", "metadata": {"page_label": "261", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "598127a403b6813e440881eed00008a450dfd238a341dcee58537d6565ba5e73", "class_name": "RelatedNodeInfo"}}, "text": "261 Pooling\n7.5.3Multiple Channels\nWhen processing multi-channel input data, the pooling layer pools each input channel sep-\narately, rather than summing the inputs up over channels as in a convolutional layer. This\nmeansthatthenumberofoutputchannelsforthepoolinglayeristhesameasthenumberof\ninput channels. Below, we will concatenate tensors XandX + 1on the channel dimension\nto construct an input with two channels.\nX=torch .cat((X, X +1),1)\nX\ntensor([[[[ 0.,1.,2.,3.],\n[4.,5.,6.,7.],\n[8.,9.,10.,11.],\n[12.,13.,14.,15.]],\n[[1.,2.,3.,4.],\n[5.,6.,7.,8.],\n[9.,10.,11.,12.],\n[13.,14.,15.,16.]]]])\nAs we can see, the number of output channels is still two after pooling.\npool2d =nn.MaxPool2d( 3, padding =1, stride =2)\npool2d(X)\ntensor([[[[ 5.,7.],\n[13.,15.]],\n[[6.,8.],\n[14.,16.]]]])\n7.5.4Summary\nPooling is an exceedingly simple operation. It does exactly what its name indicates, ag-\ngregate results over a window of values. All convolution semantics, such as strides and\npadding apply in the same way as they did previously. Note that pooling is indifferent to\nchannels, i.e., it leaves the number of channels unchanged and it applies to each channel\nseparately. Lastly,ofthetwopopularpoolingchoices,max-poolingispreferabletoaverage\npooling, as it confers some degree of invariance to output. A popular choice is to pick a\npooling window size of 2\u00022to quarter the spatial resolution of output.\nNotethattherearemanymorewaysofreducingresolutionbeyondpooling. Forinstance,in\nstochastic pooling ( Zeiler and Fergus, 2013 ) and fractional max-pooling ( Graham, 2014 )\naggregation is combined with randomization. This can slightly improve the accuracy in\nsome cases. Lastly, as we will see later with the attention mechanism, there are more\nrefined ways of aggregating over outputs, e.g., by using the alignment between a query and\nrepresentation vectors.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1849, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd62ba68-90c0-4918-a4fc-fd3dc2085175": {"__data__": {"id_": "cd62ba68-90c0-4918-a4fc-fd3dc2085175", "embedding": null, "metadata": {"page_label": "262", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "066828ac-6f9d-4493-8586-5ab0ab80fca2", "node_type": "4", "metadata": {"page_label": "262", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "37d297d49a3e87ca9a09ccabaeb8eafab3e3e69ddf39e343fa40d34b3754bd15", "class_name": "RelatedNodeInfo"}}, "text": "262 Convolutional Neural Networks\n1237.5.5Exercises\n1.Implement average pooling through a convolution.\n2.Prove that max-pooling cannot be implemented through a convolution alone.\n3.Max-pooling can be accomplished using ReLU operations, i.e., ReLU \u00b9\ud835\udc65\u00ba=max\u00b90,\ud835\udc65\u00ba.\n1.Express max\u00b9\ud835\udc4e,\ud835\udc4f\u00baby using only ReLU operations.\n2.Use this to implement max-pooling by means of convolutions and ReLU layers.\n3.How many channels and layers do you need for a 2\u00022convolution? How many for\na3\u00023convolution?\n4.Whatisthecomputationalcostofthepoolinglayer? Assumethattheinputtothepooling\nlayer is of size \ud835\udc50\u0002\u210e\u0002\ud835\udc64, the pooling window has a shape of \ud835\udc5dh\u0002\ud835\udc5dwwith a padding of\n\u00b9\ud835\udc5dh,\ud835\udc5dw\u00baand a stride of\u00b9\ud835\udc60h,\ud835\udc60w\u00ba.\n5.Why do you expect max-pooling and average pooling to work differently?\n6.Do we need a separate minimum pooling layer? Can you replace it with another opera-\ntion?\n7.We could use the softmax operation for pooling. Why might it not be so popular?\nDiscussions123.\n7.6ConvolutionalNeuralNetworks(LeNet)\nWenowhavealltheingredientsrequiredtoassembleafully-functionalCNN.Inourearlier\nencounterwithimagedata,weappliedalinearmodelwithsoftmaxregression( Section4.4 )\nand an MLP ( Section 5.2 ) to pictures of clothing in the Fashion-MNIST dataset. To make\nsuch data amenable we first flattened each image from a 28\u000228matrix into a fixed-length\n784-dimensional vector, and thereafter processed them in fully connected layers. Now that\nwe have a handle on convolutional layers, we can retain the spatial structure in our images.\nAs an additional benefit of replacing fully connected layers with convolutional layers, we\nwill enjoy more parsimonious models that require far fewer parameters.\nIn this section, we will introduce LeNet, among the first published CNNs to capture wide\nattention for its performance on computer vision tasks. The model was introduced by (and\nnamed for) Yann LeCun, then a researcher at AT&T Bell Labs, for the purpose of rec-\nognizing handwritten digits in images ( LeCunet al., 1998). This work represented the\nculmination of a decade of research developing the technology; LeCun\u2019s team published\nthefirststudytosuccessfullytrainCNNsviabackpropagation( LeCunetal.,1989).\nAtthetimeLeNetachievedoutstandingresultsmatchingtheperformanceofsupportvector\nmachines, then a dominant approach in supervised learning, achieving an error rate of less", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a68d825-32a7-48ff-9bb5-31b3fad2a607": {"__data__": {"id_": "7a68d825-32a7-48ff-9bb5-31b3fad2a607", "embedding": null, "metadata": {"page_label": "263", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a66eadd1-079f-4afc-8483-fbdd56f3312c", "node_type": "4", "metadata": {"page_label": "263", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ee4b7cc2a1cc8447559f422abdc6b1ab2b9caddb81d7aa1dd766dc3b2178870f", "class_name": "RelatedNodeInfo"}}, "text": "263 Convolutional Neural Networks (LeNet)\nthan1%perdigit. LeNetwaseventuallyadaptedtorecognizedigitsforprocessingdeposits\nin ATM machines. To this day, some ATMs still run the code that Yann LeCun and his\ncolleague Leon Bottou wrote in the 1990s!\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n7.6.1LeNet\nAt a high level, LeNet (LeNet-5) consists of two parts: (i) a convolutional encoder consist-\ning of two convolutional layers; and (ii) a dense block consisting of three fully connected\nlayers. The architecture is summarized in Fig. 7.6.1 .\ntFig. 7.6.1 Data \ufb02ow in LeNet. The input is a handwritten digit, the output is a probability over 10\npossible outcomes.\nThe basic units in each convolutional block are a convolutional layer, a sigmoid activation\nfunction, and a subsequent average pooling operation. Note that while ReLUs and max-\npoolingworkbetter,theyhadnotyetbeendiscovered. Eachconvolutionallayerusesa 5\u00025\nkernel and a sigmoid activation function. These layers map spatially arranged inputs to a\nnumberoftwo-dimensionalfeaturemaps,typicallyincreasingthenumberofchannels. The\nfirstconvolutionallayerhas6outputchannels,whilethesecondhas16. Each 2\u00022pooling\noperation (stride 2) reduces dimensionality by a factor of 4via spatial downsampling. The\nconvolutional block emits an output with shape given by (batch size, number of channel,\nheight, width).\nInordertopassoutputfromtheconvolutionalblocktothedenseblock,wemustflatteneach\nexampleintheminibatch. Inotherwords,wetakethisfour-dimensionalinputandtransform\nitintothetwo-dimensionalinputexpectedbyfullyconnectedlayers: asareminder,thetwo-\ndimensional representation that we desire uses the first dimension to index examples in the\nminibatch and the second to give the flat vector representation of each example. LeNet\u2019s\ndense block has three fully connected layers, with 120, 84, and 10 outputs, respectively.\nBecausewearestillperformingclassification,the10-dimensionaloutputlayercorresponds\nto the number of possible output classes.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2011, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "851ac805-6411-42f1-86fb-8ccfc6d627e1": {"__data__": {"id_": "851ac805-6411-42f1-86fb-8ccfc6d627e1", "embedding": null, "metadata": {"page_label": "264", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "551d0ed5-7629-4421-8a4f-41ce62de51fb", "node_type": "4", "metadata": {"page_label": "264", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e9fe242a05a0291107e080aa559da2752198526423cca4a6683d5760258e9594", "class_name": "RelatedNodeInfo"}}, "text": "264 Convolutional Neural Networks\nWhile getting to the point where you truly understand what is going on inside LeNet may\nhave taken a bit of work, we hope that the following code snippet will convince you that\nimplementing such models with modern deep learning frameworks is remarkably simple.\nWe need only to instantiate a Sequential block and chain together the appropriate layers,\nusing Xavier initialization as introduced in Section 5.4.2 .\ndef init_cnn (module): #@save\n\"\"\"Initialize weights for CNNs.\"\"\"\niftype (module) ==nn.Linear ortype (module) ==nn.Conv2d:\nnn.init .xavier_uniform_(module .weight)\nclass LeNet (d2l .Classifier): #@save\n\"\"\"The LeNet-5 model.\"\"\"\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(\nnn.LazyConv2d( 6, kernel_size =5, padding =2), nn .Sigmoid(),\nnn.AvgPool2d(kernel_size =2, stride =2),\nnn.LazyConv2d( 16, kernel_size =5), nn .Sigmoid(),\nnn.AvgPool2d(kernel_size =2, stride =2),\nnn.Flatten(),\nnn.LazyLinear( 120), nn .Sigmoid(),\nnn.LazyLinear( 84), nn .Sigmoid(),\nnn.LazyLinear(num_classes))\nWe have taken some liberty in the reproduction of LeNet insofar as we have replaced the\nGaussian activation layer by a softmax layer. This greatly simplifies the implementation,\nnotleastdue tothefactthatthe Gaussiandecoderis rarelyused nowadays. Other thanthat,\nthis network matches the original LeNet-5 architecture.\nLet\u2019s see what happens inside the network. By passing a single-channel (black and white)\n28\u000228imagethroughthenetworkandprintingtheoutputshapeateachlayer,wecaninspect\nthe model to ensure that its operations line up with what we expect from Fig. 7.6.2 .\ntFig. 7.6.2 Compressed notation for LeNet-5.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1718, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "480d0216-b617-44eb-94bf-09aee77bc2d3": {"__data__": {"id_": "480d0216-b617-44eb-94bf-09aee77bc2d3", "embedding": null, "metadata": {"page_label": "265", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3a9e826-5924-4523-b678-f5a8926c79ed", "node_type": "4", "metadata": {"page_label": "265", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "86e661d1b7c76751ee2d4fe00f4a3cfc69a963a6acc4fbb949de0a07d41f8a56", "class_name": "RelatedNodeInfo"}}, "text": "265 Convolutional Neural Networks (LeNet)\n@d2l .add_to_class(d2l .Classifier) #@save\ndef layer_summary (self , X_shape):\nX=torch .randn( *X_shape)\nfor layer inself .net:\nX=layer(X)\nprint (layer .__class__ .__name__ ,'output shape: \\t', X.shape)\nmodel =LeNet()\nmodel .layer_summary(( 1,1,28,28))\nConv2d output shape: torch .Size([ 1,6,28,28])\nSigmoid output shape: torch .Size([ 1,6,28,28])\nAvgPool2d output shape: torch .Size([ 1,6,14,14])\nConv2d output shape: torch .Size([ 1,16,10,10])\nSigmoid output shape: torch .Size([ 1,16,10,10])\nAvgPool2d output shape: torch .Size([ 1,16,5,5])\nFlatten output shape: torch .Size([ 1,400])\nLinear output shape: torch .Size([ 1,120])\nSigmoid output shape: torch .Size([ 1,120])\nLinear output shape: torch .Size([ 1,84])\nSigmoid output shape: torch .Size([ 1,84])\nLinear output shape: torch .Size([ 1,10])\nNote that the height and width of the representation at each layer throughout the convolu-\ntional block is reduced (compared with the previous layer). The first convolutional layer\nuses two pixels of padding to compensate for the reduction in height and width that would\notherwise result from using a 5\u00025kernel. As an aside, the image size of 28\u000228pixels in\ntheoriginalMNISTOCRdatasetisaresultof trimming twopixelrows(andcolumns)from\nthe original scans that measured 32\u000232pixels. This was done primarily to save space (a\n30% reduction) at a time when megabytes mattered.\nIn contrast, the second convolutional layer forgoes padding, and thus the height and width\nare both reduced by four pixels. As we go up the stack of layers, the number of channels\nincreases layer-over-layer from 1 in the input to 6 after the first convolutional layer and\n16 after the second convolutional layer. However, each pooling layer halves the height and\nwidth. Finally,eachfullyconnectedlayerreducesdimensionality,finallyemittinganoutput\nwhose dimension matches the number of classes.\n7.6.2Training\nNow that we have implemented the model, let\u2019s run an experiment to see how the LeNet-5\nmodel fares on Fashion-MNIST.\nWhile CNNs have fewer parameters, they can still be more expensive to compute than\nsimilarly deep MLPs because each parameter participates in many more multiplications.\nIf you have access to a GPU, this might be a good time to put it into action to speed up\ntraining. Note that the d2l.Trainer class takes care of all details. By default, it initializes\nthe model parameters on the available devices. Just as with MLPs, our loss function is\ncross-entropy, and we minimize it via minibatch stochastic gradient descent.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c372395-a0ef-46b8-afe1-185d2222344d": {"__data__": {"id_": "8c372395-a0ef-46b8-afe1-185d2222344d", "embedding": null, "metadata": {"page_label": "266", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d16353fc-f3c9-4b37-9757-265c6993b23e", "node_type": "4", "metadata": {"page_label": "266", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cf3539c4eea6fdc62d2247ad902c402786b78d1545218d4283df1281e4c18cca", "class_name": "RelatedNodeInfo"}}, "text": "266 Convolutional Neural Networks\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128)\nmodel =LeNet(lr =0.1)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], init_cnn)\ntrainer .fit(model, data)\n7.6.3Summary\nWe have made significant progress in this chapter. We moved from the MLPs of the 1980s\nto the CNNs of the 1990s and early 2000s. The architectures proposed, e.g., in the form\nof LeNet-5 remain meaningful, even to this day. It is worth comparing the error rates on\nFashion-MNISTachievablewithLeNet-5bothtotheverybestpossiblewithMLPs( Section\n5.2)andthosewithsignificantlymoreadvancedarchitecturessuchasResNet( Section8.6 ).\nLeNetismuchmoresimilartothelatterthantotheformer. Oneoftheprimarydifferences,\nasweshallsee, isthatgreateramountsofcomputationenabledsignificantlymorecomplex\narchitectures.\nAseconddifferenceistherelativeeasewithwhichwewereabletoimplementLeNet. What\nused to be an engineering challenge worth months of C++ and assembly code, engineering\nto improve SN, an early Lisp-based deep learning tool ( Bottou and Le Cun, 1988 ), and fi-\nnallyexperimentationwithmodelscannowbeaccomplishedinminutes. Itisthisincredible\nproductivity boost that has democratized deep learning model development tremendously.\nIn the next chapter we will journey down this rabbit to hole to see where it takes us.\n7.6.4Exercises\n1.Let\u2019s modernize LeNet. Implement and test the following changes:\n1.Replace average pooling with max-pooling.\n2.Replace the softmax layer with ReLU.\n2.Try to change the size of the LeNet style network to improve its accuracy in addition to\nmax-pooling and ReLU.\n1.Adjust the convolution window size.\n2.Adjust the number of output channels.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0fd2e343-9acd-4b12-9d36-dd53e6670ffc": {"__data__": {"id_": "0fd2e343-9acd-4b12-9d36-dd53e6670ffc", "embedding": null, "metadata": {"page_label": "267", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "40271db9-e37b-4768-9a86-07238d1bd624", "node_type": "4", "metadata": {"page_label": "267", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fc0b3c1f362a6b4ee533b3a15d133a5a99ba445f814dc03cf37c8f87efe05dab", "class_name": "RelatedNodeInfo"}}, "text": "267 Convolutional Neural Networks (LeNet)\n1243.Adjust the number of convolution layers.\n4.Adjust the number of fully connected layers.\n5.Adjustthe learning rates and other training details (e.g., initialization and number of\nepochs).\n3.Try out the improved network on the original MNIST dataset.\n4.Display the activations of the first and second layer of LeNet for different inputs (e.g.,\nsweaters and coats).\n5.What happens to the activations when you feed significantly different images into the\nnetwork (e.g., cats, cars, or even random noise)?\nDiscussions124.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74b0b8de-94f4-40e6-ab33-ecfd07fa510c": {"__data__": {"id_": "74b0b8de-94f4-40e6-ab33-ecfd07fa510c", "embedding": null, "metadata": {"page_label": "268", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9fb68c1d-a2bd-47b8-be16-f8e32ffab8f2", "node_type": "4", "metadata": {"page_label": "268", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ab3bcf008bcabf6b14b796bff127d0773b19303932ec85928e406d13ab811711", "class_name": "RelatedNodeInfo"}}, "text": "125\n8 Modern Convolutional Neural Networks\nNow that we understand the basics of wiring together CNNs, let\u2019s take a tour of modern\nCNN architectures. This tour is, by necessity, incomplete, thanks to the plethora of excit-\ningnewdesignsbeingadded. Theirimportancederivesfromthefactthatnotonlycanthey\nbe used directly for vision tasks, but they also serve as basic feature generators for more\nadvancedtaskssuchastracking( Zhangetal., 2021), segmentation( Longetal., 2015), ob-\nject detection ( Redmon and Farhadi, 2018 ), or style transformation ( Gatysetal., 2016). In\nthis chapter, most sections correspond to a significant CNN architecture that was at some\npoint (or currently) the base model upon which many research projects and deployed sys-\ntemswerebuilt. Eachofthesenetworkswasbrieflyadominantarchitectureandmanywere\nwinners or runners-up in the ImageNet competition125which has served as a barometer\nof progress on supervised learning in computer vision since 2010. It is only recently that\nTransformers have begun to displace CNNs, starting with Dosovitskiy et al.(2021) and\nfollowed by the Swin Transformer ( Liuet al., 2021). We will cover this development later\ninChapter 11 .\nWhile the idea of deepneural networks is quite simple (stack together a bunch of layers),\nperformance can vary wildly across architectures and hyperparameter choices. The neural\nnetworksdescribedinthischapteraretheproductofintuition,afewmathematicalinsights,\nandalotoftrialanderror. Wepresentthesemodelsinchronologicalorder,partlytoconvey\na sense of the history so that you can form your own intuitions about where the field is\nheadingandperhapsdevelopyourownarchitectures. Forinstance,batchnormalizationand\nresidual connections described in this chapter have offered two popular ideas for training\nand designing deep models, both of which have since also been applied to architectures\nbeyond computer vision.\nWe begin our tour of modern CNNs with AlexNet ( Krizhevsky etal., 2012), the firstlarge-\nscale network deployed to beat conventional computer vision methods on a large-scale vi-\nsion challenge; the VGG network ( Simonyan and Zisserman, 2014 ), which makes use of a\nnumberofrepeatingblocksofelements;thenetworkinnetwork(NiN)thatconvolveswhole\nneural networks patch-wise over inputs ( Linet al., 2013); GoogLeNet that uses networks\nwith multi-branch convolutions ( Szegedyet al., 2015); the residual network (ResNet) ( He\netal.,2016),whichremainsoneofthemostpopularoff-the-shelfarchitecturesincomputer\nvision; ResNeXt blocks ( Xieet al., 2017) for sparser connections; and DenseNet ( Huang\net al., 2017) for a generalization of the residual architecture. Over time many special opti-\nmizations for efficient networks have been developed, such as coordinate shifts (ShiftNet)\n(Wuet al., 2018). This culminated in the automatic search for efficient architectures such\n268", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2865, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5e21c7e-c5a9-4f7a-85a9-d08d7d90b941": {"__data__": {"id_": "b5e21c7e-c5a9-4f7a-85a9-d08d7d90b941", "embedding": null, "metadata": {"page_label": "269", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "afce9df1-82b9-439a-91c2-585166713ccd", "node_type": "4", "metadata": {"page_label": "269", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c12264155b28f6303a5e2af7add60ed85d1e219ba9e763c5feb50254abc71b76", "class_name": "RelatedNodeInfo"}}, "text": "269 Deep Convolutional Neural Networks (AlexNet)\nas MobileNet v3 ( Howardet al., 2019). It also includes the semi-automatic design explo-\nration of Radosavovic et al.(2020) that led to the RegNetX/Y which we will discuss later\nin this chapter. The work is instructive insofar as it offers a path for marrying brute force\ncomputationwiththeingenuityofanexperimenterinthesearchforefficientdesignspaces.\nOf note is also the work of Liu et al.(2022) as it shows that training techniques (e.g., op-\ntimizers, data augmentation, and regularization) play a pivotal role in improving accuracy.\nIt also shows that long-held assumptions, such as the size of a convolution window, may\nneed to be revisited, given the increase in computation and data. We will cover this and\nmany more questions in due course throughout this chapter.\n8.1DeepConvolutionalNeuralNetworks(AlexNet)\nAlthough CNNs were well known in the computer vision and machine learning commu-\nnities following the introduction of LeNet ( LeCunet al., 1995), they did not immediately\ndominate the field. Although LeNet achieved good results on early small datasets, the per-\nformance and feasibility of training CNNs on larger, more realistic datasets had yet to be\nestablished. In fact, for much of the intervening time between the early 1990s and the wa-\ntershed results of 2012 ( Krizhevsky et al., 2012), neural networks were often surpassed\nby other machine learning methods, such as kernel methods ( Sch\u00f6lkopf and Smola, 2002 ),\nensemble methods ( Freund and Schapire, 1996 ), and structured estimation ( Taskaret al.,\n2004).\nFor computer vision, this comparison is perhaps not entirely accurate. That is, although\nthe inputs to convolutional networks consist of raw or lightly-processed (e.g., by center-\ning) pixel values, practitioners would never feed raw pixels into traditional models. In-\nstead, typical computer vision pipelines consisted of manually engineering feature extrac-\ntion pipelines, such as SIFT ( Lowe, 2004 ), SURF ( Bayet al., 2006), and bags of visual\nwords (Sivic and Zisserman, 2003 ). Rather than learning the features, the features were\ncrafted. Most of the progress came from having more clever ideas for feature extraction on\nthe one hand and deep insight into geometry ( Hartley and Zisserman, 2000 ) on the other.\nThe learning algorithm was often considered an afterthought.\nAlthough some neural network accelerators were available in the 1990s, they were not yet\nsufficiently powerful to make deep multichannel, multilayer CNNs with a large number\nof parameters. For instance, NVIDIA\u2019s GeForce 256 from 1999 was able to process at\nmost 480 million floating-point operations, such as additions and multiplications, per sec-\nond (MFLOPS), without any meaningful programming framework for operations beyond\ngames. Today\u2019s accelerators are able to perform in excess of 1000 TFLOPs per device.\nMoreover, datasets were still relatively small: OCR on 60,000 low-resolution 28\u000228pixel\nimages was considered a highly challenging task. Added to these obstacles, key tricks for\ntraining neural networks including parameter initialization heuristics ( Glorot and Bengio,\n2010),clevervariantsofstochasticgradientdescent( KingmaandBa,2014 ),non-squashing", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a36bd654-ffdb-4c54-ad1a-e9784d0f9f9e": {"__data__": {"id_": "a36bd654-ffdb-4c54-ad1a-e9784d0f9f9e", "embedding": null, "metadata": {"page_label": "270", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "385d68ac-6a22-410b-8b61-4f411d2142d2", "node_type": "4", "metadata": {"page_label": "270", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2153a56a123dc1107889b1c2022b64746f27a7d917491bef9b22fe21c89676de", "class_name": "RelatedNodeInfo"}}, "text": "270 Modern Convolutional Neural Networks\n126activation functions ( Nair and Hinton, 2010 ), and effective regularization techniques ( Sri-\nvastavaetal., 2014) were still missing.\nThus, rather than training end-to-end (pixel to classification) systems, classical pipelines\nlooked more like this:\n1.Obtain an interesting dataset. In the early days, these datasets required expensive sen-\nsors. For instance, the Apple QuickTake 100126of 1994 sported a whopping 0.3\nmegapixel(VGA)resolution,capableofstoringupto8images,allforthepriceof$1000.\n2.Preprocess the dataset with hand-crafted features based on some knowledge of optics,\ngeometry, other analytic tools, and occasionally on the serendipitous discoveries by\nlucky graduate students.\n3.Feed the data through a standard set of feature extractors such as the SIFT (scale-\ninvariant feature transform) ( Lowe, 2004 ), the SURF (speeded up robust features) ( Bay\netal., 2006), or anynumber of other hand-tuned pipelines. OpenCV stillprovidesSIFT\nextractors to this day!\n4.Dump the resulting representations into your favorite classifier, likely a linear model or\nkernel method, to train a classifier.\nIf you spoke to machine learning researchers, they would reply that machine learning was\nboth important and beautiful. Elegant theories proved the properties of various classifiers\n(Boucheron et al., 2005) and convex optimization ( Boyd and Vandenberghe, 2004 ) had\nbecome the mainstay for obtaining them. The field of machine learning was thriving, rig-\norous, and eminently useful. However, if you spoke to a computer vision researcher, you\nwould hear a very different story. The dirty truth of image recognition, they would tell\nyou, is that features, geometry ( Hartley and Zisserman, 2000 ,Hartley and Kahl, 2009 ),\nand engineering, rather than novel learning algorithms, drove progress. Computer vision\nresearchers justifiably believed that a slightly bigger or cleaner dataset or a slightly im-\nprovedfeature-extractionpipelinematteredfarmoretothefinalaccuracythananylearning\nalgorithm.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n8.1.1Representation Learning\nAnotherwaytocastthestateofaffairsisthatthemostimportantpartofthepipelinewasthe\nrepresentation. And up until 2012 the representation was calculated mostly mechanically.\nIn fact, engineering a new set of feature functions, improving results, and writing up the\nmethod all featured prominently in papers. SIFT ( Lowe, 2004 ), SURF ( Bayet al., 2006),\nHOG (histograms of oriented gradient) ( Dalal and Triggs, 2005 ), bags of visual words\n(Sivic and Zisserman, 2003 ), and similar feature extractors ruled the roost.\nAnother group of researchers, including Yann LeCun, Geoff Hinton, Yoshua Bengio, An-\ndrew Ng, Shun-ichi Amari, and Juergen Schmidhuber, had different plans. They believed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f98fa6e-2be7-44ff-8b2f-0d90db9e27ef": {"__data__": {"id_": "1f98fa6e-2be7-44ff-8b2f-0d90db9e27ef", "embedding": null, "metadata": {"page_label": "271", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb998b89-ef7a-4209-8d93-d4330e4498d2", "node_type": "4", "metadata": {"page_label": "271", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "56ff89392c9e97caaa5b9ffa778f4a5385eb1a790258f8db37187cd8f79bfcab", "class_name": "RelatedNodeInfo"}}, "text": "271 Deep Convolutional Neural Networks (AlexNet)\nthat features themselves ought to be learned. Moreover, they believed that to be reasonably\ncomplex, the features ought to be hierarchically composed with multiple jointly learned\nlayers, each with learnable parameters. In the case of an image, the lowest layers might\ncome to detect edges, colors, and textures, by analogy with how the visual system in ani-\nmalsprocessesitsinput. Inparticular, theautomaticdesignofvisualfeaturessuchasthose\nobtained by sparse coding ( Olshausen and Field, 1996 ) remained an open challenge until\nthe advent of modern CNNs. It was not until Dean etal.(2012), Le (2013) that the idea of\ngenerating features from image data automatically gained significant traction.\nThe first modern CNN ( Krizhevsky etal., 2012), namedAlexNet after one of its inventors,\nAlex Krizhevsky, is largely an evolutionary improvementoverLeNet. It achievedexcellent\nperformance in the 2012 ImageNet challenge.\ntFig. 8.1.1 Image \ufb01lters learned by the \ufb01rst layer of AlexNet. Reproduction courtesy of Krizhevsky\net al. ( 2012 ).\nInterestingly, in the lowest layers of the network, the model learned feature extractors that\nresembled some traditional filters. Fig. 8.1.1 shows lower-level image descriptors. Higher\nlayers in the network might build upon these representations to represent larger structures,\nlike eyes, noses, blades of grass, and so on. Even higher layers might represent whole\nobjects like people, airplanes, dogs, or frisbees. Ultimately, the final hidden state learns a\ncompact representation of the image that summarizes its contents such that data belonging\nto different categories can be easily separated.\nAlexNet (2012) and its precursor LeNet (1995) share many architectural elements. This\nbegsthequestion: whydidittakesolong? Akeydifferencewasthat,overtheprevioustwo\ndecades,theamountofdataandthecomputingpoweravailablehadincreasedsignificantly.\nAs such AlexNet was much larger: it was trained on much more data, and on much faster\nGPUs compared to the CPUs available in 1995.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "482613fe-1d7d-4d6e-81f6-a549eea1a91d": {"__data__": {"id_": "482613fe-1d7d-4d6e-81f6-a549eea1a91d", "embedding": null, "metadata": {"page_label": "272", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "200df845-02fb-424d-80ab-40a2bffa97f3", "node_type": "4", "metadata": {"page_label": "272", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "57b147d94c035118b54f1c0081cc51603c76dceeeaf823cf173c11b3796a7857", "class_name": "RelatedNodeInfo"}}, "text": "272 Modern Convolutional Neural Networks\nMissing Ingredient: Data\nDeep models with many layers require large amounts of data in order to enter the regime\nwhere they significantly outperform traditional methods based on convex optimizations\n(e.g.,linearandkernelmethods). However,giventhelimitedstoragecapacityofcomputers,\nthe relative expense of (imaging) sensors, and the comparatively tighter research budgets\nin the 1990s, most research relied on tiny datasets. Numerous papers relied on the UCI\ncollection of datasets, many of which contained only hundreds or (a few) thousands of\nimages captured in low resolution and often with an artificially clean background.\nIn 2009, the ImageNet dataset was released ( Denget al., 2009), challenging researchers\nto learn models from 1 million examples, 1000 each from 1000 distinct categories of ob-\njects. The categories themselves were based on the most popular noun nodes in WordNet\n(Miller, 1995 ). The ImageNet team used Google Image Search to prefilter large candidate\nsets foreachcategory and employedthe Amazon MechanicalTurkcrowdsourcingpipeline\ntoconfirmforeachimagewhetheritbelongedtotheassociatedcategory. Thisscalewasun-\nprecedented, exceeding others by over an order of magnitude (e.g., CIFAR-100 has 60,000\nimages). Anotheraspectwasthattheimageswereatrelativelyhighresolutionof 224\u0002224\npixels, unlike the 80 million-sized TinyImages dataset ( Torralbaet al., 2008), consisting\nof32\u000232pixel thumbnails. This allowed for the formation of higher-level features. The\nassociated competition, dubbed the ImageNet Large Scale Visual Recognition Challenge\n(Russakovsky et al., 2015), pushed computer vision and machine learning research for-\nward, challenging researchers to identify which models performed best at a greater scale\nthanacademicshadpreviouslyconsidered. Thelargestvisiondatasets, suchasLAION-5B\n(Schuhmann etal., 2022) contain billions of images with additional metadata.\nMissing Ingredient: Hardware\nDeep learning models are voracious consumers of compute cycles. Training can take hun-\ndreds of epochs, and each iteration requires passing data through many layers of compu-\ntationally expensive linear algebra operations. This is one of the main reasons why in the\n1990s and early 2000s, simple algorithms based on the more-efficiently optimized convex\nobjectives were preferred.\nGraphical processing units (GPUs) proved to be a game changer in making deep learn-\ning feasible. These chips had earlier been developed for accelerating graphics processing\nto benefit computer games. In particular, they were optimized for high throughput 4\u00024\nmatrix\u2013vector products, which are needed for many computer graphics tasks. Fortunately,\nthe math is strikingly similar to that required for calculating convolutional layers. Around\nthat time, NVIDIA and ATI had begun optimizing GPUs for general computing opera-\ntions (Fernando, 2004 ), going as far as to market them as general-purpose GPUs (GPG-\nPUs).\nTo provide some intuition, consider the cores of a modern microprocessor (CPU). Each\nof the cores is fairly powerful running at a high clock frequency and sporting large caches\n(up to several megabytes of L3). Each core is well-suited to executing a wide range of in-\nstructions, with branch predictors, a deep pipeline, specialized execution units, speculative", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3323, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9eded505-7113-4d6f-aa02-42f709a5da66": {"__data__": {"id_": "9eded505-7113-4d6f-aa02-42f709a5da66", "embedding": null, "metadata": {"page_label": "273", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9af46be-9a60-4040-ac59-7ed70a33a7b2", "node_type": "4", "metadata": {"page_label": "273", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "59d90cde8e46f8a75edfdf2d06015d009701050f4372763709f3b6424eeb9cc2", "class_name": "RelatedNodeInfo"}}, "text": "273 Deep Convolutional Neural Networks (AlexNet)\n127execution, and many other bells and whistles that enable it to run a large variety of pro-\ngrams with sophisticated control flow. This apparent strength, however, is also its Achilles\nheel: general-purpose cores are very expensive to build. They excel at general-purpose\ncode with lots of control flow. This requires lots of chip area, not just for the actual ALU\n(arithmetic logical unit) where computation happens, but also for all the aforementioned\nbells and whistles, plus memory interfaces, caching logic between cores, high-speed in-\nterconnects, and so on. CPUs are comparatively bad at any single task when compared\nwithdedicatedhardware. Modernlaptopshave4\u20138cores,andevenhigh-endserversrarely\nexceed 64 cores per socket, simply because it is not cost-effective.\nBy comparison, GPUs can consist of thousands of small processing elements (NIVIDA\u2019s\nlatestAmperechipshaveupto6912CUDAcores),oftengroupedintolargergroups(NVIDIA\ncalls them warps). The details differ somewhat between NVIDIA, AMD, ARM and other\nchip vendors. While each core is relatively weak, running at about 1GHz clock frequency,\nit is the total number of such cores that makes GPUs orders of magnitude faster than\nCPUs. For instance, NVIDIA\u2019s recent Ampere A100 GPU offers over 300 TFLOPs per\nchip for specialized 16-bit precision (BFLOAT16) matrix-matrix multiplications, and up\nto 20 TFLOPs for more general-purpose floating point operations (FP32). At the same\ntime, floating point performance of CPUs rarely exceeds 1 TFLOPs. For instance, Ama-\nzon\u2019s Graviton 3 reaches 2 TFLOPs peak performance for 16-bit precision operations, a\nnumber similar to the GPU performance of Apple\u2019s M1 processor.\nThere are many reasons why GPUs are much faster than CPUs in terms of FLOPs. First,\npowerconsumptiontendstogrow quadratically withclockfrequency. Hence,forthepower\nbudget of a CPU core that runs four times faster (a typical number), you can use 16 GPU\ncoresat1\n4thespeed,whichyields 16\u00021\n4=4timestheperformance. Second,GPUcoresare\nmuch simpler (in fact, for a long time they were not even ableto execute general-purpose\ncode), which makes them more energy efficient. For instance, (i) they tend not to support\nspeculative evaluation, (ii) it typically is not possible to program each processing element\nindividually, and (iii) the caches per core tend to be much smaller. Last, many operations\nin deep learning require high memory bandwidth. Again, GPUs shine here with buses that\nare at least 10 times as wide as many CPUs.\nBack to 2012. A major breakthrough came when Alex Krizhevsky and Ilya Sutskever im-\nplemented a deep CNN that could run on GPUs. They realized that the computational bot-\ntlenecksinCNNs, convolutionsandmatrixmultiplications, arealloperationsthatcouldbe\nparallelized in hardware. Using two NVIDIA GTX 580s with 3GB of memory, either of\nwhichwascapableof1.5TFLOPs(stillachallengeformostCPUsadecadelater),theyim-\nplemented fast convolutions. The cuda-convnet127code was good enough that for several\nyearsitwastheindustrystandardandpoweredthefirstcoupleofyearsofthedeeplearning\nboom.\n8.1.2AlexNet\nAlexNet, which employed an 8-layer CNN, won the ImageNet Large Scale Visual Recog-\nnition Challenge 2012 by a large margin ( Russakovsky etal., 2013). This network showed,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3313, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fbe713bf-768c-4826-b541-800b2333a430": {"__data__": {"id_": "fbe713bf-768c-4826-b541-800b2333a430", "embedding": null, "metadata": {"page_label": "274", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "607f1e33-2605-491b-a23c-b8d81f574279", "node_type": "4", "metadata": {"page_label": "274", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b1da95bd8030e8b7eafc6fee6a05a3b268697e62ab3a66bc8ea1a8aad240e483", "class_name": "RelatedNodeInfo"}}, "text": "274 Modern Convolutional Neural Networks\nfor the first time, that the features obtained by learning can transcend manually-designed\nfeatures, breaking the previous paradigm in computer vision.\nThearchitecturesofAlexNetandLeNetarestrikinglysimilar,as Fig.8.1.2 illustrates. Note\nthat we provide a slightly streamlined version of AlexNet removing some of the design\nquirks that were needed in 2012 to make the model fit on two small GPUs.\ntFig. 8.1.2 From LeNet (left) to AlexNet (right).\nThere are also significant differences between AlexNet and LeNet. First, AlexNet is much\ndeeper than the comparatively small LeNet-5. AlexNet consists of eight layers: five con-\nvolutional layers, two fully connected hidden layers, and one fully connected output layer.\nSecond, AlexNet used the ReLU instead of the sigmoid as its activation function. Let\u2019s\ndelve into the details below.\nArchitecture\nIn AlexNet\u2019s first layer, the convolution window shape is 11\u000211. Since the images in\nImageNet are eight times taller and wider than the MNIST images, objects in ImageNet\ndatatendtooccupymorepixelswithmorevisualdetail. Consequently,alargerconvolution\nwindow is needed to capture the object. The convolution window shape in the second\nlayer is reduced to 5\u00025, followed by 3\u00023. In addition, after the first, second, and fifth\nconvolutional layers, the network adds max-pooling layers with a window shape of 3\u0002\n3and a stride of 2. Moreover, AlexNet has ten times more convolution channels than\nLeNet.\nAfterthefinalconvolutionallayer,therearetwohugefullyconnectedlayerswith4096out-\nputs. These layers require nearly 1GB model parameters. Because of the limited memory", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9acd5be2-cb6c-4709-a496-14bbb8ffc34c": {"__data__": {"id_": "9acd5be2-cb6c-4709-a496-14bbb8ffc34c", "embedding": null, "metadata": {"page_label": "275", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a744686d-1b9a-4f9d-aff9-2de55375798f", "node_type": "4", "metadata": {"page_label": "275", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "969f17ce901dfa1da22cb5b00141c29348a019319f79f9cc0a72a184bfde2cfe", "class_name": "RelatedNodeInfo"}}, "text": "275 Deep Convolutional Neural Networks (AlexNet)\nin early GPUs, the original AlexNet used a dual data stream design, so that each of their\ntwoGPUscouldberesponsibleforstoringandcomputingonlyitshalfofthemodel. Fortu-\nnately,GPUmemoryiscomparativelyabundantnow,sowerarelyneedtobreakupmodels\nacrossGPUsthesedays(ourversionoftheAlexNetmodeldeviatesfromtheoriginalpaper\nin this aspect).\nActivationFunctions\nFurthermore, AlexNet changed the sigmoid activation function to a simpler ReLU activa-\ntionfunction. Ontheonehand,thecomputationoftheReLUactivationfunctionissimpler.\nFor example, it does not have the exponentiation operation found in the sigmoid activation\nfunction. On the other hand, the ReLU activation function makes model training easier\nwhen using different parameter initialization methods. This is because, when the output\nof the sigmoid activation function is very close to 0 or 1, the gradient of these regions is\nalmost0,sothatbackpropagationcannotcontinuetoupdatesomeofthemodelparameters.\nBycontrast,thegradientoftheReLUactivationfunctioninthepositiveintervalisalways1\n(Section5.1.2 ). Therefore,ifthemodelparametersarenotproperlyinitialized,thesigmoid\nfunction may obtain a gradient of almost 0 in the positive interval, meaning that the model\ncannot be effectively trained.\nCapacity Controland Preprocessing\nAlexNet controls the model complexity of the fully connected layer by dropout ( Section\n5.6), while LeNet only uses weight decay. To augment the data even further, the training\nloop of AlexNet added a great deal of image augmentation, such as flipping, clipping, and\ncolor changes. This makes the model more robust and the larger sample size effectively\nreduces overfitting. See Buslaev etal.(2020) for an in-depth review of such preprocessing\nsteps.\nclass AlexNet (d2l .Classifier):\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(\nnn.LazyConv2d( 96, kernel_size =11, stride =4, padding =1),\nnn.ReLU(), nn .MaxPool2d(kernel_size =3, stride =2),\nnn.LazyConv2d( 256, kernel_size =5, padding =2), nn .ReLU(),\nnn.MaxPool2d(kernel_size =3, stride =2),\nnn.LazyConv2d( 384, kernel_size =3, padding =1), nn .ReLU(),\nnn.LazyConv2d( 384, kernel_size =3, padding =1), nn .ReLU(),\nnn.LazyConv2d( 256, kernel_size =3, padding =1), nn .ReLU(),\nnn.MaxPool2d(kernel_size =3, stride =2), nn .Flatten(),\nnn.LazyLinear( 4096 ), nn .ReLU(), nn .Dropout(p =0.5),\nnn.LazyLinear( 4096 ), nn .ReLU(),nn .Dropout(p =0.5),\nnn.LazyLinear(num_classes))\nself .net.apply(d2l .init_cnn)\nWe construct a single-channel data example with both height and width of 224 to observe\nthe output shape of each layer. It matches the AlexNet architecture in Fig. 8.1.2 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89b6a3e9-902b-41f3-8af3-6e4dfc7677dc": {"__data__": {"id_": "89b6a3e9-902b-41f3-8af3-6e4dfc7677dc", "embedding": null, "metadata": {"page_label": "276", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1dcefb27-b1a2-4c71-a453-bfb30aafe29a", "node_type": "4", "metadata": {"page_label": "276", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "38cbaff4a8ded4052c0aab919e6af708317c6f69cffbec235f83e21448786444", "class_name": "RelatedNodeInfo"}}, "text": "276 Modern Convolutional Neural Networks\nAlexNet() .layer_summary(( 1,1,224,224))\nConv2d output shape: torch .Size([ 1,96,54,54])\nReLU output shape: torch .Size([ 1,96,54,54])\nMaxPool2d output shape: torch .Size([ 1,96,26,26])\nConv2d output shape: torch .Size([ 1,256,26,26])\nReLU output shape: torch .Size([ 1,256,26,26])\nMaxPool2d output shape: torch .Size([ 1,256,12,12])\nConv2d output shape: torch .Size([ 1,384,12,12])\nReLU output shape: torch .Size([ 1,384,12,12])\nConv2d output shape: torch .Size([ 1,384,12,12])\nReLU output shape: torch .Size([ 1,384,12,12])\nConv2d output shape: torch .Size([ 1,256,12,12])\nReLU output shape: torch .Size([ 1,256,12,12])\nMaxPool2d output shape: torch .Size([ 1,256,5,5])\nFlatten output shape: torch .Size([ 1,6400 ])\nLinear output shape: torch .Size([ 1,4096 ])\nReLU output shape: torch .Size([ 1,4096 ])\nDropout output shape: torch .Size([ 1,4096 ])\nLinear output shape: torch .Size([ 1,4096 ])\nReLU output shape: torch .Size([ 1,4096 ])\nDropout output shape: torch .Size([ 1,4096 ])\nLinear output shape: torch .Size([ 1,10])\n8.1.3Training\nAlthough AlexNet was trained on ImageNet in Krizhevsky et al.(2012), we use Fashion-\nMNIST here since training an ImageNet model to convergence could take hours or days\neven on a modern GPU. One of the problems with applying AlexNet directly on Fashion-\nMNIST is that its images have lower resolution ( 28\u000228pixels) than ImageNet images. To\nmakethingswork,weupsamplethemto 224\u0002224. Thisisgenerallynotasmartpractice,as\nitsimplyincreasesthecomputationalcomplexitywithoutaddinginformation. Nonetheless,\nwe do it here to be faithful to the AlexNet architecture. We perform this resizing with the\nresizeargument in the d2l.FashionMNIST constructor.\nNow, we can start training AlexNet. Compared to LeNet in Section 7.6 , the main change\nhere is the use of a smaller learning rate and much slower training due to the deeper and\nwider network, the higher image resolution, and the more costly convolutions.\nmodel =AlexNet(lr =0.01 )\ndata =d2l.FashionMNIST(batch_size =128, resize =(224,224))\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ntrainer .fit(model, data)\n8.1.4Discussion\nAlexNet\u2019s structure bears a striking resemblance to LeNet, with a number of critical im-\nprovements, both for accuracy (dropout) and for ease of training (ReLU). What is equally\nstriking is the amount of progress that has been made in terms of deep learning tooling.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45050730-de8d-4ee4-8af4-7ea0725d9577": {"__data__": {"id_": "45050730-de8d-4ee4-8af4-7ea0725d9577", "embedding": null, "metadata": {"page_label": "277", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "864edd93-b1d8-4b2b-b4e9-3213cad3ef49", "node_type": "4", "metadata": {"page_label": "277", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d889ceb71478f27f510e5007f2c7229f186d5c71ac76ae2cbcec8a35a513ad10", "class_name": "RelatedNodeInfo"}}, "text": "277 Deep Convolutional Neural Networks (AlexNet)\nWhat was several months of work in 2012 can now be accomplished in a dozen lines of\ncode using any modern framework.\nReviewingthe architecture, wesee thatAlexNethas anAchillesheel whenit comes toeffi-\nciency: thelasttwohiddenlayersrequirematricesofsize 6400\u00024096and4096\u00024096,re-\nspectively. This corresponds to 164 MB of memory and 81 MFLOPs of computation, both\nofwhichareanontrivialoutlay,especiallyonsmallerdevices,suchasmobilephones. This\nisoneofthereasonswhyAlexNethasbeensurpassedbymuchmoreeffectivearchitectures\nthat we will cover in the following sections. Nonetheless, it is a key step from shallow to\ndeep networks that are used nowadays. Note that even though the number of parameters\nexceedsbyfartheamountoftrainingdatainourexperiments(thelasttwolayershavemore\nthan 40 million parameters, trained on a datasets of 60 thousand images), there is hardly\nanyoverfitting: trainingandvalidationlossarevirtuallyidenticalthroughouttraining. This\nis due to the improved regularization, such as dropout, inherent in modern deep network\ndesigns.\nAlthough it seems that there are only a few more lines in AlexNet\u2019s implementation than\ninLeNet\u2019s, ittooktheacademiccommunitymanyyearstoembracethisconceptualchange\nand take advantage of its excellent experimental results. This was also due to the lack of\nefficient computational tools. At the time neither DistBelief ( Deanet al., 2012) nor Caffe\n(Jiaetal.,2014)existed,andTheano( Bergstraetal.,2010)stilllackedmanydistinguishing\nfeatures. ItwastheavailabilityofTensorFlow( Abadietal.,2016)thatdramaticallychanged\nthe situation.\n8.1.5Exercises\n1.Followinguponthediscussionabove,analyzethecomputationalpropertiesofAlexNet.\n1.Compute the memory footprint for convolutions and fully connected layers, respec-\ntively. Which one dominates?\n2.Calculatethecomputationalcostfortheconvolutionsandthefullyconnectedlayers.\n3.Howdoesthememory(readandwritebandwidth,latency,size)affectcomputation?\nIs there any difference in its effects for training and inference?\n2.You are a chip designer and need to trade off computation and memory bandwidth.\nFor example, a faster chip requires more power and possibly a larger chip area. More", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "334f86ee-0a05-42d7-be55-bd48c6cef81c": {"__data__": {"id_": "334f86ee-0a05-42d7-be55-bd48c6cef81c", "embedding": null, "metadata": {"page_label": "278", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29aee9ab-d5b9-42e1-89b7-4c02e450513c", "node_type": "4", "metadata": {"page_label": "278", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "49ba3da76dff00fae7eebec4d3c4dfa2fa460ef148aa17410061b9f08465c26c", "class_name": "RelatedNodeInfo"}}, "text": "278 Modern Convolutional Neural Networks\n128memory bandwidth requires more pins and control logic, thus also more area. How do\nyou optimize?\n3.Why do engineers no longer report performance benchmarks on AlexNet?\n4.Try increasing the number of epochs when training AlexNet. Compared with LeNet,\nhow do the results differ? Why?\n5.AlexNet may be too complex for the Fashion-MNIST dataset, in particular due to the\nlow resolution of the initial images.\n1.Try simplifying the model to make the training faster, while ensuring that the accu-\nracy does not drop significantly.\n2.Design a better model that works directly on 28\u000228images.\n6.Modify the batch size, and observe the changes in throughput (images/s), accuracy, and\nGPU memory.\n7.ApplydropoutandReLUtoLeNet-5. Doesitimprove? Canyouimprovethingsfurther\nby preprocessing to take advantage of the invariances inherent in the images?\n8.CanyoumakeAlexNetoverfit? Whichfeaturedoyouneedtoremoveorchangetobreak\ntraining?\nDiscussions128.\n8.2NetworksUsing Blocks(VGG)\nWhile AlexNet offered empirical evidence that deep CNNs can achieve good results, it did\nnot provide a general template to guide subsequent researchers in designing new networks.\nIn the following sections, we will introduce several heuristic concepts commonly used to\ndesign deep networks.\nProgressinthisfieldmirrorsthatofVLSI(verylargescaleintegration)inchipdesignwhere\nengineersmovedfromplacingtransistorstologicalelementstologicblocks( Mead,1980 ).\nSimilarly,thedesignofneuralnetworkarchitectureshasgrownprogressivelymoreabstract,\nwith researchers moving from thinking in terms of individual neurons to whole layers,\nand now to blocks, repeating patterns of layers. A decade later, this has now progressed\nto researchers using entire trained models to repurpose them for different, albeit related,\ntasks. Such large pretrained models are typically called foundationmodels (Bommasani et\nal., 2021).\nBack to network design. The idea of using blocks first emerged from the Visual Geometry\nGroup(VGG)atOxfordUniversity,intheireponymously-named VGGnetwork( Simonyan\nand Zisserman, 2014 ). It is easy to implement these repeated structures in code with any\nmodern deep learning framework by using loops and subroutines.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2227, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9eb34c8-683b-4bab-83b2-6d393c868de4": {"__data__": {"id_": "a9eb34c8-683b-4bab-83b2-6d393c868de4", "embedding": null, "metadata": {"page_label": "279", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "016b6917-a164-4316-b09c-425ed49c2f2c", "node_type": "4", "metadata": {"page_label": "279", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d531ab8b4e8278dc48a22eb8b522ff52c4de2b18486fd2a227c15935405a0f76", "class_name": "RelatedNodeInfo"}}, "text": "279 Networks Using Blocks (VGG)\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n8.2.1VGGBlocks\nThe basic building block of CNNs is a sequence of the following: (i) a convolutional layer\nwith padding to maintain the resolution, (ii) a nonlinearity such as a ReLU, (iii) a pooling\nlayersuchasmax-poolingtoreducetheresolution. Oneoftheproblemswiththisapproach\nis that the spatial resolution decreases quite rapidly. In particular, this imposes a hard limit\noflog2\ud835\udc51convolutional layers on the network before all dimensions ( \ud835\udc51) are used up. For\ninstance,inthecaseofImageNet,itwouldbeimpossibletohavemorethan8convolutional\nlayers in this way.\nThe key idea of Simonyan and Zisserman ( 2014) was to use multiple convolutions in be-\ntween downsampling via max-pooling in the form of a block. They were primarily in-\nterested in whether deep or wide networks perform better. For instance, the successive\napplication of two 3\u00023convolutions touches the same pixels as a single 5\u00025convolution\ndoes. At the same time, the latter uses approximately as many parameters ( 25\u0001\ud835\udc502) as three\n3\u00023convolutionsdo( 3\u00019\u0001\ud835\udc502). Inaratherdetailedanalysistheyshowedthatdeepandnar-\nrow networks significantly outperform their shallow counterparts. This set deep learning\non a quest for ever deeper networks with over 100 layers for typical applications. Stacking\n3\u00023convolutions has become a gold standard in later deep networks (a design decision\nonly to be revisited recently by Liu et al.(2022)). Consequently, fast implementations for\nsmall convolutions have become a staple on GPUs ( Lavin and Gray, 2016 ).\nBack to VGG: a VGG block consists of a sequence of convolutions with 3\u00023kernels with\npadding of 1 (keeping height and width) followed by a 2\u00022max-pooling layer with stride\nof 2 (halving height and width after each block). In the code below, we define a function\ncalled vgg_block to implement one VGG block.\nThe function below takes two arguments, corresponding to the number of convolutional\nlayers num_convs and the number of output channels num_channels .\ndef vgg_block (num_convs, out_channels):\nlayers =[]\nfor _inrange (num_convs):\nlayers .append(nn .LazyConv2d(out_channels, kernel_size =3, padding =1))\nlayers .append(nn .ReLU())\nlayers .append(nn .MaxPool2d(kernel_size =2,stride =2))\nreturn nn.Sequential( *layers)\n8.2.2VGGNetwork\nLike AlexNet and LeNet, the VGG Network can be partitioned into two parts: the first\nconsisting mostly of convolutional and pooling layers and the second consisting of fully", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd2c3286-9b73-4433-82bf-1639d30a0533": {"__data__": {"id_": "cd2c3286-9b73-4433-82bf-1639d30a0533", "embedding": null, "metadata": {"page_label": "280", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f29676bf-e0a2-465a-9166-f13f8e78abba", "node_type": "4", "metadata": {"page_label": "280", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b762dd5ce2406d6099a65397b169cace25f6e1ca7c1357732df443b15bad78a7", "class_name": "RelatedNodeInfo"}}, "text": "280 Modern Convolutional Neural Networks\nconnected layers that are identical to those in AlexNet. The key difference is that the con-\nvolutional layers are grouped in nonlinear transformations that leave the dimensonality un-\nchanged, followed by a resolution-reduction step, as depicted in Fig. 8.2.1 .\ntFig. 8.2.1 From AlexNet to VGG. The key difference is that VGG consists of blocks of layers,\nwhereas AlexNet\u2019s layers are all designed individually.\nThe convolutional part of the network connects several VGG blocks from Fig. 8.2.1 (also\ndefined in the vgg_block function) in succession. This grouping of convolutions is a pat-\ntern that has remained almost unchanged over the past decade, although the specific choice\nof operations has undergone considerable modifications. The variable archconsists of a\nlist of tuples (one per block), where eachcontains two values: the number of convolutional\nlayers and the number of output channels, which are precisely the arguments required to\ncall the vgg_block function. As such, VGG defines a familyof networks rather than just a\nspecific manifestation. To build a specific network we simply iterate over archto compose\nthe blocks.\nclass VGG(d2l .Classifier):\ndef __init__ (self , arch, lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nconv_blks =[]\nfor (num_convs, out_channels) inarch:\nconv_blks .append(vgg_block(num_convs, out_channels))\nself .net =nn.Sequential(\n*conv_blks, nn .Flatten(),\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1495, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f597bd64-b5f4-4af4-9067-4c6ebf724b56": {"__data__": {"id_": "f597bd64-b5f4-4af4-9067-4c6ebf724b56", "embedding": null, "metadata": {"page_label": "281", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a69dcd48-f575-4887-b361-345ad010b03c", "node_type": "4", "metadata": {"page_label": "281", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "997d16a63b317468166e7ae7b0e09b39de0bb722a2ac7fa990f20ca1334f5b93", "class_name": "RelatedNodeInfo"}}, "text": "281 Networks Using Blocks (VGG)\n(continued from previous page)\nnn.LazyLinear( 4096 ), nn .ReLU(), nn .Dropout( 0.5),\nnn.LazyLinear( 4096 ), nn .ReLU(), nn .Dropout( 0.5),\nnn.LazyLinear(num_classes))\nself .net.apply(d2l .init_cnn)\nThe original VGG network had five convolutional blocks, among which the first two have\noneconvolutionallayereachandthelatterthreecontaintwoconvolutionallayerseach. The\nfirst blockhas 64 output channelsand each subsequentblockdoubles the number of output\nchannels,untilthatnumberreaches512. Sincethisnetworkuseseightconvolutionallayers\nand three fully connected layers, it is often called VGG-11.\nVGG(arch =((1,64), ( 1,128), ( 2,256), ( 2,512), ( 2,512))).layer_summary(\n(1,1,224,224))\nSequential output shape: torch .Size([ 1,64,112,112])\nSequential output shape: torch .Size([ 1,128,56,56])\nSequential output shape: torch .Size([ 1,256,28,28])\nSequential output shape: torch .Size([ 1,512,14,14])\nSequential output shape: torch .Size([ 1,512,7,7])\nFlatten output shape: torch .Size([ 1,25088 ])\nLinear output shape: torch .Size([ 1,4096 ])\nReLU output shape: torch .Size([ 1,4096 ])\nDropout output shape: torch .Size([ 1,4096 ])\nLinear output shape: torch .Size([ 1,4096 ])\nReLU output shape: torch .Size([ 1,4096 ])\nDropout output shape: torch .Size([ 1,4096 ])\nLinear output shape: torch .Size([ 1,10])\nAsyoucansee,wehalveheightandwidthateachblock,finallyreachingaheightandwidth\nof 7 before flattening the representations for processing by the fully connected part of the\nnetwork. Simonyan and Zisserman ( 2014) described several other variants of VGG. In\nfact,ithasbecomethenormtopropose families ofnetworkswithdifferentspeed\u2013accuracy\ntrade-off when introducing a new architecture.\n8.2.3Training\nSince VGG-11 is computationally more demanding than AlexNet we construct a network\nwith a smaller number of channels. This is more than sufficient for training on Fashion-\nMNIST. The model training process is similar to that of AlexNet in Section 8.1 . Again ob-\nservetheclosematchbetweenvalidationandtrainingloss, suggestingonlyasmallamount\nof overfitting.\nmodel =VGG(arch =((1,16), ( 1,32), ( 2,64), ( 2,128), ( 2,128)), lr =0.01 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(224,224))\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e593a962-ee8d-4263-9d0a-898ad50ad91d": {"__data__": {"id_": "e593a962-ee8d-4263-9d0a-898ad50ad91d", "embedding": null, "metadata": {"page_label": "282", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31db41e1-2fad-485c-9e9f-55c0cab8bd8e", "node_type": "4", "metadata": {"page_label": "282", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "48eac30281b105341559a5b430435037169ab2d876bfb5f726622abac6bf3cf8", "class_name": "RelatedNodeInfo"}}, "text": "282 Modern Convolutional Neural Networks\n8.2.4Summary\nOne might argue that VGG is the first truly modern convolutional neural network. While\nAlexNetintroducedmanyofthecomponentsofwhatmakedeeplearningeffectiveatscale,\nit is VGG that arguably introduced key properties such as blocks of multiple convolutions\nand a preference for deep and narrow networks. It is also the first network that is actually\nan entire family of similarly parametrized models, giving the practitioner ample trade-off\nbetween complexity and speed. This is also the place where modern deep learning frame-\nworks shine. It is no longer necessary to generate XML configuration files to specify a\nnetwork but rather, to assemble said networks through simple Python code.\nMore recently ParNet ( Goyalet al., 2021) demonstrated that it is possible to achieve com-\npetitive performance using a much more shallow architecture through a large number of\nparallel computations. This is an exciting development and there is hope that it will influ-\nence architecture designs in the future. For the remainder of the chapter, though, we will\nfollow the path of scientific progress over the past decade.\n8.2.5Exercises\n1.ComparedwithAlexNet,VGGismuchslowerintermsofcomputation,anditalsoneeds\nmore GPU memory.\n1.Compare the number of parameters needed for AlexNet and VGG.\n2.Compare the number of floating point operations used in the convolutional layers\nand in the fully connected layers.\n3.How could you reduce the computational cost created by the fully connected layers?\n2.When displaying the dimensions associated with the various layers of the network, we\nonly see the information associated with eight blocks (plus some auxiliary transforms),\neven though the network has 11 layers. Where did the remaining three layers go?\n3.UseTable1intheVGGpaper( SimonyanandZisserman,2014 )toconstructothercom-\nmon models, such as VGG-16 or VGG-19.\n4.Upsampling the resolution in Fashion-MNIST eight-fold from 28\u000228to224\u0002224\ndimensions is very wasteful. Try modifying the network architecture and resolution\nconversion, e.g., to 56 or to 84 dimensions for its input instead. Can you do so without", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a282996b-2ec7-4b94-bec7-ae02483990e8": {"__data__": {"id_": "a282996b-2ec7-4b94-bec7-ae02483990e8", "embedding": null, "metadata": {"page_label": "283", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1042402b-bbcc-407b-a10e-233a280892e3", "node_type": "4", "metadata": {"page_label": "283", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7b210c3d76c0f78c9db14d25755dc425543064d25556998b66cebc08e643ad3a", "class_name": "RelatedNodeInfo"}}, "text": "283 Network in Network (NiN)\n129reducing the accuracy of the network? Consult the VGG paper ( Simonyan and Zisser-\nman, 2014 ) for ideas on adding more nonlinearities prior to downsampling.\nDiscussions129.\n8.3Networkin Network(NiN)\nLeNet, AlexNet, and VGG all share a common design pattern: extract features exploiting\nspatialstructure via a sequence of convolutions and pooling layers and post-process the\nrepresentations via fully connected layers. The improvements upon LeNet by AlexNet and\nVGG mainly lie in how these later networks widen and deepen these two modules.\nThis design poses two major challenges. First, the fully connected layers at the end of\nthe architecture consume tremendous numbers of parameters. For instance, even a simple\nmodel such as VGG-11 requires a monstrous matrix, occupying almost 400MB of RAM\nin single precision (FP32). This is a significant impediment to computation, in particular\non mobile and embedded devices. After all, even high-end mobile phones sport no more\nthan 8GB of RAM. At the time VGG was invented, this was an order of magnitude less\n(the iPhone 4S had 512MB). As such, it would have been difficult to justify spending the\nmajority of memory on an image classifier.\nSecond, it is equally impossible to add fully connected layers earlier in the network to\nincreasethedegreeofnonlinearity: doingsowoulddestroythespatialstructureandrequire\npotentially even more memory.\nThenetwork in network (NiN) blocks ( Linet al., 2013) offer an alternative, capable of\nsolving both problems in one simple strategy. They were proposed based on a very simple\ninsight: (i) use 1\u00021convolutions to add local nonlinearities across the channel activations\nand(ii)useglobalaveragepoolingtointegrateacrossalllocationsinthelastrepresentation\nlayer. Note that global average pooling would not be effective, were it not for the added\nnonlinearities. Let\u2019s dive into this in detail.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n8.3.1NiN Blocks\nRecallSection7.4.3 . Initwesaidthattheinputsandoutputsofconvolutionallayersconsist\nof four-dimensional tensors with axes corresponding to the example, channel, height, and\nwidth. Also recall that the inputs and outputs of fully connected layers are typically two-\ndimensional tensors corresponding to the example and feature. The idea behind NiN is\nto apply a fully connected layer at each pixel location (for each height and width). The", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be6e06db-9ab2-4049-a181-5c00e3bef0fa": {"__data__": {"id_": "be6e06db-9ab2-4049-a181-5c00e3bef0fa", "embedding": null, "metadata": {"page_label": "284", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6cc5711f-be69-4bd5-ad67-7e64f9cc7552", "node_type": "4", "metadata": {"page_label": "284", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1232d83eefb186de7167adcaaa462d52cf44d457b68a6be5bde993ea759ce225", "class_name": "RelatedNodeInfo"}}, "text": "284 Modern Convolutional Neural Networks\nresulting 1\u00021convolutioncanbethoughtofasafullyconnectedlayeractingindependently\non each pixel location.\nFig.8.3.1 illustratesthemainstructuraldifferencesbetweenVGGandNiN,andtheirblocks.\nNoteboththedifferenceintheNiNblocks(theinitialconvolutionisfollowedby 1\u00021con-\nvolutions, whereas VGG retains 3\u00023convolutions) and at the end where we no longer\nrequire a giant fully connected layer.\ntFig. 8.3.1 Comparing the architectures of VGG and NiN, and of their blocks.\ndef nin_block (out_channels, kernel_size, strides, padding):\nreturn nn.Sequential(\nnn.LazyConv2d(out_channels, kernel_size, strides, padding), nn .ReLU(),\nnn.LazyConv2d(out_channels, kernel_size =1), nn .ReLU(),\nnn.LazyConv2d(out_channels, kernel_size =1), nn .ReLU())\n8.3.2NiN Model\nNiNusesthesameinitialconvolutionsizesasAlexNet(itwasproposedshortlythereafter).\nThe kernel sizes are 11\u000211,5\u00025, and 3\u00023, respectively, and the numbers of output", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06d3729f-e058-473b-b7b9-f1bcf1607c22": {"__data__": {"id_": "06d3729f-e058-473b-b7b9-f1bcf1607c22", "embedding": null, "metadata": {"page_label": "285", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "defd9b5f-5f24-4de5-aabe-73c2fe626903", "node_type": "4", "metadata": {"page_label": "285", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9cf1ef06b3310ce78357890df19368ab2ca287a9db90ad938af9379a579b0dc1", "class_name": "RelatedNodeInfo"}}, "text": "285 Network in Network (NiN)\nchannelsmatchthoseofAlexNet. EachNiNblockisfollowedbyamax-poolinglayerwith\na stride of 2 and a window shape of 3\u00023.\nThe second significant difference between NiN and both AlexNet and VGG is that NiN\navoids fully connected layers altogether. Instead, NiN uses a NiN block with a number of\noutput channels equal to the number of label classes, followed by a globalaverage pooling\nlayer, yielding a vector of logits. This design significantly reduces the number of required\nmodel parameters, albeit at the expense of a potential increase in training time.\nclass NiN(d2l .Classifier):\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(\nnin_block( 96, kernel_size =11, strides =4, padding =0),\nnn.MaxPool2d( 3, stride =2),\nnin_block( 256, kernel_size =5, strides =1, padding =2),\nnn.MaxPool2d( 3, stride =2),\nnin_block( 384, kernel_size =3, strides =1, padding =1),\nnn.MaxPool2d( 3, stride =2),\nnn.Dropout( 0.5),\nnin_block(num_classes, kernel_size =3, strides =1, padding =1),\nnn.AdaptiveAvgPool2d(( 1,1)),\nnn.Flatten())\nself .net.apply(d2l .init_cnn)\nWe create a data example to see the output shape of each block.\nNiN() .layer_summary(( 1,1,224,224))\nSequential output shape: torch .Size([ 1,96,54,54])\nMaxPool2d output shape: torch .Size([ 1,96,26,26])\nSequential output shape: torch .Size([ 1,256,26,26])\nMaxPool2d output shape: torch .Size([ 1,256,12,12])\nSequential output shape: torch .Size([ 1,384,12,12])\nMaxPool2d output shape: torch .Size([ 1,384,5,5])\nDropout output shape: torch .Size([ 1,384,5,5])\nSequential output shape: torch .Size([ 1,10,5,5])\nAdaptiveAvgPool2d output shape: torch .Size([ 1,10,1,1])\nFlatten output shape: torch .Size([ 1,10])\n8.3.3Training\nAsbeforeweuseFashion-MNISTtotrainthemodelusingthesameoptimizerthatweused\nfor AlexNet and VGG.\nmodel =NiN(lr =0.05 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(224,224))\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4983989b-04e5-4f34-a9bc-a18aa3f381fb": {"__data__": {"id_": "4983989b-04e5-4f34-a9bc-a18aa3f381fb", "embedding": null, "metadata": {"page_label": "286", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9c09b94-0484-43e5-86ea-a619d09dff35", "node_type": "4", "metadata": {"page_label": "286", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "450f57eea0f6bfb184e71581ca0de4daf36b8a3deed863e6a75f11982aa5105b", "class_name": "RelatedNodeInfo"}}, "text": "286 Modern Convolutional Neural Networks\n8.3.4Summary\nNiN has dramatically fewer parameters than AlexNet and VGG. This stems primarily from\nthefactthatitneedsnogiantfullyconnectedlayers. Instead, itusesglobalaveragepooling\nto aggregate across all image locations after the last stage of the network body. This obvi-\nates the need for expensive (learned) reduction operations and replaces them by a simple\naverage. What surprised researchers at the time was the fact that this averaging operation\ndid not harm accuracy. Note that averaging across a low-resolution representation (with\nmany channels) also adds to the amount of translation invariance that the network can han-\ndle.\nChoosing fewer convolutions with wide kernels and replacing them by 1\u00021convolutions\naids the quest for fewer parameters further. It can cater for a significant amount of non-\nlinearity across channels within any given location. Both 1\u00021convolutions and global\naverage pooling significantly influenced subsequent CNN designs.\n8.3.5Exercises\n1.Why are there two 1\u00021convolutional layers per NiN block? Increase their number to\nthree. Reduce their number to one. What changes?\n2.What changes if you replace the 1\u00021convolutions by 3\u00023convolutions?\n3.What happens if you replace the global average pooling by a fully connected layer\n(speed, accuracy, number of parameters)?\n4.Calculate the resource usage for NiN.\n1.What is the number of parameters?\n2.What is the amount of computation?\n3.What is the amount of memory needed during training?\n4.What is the amount of memory needed during prediction?\n5.What are possible problems with reducing the 384\u00025\u00025representation to a 10\u00025\u00025\nrepresentation in one step?\n6.Use the structural design decisions in VGG that led to VGG-11, VGG-16, and VGG-19\nto design a family of NiN-like networks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f8ea305-5e32-484d-90e0-61a7d3da3311": {"__data__": {"id_": "6f8ea305-5e32-484d-90e0-61a7d3da3311", "embedding": null, "metadata": {"page_label": "287", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2c03e3b-0410-4789-856b-2fd854d05840", "node_type": "4", "metadata": {"page_label": "287", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2020e8e82464804368de9ed6fcb99bf715c1d15c86fd08b5be8c62f9b94650d7", "class_name": "RelatedNodeInfo"}}, "text": "287 Multi-Branch Networks (GoogLeNet)\n130Discussions130.\n8.4Multi-BranchNetworks(GoogLeNet)\nIn 2014,GoogLeNet won the ImageNet Challenge ( Szegedyetal., 2015), using a structure\nthat combined the strengths of NiN ( Linetal., 2013), repeated blocks ( Simonyan and Zis-\nserman, 2014 ), and a cocktail of convolution kernels. It was arguablyalso the firstnetwork\nthat exhibited a clear distinction among the stem (data ingest), body (data processing), and\nhead (prediction) in a CNN. This design pattern has persisted ever since in the design of\ndeep networks: the stemis given by the first two or three convolutions that operate on the\nimage. They extract low-level features from the underlying images. This is followed by a\nbodyof convolutional blocks. Finally, the headmaps the features obtained so far to the\nrequired classification, segmentation, detection, or tracking problem at hand.\nThekeycontributioninGoogLeNetwasthedesignofthenetworkbody. Itsolvedtheprob-\nlem of selecting convolution kernels in an ingenious way. While other works tried to iden-\ntifywhichconvolution,rangingfrom 1\u00021to11\u000211wouldbebest,itsimply concatenated\nmulti-branch convolutions. In what follows we introduce a slightly simplified version of\nGoogLeNet: theoriginaldesignincludedanumberoftricksforstabilizingtrainingthrough\nintermediate loss functions, applied to multiple layers of the network. They are no longer\nnecessary due to the availability of improved training algorithms.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n8.4.1InceptionBlocks\nThe basic convolutional block in GoogLeNet is called an Inception block , stemming from\nthe meme \u201cwe need to go deeper\u201d from the movie Inception .\ntFig. 8.4.1 Structure of the Inception block.\nAs depicted in Fig. 8.4.1 , the inception block consists of four parallel branches. The first\nthree branches use convolutional layers with window sizes of 1\u00021,3\u00023, and 5\u00025to\nextract information from different spatial sizes. The middle two branches also add a 1\u00021", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a1e79c6-edbd-4508-b012-3ebdbcee3985": {"__data__": {"id_": "9a1e79c6-edbd-4508-b012-3ebdbcee3985", "embedding": null, "metadata": {"page_label": "288", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1e45ce4f-4856-4da2-8c64-fbeb77e6bf20", "node_type": "4", "metadata": {"page_label": "288", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "760aac54e5dc23de254f491d70e41ca83524b2e0f49e220b7849a0308d71cf2b", "class_name": "RelatedNodeInfo"}}, "text": "288 Modern Convolutional Neural Networks\nconvolution of the input to reduce the number of channels, reducing the model\u2019s complex-\nity. The fourth branch uses a 3\u00023max-pooling layer, followed by a 1\u00021convolutional\nlayer to change the number of channels. The four branches all use appropriate padding\nto give the input and output the same height and width. Finally, the outputs along each\nbranchareconcatenatedalongthechanneldimensionandcomprisetheblock\u2019soutput. The\ncommonly-tunedhyperparametersoftheInceptionblockarethenumberofoutputchannels\nper layer, i.e., how to allocate capacity among convolutions of different size.\nclass Inception (nn.Module):\n# c1--c4 are the number of output channels for each branch\ndef __init__ (self , c1, c2, c3, c4, **kwargs):\nsuper (Inception, self ).__init__ (**kwargs)\n# Branch 1\nself .b1_1 =nn.LazyConv2d(c1, kernel_size =1)\n# Branch 2\nself .b2_1 =nn.LazyConv2d(c2[ 0], kernel_size =1)\nself .b2_2 =nn.LazyConv2d(c2[ 1], kernel_size =3, padding =1)\n# Branch 3\nself .b3_1 =nn.LazyConv2d(c3[ 0], kernel_size =1)\nself .b3_2 =nn.LazyConv2d(c3[ 1], kernel_size =5, padding =2)\n# Branch 4\nself .b4_1 =nn.MaxPool2d(kernel_size =3, stride =1, padding =1)\nself .b4_2 =nn.LazyConv2d(c4, kernel_size =1)\ndef forward (self , x):\nb1=F.relu( self .b1_1(x))\nb2=F.relu( self .b2_2(F .relu( self .b2_1(x))))\nb3=F.relu( self .b3_2(F .relu( self .b3_1(x))))\nb4=F.relu( self .b4_2( self .b4_1(x)))\nreturn torch .cat((b1, b2, b3, b4), dim =1)\nTo gain some intuition for why this network works so well, consider the combination of\nthe filters. They explore the image in a variety of filter sizes. This means that details at\ndifferentextentscanberecognizedefficientlybyfiltersofdifferentsizes. Atthesametime,\nwe can allocate different amounts of parameters for different filters.\n8.4.2GoogLeNetModel\nAs shown in Fig. 8.4.2 , GoogLeNet uses a stack of a total of 9 inception blocks, arranged\ninto three groups with max-pooling in between, and global average pooling in its head to\ngenerate its estimates. Max-pooling between inception blocks reduces the dimensionality.\nAt its stem, the first module is similar to AlexNet and LeNet.\ntFig. 8.4.2 The GoogLeNet architecture.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1b0203d-0599-4314-b5df-31d85b348f6b": {"__data__": {"id_": "f1b0203d-0599-4314-b5df-31d85b348f6b", "embedding": null, "metadata": {"page_label": "289", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73d64630-11d0-4470-8951-80633c25e02b", "node_type": "4", "metadata": {"page_label": "289", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "116e44aee401c902e43ac75679bb7cc0793238bb490fc7badb6a055098bda993", "class_name": "RelatedNodeInfo"}}, "text": "289 Multi-Branch Networks (GoogLeNet)\nWe can now implement GoogLeNet piece by piece. Let\u2019s begin with the stem. The first\nmodule uses a 64-channel 7\u00027convolutional layer.\nclass GoogleNet (d2l .Classifier):\ndef b1(self ):\nreturn nn.Sequential(\nnn.LazyConv2d( 64, kernel_size =7, stride =2, padding =3),\nnn.ReLU(), nn .MaxPool2d(kernel_size =3, stride =2, padding =1))\nThe second module uses two convolutional layers: first, a 64-channel 1\u00021convolutional\nlayer, followed by a 3\u00023convolutional layer that triples the number of channels. This\ncorresponds to the second branch in the Inception block and concludes the design of the\nbody. At this point we have 192 channels.\n@d2l .add_to_class(GoogleNet)\ndef b2(self ):\nreturn nn.Sequential(\nnn.LazyConv2d( 64, kernel_size =1), nn .ReLU(),\nnn.LazyConv2d( 192, kernel_size =3, padding =1), nn .ReLU(),\nnn.MaxPool2d(kernel_size =3, stride =2, padding =1))\nThe third module connects two complete Inception blocks in series. The number of output\nchannels of the first Inception block is 64\u00b8128\u00b832\u00b832=256. This amounts to a ratio of\nthe number of output channels among the four branches of 2 : 4 : 1 : 1 . To achieve this, we\nfirstreducetheinputdimensionsby1\n2andby1\n12inthesecondandthirdbranchrespectively\nto arrive at 96=192\u009d2and16=192\u009d12channels respectively.\nThe number of output channels of the second Inception block is increased to 128\u00b8192\u00b8\n96\u00b864=480, yielding a ratio of 128 : 192 : 96 : 64 =4 : 6 : 3 : 2 . As before, we need to\nreduce the number of intermediate dimensions in the second and third channel. A scale of\n1\n2and1\n8respectively suffices, yielding 128and32channels respectively. This is captured\nby the arguments of the following Inception block constructors.\n@d2l .add_to_class(GoogleNet)\ndef b3(self ):\nreturn nn.Sequential(Inception( 64, (96,128), ( 16,32),32),\nInception( 128, (128,192), ( 32,96),64),\nnn.MaxPool2d(kernel_size =3, stride =2, padding =1))\nThe fourth module is more complicated. It connects five Inception blocks in series, and\nthey have 192\u00b8208\u00b848\u00b864=512,160\u00b8224\u00b864\u00b864=512,128\u00b8256\u00b864\u00b864=512,\n112\u00b8288\u00b864\u00b864=528, and 256\u00b8320\u00b8128\u00b8128=832output channels, respectively.\nThe number of channels assigned to these branches is similar to that in the third module:\nthesecondbranchwiththe 3\u00023convolutionallayeroutputsthelargestnumberofchannels,\nfollowed by the first branch with only the 1\u00021convolutional layer, the third branch with\nthe5\u00025convolutional layer, and the fourth branch with the 3\u00023max-pooling layer. The\nsecond and third branches will first reduce the number of channels according to the ratio.\nThese ratios are slightly different in different Inception blocks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea07a59f-b7de-403f-9307-386f55fbd623": {"__data__": {"id_": "ea07a59f-b7de-403f-9307-386f55fbd623", "embedding": null, "metadata": {"page_label": "290", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2224b39a-bf57-4926-9b66-eab46b49fcd7", "node_type": "4", "metadata": {"page_label": "290", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f099aa2300e5646c6adac33e75ab2eb98d7272065c8f6fbfcda934ab6fb5765a", "class_name": "RelatedNodeInfo"}}, "text": "290 Modern Convolutional Neural Networks\n@d2l .add_to_class(GoogleNet)\ndef b4(self ):\nreturn nn.Sequential(Inception( 192, (96,208), ( 16,48),64),\nInception( 160, (112,224), ( 24,64),64),\nInception( 128, (128,256), ( 24,64),64),\nInception( 112, (144,288), ( 32,64),64),\nInception( 256, (160,320), ( 32,128),128),\nnn.MaxPool2d(kernel_size =3, stride =2, padding =1))\nThefifthmodulehastwoInceptionblockswith 256\u00b8320\u00b8128\u00b8128=832and384\u00b8384\u00b8\n128\u00b8128=1024output channels. The number of channels assigned to each branch is the\nsame as that in the third and fourth modules, but differs in specific values. It should be\nnotedthat thefifth blockis followedbytheoutput layer. Thisblockusesthe globalaverage\npooling layer to change the height and width of each channel to 1, just as in NiN. Finally,\nwe turn the output into a two-dimensional array followed by a fully connected layer whose\nnumber of outputs is the number of label classes.\n@d2l .add_to_class(GoogleNet)\ndef b5(self ):\nreturn nn.Sequential(Inception( 256, (160,320), ( 32,128),128),\nInception( 384, (192,384), ( 48,128),128),\nnn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten())\nNowthatwedefinedallblocks b1through b5, itisjustamatterofassemblingthemallinto\na full network.\n@d2l .add_to_class(GoogleNet)\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper (GoogleNet, self ).__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential( self .b1(), self .b2(), self .b3(), self .b4(),\nself .b5(), nn .LazyLinear(num_classes))\nself .net.apply(d2l .init_cnn)\nThe GoogLeNet model is computationally complex. Note the large number of relatively\narbitraryhyperparametersintermsofthenumberofchannelschosen,thenumberofblocks\nprior to dimensionality reduction, the relative partitioning of capacity across channels, etc.\nMuch of it is due to the fact that at the time when GoogLeNet was introduced, automatic\ntools for network definition or design exploration were not yet available. For instance, by\nnowwetakeitforgrantedthatacompetentdeeplearningframeworkiscapableofinferring\ndimensionalities of input tensors automatically. At the time, many such configurations had\nto be specified explicitly by the experimenter, thus often slowing down active experimen-\ntation. Moreover, the tools needed for automatic exploration were still in flux and initial\nexperiments largely amounted to costly brute-force exploration, genetic algorithms, and\nsimilar strategies.\nFor now the only modification we will carry out is to reduce the input height and width\nfrom 224 to 96 to have a reasonable training time on Fashion-MNIST. This simplifies the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90fbd74a-c31f-4cf4-9967-3effcb9fe875": {"__data__": {"id_": "90fbd74a-c31f-4cf4-9967-3effcb9fe875", "embedding": null, "metadata": {"page_label": "291", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "830f7bff-012f-4563-824e-ee7b19174305", "node_type": "4", "metadata": {"page_label": "291", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b6be63fa1a57cdf1472797cf75f2ebfae518fc290d7273a4f811319460d94419", "class_name": "RelatedNodeInfo"}}, "text": "291 Multi-Branch Networks (GoogLeNet)\ncomputation. Let\u2019shavealookatthechangesintheshapeoftheoutputbetweenthevarious\nmodules.\nmodel =GoogleNet() .layer_summary(( 1,1,96,96))\nSequential output shape: torch .Size([ 1,64,24,24])\nSequential output shape: torch .Size([ 1,192,12,12])\nSequential output shape: torch .Size([ 1,480,6,6])\nSequential output shape: torch .Size([ 1,832,3,3])\nSequential output shape: torch .Size([ 1,1024 ])\nLinear output shape: torch .Size([ 1,10])\n8.4.3Training\nAsbefore,wetrainourmodelusingtheFashion-MNISTdataset. Wetransformitto 96\u000296\npixel resolution before invoking the training procedure.\nmodel =GoogleNet(lr =0.01 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(96,96))\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)\n8.4.4Discussion\nA key feature of GoogLeNet is that it is actually cheaper to compute than its predecessors\nwhile simultaneously providing improved accuracy. This marks the beginning of a much\nmoredeliberatenetworkdesignthattradesoffthecostofevaluatinganetworkwithareduc-\ntion in errors. It also marks the beginning of experimentation at a block level with network\ndesignhyperparameters,eventhoughitwasentirelymanualatthetime. Wewillrevisitthis\ntopic inSection 8.8 when discussing strategies for network structure exploration.\nOver the following sections we will encounter a number of design choices (e.g., batch nor-\nmalization, residual connections, and channel grouping) that allow us to improve networks\nsignificantly. For now, you can be proud to have implemented what is arguably the first\ntruly modern CNN.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f0df830-fa96-45cb-8308-9aa035c49b7c": {"__data__": {"id_": "2f0df830-fa96-45cb-8308-9aa035c49b7c", "embedding": null, "metadata": {"page_label": "292", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c9329fb-05a7-4a0a-b157-6b2e1a6301fa", "node_type": "4", "metadata": {"page_label": "292", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "523c8a799342c6f96c53d10ebec3a94ccc64abe15d68dd3939699065eef9a8c1", "class_name": "RelatedNodeInfo"}}, "text": "292 Modern Convolutional Neural Networks\n1318.4.5Exercises\n1.GoogLeNetwasso successfulthat it wentthrough a numberof iterations, progressively\nimproving speed and accuracy. Try to implement and run some of them. They include\nthe following:\n1.Add a batch normalization layer ( Ioffe and Szegedy, 2015 ), as described later in\nSection 8.5 .\n2.Make adjustments to the Inception block (width, choice and order of convolutions),\nas described in Szegedy etal.(2016).\n3.Uselabelsmoothingformodelregularization, asdescribedinSzegedy etal.(2016).\n4.MakefurtheradjustmentstotheInceptionblockbyaddingresidualconnection( Szegedy\netal., 2017), as described later in Section 8.6 .\n2.What is the minimum image size needed for GoogLeNet to work?\n3.Can you design a variant of GoogLeNet that works on Fashion-MNIST\u2019s native resolu-\ntion of 28\u000228pixels? How would you need to change the stem, the body, and the head\nof the network, if anything at all?\n4.Compare the model parameter sizes of AlexNet, VGG, NiN, and GoogLeNet. How do\nthe latter two network architectures significantly reduce the model parameter size?\n5.ComparetheamountofcomputationneededinGoogLeNetandAlexNet. Howdoesthis\naffect the design of an accelerator chip, e.g., in terms of memory size, memory band-\nwidth,cachesize,theamountofcomputation,andthebenefitofspecializedoperations?\nDiscussions131.\n8.5BatchNormalization\nTrainingdeepneuralnetworksisdifficult. Gettingthemtoconvergeinareasonableamount\nof time can be tricky. In this section, we describe batch normalization , a popular and\neffective technique that consistently accelerates the convergence of deep networks ( Ioffe\nand Szegedy, 2015 ). Together with residual blocks\u2014covered later in Section 8.6 \u2014batch\nnormalization has made it possible for practitioners to routinely train networks with over\n100 layers. A secondary (serendipitous) benefit of batch normalization lies in its inherent\nregularization.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d35a654e-01d8-4ded-8751-7408b045e61c": {"__data__": {"id_": "d35a654e-01d8-4ded-8751-7408b045e61c", "embedding": null, "metadata": {"page_label": "293", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d5e0954-8283-40bf-95f3-4151f71086c5", "node_type": "4", "metadata": {"page_label": "293", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "16ea10e181f6e61ddafece5feef1943b50da813ad427144504e210a17b8b1bb5", "class_name": "RelatedNodeInfo"}}, "text": "293 Batch Normalization\n8.5.1TrainingDeep Networks\nWhen working with data, we often preprocess before training. Choices regarding data pre-\nprocessingoftenmakeanenormousdifferenceinthefinalresults. Recallourapplicationof\nMLPs to predicting house prices ( Section 5.7 ). Our first step when working with real data\nwastostandardizeourinputfeaturestohavezeromean \ud835\udf41=0andunitvariance \ud835\udeba=1across\nmultiple observations ( Friedman, 1987 ), frequently rescaling the latter so that the diagonal\nis unity, i.e., \u03a3\ud835\udc56\ud835\udc56=1. Yet another strategy is to rescale vectors to unit length, possibly\nzero mean per observation . This can work well, e.g., for spatial sensor data. These pre-\nprocessing techniques and many others, are beneficial for keeping the estimation problem\nwell controlled. For a review of feature selection and extraction see the article of Guyon et\nal.(2008), for example. Standardizing vectors also has the nice side-effect of constraining\nthe function complexity of functions that act upon it. For instance, the celebrated radius-\nmargin bound ( Vapnik, 1995 ) in support vector machines and the Perceptron Convergence\nTheorem ( Novikoff, 1962 ) rely on inputs of bounded norm.\nIntuitively,thisstandardizationplaysnicelywithouroptimizerssinceitputstheparameters\na priori on a similar scale. As such, it is only natural to ask whether a corresponding\nnormalization step insidea deep network might not be beneficial. While this is not quite\nthe reasoning that led to the invention of batch normalization ( Ioffe and Szegedy, 2015 ),\nit is a useful way of understanding it and its cousin, layer normalization ( Baet al., 2016),\nwithin a unified framework.\nSecond, for a typical MLP or CNN, as we train, the variables in intermediate layers (e.g.,\naffine transformation outputs in MLP) may take values with widely varying magnitudes:\nwhether along the layers from input to output, across units in the same layer, and over\ntime due to our updates to the model parameters. The inventors of batch normalization\npostulated informally that this drift in the distribution of such variables could hamper the\nconvergence of the network. Intuitively, we might conjecture that if one layer has variable\nactivations that are 100 times that of another layer, this might necessitate compensatory\nadjustments in the learning rates. Adaptive solvers such as AdaGrad ( Duchiet al., 2011),\nAdam (Kingma and Ba, 2014 ), Yogi ( Zaheeret al., 2018), or Distributed Shampoo ( Anil\netal., 2020)aimtoaddressthisfromtheviewpointofoptimization, e.g., byaddingaspects\nofsecond-ordermethods. Thealternativeistopreventtheproblemfromoccurring, simply\nby adaptive normalization.\nThird, deeper networks are complex and tend to be more liable to overfitting. This means\nthat regularization becomes more critical. A common technique for regularization is noise\ninjection. This has been known for a long time, e.g., with regard to noise injection for\nthe inputs ( Bishop, 1995 ). It also forms the basis of dropout in Section 5.6 . As it turns\nout, quite serendipitously, batch normalization conveys all three benefits: preprocessing,\nnumerical stability, and regularization.\nBatch normalization is applied to individual layers, or optionally, to all of them: In each\ntrainingiteration,wefirstnormalizetheinputs(ofbatchnormalization)bysubtractingtheir\nmeananddividingbytheirstandarddeviation,wherebothareestimatedbasedonthestatis-\ntics of the current minibatch. Next, we apply a scale coefficient and an offset to recover the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fa234ca-d799-4869-9cdc-0653cfd177b7": {"__data__": {"id_": "2fa234ca-d799-4869-9cdc-0653cfd177b7", "embedding": null, "metadata": {"page_label": "294", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab2c582c-4fcb-44f8-8b1d-b004a4f60230", "node_type": "4", "metadata": {"page_label": "294", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "130567dd9177c11baf593c3e662d092a95272ed9c49725b0155f2f765a1f6ea0", "class_name": "RelatedNodeInfo"}}, "text": "294 Modern Convolutional Neural Networks\nlost degrees of freedom. It is precisely due to this normalization based on batchstatistics\nthatbatchnormalization derives its name.\nNote that if wetried to applybatch normalization with minibatches of size 1, wewouldnot\nbe able to learn anything. That is because after subtracting the means, each hidden unit\nwould take value 0. As you might guess, since we are devoting a whole section to batch\nnormalization, with large enough minibatches the approach proves effective and stable.\nOne takeaway here is that when applying batch normalization, the choice of batch size is\neven more significant than without batch normalization, or at least, suitable calibration is\nneeded as we might adjust batch size.\nDenote byBa minibatch and let x2Bbe an input to batch normalization (BN). In this\ncase the batch normalization is defined as follows:\nBN\u00b9x\u00ba=\ud835\udf38\fx\u0000\u02c6\ud835\udf41B\n\u02c6\ud835\udf48B\u00b8\ud835\udf37. (8.5.1)\nIn(8.5.1 ),\u02c6\ud835\udf41Bisthesamplemeanand \u02c6\ud835\udf48Bisthesamplestandarddeviationoftheminibatch\nB. Afterapplyingstandardization,theresultingminibatchhaszeromeanandunitvariance.\nThechoiceofunitvariance(ratherthansomeothermagicnumber)isarbitrary. Werecover\nthisdegreeoffreedombyincludinganelementwise scaleparameter \ud835\udf38andshiftparameter\n\ud835\udf37that have the same shape as x. Both are parameters that need to be learned as part of\nmodel training.\nThe variable magnitudes for intermediate layers cannot diverge during training since batch\nnormalizationactivelycentersandrescalesthembacktoagivenmeanandsize(via \u02c6\ud835\udf41Band\n\u02c6\ud835\udf48B). Practical experience confirms that, as alluded to when discussing feature rescaling,\nbatch normalization seems to allow for more aggressive learning rates. We calculate \u02c6\ud835\udf41B\nand \u02c6\ud835\udf48Bin(8.5.1 )as follows:\n\u02c6\ud835\udf41B=1\njBj\u00d5\nx2Bxand \u02c6\ud835\udf482\nB=1\njBj\u00d5\nx2B\u00b9x\u0000\u02c6\ud835\udf41B\u00ba2\u00b8\ud835\udf16. (8.5.2)\nNote that we add a small constant \ud835\udf16 > 0to the variance estimate to ensure that we never\nattempt division by zero, even in cases where the empirical variance estimate might be\nverysmallorvanish. Theestimates \u02c6\ud835\udf41Band \u02c6\ud835\udf48Bcounteractthescalingissuebyusingnoisy\nestimates of mean and variance. You might think that this noisiness should be a problem.\nOn the contrary, it is actually beneficial.\nThis turns out to be a recurring theme in deep learning. For reasons that are not yet well-\ncharacterized theoretically, various sources of noise in optimization often lead to faster\ntraining and less overfitting: this variation appears to act as a form of regularization. Teye\netal.(2018)andLuo etal.(2018)relatedthepropertiesofbatchnormalizationtoBayesian\npriors and penalties, respectively. In particular, this sheds some light on the puzzle of why\nbatch normalization works best for moderate minibatch sizes in the 50\u2013100 range. This\nparticular size of minibatch seems to inject just the \u201cright amount\u201d of noise per layer, both\nintermsofscalevia \u02c6\ud835\udf48, and intermsofoffsetvia \u02c6\ud835\udf41: a largerminibatchregularizeslessdue\nto the more stable estimates, whereas tiny minibatches destroy useful signal due to high\nvariance. Exploring this direction further, considering alternative types of preprocessing\nand filtering may yet lead to other effective types of regularization.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed814763-1e6b-4d9d-8d74-9e10cf909801": {"__data__": {"id_": "ed814763-1e6b-4d9d-8d74-9e10cf909801", "embedding": null, "metadata": {"page_label": "295", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be85e1f2-50de-45b6-abb1-53da29a52fa1", "node_type": "4", "metadata": {"page_label": "295", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c96ea26ff9a9110cd523c7355f059f716f406d3291760f945e3fea48cef18447", "class_name": "RelatedNodeInfo"}}, "text": "295 Batch Normalization\nFixing a trained model, you might think that we would prefer using the entire dataset to\nestimate the mean and variance. Once training is complete, why would we want the same\nimage to be classified differently, depending on the batch in which it happens to reside?\nDuring training, such exact calculation is infeasible because the intermediate variables for\nall data examples change every time we update our model. However, once the model is\ntrained, we can calculate the means and variances of each layer\u2019s variables based on the\nentire dataset. Indeed this is standard practice for models employing batch normalization;\nthusbatchnormalizationlayersfunctiondifferentlyin trainingmode (normalizingbymini-\nbatchstatistics)thanin predictionmode (normalizingbydatasetstatistics). Inthisformthey\nclosely resemble the behavior of dropout regularization of Section 5.6 , where noise is only\ninjected during training.\n8.5.2BatchNormalizationLayers\nBatch normalization implementations for fully connected layers and convolutional layers\nare slightly different. One key difference between batch normalization and other layers is\nthat because the former operates on a full minibatch at a time, we cannot just ignore the\nbatch dimension as we did before when introducing other layers.\nFullyConnected Layers\nWhenapplyingbatchnormalizationtofullyconnectedlayers, IoffeandSzegedy( 2015), in\ntheir original paper inserted batch normalization after the affine transformation and before\nthe nonlinear activation function. Later applications experimented with inserting batch\nnormalization right afteractivation functions. Denoting the input to the fully connected\nlayer by x, the affine transformation by Wx\u00b8b(with the weight parameter Wand the\nbias parameter b), and the activation function by \ud835\udf19, we can express the computation of a\nbatch-normalization-enabled, fully connected layer output has follows:\nh=\ud835\udf19\u00b9BN\u00b9Wx\u00b8b\u00ba\u00ba. (8.5.3)\nRecall that mean and variance are computed on the sameminibatch on which the transfor-\nmation is applied.\nConvolutionalLayers\nSimilarly,withconvolutionallayers,wecanapplybatchnormalizationaftertheconvolution\nbut before the nonlinear activation function. The key difference from batch normalization\nin fully connected layers is that we apply the operation on a per-channel basis across all\nlocations . This is compatible with our assumption of translation invariance that led to\nconvolutions: we assumed that the specific location of a pattern within an image was not\ncritical for the purpose of understanding.\nAssume that our minibatches contain \ud835\udc5aexamples and that for each channel, the output\nof the convolution has height \ud835\udc5dand width\ud835\udc5e. For convolutional layers, we carry out each\nbatch normalization over the \ud835\udc5a\u0001\ud835\udc5d\u0001\ud835\udc5eelements per output channel simultaneously. Thus,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6e34e56-6fa3-45bc-8ed2-e89530c71574": {"__data__": {"id_": "c6e34e56-6fa3-45bc-8ed2-e89530c71574", "embedding": null, "metadata": {"page_label": "296", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e3ccea7-4ea6-4b80-85b1-0192b485113c", "node_type": "4", "metadata": {"page_label": "296", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "853dda841eca1d9a1321b9e976c87ce3dc7b9d06428497f316b460976a3db243", "class_name": "RelatedNodeInfo"}}, "text": "296 Modern Convolutional Neural Networks\nwe collect the values over all spatial locations when computing the mean and variance and\nconsequently apply the same mean and variance within a given channel to normalize the\nvalue at each spatial location. Each channel has its own scale and shift parameters, both of\nwhich are scalars.\nLayerNormalization\nNote that in the context of convolutions the batch normalization is well defined even for\nminibatches of size 1: after all, we have all the locations across an image to average. Con-\nsequently, mean and variance are well defined, even if it is just within a single observation.\nThis consideration led Ba et al.(2016) to introduce the notion of layer normalization . It\nworks just like a batch norm, only that it is applied to one observation at a time. Conse-\nquently both the offset and the scaling factor are scalars. For an \ud835\udc5b-dimensional vector x,\nlayer norms are given by\nx!LN\u00b9x\u00ba=x\u0000\u02c6\ud835\udf07\n\u02c6\ud835\udf0e, (8.5.4)\nwhere scaling and offset are applied coefficient-wise and given by\n\u02c6\ud835\udf07def=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc65\ud835\udc56and \u02c6\ud835\udf0e2def=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1\u00b9\ud835\udc65\ud835\udc56\u0000\u02c6\ud835\udf07\u00ba2\u00b8\ud835\udf16. (8.5.5)\nAsbeforeweaddasmalloffset \ud835\udf16 >0topreventdivisionbyzero. Oneofthemajorbenefits\nof using layer normalization is that it prevents divergence. After all, ignoring \ud835\udf16, the output\nofthelayernormalizationisscaleindependent. Thatis, wehaveLN \u00b9x\u00ba\u0019LN\u00b9\ud835\udefcx\u00baforany\nchoice of\ud835\udefc\u22600. This becomes an equality for j\ud835\udefcj!1(the approximate equality is due\nto the offset\ud835\udf16for the variance).\nAnother advantage of the layer normalization is that it does not depend on the minibatch\nsize. Itisalsoindependentofwhetherweareintrainingortestregime. Inotherwords,itis\nsimplyadeterministictransformationthatstandardizestheactivationstoagivenscale. This\ncanbeverybeneficialinpreventingdivergenceinoptimization. Weskipfurtherdetailsand\nrecommend that interested readers consult the original paper.\nBatchNormalization During Prediction\nAswementionedearlier,batchnormalizationtypicallybehavesdifferentlyintrainingmode\nthaninpredictionmode. First,thenoiseinthesamplemeanandthesamplevariancearising\nfromestimatingeachonminibatchesisnolongerdesirableoncewehavetrainedthemodel.\nSecond, we might not have the luxury of computing per-batch normalization statistics. For\nexample, we might need to apply our model to make one prediction at a time.\nTypically, after training, we use the entire dataset to compute stable estimates of the vari-\nable statistics and then fix them at prediction time. Hence, batch normalization behaves\ndifferently during training than at test time. Recall that dropout also exhibits this charac-\nteristic.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c8733b6-5f88-4c93-9d8d-66ded4599fe8": {"__data__": {"id_": "7c8733b6-5f88-4c93-9d8d-66ded4599fe8", "embedding": null, "metadata": {"page_label": "297", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d66172d5-5c87-4774-b04b-9d75d2acd12b", "node_type": "4", "metadata": {"page_label": "297", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "26c46ce086ae4acfbd3c5fb78010fd07e8f2b4b2fc9b161d03b94b4aed1aca18", "class_name": "RelatedNodeInfo"}}, "text": "297 Batch Normalization\n8.5.3Implementation fromScratch\nTo see how batch normalization works in practice, we implement one from scratch be-\nlow.\ndef batch_norm (X, gamma, beta, moving_mean, moving_var, eps, momentum):\n# Use is_grad_enabled to determine whether we are in training mode\nifnot torch .is_grad_enabled():\n# In prediction mode, use mean and variance obtained by moving average\nX_hat =(X-moving_mean) /torch .sqrt(moving_var +eps)\nelse :\nassert len(X.shape) in(2,4)\niflen(X.shape) ==2:\n# When using a fully connected layer, calculate the mean and\n# variance on the feature dimension\nmean =X.mean(dim =0)\nvar =((X -mean) **2).mean(dim =0)\nelse :\n# When using a two-dimensional convolutional layer, calculate the\n# mean and variance on the channel dimension (axis=1). Here we\n# need to maintain the shape of X, so that the broadcasting\n# operation can be carried out later\nmean =X.mean(dim =(0,2,3), keepdim =True )\nvar =((X -mean) **2).mean(dim =(0,2,3), keepdim =True )\n# In training mode, the current mean and variance are used\nX_hat =(X-mean) /torch .sqrt(var +eps)\n# Update the mean and variance using moving average\nmoving_mean =(1.0 -momentum) *moving_mean +momentum *mean\nmoving_var =(1.0 -momentum) *moving_var +momentum *var\nY=gamma *X_hat +beta # Scale and shift\nreturn Y, moving_mean .data, moving_var .data\nWecannowcreateaproper BatchNorm layer. Ourlayerwillmaintainproperparametersfor\nscale gammaand shift beta, both of which will be updated in the course of training. Addi-\ntionally,ourlayerwillmaintainmovingaveragesofthemeansandvariancesforsubsequent\nuse during model prediction.\nPuttingasidethealgorithmicdetails,notethedesignpatternunderlyingourimplementation\nof the layer. Typically, we define the mathematics in a separate function, say batch_norm .\nWethenintegratethisfunctionalityintoacustomlayer,whosecodemostlyaddressesbook-\nkeepingmatters, suchasmovingdatatotherightdevicecontext, allocatingandinitializing\nany required variables, keeping track of moving averages (here for mean and variance),\nand so on. This pattern enables a clean separation of mathematics from boilerplate code.\nAlso note that for the sake of convenience we did not worry about automatically inferring\nthe input shape here; thus we need to specify the number of features throughout. By now\nall modern deep learning frameworks offer automatic detection of size and shape in the\nhigh-level batch normalization APIs (in practice we will use this instead).\nclass BatchNorm (nn.Module):\n# num_features: the number of outputs for a fully connected layer or the\n# number of output channels for a convolutional layer. num_dims: 2 for a\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63b66362-2106-4ca8-a5fd-04b48c41ec1a": {"__data__": {"id_": "63b66362-2106-4ca8-a5fd-04b48c41ec1a", "embedding": null, "metadata": {"page_label": "298", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f4cf67f-16c1-4c00-8f21-148004e01c72", "node_type": "4", "metadata": {"page_label": "298", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f7515b7823e8f828d2191fb6145384e7be4c88838bc86547de9552cb9c37f210", "class_name": "RelatedNodeInfo"}}, "text": "298 Modern Convolutional Neural Networks\n(continued from previous page)\n# fully connected layer and 4 for a convolutional layer\ndef __init__ (self , num_features, num_dims):\nsuper ().__init__ ()\nifnum_dims ==2:\nshape =(1, num_features)\nelse :\nshape =(1, num_features, 1,1)\n# The scale parameter and the shift parameter (model parameters) are\n# initialized to 1 and 0, respectively\nself .gamma =nn.Parameter(torch .ones(shape))\nself .beta =nn.Parameter(torch .zeros(shape))\n# The variables that are not model parameters are initialized to 0 and\n# 1\nself .moving_mean =torch .zeros(shape)\nself .moving_var =torch .ones(shape)\ndef forward (self , X):\n# If X is not on the main memory, copy moving_mean and moving_var to\n# the device where X is located\nifself .moving_mean .device !=X.device:\nself .moving_mean =self .moving_mean .to(X .device)\nself .moving_var =self .moving_var .to(X .device)\n# Save the updated moving_mean and moving_var\nY,self .moving_mean, self .moving_var =batch_norm(\nX,self .gamma, self .beta, self .moving_mean,\nself .moving_var, eps =1e-5 , momentum =0.1)\nreturn Y\nWe used momentum to govern the aggregation over past mean and variance estimates. This\nis somewhat of a misnomer as it has nothing whatsoever to do with the momentum term of\noptimization. Nonetheless, itisthecommonlyadoptednameforthistermandindeference\nto API naming convention we use the same variable name in our code.\n8.5.4LeNetwith BatchNormalization\nTo see how to apply BatchNorm in context, below we apply it to a traditional LeNet model\n(Section 7.6 ). Recall that batch normalization is applied after the convolutional layers or\nfully connected layers but before the corresponding activation functions.\nclass BNLeNetScratch (d2l .Classifier):\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(\nnn.LazyConv2d( 6, kernel_size =5), BatchNorm( 6, num_dims =4),\nnn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),\nnn.LazyConv2d( 16, kernel_size =5), BatchNorm( 16, num_dims =4),\nnn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),\nnn.Flatten(), nn .LazyLinear( 120),\nBatchNorm( 120, num_dims =2), nn .Sigmoid(), nn .LazyLinear( 84),\nBatchNorm( 84, num_dims =2), nn .Sigmoid(),\nnn.LazyLinear(num_classes))", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cac38340-0975-40a9-9190-14bfcc9ae555": {"__data__": {"id_": "cac38340-0975-40a9-9190-14bfcc9ae555", "embedding": null, "metadata": {"page_label": "299", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f54e4bc0-7397-43c5-9a9c-c2c90f82faa9", "node_type": "4", "metadata": {"page_label": "299", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f06a117e1295bc66ae6c2e80f1711ee1095db0d65cbb8410765b40414691c1b2", "class_name": "RelatedNodeInfo"}}, "text": "299 Batch Normalization\nAs before, we will train our network on the Fashion-MNIST dataset. This code is virtually\nidentical to that when we first trained LeNet.\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128)\nmodel =BNLeNetScratch(lr =0.1)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)\nLet\u2019s have a look at the scale parameter gammaand the shift parameter betalearned from\nthe first batch normalization layer.\nmodel .net[ 1].gamma .reshape(( -1,)), model .net[ 1].beta .reshape(( -1,))\n(tensor([ 1.4334 ,1.9905 ,1.8584 ,2.0740 ,2.0522 ,1.8877 ], device ='cuda:0 ',\ngrad_fn =<ViewBackward0 >),\ntensor([ 0.7354 ,-1.3538 ,-0.2567 ,-0.9991 ,-0.3028 ,1.3125 ], device ='cuda:0\n\u21a9!',\ngrad_fn =<ViewBackward0 >))\n8.5.5Concise Implementation\nCompared with the BatchNorm class, which we just defined ourselves, we can use the\nBatchNorm class defined in high-level APIs from the deep learning framework directly.\nThe code looks virtually identical to our implementation above, except that we no longer\nneed to provide additional arguments for it to get the dimensions right.\nclass BNLeNet (d2l .Classifier):\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential(\nnn.LazyConv2d( 6, kernel_size =5), nn .LazyBatchNorm2d(),\nnn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),\nnn.LazyConv2d( 16, kernel_size =5), nn .LazyBatchNorm2d(),\nnn.Sigmoid(), nn .AvgPool2d(kernel_size =2, stride =2),\nnn.Flatten(), nn .LazyLinear( 120), nn .LazyBatchNorm1d(),\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1639, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c25687ff-5ab1-44c7-b027-7c29d813d957": {"__data__": {"id_": "c25687ff-5ab1-44c7-b027-7c29d813d957", "embedding": null, "metadata": {"page_label": "300", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a82127f1-11be-4b79-ac97-22056bd3b524", "node_type": "4", "metadata": {"page_label": "300", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c150436e753eeb133a7e8f0629b42073e30607c3d597649e2c5cf32d428df94c", "class_name": "RelatedNodeInfo"}}, "text": "300 Modern Convolutional Neural Networks\n(continued from previous page)\nnn.Sigmoid(), nn .LazyLinear( 84), nn .LazyBatchNorm1d(),\nnn.Sigmoid(), nn .LazyLinear(num_classes))\nBelow, we use the same hyperparameters to train our model. Note that as usual, the high-\nlevel API variant runs much faster because its code has been compiled to C++ or CUDA\nwhile our custom implementation must be interpreted by Python.\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128)\nmodel =BNLeNet(lr =0.1)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)\n8.5.6Discussion\nIntuitively, batch normalization is thought to make the optimization landscape smoother.\nHowever, we must be careful to distinguish between speculative intuitions and true expla-\nnations for the phenomena that we observe when training deep models. Recall that we do\nnotevenknowwhysimplerdeepneuralnetworks(MLPsandconventionalCNNs)general-\nize well in the first place. Even with dropout and weight decay, they remain so flexible that\ntheir ability to generalize to unseen data likely needs significantly more refined learning-\ntheoretic generalization guarantees.\nTheoriginalpaperproposingbatchnormalization( IoffeandSzegedy, 2015 ), inadditionto\nintroducingapowerfulandusefultool,offeredanexplanationforwhyitworks: byreducing\ninternal covariate shift . Presumably by internal covariate shift they meant something like\nthe intuition expressed above\u2014the notion that the distribution of variable values changes\noverthecourseoftraining. However,thereweretwoproblemswiththisexplanation: i)This\ndrift is very different from covariate shift , rendering the name a misnomer. If anything, it\nis closer to concept drift. ii) The explanation offers an under-specified intuition but leaves\nthe question of whypreciselythistechniqueworks an open question wanting for a rigorous\nexplanation. Throughoutthisbook,weaimtoconveytheintuitionsthatpractitionersuseto\nguide their development of deep neural networks. However, we believe that it is important\nto separate these guiding intuitions from established scientific fact. Eventually, when you", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2177, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce111cd6-6b11-4ebb-97e7-da8b353e0265": {"__data__": {"id_": "ce111cd6-6b11-4ebb-97e7-da8b353e0265", "embedding": null, "metadata": {"page_label": "301", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a398fe64-1b5b-4eb4-8e1d-a57663a2f83f", "node_type": "4", "metadata": {"page_label": "301", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "97572c9f995c4036632661b7960a6795c533e8b40507af9d5b20f5ee607e03db", "class_name": "RelatedNodeInfo"}}, "text": "301 Batch Normalization\nmaster this material and start writing your own research papers you will want to be clear to\ndelineate between technical claims and hunches.\nFollowingthesuccessofbatchnormalization,itsexplanationintermsof internalcovariate\nshifthas repeatedly surfaced in debates in the technical literature and broader discourse\nabout how to present machine learning research. In a memorable speech given while ac-\ncepting a Test of Time Award at the 2017 NeurIPS conference, Ali Rahimi used internal\ncovariateshift asafocalpointinanargumentlikeningthemodernpracticeofdeeplearning\nto alchemy. Subsequently, the example was revisited in detail in a position paper outlining\ntroubling trends in machine learning ( Lipton and Steinhardt, 2018 ). Other authors have\nproposed alternative explanations for the success of batch normalization, some ( Santurkar\netal.,2018)claimingthatbatchnormalization\u2019ssuccesscomesdespiteexhibitingbehavior\nthat is in some ways opposite to those claimed in the original paper.\nWe note that the internal covariate shift is no more worthy of criticism than any of thou-\nsandsofsimilarlyvagueclaimsmadeeveryyearinthetechnicalmachinelearningliterature.\nLikely, its resonance as a focal point of these debates owes to its broad recognizability for\nthe target audience. Batch normalization has proven an indispensable method, applied in\nnearlyalldeployedimageclassifiers,earningthepaperthatintroducedthetechniquetensof\nthousandsofcitations. Weconjecture, though, thattheguidingprinciplesofregularization\nthrough noise injection, acceleration through rescaling and lastly preprocessing may well\nlead to further inventions of layers and techniques in the future.\nOn a more practical note, there are a number of aspects worth remembering about batch\nnormalization:\n\u000fDuringmodeltraining,batchnormalizationcontinuouslyadjuststheintermediateoutput\nof the network by utilizing the mean and standard deviation of the minibatch, so that\nthe values of the intermediate output in each layer throughout the neural network are\nmore stable.\n\u000fBatchnormalizationisslightlydifferentforfullyconnectedlayersthanforconvolutional\nlayers. In fact, for convolutional layers, layer normalization can sometimes be used as\nan alternative.\n\u000fLikeadropoutlayer,batchnormalizationlayershavedifferentbehaviorsintrainingmode\nthan in prediction mode.\n\u000fBatchnormalizationisusefulforregularizationandimprovingconvergenceinoptimiza-\ntion. By contrast, the original motivation of reducing internal covariate shift seems\nnot to be a valid explanation.\n\u000fFor more robust models that are less sensitive to input perturbations, consider removing\nbatch normalization ( Wangetal., 2022).\n8.5.7Exercises\n1.Shouldweremovethebiasparameterfromthefullyconnectedlayerortheconvolutional\nlayer before the batch normalization? Why?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2807, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ed436ea-8c07-4280-bd4e-b37241178a0a": {"__data__": {"id_": "6ed436ea-8c07-4280-bd4e-b37241178a0a", "embedding": null, "metadata": {"page_label": "302", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7dbb6d46-82bb-4b56-aa33-d78c00d7ece2", "node_type": "4", "metadata": {"page_label": "302", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "82ff1c0bd4fea2b3d1f4c5bba023194d9c216610af01a6483f78780ad8036fec", "class_name": "RelatedNodeInfo"}}, "text": "302 Modern Convolutional Neural Networks\n1322.Compare the learning rates for LeNet with and without batch normalization.\n1.Plot the increase in validation accuracy.\n2.Howlargecanyoumakethelearningratebeforetheoptimizationfailsinbothcases?\n3.Do we need batch normalization in every layer? Experiment with it.\n4.Implement a \u201clite\u201d version of batch normalization that only removes the mean, or alter-\nnatively one that only removes the variance. How does it behave?\n5.Fix the parameters betaandgamma. Observe and analyze the results.\n6.Can you replace dropout by batch normalization? How does the behavior change?\n7.Research ideas: think of other normalization transforms that you can apply:\n1.Can you apply the probability integral transform?\n2.Can you use a full-rank covariance estimate? Why should you probably not do that?\n3.Can you use other compact matrix variants (block-diagonal, low-displacement rank,\nMonarch, etc.)?\n4.Does a sparsification compression act as a regularizer?\n5.Are there other projections (e.g., convex cone, symmetry group-specific transforms)\nthat you can use?\nDiscussions132.\n8.6ResidualNetworks(ResNet)and ResNeXt\nAs we design ever deeper networks it becomes imperative to understand how adding layers\ncan increase the complexity and expressiveness of the network. Even more important is\ntheabilitytodesignnetworkswhereaddinglayersmakesnetworksstrictlymoreexpressive\nrather than just different. To make some progress we need a bit of mathematics.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n8.6.1Function Classes\nConsiderF,theclassoffunctionsthataspecificnetworkarchitecture(togetherwithlearn-\ning rates and other hyperparameter settings) can reach. That is, for all \ud835\udc532Fthere exists\nsome set of parameters (e.g., weights and biases) that can be obtained through training on\na suitable dataset. Let\u2019s assume that \ud835\udc53\u0003is the \u201ctruth\u201d function that we really would like to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce52a2ef-9b41-410a-b104-d0f6e6392843": {"__data__": {"id_": "ce52a2ef-9b41-410a-b104-d0f6e6392843", "embedding": null, "metadata": {"page_label": "303", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5651aed3-a2a3-4170-866c-3f61cdeb7a63", "node_type": "4", "metadata": {"page_label": "303", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "20fe5fd2cb3133530722a82491b0e262ea75ea16ffc7f6ec75c75b05da3f9c60", "class_name": "RelatedNodeInfo"}}, "text": "303 Residual Networks (ResNet) and ResNeXt\nfind. If it is inF, we are in good shape but typically we will not be quite so lucky. Instead,\nwe will try to find some \ud835\udc53\u0003\nFwhich is our best bet within F. For instance, given a dataset\nwith features Xand labels y, we might try finding it by solving the following optimization\nproblem:\n\ud835\udc53\u0003\nFdef=argmin\n\ud835\udc53\ud835\udc3f\u00b9X,y, \ud835\udc53\u00basubject to\ud835\udc532F. (8.6.1)\nWe know that regularization ( Morozov, 1984 ,Tikhonov and Arsenin, 1977 ) may control\ncomplexityofFandachieveconsistency,soalargersizeoftrainingdatagenerallyleadsto\nbetter\ud835\udc53\u0003\nF. It is only reasonable to assume that if we design a different and more powerful\narchitectureF0we should arrive at a better outcome. In other words, we would expect\nthat\ud835\udc53\u0003\nF0is \u201cbetter\u201d than \ud835\udc53\u0003\nF. However, ifF\u2288F0there is no guarantee that this should\neven happen. In fact, \ud835\udc53\u0003\nF0might well be worse. As illustrated by Fig. 8.6.1 , for non-nested\nfunctionclasses, alargerfunctionclassdoesnotalwaysmoveclosertothe\u201ctruth\u201dfunction\n\ud835\udc53\u0003. Forinstance, ontheleftof Fig.8.6.1 , thoughF3iscloserto\ud835\udc53\u0003thanF1,F6movesaway\nand there is no guarantee that further increasing the complexity can reduce the distance\nfrom\ud835\udc53\u0003. With nested function classes where F1\u0012\u0001\u0001\u0001\u0012F 6on the right of Fig. 8.6.1 , we\ncan avoid the aforementioned issue from the non-nested function classes.\ntFig. 8.6.1 For non-nested function classes, a larger (indicated by area) function class does not\nguarantee we will get closer to the \u201ctruth\u201d function ( f\u0003). This does not happen in nested\nfunction classes.\nThus,onlyiflargerfunctionclassescontainthesmalleronesareweguaranteedthatincreas-\ning them strictly increases the expressive power of the network. For deep neural networks,\nif we can train the newly-added layer into an identity function \ud835\udc53\u00b9x\u00ba=x, the new model\nwill be as effective as the original model. As the new model may get a better solution to fit\nthe training dataset, the added layer might make it easier to reduce training errors.\nThis is the question that He et al.(2016) considered when working on very deep com-\nputer vision models. At the heart of their proposed residual network (ResNet) is the idea\nthat every additional layer should more easily contain the identity function as one of its\nelements. These considerations are rather profound but they led to a surprisingly simple\nsolution, a residualblock . With it, ResNet won the ImageNet Large Scale Visual Recogni-\ntion Challenge in 2015. The design had a profound influence on how to build deep neural\nnetworks. For instance, residual blocks have been added to recurrent networks ( Kimetal.,\n2017,Prakashet al., 2016). Likewise, Transformers ( Vaswaniet al., 2017) use them to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f755184-ed9b-40f0-9621-f70df9585ce5": {"__data__": {"id_": "1f755184-ed9b-40f0-9621-f70df9585ce5", "embedding": null, "metadata": {"page_label": "304", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "798cae7e-23d0-4662-816e-c08708bcdb07", "node_type": "4", "metadata": {"page_label": "304", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ee8ef931184f338b5028ac06189c32d3232406fc134abdf6bd97a2ec4e1eaf50", "class_name": "RelatedNodeInfo"}}, "text": "304 Modern Convolutional Neural Networks\nstack many layers of networks efficiently. It is also used in graph neural networks ( Kipf\nandWelling, 2016 )and, asabasicconcept, ithasbeenusedextensivelyincomputervision\n(Redmonand Farhadi, 2018 ,Renetal., 2015). Notethat residual networksare predated by\nhighwaynetworks( Srivastava etal.,2015)thatsharesomeofthemotivation,albeitwithout\nthe elegant parametrization around the identity function.\n8.6.2ResidualBlocks\nLet\u2019s focus on a local part of a neural network, as depicted in Fig. 8.6.2 . Denote the input\nbyx. Weassumethat \ud835\udc53\u00b9x\u00ba,thedesiredunderlyingmappingwewanttoobtainbylearning,\nistobeusedasinputtotheactivationfunctiononthetop. Ontheleft,theportionwithinthe\ndotted-line box must directly learn \ud835\udc53\u00b9x\u00ba. On the right, the portion within the dotted-line\nbox needs to learn the residualmapping \ud835\udc54\u00b9x\u00ba=\ud835\udc53\u00b9x\u00ba\u0000x, which is how the residual block\nderives its name. If the identity mapping \ud835\udc53\u00b9x\u00ba=xis the desired underlying mapping, the\nresidualmappingamountsto \ud835\udc54\u00b9x\u00ba=0anditisthuseasiertolearn: weonlyneedtopushthe\nweights and biases of the upper weight layer (e.g., fully connected layer and convolutional\nlayer) within the dotted-line box to zero. The right figure illustrates the residual block of\nResNet, where the solid line carrying the layer input xto the addition operator is called\naresidual connection (orshortcut connection ). With residual blocks, inputs can forward\npropagate faster through the residual connections across layers. In fact, the residual block\ncanbethoughtofasaspecialcaseofthemulti-branchInceptionblock: ithastwobranches\none of which is the identity mapping.\ntFig. 8.6.2 In a regular block (left), the portion within the dotted-line box must directly learn the\nmapping f\u00b9x\u00ba. In a residual block (right), the portion within the dotted-line box needs to\nlearn the residual mapping g\u00b9x\u00ba=f\u00b9x\u00ba\u0000x, making the identity mapping f\u00b9x\u00ba=xeasier\nto learn.\nResNet has VGG\u2019s full 3\u00023convolutional layer design. The residual block has two 3\u00023\nconvolutional layers with the same number of output channels. Each convolutional layer\nis followed by a batch normalization layer and a ReLU activation function. Then, we skip\nthesetwoconvolutionoperationsandaddtheinputdirectlybeforethefinalReLUactivation\nfunction. This kind of design requires that the output of the twoconvolutional layers has to\nbe of the same shape as the input, so that they can be added together. If we want to change\nthe number of channels, we need to introduce an additional 1\u00021convolutional layer to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90972da9-91c5-4412-9f88-ce362199d172": {"__data__": {"id_": "90972da9-91c5-4412-9f88-ce362199d172", "embedding": null, "metadata": {"page_label": "305", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "890f675c-9b03-48e6-8dfa-faaa19476c18", "node_type": "4", "metadata": {"page_label": "305", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9de41ceabfb8fa65356260b49f87f552019df1272194746e6d91ea615ff20ea0", "class_name": "RelatedNodeInfo"}}, "text": "305 Residual Networks (ResNet) and ResNeXt\ntransform the input into the desired shape for the addition operation. Let\u2019s have a look at\nthe code below.\nclass Residual (nn.Module): #@save\n\"\"\"The Residual block of ResNet models.\"\"\"\ndef __init__ (self , num_channels, use_1x1conv =False , strides =1):\nsuper ().__init__ ()\nself .conv1 =nn.LazyConv2d(num_channels, kernel_size =3, padding =1,\nstride =strides)\nself .conv2 =nn.LazyConv2d(num_channels, kernel_size =3, padding =1)\nifuse_1x1conv:\nself .conv3 =nn.LazyConv2d(num_channels, kernel_size =1,\nstride =strides)\nelse :\nself .conv3 =None\nself .bn1 =nn.LazyBatchNorm2d()\nself .bn2 =nn.LazyBatchNorm2d()\ndef forward (self , X):\nY=F.relu( self .bn1( self .conv1(X)))\nY=self .bn2( self .conv2(Y))\nifself .conv3:\nX=self .conv3(X)\nY+=X\nreturn F.relu(Y)\nThiscodegeneratestwotypesofnetworks: onewhereweaddtheinputtotheoutputbefore\napplying the ReLU nonlinearity whenever use_1x1conv=False ; and one where we adjust\nchannelsandresolutionbymeansofa 1\u00021convolutionbeforeadding. Fig.8.6.3 illustrates\nthis.\ntFig. 8.6.3 ResNet block with and without 1 \u00021 convolution, which transforms the input into the\ndesired shape for the addition operation.\nNow let\u2019s look at a situation where the input and output are of the same shape, where 1\u00021\nconvolution is not needed.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfe93013-57a3-445e-a4c8-5ba2d4680089": {"__data__": {"id_": "bfe93013-57a3-445e-a4c8-5ba2d4680089", "embedding": null, "metadata": {"page_label": "306", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd796fd0-bd66-4d36-bad0-d6092c6791fb", "node_type": "4", "metadata": {"page_label": "306", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7748b0f857c98567dd2b0ce3ab900eb7d5d91407b26d6d4ef3bac0d9818b9e22", "class_name": "RelatedNodeInfo"}}, "text": "306 Modern Convolutional Neural Networks\nblk =Residual( 3)\nX=torch .randn( 4,3,6,6)\nblk(X) .shape\ntorch .Size([ 4,3,6,6])\nWe also have the option to halve the output height and width while increasing the number\nof output channels. In this case we use 1\u00021convolutions via use_1x1conv=True . This\ncomes in handy at the beginning of each ResNet block to reduce the spatial dimensionality\nviastrides=2 .\nblk =Residual( 6, use_1x1conv =True , strides =2)\nblk(X) .shape\ntorch .Size([ 4,6,3,3])\n8.6.3ResNetModel\nThefirsttwolayersofResNetarethesameasthoseoftheGoogLeNetwedescribedbefore:\nthe7\u00027convolutional layer with 64 output channels and a stride of 2 is followed by the\n3\u00023max-pooling layer with a stride of 2. The difference is the batch normalization layer\nadded after each convolutional layer in ResNet.\nclass ResNet (d2l .Classifier):\ndef b1(self ):\nreturn nn.Sequential(\nnn.LazyConv2d( 64, kernel_size =7, stride =2, padding =3),\nnn.LazyBatchNorm2d(), nn .ReLU(),\nnn.MaxPool2d(kernel_size =3, stride =2, padding =1))\nGoogLeNet uses four modules made up of Inception blocks. However, ResNet uses four\nmodules made up of residual blocks, each of which uses several residual blocks with the\nsame number of output channels. The number of channels in the first module is the same\nas the number of input channels. Since a max-pooling layer with a stride of 2 has already\nbeen used, it is not necessary to reduce the height and width. In the first residual block for\neach of the subsequent modules, the number of channels is doubled compared with that of\nthe previous module, and the height and width are halved.\n@d2l .add_to_class(ResNet)\ndef block (self , num_residuals, num_channels, first_block =False ):\nblk =[]\nfor iinrange (num_residuals):\nifi==0and not first_block:\nblk.append(Residual(num_channels, use_1x1conv =True , strides =2))\nelse :\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45465ea9-19e4-4d97-8ca6-791c46b65dd3": {"__data__": {"id_": "45465ea9-19e4-4d97-8ca6-791c46b65dd3", "embedding": null, "metadata": {"page_label": "307", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a044a89-72a3-4676-88e5-e6830a53ce30", "node_type": "4", "metadata": {"page_label": "307", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2d508ccbeead53d2e54a133a5aae03ed41319baa81f0e61362c1a9aaf1070a70", "class_name": "RelatedNodeInfo"}}, "text": "307 Residual Networks (ResNet) and ResNeXt\n(continued from previous page)\nblk.append(Residual(num_channels))\nreturn nn.Sequential( *blk)\nThen, we add all the modules to ResNet. Here, two residual blocks are used for each mod-\nule. Lastly, just like GoogLeNet, we add a global average pooling layer, followed by the\nfully connected layer output.\n@d2l .add_to_class(ResNet)\ndef __init__ (self , arch, lr =0.1, num_classes =10):\nsuper (ResNet, self ).__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential( self .b1())\nfor i, b inenumerate (arch):\nself .net.add_module( f'b{i+2}',self .block( *b, first_block =(i==0)))\nself .net.add_module( 'last ', nn .Sequential(\nnn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten(),\nnn.LazyLinear(num_classes)))\nself .net.apply(d2l .init_cnn)\nTherearefourconvolutionallayersineachmodule(excludingthe 1\u00021convolutionallayer).\nTogetherwiththefirst 7\u00027convolutionallayerandthefinalfullyconnectedlayer,thereare\n18layersintotal. Therefore,thismodeliscommonlyknownasResNet-18. Byconfiguring\ndifferent numbers of channels and residual blocks in the module, we can create different\nResNet models, such as the deeper 152-layer ResNet-152. Although the main architecture\nofResNetissimilartothatofGoogLeNet, ResNet\u2019sstructureissimplerandeasiertomod-\nify. All these factors have resulted in the rapid and widespread use of ResNet. Fig. 8.6.4\ndepicts the full ResNet-18.\ntFig. 8.6.4 The ResNet-18 architecture.\nBeforetrainingResNet,let\u2019sobservehowtheinputshapechangesacrossdifferentmodules\nin ResNet. As in all the previous architectures, the resolution decreases while the number\nof channels increases up until the point where a global average pooling layer aggregates all\nfeatures.\nclass ResNet18 (ResNet):\ndef __init__ (self , lr =0.1, num_classes =10):\nsuper ().__init__ (((2,64), ( 2,128), ( 2,256), ( 2,512)),\nlr, num_classes)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "579c5922-2e00-40f1-847e-ab65500afeca": {"__data__": {"id_": "579c5922-2e00-40f1-847e-ab65500afeca", "embedding": null, "metadata": {"page_label": "308", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c043c329-0236-4773-b612-e98b3aacce30", "node_type": "4", "metadata": {"page_label": "308", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e880cbbef6cdb048ea3eb54cb3ee849a5e318f71d9877f849c9391544e76d984", "class_name": "RelatedNodeInfo"}}, "text": "308 Modern Convolutional Neural Networks\nResNet18() .layer_summary(( 1,1,96,96))\nSequential output shape: torch .Size([ 1,64,24,24])\nSequential output shape: torch .Size([ 1,64,24,24])\nSequential output shape: torch .Size([ 1,128,12,12])\nSequential output shape: torch .Size([ 1,256,6,6])\nSequential output shape: torch .Size([ 1,512,3,3])\nSequential output shape: torch .Size([ 1,10])\n8.6.4Training\nWe train ResNet on the Fashion-MNIST dataset, just like before. ResNet is quite a pow-\nerful and flexible architecture. The plot capturing training and validation loss illustrates a\nsignificant gap between both graphs, with the training loss being considerably lower. For\na network of this flexibility, more training data would offer distinct benefit in closing the\ngap and improving accuracy.\nmodel =ResNet18(lr =0.01 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(96,96))\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model, data)\n8.6.5ResNeXt\nOne of the challenges one encounters in the design of ResNet is the trade-off between non-\nlinearity and dimensionality within a given block. That is, we could add more nonlinearity\nby increasing the number of layers, or by increasing the width of the convolutions. An al-\nternative strategy is to increase the number of channels that can carry information between\nblocks. Unfortunately, the latter comes with a quadratic penalty since the computational\ncost of ingesting \ud835\udc50ichannels and emitting \ud835\udc50ochannels is proportional to O\u00b9\ud835\udc50i\u0001\ud835\udc50o\u00ba(see our\ndiscussion in Section 7.4 ).\nWe can take some inspiration from the Inception block of Fig. 8.4.1 which has informa-\ntion flowing through the block in separate groups. Applying the idea of multiple indepen-\ndent groups to the ResNet block of Fig. 8.6.3 led to the design of ResNeXt ( Xieet al.,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e0b096f-8251-4a24-9ad2-5f38c2e6e3fa": {"__data__": {"id_": "4e0b096f-8251-4a24-9ad2-5f38c2e6e3fa", "embedding": null, "metadata": {"page_label": "309", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "081ac93b-bc28-46c8-ab99-3d21f40c5b25", "node_type": "4", "metadata": {"page_label": "309", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "84cd6561a88a5193c549c109aff02469ca4d39c7e913dbf98f714ee9f5685817", "class_name": "RelatedNodeInfo"}}, "text": "309 Residual Networks (ResNet) and ResNeXt\n2017). Different from the smorgasbord of transformations in Inception, ResNeXt adopts\nthesametransformation in all branches, thus minimizing the need for manual tuning of\neach branch.\ntFig. 8.6.5 The ResNeXt block. The use of grouped convolution with ggroups is gtimes faster than\na dense convolution. It is a bottleneck residual block when the number of intermediate\nchannels bis less than c.\nBreaking up a convolution from \ud835\udc50ito\ud835\udc50ochannels into one of \ud835\udc54groups of size \ud835\udc50i\u009d\ud835\udc54gener-\nating\ud835\udc54outputs of size \ud835\udc50o\u009d\ud835\udc54is called, quite fittingly, a groupedconvolution . The computa-\ntionalcost(proportionally)isreducedfrom O\u00b9\ud835\udc50i\u0001\ud835\udc50o\u00batoO\u00b9\ud835\udc54\u0001\u00b9\ud835\udc50i\u009d\ud835\udc54\u00ba\u0001\u00b9\ud835\udc50o\u009d\ud835\udc54\u00ba\u00ba=O\u00b9\ud835\udc50i\u0001\ud835\udc50o\u009d\ud835\udc54\u00ba,\ni.e.,itis\ud835\udc54timesfaster. Evenbetter,thenumberofparametersneededtogeneratetheoutput\nis also reduced from a \ud835\udc50i\u0002\ud835\udc50omatrix to\ud835\udc54smaller matrices of size \u00b9\ud835\udc50i\u009d\ud835\udc54\u00ba\u0002\u00b9\ud835\udc50o\u009d\ud835\udc54\u00ba, again a\n\ud835\udc54timesreduction. Inwhatfollowsweassumethatboth \ud835\udc50iand\ud835\udc50oaredivisibleby \ud835\udc54.\nTheonlychallengeinthisdesignisthatnoinformationisexchangedbetweenthe \ud835\udc54groups.\nThe ResNeXt block of Fig. 8.6.5 amends this in two ways: the grouped convolution with\na3\u00023kernel is sandwiched in between two 1\u00021convolutions. The second one serves\ndouble duty in changing the number of channels back. The benefit is that we only pay the\nO\u00b9\ud835\udc50\u0001\ud835\udc4f\u00bacost for 1\u00021kernels and can make do with an O\u00b9\ud835\udc4f2\u009d\ud835\udc54\u00bacost for 3\u00023kernels.\nSimilar to the residual block implementation in Section 8.6.2 , the residual connection is\nreplaced (thus generalized) by a 1\u00021convolution.\nTheright-handfigurein Fig.8.6.5 providesamuchmoreconcisesummaryoftheresulting\nnetwork block. It will also play a major role in the design of generic modern CNNs in\nSection 8.8 . Note that the idea of grouped convolutions dates back to the implementation\nof AlexNet ( Krizhevsky et al., 2012). When distributing the network across two GPUs\nwith limited memory, the implementation treated each GPU as its own channel with no ill\neffects.\nThe following implementation of the ResNeXtBlock class takes as argument groups(\ud835\udc54),\nwith bot_channels (\ud835\udc4f)intermediate(bottleneck)channels. Lastly,whenweneedtoreduce", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ada7371e-9776-4c83-bc3e-1377ec6a6699": {"__data__": {"id_": "ada7371e-9776-4c83-bc3e-1377ec6a6699", "embedding": null, "metadata": {"page_label": "310", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf47a3af-25e3-4f13-82b1-e119012a29e3", "node_type": "4", "metadata": {"page_label": "310", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0a1275231078217e9829d44a53db4013a2e2f46556603385e87df12db03e93cf", "class_name": "RelatedNodeInfo"}}, "text": "310 Modern Convolutional Neural Networks\ntheheightandwidthoftherepresentation,weaddastrideof 2bysetting use_1x1conv=True,\nstrides=2 .\nclass ResNeXtBlock (nn.Module): #@save\n\"\"\"The ResNeXt block.\"\"\"\ndef __init__ (self , num_channels, groups, bot_mul, use_1x1conv =False ,\nstrides =1):\nsuper ().__init__ ()\nbot_channels =int(round (num_channels *bot_mul))\nself .conv1 =nn.LazyConv2d(bot_channels, kernel_size =1, stride =1)\nself .conv2 =nn.LazyConv2d(bot_channels, kernel_size =3,\nstride =strides, padding =1,\ngroups =bot_channels //groups)\nself .conv3 =nn.LazyConv2d(num_channels, kernel_size =1, stride =1)\nself .bn1 =nn.LazyBatchNorm2d()\nself .bn2 =nn.LazyBatchNorm2d()\nself .bn3 =nn.LazyBatchNorm2d()\nifuse_1x1conv:\nself .conv4 =nn.LazyConv2d(num_channels, kernel_size =1,\nstride =strides)\nself .bn4 =nn.LazyBatchNorm2d()\nelse :\nself .conv4 =None\ndef forward (self , X):\nY=F.relu( self .bn1( self .conv1(X)))\nY=F.relu( self .bn2( self .conv2(Y)))\nY=self .bn3( self .conv3(Y))\nifself .conv4:\nX=self .bn4( self .conv4(X))\nreturn F.relu(Y +X)\nItsuseisentirelyanalogoustothatofthe ResNetBlock discussedpreviously. Forinstance,\nwhen using ( use_1x1conv=False, strides=1 ), the input and output are of the same\nshape. Alternatively, setting use_1x1conv=True, strides=2 halves the output height\nand width.\nblk =ResNeXtBlock( 32,16,1)\nX=torch .randn( 4,32,96,96)\nblk(X) .shape\ntorch .Size([ 4,32,96,96])\n8.6.6Summaryand Discussion\nNested function classes are desirable since they allow us to obtain strictly more power-\nfulrather than also subtly different function classes when adding capacity. One way of\naccomplishing this is by letting additional layers to simply pass through the input to the\noutput. Residual connections allow for this. As a consequence, this changes the inductive\nbias from simple functions being of the form \ud835\udc53\u00b9x\u00ba=0to simple functions looking like\n\ud835\udc53\u00b9x\u00ba=x.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "840024f5-b890-459f-befa-db53fb8f8adc": {"__data__": {"id_": "840024f5-b890-459f-befa-db53fb8f8adc", "embedding": null, "metadata": {"page_label": "311", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ee68b7f-2c5b-4dc6-8311-460a38e5977c", "node_type": "4", "metadata": {"page_label": "311", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ea1b12a7b432786d35eab7c09f1e5c0eaa7bfc1ace49b24c9c33de50c87427a0", "class_name": "RelatedNodeInfo"}}, "text": "311 Residual Networks (ResNet) and ResNeXt\nThe residual mapping can learn the identity function more easily, such as pushing param-\neters in the weight layer to zero. We can train an effective deepneural network by having\nresidualblocks. Inputscanforwardpropagatefasterthroughtheresidualconnectionsacross\nlayers. Asaconsequence, wecanthustrainmuchdeepernetworks. Forinstance, theorigi-\nnal ResNetpaper ( Heetal., 2016) allowedforup to 152 layers. Another benefit of residual\nnetworks is that it allows us to add layers, initialized as the identity function, duringthe\ntraining process. After all, the default behavior of a layer is to let the data pass through\nunchanged. This can accelerate the training of very large networks in some cases.\nPrior to residual connections, bypassing paths with gating units were introduced to effec-\ntively train highway networks with over 100 layers ( Srivastava etal., 2015). Using identity\nfunctions as bypassing paths, ResNet performed remarkably well on multiple computer vi-\nsion tasks. Residual connections had a major influence on the design of subsequent deep\nneural networks, of either convolutional or sequential nature. As we will introduce later,\nthe Transformer architecture ( Vaswaniet al., 2017) adopts residual connections (together\nwith other design choices) and is pervasive in areas as diverse as language, vision, speech,\nand reinforcement learning.\nResNeXt is an example for how the design of convolutional neural networks has evolved\nover time: by being more frugal with computation and trading it off against the size of the\nactivations (number of channels), it allows for faster and more accurate networks at lower\ncost. An alternative way of viewing grouped convolutions is to think of a block-diagonal\nmatrix for the convolutional weights. Note that there are quite a few such \u201ctricks\u201d that lead\nto more efficient networks. For instance, ShiftNet ( Wuet al., 2018) mimicks the effects of\na3\u00023convolution,simplybyaddingshiftedactivationstothechannels,offeringincreased\nfunction complexity, this time without any computational cost.\nA common feature of the designs we have discussed so far is that the network design is\nfairlymanual, primarilyrelyingontheingenuityofthedesignertofindthe\u201cright\u201dnetwork\nhyperparameters. While clearly feasible, it is also very costly in terms of human time and\nthereisnoguaranteethattheoutcomeisoptimalinanysense. In Section8.8 wewilldiscuss\na number of strategies for obtaining high quality networks in a more automated fashion. In\nparticular, we will review the notion of network design spaces that led to the RegNetX/Y\nmodels ( Radosavovic etal., 2020).\n8.6.7Exercises\n1.WhatarethemajordifferencesbetweentheInceptionblockin Fig.8.4.1 andtheresidual\nblock? How do they compare in terms of computation, accuracy, and the classes of\nfunctions they can describe?\n2.Refer to Table 1 in the ResNet paper ( Heetal., 2016) to implement different variants of\nthe network.\n3.For deeper networks, ResNet introduces a \u201cbottleneck\u201d architecture to reduce model\ncomplexity. Try to implement it.\n4.In subsequent versions of ResNet, the authors changed the \u201cconvolution, batch normal-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3161, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4138e68d-08e3-44c0-8e85-14215ddb9385": {"__data__": {"id_": "4138e68d-08e3-44c0-8e85-14215ddb9385", "embedding": null, "metadata": {"page_label": "312", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79586157-f9ae-4ec8-8314-1e7bfe7d5c50", "node_type": "4", "metadata": {"page_label": "312", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "340093cb69a9da782a9479603abfdd2132d7d47045376fbbc69317ab9ebd7420", "class_name": "RelatedNodeInfo"}}, "text": "312 Modern Convolutional Neural Networks\n133ization, and activation\u201d structure to the \u201cbatch normalization, activation, and convolu-\ntion\u201d structure. Make this improvement yourself. See Figure 1 in He et al.(2016) for\ndetails.\n5.Why can\u2019t we just increase the complexity of functions without bound, even if the func-\ntion classes are nested?\nDiscussions133.\n8.7DenselyConnected Networks(DenseNet)\nResNet significantly changed the view of how to parametrize the functions in deep net-\nworks.DenseNet (dense convolutional network) is to some extent the logical extension of\nthis(Huangetal.,2017). DenseNetischaracterizedbyboththeconnectivitypatternwhere\neach layer connects to all the preceding layers and the concatenation operation (rather than\nthe addition operator in ResNet) to preserve and reuse features from earlier layers. To un-\nderstand how to arrive at it, let\u2019s take a small detour to mathematics.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n8.7.1FromResNetto DenseNet\nRecall the Taylor expansion for functions. At the point \ud835\udc65=0it can be written as\n\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc53\u00b90\u00ba\u00b8\ud835\udc65\u0001\u0014\n\ud835\udc530\u00b90\u00ba\u00b8\ud835\udc65\u0001\u0014\ud835\udc5300\u00b90\u00ba\n2!\u00b8\ud835\udc65\u0001\u0014\ud835\udc53000\u00b90\u00ba\n3!\u00b8\u0001\u0001\u0001\u0015\u0015\u0015\n. (8.7.1)\nThe key point is that it decomposes a function into terms of increasingly higher order. In a\nsimilar vein, ResNet decomposes functions into\n\ud835\udc53\u00b9x\u00ba=x\u00b8\ud835\udc54\u00b9x\u00ba. (8.7.2)\nThatis,ResNetdecomposes \ud835\udc53intoasimplelineartermandamorecomplexnonlinearone.\nWhat if we wanted to capture (not necessarily add) information beyond two terms? One\nsuch solution is DenseNet ( Huangetal., 2017).\ntFig. 8.7.1 The main difference between ResNet (left) and DenseNet (right) in cross-layer\nconnections: use of addition and use of concatenation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef14457b-712c-4319-a384-bca42703e430": {"__data__": {"id_": "ef14457b-712c-4319-a384-bca42703e430", "embedding": null, "metadata": {"page_label": "313", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ff34026-3b3b-4004-8dad-7c89304e7d83", "node_type": "4", "metadata": {"page_label": "313", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c044ac6bd664cd21428b5c463426d142cf19af1d57e7e90429630d5093917c6b", "class_name": "RelatedNodeInfo"}}, "text": "313 Densely Connected Networks (DenseNet)\nAs shown in Fig. 8.7.1 , the key difference between ResNet and DenseNet is that in the\nlatter case outputs are concatenated (denoted by\u00bb,\u00bc) rather than added. As a result, we\nperform a mapping from xto its values after applying an increasingly complex sequence\nof functions:\nx!\u00bbx, \ud835\udc531\u00b9x\u00ba, \ud835\udc532\u00b9\u00bbx, \ud835\udc531\u00b9x\u00ba\u00bc\u00ba, \ud835\udc533\u00b9\u00bbx, \ud835\udc531\u00b9x\u00ba, \ud835\udc532\u00b9\u00bbx, \ud835\udc531\u00b9x\u00ba\u00bc\u00ba\u00bc\u00ba,...\u00bc. (8.7.3)\nIntheend,allthesefunctionsarecombinedinMLPtoreducethenumberoffeaturesagain.\nIn terms of implementation this is quite simple: rather than adding terms, we concatenate\nthem. ThenameDenseNetarisesfromthefactthatthedependencygraphbetweenvariables\nbecomes quite dense. The final layer of such a chain is densely connected to all previous\nlayers. The dense connections are shown in Fig. 8.7.2 .\ntFig. 8.7.2 Dense connections in DenseNet. Note how the dimensionality increases with depth.\nThemaincomponentsthatcompriseaDenseNetare denseblocks andtransitionlayers . The\nformerdefinehowtheinputsandoutputsareconcatenated,whilethelattercontrolthenum-\nberofchannelssothatitisnottoolarge,sincetheexpansion x!\u00bbx, \ud835\udc531\u00b9x\u00ba, \ud835\udc532\u00b9\u00bbx, \ud835\udc531\u00b9x\u00ba\u00bc\u00ba,...\u00bc\ncan be quite high-dimensional.\n8.7.2DenseBlocks\nDenseNet uses the modified \u201cbatch normalization, activation, and convolution\u201d structure\nof ResNet (see the exercise in Section 8.6 ). First, we implement this convolution block\nstructure.\ndef conv_block (num_channels):\nreturn nn.Sequential(\nnn.LazyBatchNorm2d(), nn .ReLU(),\nnn.LazyConv2d(num_channels, kernel_size =3, padding =1))\nAdense block consists of multiple convolution blocks, each using the same number of\noutput channels. In the forward propagation, however, weconcatenate the input and output\nof each convolution block on the channel dimension. Lazy evaluation allows us to adjust\nthe dimensionality automatically.\nclass DenseBlock (nn.Module):\ndef __init__ (self , num_convs, num_channels):\nsuper (DenseBlock, self ).__init__ ()\nlayer =[]\nfor iinrange (num_convs):\nlayer .append(conv_block(num_channels))\nself .net =nn.Sequential( *layer)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2042, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc6f189e-2359-4374-a859-23f72580b43f": {"__data__": {"id_": "cc6f189e-2359-4374-a859-23f72580b43f", "embedding": null, "metadata": {"page_label": "314", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f55f959-7b4f-4e17-a63d-1675484569b3", "node_type": "4", "metadata": {"page_label": "314", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d5d0bf632b94de4b92c8004947497b1cbb4ec1f7ae54c2b0f994f6f33dde2e93", "class_name": "RelatedNodeInfo"}}, "text": "314 Modern Convolutional Neural Networks\n(continued from previous page)\ndef forward (self , X):\nfor blk inself .net:\nY=blk(X)\n# Concatenate input and output of each block along the channels\nX=torch .cat((X, Y), dim =1)\nreturn X\nInthefollowingexample,wedefinea DenseBlock instancewithtwoconvolutionblocksof\n10 output channels. When using an input with three channels, we will get an output with\n3\u00b810\u00b810=23channels. The number of convolution block channels controls the growth\nin the number of output channels relative to the number of input channels. This is also\nreferred to as the growthrate .\nblk =DenseBlock( 2,10)\nX=torch .randn( 4,3,8,8)\nY=blk(X)\nY.shape\ntorch .Size([ 4,23,8,8])\n8.7.3TransitionLayers\nSinceeachdenseblockwillincreasethenumberofchannels,addingtoomanyofthemwill\nlead to an excessively complex model. A transitionlayer is used to control the complexity\nof the model. It reduces the number of channels by using a 1\u00021convolution. Moreover, it\nhalves the height and width via average pooling with a stride of 2.\ndef transition_block (num_channels):\nreturn nn.Sequential(\nnn.LazyBatchNorm2d(), nn .ReLU(),\nnn.LazyConv2d(num_channels, kernel_size =1),\nnn.AvgPool2d(kernel_size =2, stride =2))\nApply a transition layer with 10 channels to the output of the dense block in the previous\nexample. This reduces the number of output channels to 10, and halves the height and\nwidth.\nblk =transition_block( 10)\nblk(Y) .shape\ntorch .Size([ 4,10,4,4])\n8.7.4DenseNetModel", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0789e879-56bd-4479-818e-5f5fb5da83e7": {"__data__": {"id_": "0789e879-56bd-4479-818e-5f5fb5da83e7", "embedding": null, "metadata": {"page_label": "315", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bcbc1947-aee9-4644-860f-2aa81a9f826d", "node_type": "4", "metadata": {"page_label": "315", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3050609df59d58b9a78336016aee7271c344db1e70f4305bb06686bbaf54910d", "class_name": "RelatedNodeInfo"}}, "text": "315 Densely Connected Networks (DenseNet)\nNext, we will construct a DenseNet model. DenseNet first uses the same single convolu-\ntional layer and max-pooling layer as in ResNet.\nclass DenseNet (d2l .Classifier):\ndef b1(self ):\nreturn nn.Sequential(\nnn.LazyConv2d( 64, kernel_size =7, stride =2, padding =3),\nnn.LazyBatchNorm2d(), nn .ReLU(),\nnn.MaxPool2d(kernel_size =3, stride =2, padding =1))\nThen, similar to the four modules made up of residual blocks that ResNet uses, DenseNet\nusesfourdenseblocks. AswithResNet,wecansetthenumberofconvolutionallayersused\nin each dense block. Here, we set it to 4, consistent with the ResNet-18 model in Section\n8.6. Furthermore, we set the number of channels (i.e., growth rate) for the convolutional\nlayers in the dense block to 32, so 128 channels will be added to each dense block.\nIn ResNet, the height and width are reduced between each module by a residual block with\na stride of 2. Here, we use the transition layer to halve the height and width and halve the\nnumber of channels. Similar to ResNet, a global pooling layer and a fully connected layer\nare connected at the end to produce the output.\n@d2l .add_to_class(DenseNet)\ndef __init__ (self , num_channels =64, growth_rate =32, arch =(4,4,4,4),\nlr=0.1, num_classes =10):\nsuper (DenseNet, self ).__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential( self .b1())\nfor i, num_convs inenumerate (arch):\nself .net.add_module( f'dense_blk {i+1}', DenseBlock(num_convs,\ngrowth_rate))\n# The number of output channels in the previous dense block\nnum_channels +=num_convs *growth_rate\n# A transition layer that halves the number of channels is added\n# between the dense blocks\nifi!=len(arch) -1:\nnum_channels //=2\nself .net.add_module( f'tran_blk {i+1}', transition_block(\nnum_channels))\nself .net.add_module( 'last ', nn .Sequential(\nnn.LazyBatchNorm2d(), nn .ReLU(),\nnn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten(),\nnn.LazyLinear(num_classes)))\nself .net.apply(d2l .init_cnn)\n8.7.5Training\nSince we are using a deeper network here, in this section, we will reduce the input height\nand width from 224 to 96 to simplify the computation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b940e7c-d5ad-44a3-b209-17d04005fd55": {"__data__": {"id_": "9b940e7c-d5ad-44a3-b209-17d04005fd55", "embedding": null, "metadata": {"page_label": "316", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9480d53f-c80b-42a3-8379-cc06459760cf", "node_type": "4", "metadata": {"page_label": "316", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4b395a72f9f6e92ec18ef165b696a9e6121a0c93b594d1374f8a9989de83c02a", "class_name": "RelatedNodeInfo"}}, "text": "316 Modern Convolutional Neural Networks\n134model =DenseNet(lr =0.01 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(96,96))\ntrainer .fit(model, data)\n8.7.6Summaryand Discussion\nThe main components that comprise DenseNet are dense blocks and transition layers. For\nthe latter, we need to keep the dimensionality under control when composing the net-\nwork by adding transition layers that shrink the number of channels again. In terms of\ncross-layer connections, in contrast to ResNet, where inputs and outputs are added to-\ngether, DenseNet concatenates inputs and outputs on the channel dimension. Although\nthese concatenation operations reuse features to achieve computational efficiency, unfortu-\nnately they lead to heavy GPU memory consumption. As a result, applying DenseNet may\nrequire more memory-efficient implementations that may increase training time ( Pleisset\nal., 2017).\n8.7.7Exercises\n1.Why do we use average pooling rather than max-pooling in the transition layer?\n2.OneoftheadvantagesmentionedintheDenseNetpaperisthatitsmodelparametersare\nsmaller than those of ResNet. Why is this the case?\n3.One problem for which DenseNet has been criticized is its high memory consumption.\n1.Is this really the case? Try to change the input shape to 224\u0002224to compare the\nactual GPU memory consumption empirically.\n2.Can you think of an alternative means of reducing the memory consumption? How\nwould you need to change the framework?\n4.Implement the various DenseNet versions presented in Table 1 of the DenseNet paper\n(Huangetal., 2017).\n5.Design an MLP-based model by applying the DenseNet idea. Apply it to the housing\nprice prediction task in Section 5.7 .\nDiscussions134.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d3567e0-f139-448d-844b-6d38f9dcecac": {"__data__": {"id_": "4d3567e0-f139-448d-844b-6d38f9dcecac", "embedding": null, "metadata": {"page_label": "317", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a6767625-e211-40d1-af1c-bcb465387b0f", "node_type": "4", "metadata": {"page_label": "317", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3a3f98c677fd2abab38e6f59c7f37e2511b8329ac00427c4685a53f1774bb421", "class_name": "RelatedNodeInfo"}}, "text": "317 Designing Convolution Network Architectures\n8.8DesigningConvolutionNetworkArchitectures\nThe previous sections have taken us on a tour of modern network design for computer\nvision. Common to all the work we covered was that it greatly relied on the intuition of\nscientists. Many of the architectures are heavily informed by human creativity and to a\nmuch lesser extent by systematic exploration of the design space that deep networks offer.\nNonetheless, this networkengineering approachhasbeentremendouslysuccessful.\nEver since AlexNet ( Section 8.1 ) beat conventional computer vision models on ImageNet,\nit has become popular to construct very deep networks by stacking blocks of convolutions,\nall designed according to the same pattern. In particular, 3\u00023convolutions were popular-\nized by VGG networks ( Section 8.2 ). NiN ( Section 8.3 ) showed that even 1\u00021convolu-\ntionscouldbebeneficialbyaddinglocalnonlinearities. Moreover,NiNsolvedtheproblem\nof aggregating information at the head of a network by aggregating across all locations.\nGoogLeNet ( Section 8.4 ) added multiple branches of different convolution width, combin-\ning the advantages of VGG and NiN in its Inception block. ResNets ( Section 8.6 ) changed\nthe inductive bias towards the identity mapping (from \ud835\udc53\u00b9\ud835\udc65\u00ba=0). This allowed for very\ndeep networks. Almost a decade later, the ResNet design is still popular, a testament to\nits design. Lastly, ResNeXt ( Section 8.6.5 ) added grouped convolutions, offering a better\ntrade-off between parameters and computation. A precursor to Transformers for vision,\nthe Squeeze-and-Excitation Networks (SENets) allow for efficient information transfer be-\ntween locations ( Huet al., 2018). This was accomplished by computing a per-channel\nglobal attention function.\nUp to now we have omitted networks obtained via neural architecture search (NAS) (Liu\net al., 2018,Zoph and Le, 2016 ). We chose to do so since their cost is usually enormous,\nrelying on brute-force search, genetic algorithms, reinforcement learning, or some other\nform of hyperparameter optimization. Given a fixed search space, NAS uses a search strat-\negy to automatically select an architecture based on the returned performance estimation.\nThe outcome of NAS is a single network instance. EfficientNets are a notable outcome of\nthis search ( Tan and Le, 2019 ).\nIn the following we discuss an idea that is quite different to the quest for the single best\nnetwork. It is computationally relatively inexpensive, it leads to scientific insights on the\nway, and it is quite effective in terms of the quality of outcomes. Let\u2019s review the strategy\nby Radosavovic et al.(2020) todesign network design spaces . The strategy combines the\nstrength of manual design and NAS. It accomplishes this by operating on distributions of\nnetworks and optimizing the distributions in a way to obtain good performance for entire\nfamilies of networks. The outcome of it are RegNets, specifically RegNetX and RegNetY,\nplus a range of guiding principles for the design of performant CNNs.\nimport torch\nfrom torch import nn\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3105, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0581543-cc73-4c8f-a235-7b185e47d800": {"__data__": {"id_": "c0581543-cc73-4c8f-a235-7b185e47d800", "embedding": null, "metadata": {"page_label": "318", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14ba5022-b35c-402e-b3d1-c3051c1c3597", "node_type": "4", "metadata": {"page_label": "318", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cc4dd73b2a73c188aacabd8779de5aba61a13020441485ff661f28a62cd974ae", "class_name": "RelatedNodeInfo"}}, "text": "318 Modern Convolutional Neural Networks\n(continued from previous page)\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n8.8.1TheAnyNetDesign Space\nThedescriptionbelowcloselyfollowsthereasoninginRadosavovic etal.(2020)withsome\nabbreviations to make it fit in the scope of the book. To begin, we need a template for the\nfamily of networks to explore. One of the commonalities of the designs in this chapter is\nthat the networks consist of a stem, abodyand ahead. The stem performs initial image\nprocessing, often through convolutions with a larger window size. The body consists of\nmultipleblocks,carryingoutthebulkofthetransformationsneededtogofromrawimages\nto object representations. Lastly, the head converts this into the desired outputs, such as\nviaasoftmaxregressorformulticlassclassification. Thebody, inturn, consistsofmultiple\nstages, operating on the image at decreasing resolutions. In fact, both the stem and each\nsubsequent stage quarter the spatial resolution. Lastly, each stage consists of one or more\nblocks. This pattern is common to all networks, from VGG to ResNeXt. Indeed, for the\ndesign of generic AnyNet networks, Radosavovic etal.(2020) used the ResNeXt block of\nFig. 8.6.5 .\ntFig. 8.8.1 The AnyNet design space. The numbers \u00b9c,r\u00baalong each arrow indicate the number of\nchannels c and the resolution r\u0002rof the images at that point. From left to right: generic\nnetwork structure composed of stem, body, and head; body composed of four stages;\ndetailed structure of a stage; two alternative structures for blocks, one without\ndownsampling and one that halves the resolution in each dimension. Design choices\ninclude depth di, the number of output channels ci, the number of groups gi, and\nbottleneck ratio kifor any stage i.\nLet\u2019sreviewthestructureoutlinedin Fig.8.8.1 indetail. Asmentioned,anAnyNetconsists\nof a stem, body, and head. The stem takes as its input RGB images (3 channels), using a\n3\u00023convolution with a stride of 2, followed by a batch norm, to halve the resolution from\n\ud835\udc5f\u0002\ud835\udc5fto\ud835\udc5f\u009d2\u0002\ud835\udc5f\u009d2. Moreover,itgenerates \ud835\udc500channelsthatserveasinputtothebody.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a336483-247f-4991-87ef-fb975d854adf": {"__data__": {"id_": "7a336483-247f-4991-87ef-fb975d854adf", "embedding": null, "metadata": {"page_label": "319", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0ab9f07-aa64-4025-b27f-055690ac78de", "node_type": "4", "metadata": {"page_label": "319", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5290b59890e3da04e42ebfd50fa7149d9b4efd5e8e851042739ebc77a2efc684", "class_name": "RelatedNodeInfo"}}, "text": "319 Designing Convolution Network Architectures\nSince the network is designed to work well with ImageNet images of shape 224\u0002224\u00023,\nthe body serves to reduce this to 7\u00027\u0002\ud835\udc504through 4 stages (recall that 224\u009d21\u00b84=7),\neach with an eventual stride of 2. Lastly, the head employs an entirely standard design via\nglobal average pooling, similar to NiN ( Section 8.3 ), followed by a fully connected layer to\nemit an\ud835\udc5b-dimensional vector for \ud835\udc5b-class classification.\nMostoftherelevantdesigndecisionsareinherenttothebodyofthenetwork. Itproceedsin\nstages, where each stage is composed of the same type of ResNeXt blocks as we discussed\ninSection 8.6.5 . The design there is again entirely generic: we begin with a block that\nhalves the resolution by using a stride of 2(the rightmost in Fig. 8.8.1 ). To match this, the\nresidualbranchoftheResNeXtblockneedstopassthrougha 1\u00021convolution. Thisblock\nis followed by a variable number of additional ResNeXt blocks that leave both resolution\nand the number of channels unchanged. Note that a common design practice is to add\na slight bottleneck in the design of convolutional blocks. As such, with bottleneck ratio\n\ud835\udc58\ud835\udc56\u00151we afford some number of channels, \ud835\udc50\ud835\udc56\u009d\ud835\udc58\ud835\udc56, within each block for stage \ud835\udc56(as the\nexperiments show, this is not really effective and should be skipped). Lastly, since we are\ndealing with ResNeXt blocks, we also need to pick the number of groups \ud835\udc54\ud835\udc56for grouped\nconvolutions at stage \ud835\udc56.\nThis seemingly generic design space provides us nonetheless with many parameters: we\ncan set the block width (number of channels) \ud835\udc500,...\ud835\udc50 4, the depth (number of blocks) per\nstage\ud835\udc511,...\ud835\udc51 4, the bottleneck ratios \ud835\udc581,...\ud835\udc58 4, and the group widths (numbers of groups)\n\ud835\udc541,...\ud835\udc54 4. Intotalthisaddsupto17parameters, resultinginanunreasonablylargenumber\nof configurations that would warrant exploring. We need some tools to reduce this huge\ndesign space effectively. This is where the conceptual beauty of design spaces comes in.\nBefore we do so, let\u2019s implement the generic design first.\nclass AnyNet (d2l .Classifier):\ndef stem (self , num_channels):\nreturn nn.Sequential(\nnn.LazyConv2d(num_channels, kernel_size =3, stride =2, padding =1),\nnn.LazyBatchNorm2d(), nn .ReLU())\nEach stage consists of depthResNeXt blocks, where num_channels specifies the block\nwidth. Note that the first block halves the height and width of input images.\n@d2l .add_to_class(AnyNet)\ndef stage (self , depth, num_channels, groups, bot_mul):\nblk =[]\nfor iinrange (depth):\nifi==0:\nblk.append(d2l .ResNeXtBlock(num_channels, groups, bot_mul,\nuse_1x1conv =True , strides =2))\nelse :\nblk.append(d2l .ResNeXtBlock(num_channels, groups, bot_mul))\nreturn nn.Sequential( *blk)\nPutting the network stem, body, and head together, we complete the implementation of\nAnyNet.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "085718de-cb0d-4e20-9c6b-3bdd4312a8ed": {"__data__": {"id_": "085718de-cb0d-4e20-9c6b-3bdd4312a8ed", "embedding": null, "metadata": {"page_label": "320", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9f6313e-d215-4074-b111-872baac079ca", "node_type": "4", "metadata": {"page_label": "320", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "eac4532ac9d4d9ea83b8fd1a7d6d520f0afe06f236dbe85223dc54abf49077b8", "class_name": "RelatedNodeInfo"}}, "text": "320 Modern Convolutional Neural Networks\n@d2l .add_to_class(AnyNet)\ndef __init__ (self , arch, stem_channels, lr =0.1, num_classes =10):\nsuper (AnyNet, self ).__init__ ()\nself .save_hyperparameters()\nself .net =nn.Sequential( self .stem(stem_channels))\nfor i, s inenumerate (arch):\nself .net.add_module( f'stage {i+1}',self .stage( *s))\nself .net.add_module( 'head ', nn .Sequential(\nnn.AdaptiveAvgPool2d(( 1,1)), nn .Flatten(),\nnn.LazyLinear(num_classes)))\nself .net.apply(d2l .init_cnn)\n8.8.2Distributions and Parametersof Design Spaces\nAs just discussed in Section 8.8.1 , parameters of a design space are hyperparameters of\nnetworks in that design space. Consider the problem of identifying good parameters in the\nAnyNet design space. We could try finding the single best parameter choice for a given\namount of computation (e.g., FLOPs and compute time). If we allowed for even only two\npossible choices for each parameter, we would have to explore 217=131072combinations\nto find the best solution. This is clearly infeasible because of its exorbitant cost. Even\nworse,wedonotreallylearnanythingfromthisexerciseintermsofhowoneshoulddesign\nanetwork. Nexttimeweadd,say,anX-stage,orashiftoperation,orsimilar,wewouldneed\nto start from scratch. Even worse, due to the stochasticity in training (rounding, shuffling,\nbit errors), no two runs are likely to produce exactly the same results. A better strategy\nwould be to try to determine general guidelines of how the choices of parameters should\nbe related. For instance, the bottleneck ratio, the number of channels, blocks, groups, or\ntheirchangebetweenlayersshouldideallybegovernedbyacollectionofsimplerules. The\napproach in Radosavovic etal.(2019) relies on the following four assumptions:\n1.We assume that general design principles actually exist, so that many networks satis-\nfying these requirements should offer good performance. Consequently, identifying a\ndistribution over networks can be a sensible strategy. In other words, we assume that\nthere are many good needles in the haystack.\n2.We need not train networks to convergence before we can assess whether a network is\ngood. Instead, it is sufficient to use the intermediate results as reliable guidance for\nfinal accuracy. Using (approximate) proxies to optimize an objective is referred to as\nmulti-fidelityoptimization( Forrester etal.,2007). Consequently,designoptimizationis\ncarried out, based on the accuracy achieved after only a few passes through the dataset,\nreducing the cost significantly.\n3.Resultsobtainedatasmallerscale(forsmallernetworks)generalizetolargerones. Con-\nsequently, optimization is carried out for networks that are structurally similar, but with\na smaller number of blocks, fewer channels, etc. Only in the end will we need to verify\nthat the so-found networks also offer good performance at scale.\n4.Aspects of the design can be approximately factorized so that it is possible to infer", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73dff742-5608-4ff9-a83f-d71571c24847": {"__data__": {"id_": "73dff742-5608-4ff9-a83f-d71571c24847", "embedding": null, "metadata": {"page_label": "321", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3468217-d6e9-4c03-b2b4-589b7ef1b3db", "node_type": "4", "metadata": {"page_label": "321", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "180378df904567ff6dd2e0540ead3c543b52521d91a95fdac9d5d944904e33cd", "class_name": "RelatedNodeInfo"}}, "text": "321 Designing Convolution Network Architectures\ntheir effect on the quality of the outcome somewhat independently. In other words, the\noptimization problem is moderately easy.\nThese assumptions allow us to test many networks cheaply. In particular, we can sample\nuniformly from the space of configurations and evaluate their performance. Subsequently,\nwe can evaluate the quality of the choice of parameters by reviewing the distribution of\nerror/accuracy that can be achieved with said networks. Denote by \ud835\udc39\u00b9\ud835\udc52\u00bathe cumulative\ndistribution function (CDF) for errors committed by networks of a given design space,\ndrawn using probability disribution \ud835\udc5d. That is,\n\ud835\udc39\u00b9\ud835\udc52,\ud835\udc5d\u00badef=\ud835\udc43net\u0018\ud835\udc5df\ud835\udc52\u00b9net\u00ba\u0014\ud835\udc52g. (8.8.1)\nOur goal is now to find a distribution \ud835\udc5dovernetworks such that most networks have a very\nlow error rate and where the support of \ud835\udc5dis concise. Of course, this is computationally\ninfeasible to perform accurately. We resort to a sample of networks Zdef=fnet1,...net\ud835\udc5bg\n(witherrors\ud835\udc521,...,\ud835\udc52\ud835\udc5b,respectively)from \ud835\udc5dandusetheempiricalCDF \u02c6\ud835\udc39\u00b9\ud835\udc52,Z\u00bainstead:\n\u02c6\ud835\udc39\u00b9\ud835\udc52,Z\u00ba=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=11\u00b9\ud835\udc52\ud835\udc56\u0014\ud835\udc52\u00ba. (8.8.2)\nWhenever the CDF for one set of choices majorizes (or matches) another CDF it follows\nthat its choice of parameters is superior (or indifferent). Accordingly Radosavovic et al.\n(2020) experimented with a shared network bottleneck ratio \ud835\udc58\ud835\udc56=\ud835\udc58for all stages \ud835\udc56of the\nnetwork. This gets rid of three of the four parameters governing the bottleneck ratio. To\nassess whether this (negatively) affects the performance one can draw networks from the\nconstrained and from the unconstrained distribution and compare the corresonding CDFs.\nIt turns out that this constraint does not affect the accuracy of the distribution of networks\nat all, as can be seen in the first panel of Fig. 8.8.2 . Likewise, we could choose to pick\nthe same group width \ud835\udc54\ud835\udc56=\ud835\udc54occurring at the various stages of the network. Again, this\ndoes not affect performance, as can be seen in the second panel of Fig. 8.8.2 . Both steps\ncombined reduce the number of free parameters by six.\ntFig. 8.8.2 Comparing error empirical distribution functions of design spaces. AnyNetAis the\noriginal design space; AnyNetBties the bottleneck ratios, AnyNetCalso ties group\nwidths,AnyNetDincreases the network depth across stages. From left to right: (i) tying\nbottleneck ratios has no effect on performance; (ii) tying group widths has no effect on\nperformance; (iii) increasing network widths (channels) across stages improves\nperformance; (iv) increasing network depths across stages improves performance. Figure\ncourtesy of Radosavovic et al. ( 2020 ).\nNextwelookforwaystoreducethemultitudeofpotentialchoicesforwidthanddepthofthe", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2651, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68cd523f-d086-4890-bbe7-d8928ef5f0af": {"__data__": {"id_": "68cd523f-d086-4890-bbe7-d8928ef5f0af", "embedding": null, "metadata": {"page_label": "322", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb3f896c-a985-4cea-9e1f-a097a70bf163", "node_type": "4", "metadata": {"page_label": "322", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "81712ef9705b7ef7d660f0678db6f807c8b642fb80be0d2974c2eab64ab2fea6", "class_name": "RelatedNodeInfo"}}, "text": "322 Modern Convolutional Neural Networks\nstages. It is a reasonable assumption that, as we go deeper, the number of channels should\nincrease, i.e., \ud835\udc50\ud835\udc56\u0015\ud835\udc50\ud835\udc56\u00001(\ud835\udc64\ud835\udc56\u00b81\u0015\ud835\udc64\ud835\udc56per their notation in Fig. 8.8.2 ), yielding AnyNetX\ud835\udc37.\nLikewise,itisequallyreasonabletoassumethatasthestagesprogress,theyshouldbecome\ndeeper,i.e.,\ud835\udc51\ud835\udc56\u0015\ud835\udc51\ud835\udc56\u00001,yieldingAnyNetX\ud835\udc38. Thiscanbeexperimentallyverifiedinthethird\nand fourth panel of Fig. 8.8.2 , respectively.\n8.8.3RegNet\nTheresultingAnyNetX\ud835\udc38designspaceconsistsofsimplenetworksfollowingeasy-to-interpret\ndesign principles:\n\u000fShare the bottleneck ratio \ud835\udc58\ud835\udc56=\ud835\udc58for all stages \ud835\udc56;\n\u000fShare the group width \ud835\udc54\ud835\udc56=\ud835\udc54for all stages \ud835\udc56;\n\u000fIncrease network width across stages: \ud835\udc50\ud835\udc56\u0014\ud835\udc50\ud835\udc56\u00b81;\n\u000fIncrease network depth across stages: \ud835\udc51\ud835\udc56\u0014\ud835\udc51\ud835\udc56\u00b81.\nThis leaves us with a final set of choices: how to pick the specific values for the above\nparameters of the eventual AnyNetX\ud835\udc38design space. By studying the best-performing\nnetworks from the distribution in AnyNetX\ud835\udc38one can observe the following: the width\nof the network ideally increases linearly with the block index across the network, i.e.,\n\ud835\udc50\ud835\udc57\u0019\ud835\udc500\u00b8\ud835\udc50\ud835\udc4e\ud835\udc57, where\ud835\udc57is the block index and slope \ud835\udc50\ud835\udc4e>0. Given that we get to choose a\ndifferent block width only per stage, wearrive at a piecewise constantfunction, engineered\nto match this dependence. Furthermore, experiments also show that a bottleneck ratio of\n\ud835\udc58=1performs best, i.e., we are advised not to use bottlenecks at all.\nWe recommend the interested reader reviews further details in the design of specific net-\nworks for different amounts of computation by perusing Radosavovic et al.(2020). For\ninstance, an effective 32-layer RegNetX variant is given by \ud835\udc58=1(no bottleneck), \ud835\udc54=16\n(group width is 16), \ud835\udc501=32and\ud835\udc502=80channels for the first and second stage, respec-\ntively, chosen to be \ud835\udc511=4and\ud835\udc512=6blocks deep. The astonishing insight from the\ndesign is that it still applies, even when investigating networks at a larger scale. Even bet-\nter, it even holds for Squeeze-and-Excitation (SE) network designs (RegNetY) that have a\nglobal channel activation ( Huetal., 2018).\nclass RegNetX32 (AnyNet):\ndef __init__ (self , lr =0.1, num_classes =10):\nstem_channels, groups, bot_mul =32,16,1\ndepths, channels =(4,6), ( 32,80)\nsuper ().__init__ (\n((depths[ 0], channels[ 0], groups, bot_mul),\n(depths[ 1], channels[ 1], groups, bot_mul)),\nstem_channels, lr, num_classes)\nWe can see that each RegNetX stage progressively reduces resolution and increases output\nchannels.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e60b7d92-49be-4e45-a9ba-677bd4426d5e": {"__data__": {"id_": "e60b7d92-49be-4e45-a9ba-677bd4426d5e", "embedding": null, "metadata": {"page_label": "323", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b4b7c84-2096-4124-8fea-b55407a030f7", "node_type": "4", "metadata": {"page_label": "323", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a6b6ab8c7ed37999622840cbf9b45bd336f6e561e9398821f71b9f9b29b1889d", "class_name": "RelatedNodeInfo"}}, "text": "323 Designing Convolution Network Architectures\nRegNetX32() .layer_summary(( 1,1,96,96))\nSequential output shape: torch .Size([ 1,32,48,48])\nSequential output shape: torch .Size([ 1,32,24,24])\nSequential output shape: torch .Size([ 1,80,12,12])\nSequential output shape: torch .Size([ 1,10])\n8.8.4Training\nTraining the 32-layer RegNetX on the Fashion-MNIST dataset is just like before.\nmodel =RegNetX32(lr =0.05 )\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(96,96))\ntrainer .fit(model, data)\n8.8.5Discussion\nWith desirable inductive biases (assumptions or preferences) like locality and translation\ninvariance( Section7.1 )forvision,CNNshavebeenthedominantarchitecturesinthisarea.\nThis remained the case from LeNet up until Transformers ( Section 11.7 ) (Dosovitskiy et\nal.,2021,Touvronetal.,2021)startedsurpassingCNNsintermsofaccuracy. Whilemuch\nof the recent progress in terms of vision Transformers canbe backported into CNNs ( Liu\net al., 2022), it is only possible at a higher computational cost. Just as importantly, recent\nhardwareoptimizations(NVIDIAAmpereandHopper)haveonlywidenedthegapinfavor\nof Transformers.\nIt is worth noting that Transformers have a significantly lower degree of inductive bias to-\nwards locality and translation invariance than CNNs. That learned structures prevailed is\ndue, not least, to the availability of large image collections, such as LAION-400m and\nLAION-5B ( Schuhmann et al., 2022) with up to 5 billion images. Quite surprisingly,\nsome of the more relevant work in this context even includes MLPs ( Tolstikhin et al.,\n2021).\nIn sum, vision Transformers ( Section 11.8 ) by now lead in terms of state-of-the-art perfor-\nmance in large-scale image classification, showing that scalabilitytrumpsinductivebiases", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ff05a485-e7a3-4baf-a65f-69e6adf0a2bc": {"__data__": {"id_": "ff05a485-e7a3-4baf-a65f-69e6adf0a2bc", "embedding": null, "metadata": {"page_label": "324", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2cf87c05-9062-4a49-8c5a-f3b4c730098a", "node_type": "4", "metadata": {"page_label": "324", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b2873e9432892721c4baa650d7f192795fcbfd7da1535df96720c4a1132b9a47", "class_name": "RelatedNodeInfo"}}, "text": "324 Modern Convolutional Neural Networks\n135(Dosovitskiy et al., 2021). This includes pretraining large-scale Transformers ( Section\n11.9) with multi-head self-attention ( Section 11.5 ). We invite the readers to dive into these\nchapters for a much more detailed discussion.\n8.8.6Exercises\n1.Increase the number of stages to four. Can you design a deeper RegNetX that performs\nbetter?\n2.De-ResNeXt-ify RegNets by replacing the ResNeXt block with the ResNet block. How\ndoes your new model perform?\n3.Implementmultipleinstancesofa\u201cVioNet\u201dfamilyby violating thedesignprinciplesof\nRegNetX. How do they perform? Which of ( \ud835\udc51\ud835\udc56,\ud835\udc50\ud835\udc56,\ud835\udc54\ud835\udc56,\ud835\udc4f\ud835\udc56) is the most important factor?\n4.Your goal is to design the \u201cperfect\u201d MLP. Can you use the design principles introduced\nabove to find good architectures? Is it possible to extrapolate from small to large net-\nworks?\nDiscussions135.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbdca2d6-7c6a-4c5a-b6e9-d3f931e05f19": {"__data__": {"id_": "bbdca2d6-7c6a-4c5a-b6e9-d3f931e05f19", "embedding": null, "metadata": {"page_label": "325", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bb34d36-b517-472c-9f1b-553bd41084ac", "node_type": "4", "metadata": {"page_label": "325", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a251e1526d6f7d9008682da6a8cf7fca7b9f726be15552aeb94e16398fb5d167", "class_name": "RelatedNodeInfo"}}, "text": "9 Recurrent Neural Networks\nUpuntilnow,wehavefocusedprimarilyonfixed-lengthdata. Whenintroducinglinearand\nlogistic regression in Chapter 3 andChapter 4 and multilayer perceptrons in Chapter 5 , we\nwerehappytoassumethateachfeaturevector x\ud835\udc56consistedofafixednumberofcomponents\n\ud835\udc651,...,\ud835\udc65\ud835\udc51, where each numerical feature \ud835\udc65\ud835\udc57corresponded to a particular attribute. These\ndatasets are sometimes called tabular, because they can be arranged in tables, where each\nexample\ud835\udc56gets its own row, and each attribute gets its own column. Crucially, with tabular\ndata, we seldom assume any particular structure over the columns.\nSubsequently, in Chapter 7 , we moved on to image data, where inputs consist of the raw\npixelvaluesateachcoordinateinanimage. Imagedatahardlyfittedthebillofaprotypical\ntabular dataset. There, we needed to call upon convolutional neural networks (CNNs) to\nhandle the hierarchical structure and invariances. However, our data were still of fixed\nlength. Every Fashion-MNIST image is represented as a 28\u000228grid of pixel values.\nMoreover,ourgoalwastodevelopamodelthatlookedatjustoneimageandthenoutputted\na single prediction. But what should we do when faced with a sequence of images, as in a\nvideo, or when tasked with producing a sequentiallystructured prediction, as in the case of\nimage captioning?\nAgreatmanylearningtasksrequiredealingwithsequentialdata. Imagecaptioning,speech\nsynthesis, and music generation all require that models produce outputs consisting of se-\nquences. In other domains, such as time series prediction, video analysis, and musical\ninformation retrieval, a model must learn from inputs that are sequences. These demands\noften arise simultaneously: tasks such as translating passages of text from one natural lan-\nguage to another, engaging in dialogue, or controlling a robot, demand that models both\ningest and output sequentially structured data.\nRecurrent neural networks (RNNs) are deep learning models that capture the dynamics of\nsequences via recurrent connections, which can be thought of as cycles in the network of\nnodes. This might seem counterintuitive at first. After all, it is the feedforward nature of\nneural networks that makes the order of computation unambiguous. However, recurrent\nedges are defined in a precise way that ensures that no such ambiguity can arise. Recurrent\nneural networks are unrolled across time steps (or sequence steps), with the sameunder-\nlying parameters applied at each step. While the standard connections are applied syn-\nchronously to propagate each layer\u2019s activations to the subsequent layer at the same time\nstep,therecurrentconnectionsare dynamic ,passinginformationacrossadjacenttimesteps.\nAs the unfolded view in Fig. 9.1 reveals, RNNs can be thought of as feedforward neural\n325", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1aede15c-2422-4080-88b9-c8959e8f33e1": {"__data__": {"id_": "1aede15c-2422-4080-88b9-c8959e8f33e1", "embedding": null, "metadata": {"page_label": "326", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1df2bddc-fcf9-47a8-95a1-abc273c6aaa7", "node_type": "4", "metadata": {"page_label": "326", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5eb01b4592a4753c33462265e508c968d3e70b27e8e53e2ac88cf9340eff1884", "class_name": "RelatedNodeInfo"}}, "text": "326 Recurrent Neural Networks\nnetworkswhereeachlayer\u2019sparameters(bothconventionalandrecurrent)aresharedacross\ntime steps.\ntFig. 9.1 On the left recurrent connections are depicted via cyclic edges. On the right, we unfold\nthe RNN over time steps. Here, recurrent edges span adjacent time steps, while\nconventional connections are computed synchronously.\nLike neural networks more broadly, RNNs have a long discipline-spanning history, origi-\nnating as models of the brain popularized by cognitive scientists and subsequently adopted\nas practical modeling tools employed by the machine learning community. As we do for\ndeeplearningmorebroadly,inthisbookweadoptthemachinelearningperspective,focus-\ning on RNNs as practical tools that rose to popularity in the 2010s owing to breakthrough\nresults on such diverse tasks as handwriting recognition ( Graveset al., 2008), machine\ntranslation( Sutskever etal.,2014),andrecognizingmedicaldiagnoses( Liptonetal.,2016).\nWepointthereaderinterestedinmorebackgroundmaterialtoapubliclyavailablecompre-\nhensivereview( Liptonetal.,2015). WealsonotethatsequentialityisnotuniquetoRNNs.\nForexample,theCNNsthatwealreadyintroducedcanbeadaptedtohandledataofvarying\nlength, e.g., images of varying resolution. Moreover, RNNs have recently ceded consider-\nable market share to Transformer models, which will be covered in Chapter 11 . However,\nRNNs rose to prominence as the default models for handling complex sequential structure\nin deep learning, and remain staple models for sequential modeling to this day. The stories\nof RNNs and of sequence modeling are inextricably linked, and this is as much a chapter\nabout the ABCs of sequence modeling problems as it is a chapter about RNNs.\nOne key insight paved the way for a revolution in sequence modeling. While the inputs\nand targets for many fundamental tasks in machine learning cannot easily be represented\nas fixed-length vectors, they can often nevertheless be represented as varying-length se-\nquences of fixed-length vectors. For example, documents can be represented as sequences\nof words; medical records can often be represented as sequences of events (encounters,\nmedications,procedures,labtests,diagnoses); videoscanberepresentedasvarying-length\nsequences of still images.\nWhilesequencemodelshavepoppedupinnumerousapplicationareas,basicresearchinthe\nareahasbeendrivenpredominantlybyadvancesoncoretasksinnaturallanguageprocess-\ning. Thus, throughout this chapter, we will focus our exposition and examples on text data.\nIf you get the hang of these examples, then applying the models to other data modalities\nshould be relatively straightforward. In the next few sections, we introduce basic notation\nforsequencesandsomeevaluationmeasuresforassessingthequalityofsequentiallystruc-\nturedmodeloutputs. Afterthat,wediscussbasicconceptsofalanguagemodelandusethis", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2846, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b554a9b8-59fb-43f8-bf37-655f523e8e09": {"__data__": {"id_": "b554a9b8-59fb-43f8-bf37-655f523e8e09", "embedding": null, "metadata": {"page_label": "327", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42a3a1f4-72f2-4ba0-a117-29b27c3bf0f6", "node_type": "4", "metadata": {"page_label": "327", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "02721c783b78c3ce7ce6177ee32c8c9c3c46e3faf39de6a682979a22f36750d7", "class_name": "RelatedNodeInfo"}}, "text": "327 Working with Sequences\ndiscussion to motivate our first RNN models. Finally, we describe the method for calculat-\ning gradients when backpropagating through RNNs and explore some challenges that are\noftenencounteredwhentrainingsuchnetworks, motivatingthemodernRNNarchitectures\nthat will follow in Chapter 10 .\n9.1Workingwith Sequences\nUp until now, we have focused on models whose inputs consisted of a single feature vector\nx2R\ud835\udc51. The main change of perspective when developing models capable of processing\nsequences is that we now focus on inputs that consist of an ordered list of feature vec-\ntorsx1,...,x\ud835\udc47, where each feature vector x\ud835\udc61is indexed by a time step \ud835\udc612Z\u00b8lying in\nR\ud835\udc51.\nSome datasets consist of a single massive sequence. Consider, for example, the extremely\nlong streams of sensor readings that might be available to climate scientists. In such cases,\nwe might create training datasets by randomly sampling subsequences of some predeter-\nmined length. More often, our data arrives as a collection of sequences. Consider the\nfollowingexamples: (i)acollectionofdocuments, eachrepresentedasitsownsequenceof\nwords,andeachhavingitsownlength \ud835\udc47\ud835\udc56;(ii)sequencerepresentationofpatientstaysinthe\nhospital, where each stay consists of a number of events and the sequence length depends\nroughly on the length of the stay.\nPreviously, when dealing with individual inputs, we assumed that they were sampled inde-\npendently from the same underlying distribution \ud835\udc43\u00b9\ud835\udc4b\u00ba. While we still assume that entire\nsequences (e.g., entire documents or patient trajectories) are sampled independently, we\ncannot assume that the data arriving at each time step are independent of each other. For\nexample, the words that likely to appear later in a document depend heavily on words oc-\ncurring earlier in the document. The medicine a patient is likely to receive on the 10th day\nof a hospital visit depends heavily on what transpired in the previous nine days.\nThis should come as no surprise. If we did not believe that the elements in a sequence\nwere related, we would not have bothered to model them as a sequence in the first place.\nConsidertheusefulnessoftheauto-fillfeaturesthatarepopularonsearchtoolsandmodern\nemail clients. They are useful precisely because it is often possible to predict (imperfectly,\nbut better than random guessing) what the likely continuations of a sequence might be,\ngiven some initial prefix. For most sequence models, we do not require independence, or\neven stationarity, of our sequences. Instead, we require only that the sequences themselves\nare sampled from some fixed underlying distribution over entire sequences.\nThis flexible approach allows for such phenomena as (i) documents looking significantly\ndifferentatthebeginningthanattheend;or(ii)patientstatusevolvingeithertowardsrecov-\neryortowardsdeathoverthecourseofahospitalstay;or(iii)customertasteevolvinginpre-\ndictable ways over the course of continued interaction with a recommender system.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2972, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ae0cc01-22dc-4ccd-953e-da54bec48fa4": {"__data__": {"id_": "9ae0cc01-22dc-4ccd-953e-da54bec48fa4", "embedding": null, "metadata": {"page_label": "328", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "476a6102-fafe-4a30-a441-61d1d688233a", "node_type": "4", "metadata": {"page_label": "328", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "da4de8177306533092bc114f41b049fafae5b0b2c372bee5fbc40baa135a672d", "class_name": "RelatedNodeInfo"}}, "text": "328 Recurrent Neural Networks\nWesometimeswishtopredictafixedtarget \ud835\udc66givensequentiallystructuredinput(e.g.,sen-\ntimentclassificationbasedonamoviereview). Atothertimes,wewishtopredictasequen-\ntially structured target ( \ud835\udc661,...,\ud835\udc66\ud835\udc47) given a fixed input (e.g., image captioning). Still other\ntimes, our goal is to predict sequentially structured targets based on sequentially structured\ninputs (e.g., machine translation or video captioning). Such sequence-to-sequence tasks\ntake two forms: (i) aligned: where the input at each time step aligns with a correspond-\ning target (e.g., part of speech tagging); (ii) unaligned : where the input and target do not\nnecessarily exhibit a step-for-step correspondence (e.g., machine translation).\nBeforeweworryabouthandlingtargetsofanykind,wecantacklethemoststraightforward\nproblem: unsupervised density modeling (also called sequence modeling ). Here, given a\ncollection of sequences, our goal is to estimate the probability mass function that tells us\nhow likely we are to see any given sequence, i.e., \ud835\udc5d\u00b9x1,...,x\ud835\udc47\u00ba.\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n9.1.1AutoregressiveModels\nBefore introducing specialized neural networks designed to handle sequentially structured\ndata, let\u2019s take a look at some actual sequence data and build up some basic intuitions\nand statistical tools. In particular, we will focus on stock price data from the FTSE 100\nindex (Fig. 9.1.1 ). At each time step\ud835\udc612Z\u00b8, we observe the price, \ud835\udc65\ud835\udc61, of the index at that\ntime.\ntFig. 9.1.1 FTSE 100 index over about 30 years.\nNow suppose that a trader would like to make short-term trades, strategically getting into\nor out of the index, depending on whether they believe that it will rise or decline in the\nsubsequent time step. Absent any other features (news, financial reporting data, etc.), the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1847, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a6af8f8-d1ae-4e70-a39a-ecc5cfe3a6c9": {"__data__": {"id_": "2a6af8f8-d1ae-4e70-a39a-ecc5cfe3a6c9", "embedding": null, "metadata": {"page_label": "329", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46cbdadf-bf54-4e41-a3d5-9b23f1ae87ce", "node_type": "4", "metadata": {"page_label": "329", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c982d719a649378f1fe92db6399ae56d7f082929758f8b38fb140310a96d57c7", "class_name": "RelatedNodeInfo"}}, "text": "329 Working with Sequences\nonly available signal for predicting the subsequent value is the history of prices to date.\nThe trader is thus interested in knowing the probability distribution\n\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1\u00ba (9.1.1)\noverpricesthattheindexmighttakeinthesubsequenttimestep. Whileestimatingtheentire\ndistribution over a continuously valued random variable can be difficult, the trader would\nbe happy to focus on a few key statistics of the distribution, particularly the expected value\nand the variance. One simple strategy for estimating the conditional expectation\nE\u00bb\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1\u00ba\u00bc, (9.1.2)\nwould be to apply a linear regression model (recall Section 3.1 ). Such models that regress\nthe value of a signal on the previous values of that same signal are naturally called au-\ntoregressive models . There is just one major problem: the number of inputs, \ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1\nvaries, depending on \ud835\udc61. In other words, the number of inputs increases with the amount of\ndata that we encounter. Thus if we want to treat our historical data as a training set, we\nare left with the problem that each example has a different number of features. Much of\nwhatfollowsinthischapterwillrevolvearoundtechniquesforovercomingthesechallenges\nwhen engaging in such autoregressive modeling problems where the object of interest is\n\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1\u00baor some statistic(s) of this distribution.\nAfewstrategiesrecurfrequently. Firstofall,wemightbelievethatalthoughlongsequences\n\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1are available, it may not be necessary to look back so far in the history when\npredicting the near future. In this case we might content ourselves to condition on some\nwindow of length \ud835\udf0fand only use \ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65\ud835\udc61\u0000\ud835\udf0fobservations. The immediate benefit is\nthat now the number of arguments is always the same, at least for \ud835\udc61 > \ud835\udf0f. This allows us to\ntrainanylinearmodelordeepnetworkthatrequiresfixed-lengthvectorsasinputs. Second,\nwe might develop models that maintain some summary \u210e\ud835\udc61of the past observations (see\nFig. 9.1.2 ) and at the same time update \u210e\ud835\udc61in addition to the prediction \u02c6\ud835\udc65\ud835\udc61. This leads to\nmodels that estimate not only \ud835\udc65\ud835\udc61with \u02c6\ud835\udc65\ud835\udc61=\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\u210e\ud835\udc61\u00babut also updates of the form \u210e\ud835\udc61=\n\ud835\udc54\u00b9\u210e\ud835\udc61\u00001,\ud835\udc65\ud835\udc61\u00001\u00ba. Since\u210e\ud835\udc61isneverobserved,thesemodelsarealsocalled latentautoregressive\nmodels.\ntFig. 9.1.2 A latent autoregressive model.\nTo construct training data from historical data, one typically creates examples by sampling\nwindows randomly. In general, we do not expect time to stand still. However, we often\nassume that while the specific values of \ud835\udc65\ud835\udc61might change, the dynamics according to which\neach subsequent observation is generated given the previous observations do not. Statisti-\ncians call dynamics that do not change stationary .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "016891c5-3670-4b8d-88f1-584a87e69449": {"__data__": {"id_": "016891c5-3670-4b8d-88f1-584a87e69449", "embedding": null, "metadata": {"page_label": "330", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0737b0b9-8d92-4191-ba95-1486ef0ba9d2", "node_type": "4", "metadata": {"page_label": "330", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7a4fdf00b7e4b08b150a2088265c1553577b6582dc3347f245fb6afa7d35985e", "class_name": "RelatedNodeInfo"}}, "text": "330 Recurrent Neural Networks\n9.1.2SequenceModels\nSometimes,especiallywhenworkingwithlanguage,wewishtoestimatethejointprobabil-\nity of an entire sequence. This is a common task when working with sequences composed\nof discrete tokens, such as words. Generally, these estimated functions are called sequence\nmodelsand for natural language data, they are called language models . The field of se-\nquence modeling has been driven so much by natural language processing, that we often\ndescribe sequence models as \u201clanguage models\u201d, even when dealing with non-language\ndata. Language models prove useful for all sorts of reasons. Sometimes we want to evalu-\nate the likelihood of sentences. For example, we might wish to compare the naturalness of\ntwocandidateoutputsgeneratedbyamachinetranslationsystemorbyaspeechrecognition\nsystem. But language modeling gives us not only the capacity to evaluate likelihood, but\ntheabilityto samplesequences, andeventooptimizeforthemostlikelysequences.\nWhile language modeling might not, at first glance, look like an autoregressive problem,\nwe can reduce language modeling to autoregressive prediction by decomposing the joint\ndensity of a sequence \ud835\udc5d\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc47\u00bainto the product of conditional densities in a left-to-\nright fashion by applying the chain rule of probability:\n\ud835\udc43\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc47\u00ba=\ud835\udc43\u00b9\ud835\udc651\u00ba\ud835\udc47\u00d6\n\ud835\udc61=2\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1\u00ba. (9.1.3)\nNote that if we are working with discrete signals such as words, then the autoregressive\nmodel must be a probabilistic classifier, outputting a full probability distribution over the\nvocabulary for whatever word will come next, given the leftwards context.\nMarkovModels\nNow suppose that we wish to employ the strategy mentioned above, where we condition\nonlyonthe\ud835\udf0fprevioustimesteps,i.e., \ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65\ud835\udc61\u0000\ud835\udf0f,ratherthantheentiresequencehistory\n\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1. Wheneverwecanthrowawaythehistorybeyondtheprevious \ud835\udf0fstepswithout\nanylossinpredictivepower,wesaythatthesequencesatisfiesa Markovcondition ,i.e.,that\nthe future is conditionally independent of the past, given the recent history . When\ud835\udf0f=1,\nwe say that the data is characterized by a first-order Markov model , and when\ud835\udf0f=\ud835\udc58, we\nsay that the data is characterized by a \ud835\udc58th-order Markov model. For when the first-order\nMarkovconditionholds( \ud835\udf0f=1)thefactorizationofourjointprobabilitybecomesaproduct\nof probabilities of each word given the previous word:\n\ud835\udc43\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc47\u00ba=\ud835\udc43\u00b9\ud835\udc651\u00ba\ud835\udc47\u00d6\n\ud835\udc61=2\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001\u00ba. (9.1.4)\nWeoftenfinditusefultoworkwithmodelsthatproceedasthoughaMarkovconditionwere\nsatisfied,evenwhenweknowthatthisisonly approximately true. Withrealtextdocuments\nwe continue to gain information as we include more and more leftwards context. But these\ngains diminish rapidly. Thus, sometimes we compromise, obviating computational and\nstatistical difficulties by training models whose validity depends on a \ud835\udc58th-order Markov\ncondition. Even today\u2019s massive RNN- and Transformer-based language models seldom\nincorporate more than thousands of words of context.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e04b4435-9a48-4a39-8e94-3ed0dc97449c": {"__data__": {"id_": "e04b4435-9a48-4a39-8e94-3ed0dc97449c", "embedding": null, "metadata": {"page_label": "331", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c24ad952-0f84-445b-aac9-5d1a63fd9a4d", "node_type": "4", "metadata": {"page_label": "331", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "392f83f7cc5c8b22942bc1f2ce1d4f8041fde3a0a7f97b34363cd55abfa765bb", "class_name": "RelatedNodeInfo"}}, "text": "331 Working with Sequences\nWith discrete data, a true Markov model simplycounts the number of times that each word\nhas occurred in each context, producing the relative frequency estimate of \ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001\u00ba.\nWhenever the data assumes only discrete values (as in language), the most likely sequence\nof words can be computed efficiently using dynamic programming.\nThe Orderof Decoding\nYoumaybewonderingwhywerepresentedthefactorizationofatextsequence \ud835\udc43\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc47\u00ba\nas a left-to-right chain of conditional probabilities. Why not right-to-left or some other,\nseeminglyrandomorder? Inprinciple,thereisnothingwrongwithunfolding \ud835\udc43\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc47\u00ba\nin reverse order. The result is a valid factorization:\n\ud835\udc43\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc47\u00ba=\ud835\udc43\u00b9\ud835\udc65\ud835\udc47\u00ba1\u00d6\n\ud835\udc61=\ud835\udc47\u00001\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00b81,...,\ud835\udc65\ud835\udc47\u00ba. (9.1.5)\nHowever, there are many reasons why factorizing text in the same direction in which we\nreadit(left-to-rightformostlanguages,butright-to-leftforArabicandHebrew)ispreferred\nfor the task of language modeling. First, this is just a more natural direction for us to think\nabout. After all we all read text every day, and this process is guided by our ability to\nanticipate which words and phrases are likely to come next. Just think of how many times\nyouhavecompletedsomeoneelse\u2019ssentence. Thus,evenifwehadnootherreasontoprefer\nsuch in-order decodings, they would be useful if only because we have better intuitions for\nwhat should be likely when predicting in this order.\nSecond, by factorizing in order, we can assign probabilities to arbitrarily long sequences\nusingthesamelanguagemodel. Toconvertaprobabilityoversteps 1through\ud835\udc61intoonethat\nextendstoword \ud835\udc61\u00b81wesimplymultiplybytheconditionalprobabilityoftheadditionalto-\nkengiventhepreviousones: \ud835\udc43\u00b9\ud835\udc65\ud835\udc61\u00b81,...,\ud835\udc65 1\u00ba=\ud835\udc43\u00b9\ud835\udc65\ud835\udc61,...,\ud835\udc65 1\u00ba\u0001\ud835\udc43\u00b9\ud835\udc65\ud835\udc61\u00b81j\ud835\udc65\ud835\udc61,...,\ud835\udc65 1\u00ba.\nThird, we have stronger predictive models for predicting adjacent words than words at ar-\nbitrary other locations. While all orders of factorization are valid, they do not necessarily\nallrepresentequallyeasypredictivemodelingproblems. Thisistruenotonlyforlanguage,\nbut for other kinds of data as well, e.g., when the data is causally structured. For example,\nwe believe that future events cannot influence the past. Hence, if we change \ud835\udc65\ud835\udc61, we may be\nable to influence what happens for \ud835\udc65\ud835\udc61\u00b81going forward but not the converse. That is, if we\nchange\ud835\udc65\ud835\udc61, the distribution over past events will not change. In some contexts, this makes\nit easier to predict \ud835\udc43\u00b9\ud835\udc65\ud835\udc61\u00b81j\ud835\udc65\ud835\udc61\u00bathan to predict \ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00b81\u00ba. For instance, in some cases,\nwe can find\ud835\udc65\ud835\udc61\u00b81=\ud835\udc53\u00b9\ud835\udc65\ud835\udc61\u00ba\u00b8\ud835\udf16for some additive noise \ud835\udf16, whereas the converse is not true\n(Hoyeret al., 2009). This is great news, since it is typically the forward direction that we\nare interested in estimating. The book by Peters et al.(2017) contains more on this topic.\nWe barely scratch the surface of it.\n9.1.3Training\nBefore we focus our attention on text data, let\u2019s first try this out with some continuous-\nvalued synthetic data.\nHere, our 1000 synthetic data will follow the trigonometric sinfunction, applied to 0.01", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "009ed324-e378-4668-afc1-b54c4af139df": {"__data__": {"id_": "009ed324-e378-4668-afc1-b54c4af139df", "embedding": null, "metadata": {"page_label": "332", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2236f0eb-2457-449a-a621-f767bac4099b", "node_type": "4", "metadata": {"page_label": "332", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ec1c00a44089a1fb90b0b2680225d8301b5162e608ae4a3a2010ff512a58e601", "class_name": "RelatedNodeInfo"}}, "text": "332 Recurrent Neural Networks\ntimes the time step. To make the problem a little more interesting, we corrupt each sample\nwith additive noise. From this sequence we extract training examples, each consisting of\nfeatures and a label.\nclass Data (d2l .DataModule):\ndef __init__ (self , batch_size =16, T=1000 , num_train =600, tau =4):\nself .save_hyperparameters()\nself .time =torch .arange( 1, T +1, dtype =torch .float32)\nself .x=torch .sin( 0.01 *self .time) +torch .randn(T) *0.2\ndata =Data()\nd2l.plot(data .time, data .x,'time ','x', xlim =[1,1000 ], figsize =(6,3))\nTo begin, we try a model that acts as if the data satisfied a \ud835\udf0fth-order Markov condition,\nand thus predicts \ud835\udc65\ud835\udc61using only the past \ud835\udf0fobservations. Thus for each time step we have an\nexamplewithlabel \ud835\udc66=\ud835\udc65\ud835\udc61andfeatures x\ud835\udc61=\u00bb\ud835\udc65\ud835\udc61\u0000\ud835\udf0f,...,\ud835\udc65\ud835\udc61\u00001\u00bc. Theastutereadermighthave\nnoticedthatthisresultsin 1000\u0000\ud835\udf0fexamples,sincewelacksufficienthistoryfor \ud835\udc661,...,\ud835\udc66\ud835\udf0f.\nWhile we could pad the first \ud835\udf0fsequences with zeros, to keep things simple, we drop them\nfor now. The resulting dataset contains \ud835\udc47\u0000\ud835\udf0fexamples, where each input to the model has\nsequence length \ud835\udf0f. We create a data iterator on the first 600 examples, covering a period of\nthe sin function.\n@d2l .add_to_class(Data)\ndef get_dataloader (self , train):\nfeatures =[self .x[i : self .T-self .tau+i]for iinrange (self .tau)]\nself .features =torch .stack(features, 1)\nself .labels =self .x[self .tau:] .reshape(( -1,1))\ni=slice (0,self .num_train) iftrain else slice (self .num_train, None )\nreturn self .get_tensorloader([ self .features, self .labels], train, i)\nIn this example our model will be a standard linear regression.\nmodel =d2l.LinearRegression(lr =0.01 )\ntrainer =d2l.Trainer(max_epochs =5)\ntrainer .fit(model, data)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fcf9d47a-3372-41c9-9387-96206f510658": {"__data__": {"id_": "fcf9d47a-3372-41c9-9387-96206f510658", "embedding": null, "metadata": {"page_label": "333", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f8fee51-edb8-4586-abf9-6df2181f773b", "node_type": "4", "metadata": {"page_label": "333", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "394f6294d42bcc84e3cbf4296b27403c6bbfc072942b12ea700858d9195cbfd9", "class_name": "RelatedNodeInfo"}}, "text": "333 Working with Sequences\n9.1.4Prediction\nToevaluateourmodel,wefirstcheckhowwellitperformsatone-step-aheadprediction.\nonestep_preds =model(data .features) .detach() .numpy()\nd2l.plot(data .time[data .tau:], [data .labels, onestep_preds], 'time ','x',\nlegend =['labels ','1-step preds '], figsize =(6,3))\nThese predictions look good, even near the end at \ud835\udc61=1000.\nBut what if we only observed sequence data up until time step 604 ( n_train + tau ) and\nwished to make predictions several stepsinto the future? Unfortunately, we cannot directly\ncompute the one-step-ahead prediction for time step 609, because we do not know the cor-\nrespondinginputs,havingseenonlyupto \ud835\udc65604. Wecanaddressthisproblembypluggingin\nourearlierpredictionsasinputstoourmodelformakingsubsequentpredictions,projecting\nforward, one step at a time, until reaching the desired time step:\n\u02c6\ud835\udc65605=\ud835\udc53\u00b9\ud835\udc65601,\ud835\udc65602,\ud835\udc65603,\ud835\udc65604\u00ba,\n\u02c6\ud835\udc65606=\ud835\udc53\u00b9\ud835\udc65602,\ud835\udc65603,\ud835\udc65604,\u02c6\ud835\udc65605\u00ba,\n\u02c6\ud835\udc65607=\ud835\udc53\u00b9\ud835\udc65603,\ud835\udc65604,\u02c6\ud835\udc65605,\u02c6\ud835\udc65606\u00ba,\n\u02c6\ud835\udc65608=\ud835\udc53\u00b9\ud835\udc65604,\u02c6\ud835\udc65605,\u02c6\ud835\udc65606,\u02c6\ud835\udc65607\u00ba,\n\u02c6\ud835\udc65609=\ud835\udc53\u00b9\u02c6\ud835\udc65605,\u02c6\ud835\udc65606,\u02c6\ud835\udc65607,\u02c6\ud835\udc65608\u00ba,\n...(9.1.6)\nGenerally, for an observed sequence \ud835\udc651,...,\ud835\udc65\ud835\udc61, its predicted output \u02c6\ud835\udc65\ud835\udc61\u00b8\ud835\udc58at time step\ud835\udc61\u00b8\ud835\udc58", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b97f0227-b1f4-4b1b-b642-119ed2af03cc": {"__data__": {"id_": "b97f0227-b1f4-4b1b-b642-119ed2af03cc", "embedding": null, "metadata": {"page_label": "334", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd63404f-c603-47d4-9230-c8b6f2009b14", "node_type": "4", "metadata": {"page_label": "334", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4d5bc4b96eab9112ed0b86fede4cf8c06b1bd612e8655fe88b67ac5524241b11", "class_name": "RelatedNodeInfo"}}, "text": "334 Recurrent Neural Networks\nis called the \ud835\udc58-step-ahead prediction . Since we have observed up to \ud835\udc65604, its\ud835\udc58-step-ahead\nprediction is \u02c6\ud835\udc65604\u00b8\ud835\udc58. In other words, we will have to keep on using our own predictions to\nmake multistep-ahead predictions. Let\u2019s see how well this goes.\nmultistep_preds =torch .zeros(data .T)\nmultistep_preds[:] =data .x\nfor iinrange (data .num_train +data .tau, data .T):\nmultistep_preds[i] =model(\nmultistep_preds[i -data .tau:i] .reshape(( 1,-1)))\nmultistep_preds =multistep_preds .detach() .numpy()\nd2l.plot([data .time[data .tau:], data .time[data .num_train +data .tau:]],\n[onestep_preds, multistep_preds[data .num_train +data .tau:]], 'time ',\n'x', legend =['1-step preds ','multistep preds '], figsize =(6,3))\nUnfortunately, in this case we fail spectacularly. The predictions decay to a constant pretty\nquickly after a few steps. Why did the algorithm perform so much worse when predicting\nfurther into the future? Ultimately, this is down to the fact that errors build up. Let\u2019s say\nthat after step 1 we have some error \ud835\udf161=\u00af\ud835\udf16. Now the inputfor step 2 is perturbed by \ud835\udf161,\nhence we suffer some error in the order of \ud835\udf162=\u00af\ud835\udf16\u00b8\ud835\udc50\ud835\udf161for some constant \ud835\udc50, and so on. The\npredictions can diverge rapidly from the true observations. You may already be familiar\nwith this common phenomenon. For instance, weather forecasts for the next 24 hours tend\nto be pretty accurate but beyond that, accuracy declines rapidly. We will discuss methods\nfor improving this throughout this chapter and beyond.\nLet\u2019s take a closer look at the difficulties in \ud835\udc58-step-ahead predictions by computing predic-\ntions on the entire sequence for \ud835\udc58=1,4,16,64.\ndef k_step_pred (k):\nfeatures =[]\nfor iinrange (data .tau):\nfeatures .append(data .x[i : i +data .T-data .tau-k+1])\n# The (i+tau)-th element stores the (i+1)-step-ahead predictions\nfor iinrange (k):\npreds =model(torch .stack(features[i : i +data .tau], 1))\nfeatures .append(preds .reshape( -1))\nreturn features[data .tau:]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9095183-c80e-45ab-a28a-69aa4593e2b3": {"__data__": {"id_": "c9095183-c80e-45ab-a28a-69aa4593e2b3", "embedding": null, "metadata": {"page_label": "335", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "896a69bc-3255-4a14-9393-63518a7b9693", "node_type": "4", "metadata": {"page_label": "335", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9907f14e4abe5c6270d89d7359f1ebc40036ebeeda53acbad05657bd6080e074", "class_name": "RelatedNodeInfo"}}, "text": "335 Working with Sequences\nsteps =(1,4,16,64)\npreds =k_step_pred(steps[ -1])\nd2l.plot(data .time[data .tau+steps[ -1]-1:],\n[preds[k -1].detach() .numpy() for kinsteps], 'time ','x',\nlegend =[f'{k}-step preds 'for kinsteps], figsize =(6,3))\nThisclearlyillustrateshowthequalityofthepredictionchangesaswetrytopredictfurther\ninto the future. While the 4-step-ahead predictions still look good, anything beyond that is\nalmost useless.\n9.1.5Summary\nThere is quite a difference in difficulty between interpolation and extrapolation. Conse-\nquently,ifyouhaveasequence,alwaysrespectthetemporalorderofthedatawhentraining,\ni.e.,nevertrainonfuturedata. Giventhiskindofdata,sequencemodelsrequirespecialized\nstatistical tools for estimation. Two popular choices are autoregressive models and latent-\nvariable autoregressive models. For causal models (e.g., time going forward), estimating\nthe forward direction is typically a lot easier than the reverse direction. For an observed\nsequenceuptotimestep \ud835\udc61, itspredictedoutputattimestep \ud835\udc61\u00b8\ud835\udc58isthe\ud835\udc58-step-aheadpredic-\ntion. As we predict further in time by increasing \ud835\udc58, the errors accumulate and the quality\nof the prediction degrades, often dramatically.\n9.1.6Exercises\n1.Improve the model in the experiment of this section.\n1.Incorporate more than the past four observations? How many do you really need?\n2.How many past observations would you need if there was no noise? Hint: you can\nwrite sinandcosas a differential equation.\n3.Can you incorporate older observations while keeping the total number of features\nconstant? Does this improve accuracy? Why?\n4.Changetheneuralnetworkarchitectureandevaluatetheperformance. Youmaytrain\nthe new model with more epochs. What do you observe?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1716, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16192993-cfc5-490b-827d-800e68238062": {"__data__": {"id_": "16192993-cfc5-490b-827d-800e68238062", "embedding": null, "metadata": {"page_label": "336", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "646b85ff-6473-4fbf-84f9-709631949fa5", "node_type": "4", "metadata": {"page_label": "336", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3d92e8952e84f52f7566b761c65c9ca910ea18ecfbf7191fd30181bca3c4aef3", "class_name": "RelatedNodeInfo"}}, "text": "336 Recurrent Neural Networks\n136\n1372.An investor wants to find a good security to buy. They look at past returns to decide\nwhich one is likely to do well. What could possibly go wrong with this strategy?\n3.Does causality also apply to text? To which extent?\n4.Give an example for when a latent autoregressive model might be needed to capture the\ndynamic of the data.\nDiscussions136.\n9.2ConvertingRawTextinto SequenceData\nThroughoutthisbook,wewilloftenworkwithtextdatarepresentedassequencesofwords,\ncharacters, or word pieces. To get going, we will need some basic tools for converting raw\ntext into sequences of the appropriate form. Typical preprocessing pipelines execute the\nfollowing steps:\n1.Load text as strings into memory.\n2.Split the strings into tokens (e.g., words or characters).\n3.Build a vocabulary dictionary to associate each vocabulary element with a numerical\nindex.\n4.Convert the text into sequences of numerical indices.\nimport collections\nimport random\nimport re\nimport torch\nfrom d2l import torch asd2l\n9.2.1Readingthe Dataset\nHere, we will work with H. G. Wells\u2019 The Time Machine137, a book containing just over\n30,000 words. While real applications will typically involve significantly larger datasets,\nthis is sufficient to demonstrate the preprocessing pipeline. The following _download\nmethod reads the raw text into a string.\nclass TimeMachine (d2l .DataModule): #@save\n\"\"\"The Time Machine dataset.\"\"\"\ndef _download (self ):\nfname =d2l.download(d2l .DATA_URL +'timemachine.txt ',self .root,\n'090b5e7e70c295757f55df93cb0a180b9691891a ')\nwith open (fname) asf:\nreturn f.read()\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80ef2de0-66c0-4c67-8f43-3e902afbe6cb": {"__data__": {"id_": "80ef2de0-66c0-4c67-8f43-3e902afbe6cb", "embedding": null, "metadata": {"page_label": "337", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a29d51f8-8480-48b6-91f6-7840d05d0343", "node_type": "4", "metadata": {"page_label": "337", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "28ca52985e69c91acd78387645cddfd9e4afaadcbd0d5de395e053f9f637a44c", "class_name": "RelatedNodeInfo"}}, "text": "337 Converting Raw Text into Sequence Data\n(continued from previous page)\ndata =TimeMachine()\nraw_text =data ._download()\nraw_text[: 60]\n'The Time Machine, by H. G. Wells [1898]nnnnnInnnThe Time Tra'\nForsimplicity,weignorepunctuationandcapitalizationwhenpreprocessingtherawtext.\n@d2l .add_to_class(TimeMachine) #@save\ndef _preprocess (self , text):\nreturn re.sub( '[^A-Za-z]+ ','', text) .lower()\ntext =data ._preprocess(raw_text)\ntext[: 60]\n'the time machine by h g wells i the time traveller for so it '\n9.2.2Tokenization\nTokensare the atomic (indivisible) units of text. Each time step corresponds to 1 token,\nbut what precisely constitutes a token is a design choice. For example, we could represent\nthe sentence \u201cBaby needs a new pair of shoes\u201d as a sequence of 7 words, where the set of\nall words comprise a large vocabulary (typically tens or hundreds of thousands of words).\nOr we would represent the same sentence as a much longer sequence of 30 characters,\nusingamuchsmallervocabulary(thereareonly256distinctASCIIcharacters). Below,we\ntokenize our preprocessed text into a sequence of characters.\n@d2l .add_to_class(TimeMachine) #@save\ndef _tokenize (self , text):\nreturn list (text)\ntokens =data ._tokenize(text)\n','.join(tokens[: 30])\n't,h,e, ,t,i,m,e, ,m,a,c,h,i,n,e, ,b,y, ,h, ,g, ,w,e,l,l,s, '\n9.2.3Vocabulary\nThese tokens are still strings. However, the inputs to our models must ultimately consist of\nnumericalinputs. Next,weintroduceaclassforconstructing vocabularies ,i.e.,objectsthat\nassociateeachdistincttokenvaluewithauniqueindex. First,wedeterminethesetofunique\ntokensinourtraining corpus. Wethenassignanumericalindextoeachuniquetoken. Rare\nvocabularyelementsareoftendroppedforconvenience. Wheneverweencounteratokenat\ntrainingor testtime that had notbeen previouslyseen or wasdropped fromthe vocabulary,\nwerepresentitbyaspecial\u201c<unk>\u201dtoken,signifyingthatthisisan unknown value.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06010588-37be-4d91-a2a2-172661c7eb90": {"__data__": {"id_": "06010588-37be-4d91-a2a2-172661c7eb90", "embedding": null, "metadata": {"page_label": "338", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd967916-493b-46fc-b39d-e9d4041bd6c7", "node_type": "4", "metadata": {"page_label": "338", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "795ff3f868037dab48bf79e99801bba53a05425e8955b90d1da4e7023634161f", "class_name": "RelatedNodeInfo"}}, "text": "338 Recurrent Neural Networks\nclass Vocab :#@save\n\"\"\"Vocabulary for text.\"\"\"\ndef __init__ (self , tokens =[], min_freq =0, reserved_tokens =[]):\n# Flatten a 2D list if needed\niftokens and isinstance (tokens[ 0],list ):\ntokens =[token for line intokens for token inline]\n# Count token frequencies\ncounter =collections .Counter(tokens)\nself .token_freqs =sorted (counter .items(), key =lambda x: x[ 1],\nreverse =True )\n# The list of unique tokens\nself .idx_to_token =list (sorted (set(['<unk> ']+reserved_tokens +[\ntoken for token, freq inself .token_freqs iffreq >=min_freq])))\nself .token_to_idx ={token: idx\nfor idx, token inenumerate (self .idx_to_token)}\ndef __len__ (self ):\nreturn len(self .idx_to_token)\ndef __getitem__ (self , tokens):\nifnot isinstance (tokens, ( list ,tuple )):\nreturn self .token_to_idx .get(tokens, self .unk)\nreturn [self .__getitem__ (token) for token intokens]\ndef to_tokens (self , indices):\nifhasattr (indices, '__len__ ')and len(indices) >1:\nreturn [self .idx_to_token[ int(index)] for index inindices]\nreturn self .idx_to_token[indices]\n@property\ndef unk(self ): # Index for the unknown token\nreturn self .token_to_idx[ '<unk> ']\nWe now construct a vocabulary for our dataset, converting the sequence of strings into a\nlist of numerical indices. Note that we have not lost any information and can easily convert\nour dataset back to its original (string) representation.\nvocab =Vocab(tokens)\nindices =vocab[tokens[: 10]]\nprint ('indices: ', indices)\nprint ('words: ', vocab .to_tokens(indices))\nindices: [ 21,9,6,0,21,10,14,6,0,14]\nwords: [ 't','h','e','','t','i','m','e','','m']\n9.2.4PuttingIt All Together\nUsing the above classes and methods, we package everything into the following build\nmethodofthe TimeMachine class,whichreturns corpus,alistoftokenindices,and vocab,\nthe vocabulary of The Time Machine corpus. The modifications we did here are: (i) we\ntokenizetextintocharacters,notwords,tosimplifythetraininginlatersections;(ii) corpus", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ec0c04c-90cc-48e0-9d95-15f9604e38b5": {"__data__": {"id_": "2ec0c04c-90cc-48e0-9d95-15f9604e38b5", "embedding": null, "metadata": {"page_label": "339", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f1fc008e-65cf-4c40-b84d-509f5ca6d561", "node_type": "4", "metadata": {"page_label": "339", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3531b39cfd0e56e31df0d4a28ef759391522c5f5f3573e3989efd0b74429596f", "class_name": "RelatedNodeInfo"}}, "text": "339 Converting Raw Text into Sequence Data\nis a single list, not a list of token lists, since each text line in The Time Machine dataset is\nnot necessarily a sentence or paragraph.\n@d2l .add_to_class(TimeMachine) #@save\ndef build (self , raw_text, vocab =None ):\ntokens =self ._tokenize( self ._preprocess(raw_text))\nifvocab isNone : vocab =Vocab(tokens)\ncorpus =[vocab[token] for token intokens]\nreturn corpus, vocab\ncorpus, vocab =data .build(raw_text)\nlen(corpus), len(vocab)\n(173428 ,28)\n9.2.5Exploratory LanguageStatistics\nUsingtherealcorpusandthe Vocabclassdefinedoverwords,wecaninspectbasicstatistics\nconcerning word use in our corpus. Below, we construct a vocabulary from words used in\nTheTimeMachine and print the ten most frequently occurring of them.\nwords =text .split()\nvocab =Vocab(words)\nvocab .token_freqs[: 10]\n[('the',2261 ),\n('i',1267 ),\n('and',1245 ),\n('of',1155 ),\n('a',816),\n('to',695),\n('was',552),\n('in',541),\n('that ',443),\n('my',440)]\nNote that the ten most frequent words are not all that descriptive. You might even imagine\nthat we might see a very similar list if we had chosen any book at random. Articles like\n\u201cthe\u201d and \u201ca\u201d, pronouns like \u201ci\u201d and \u201cmy\u201d, and prepositions like \u201cof\u201d, \u201cto\u201d, and \u201cin\u201d occur\noften because they serve common syntactic roles. Such words that are common but not\nparticularly descriptive are often called stop words and, in previous generations of text\nclassifiers based on so-called bag-of-words representations, they were most often filtered\nout. However, they carry meaning and it is not necessary to filter them out when working\nwith modern RNN- and Transformer-based neural models. If you look further down the\nlist, you will notice that word frequency decays quickly. The 10thmost frequent word is\nless than 1\u009d5as common as the most popular. Word frequency tends to follow a power law\ndistribution(specificallytheZipfian)aswegodowntheranks. Togetabetteridea, weplot\nthe figure of the word frequency.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29c89dbd-6326-4cbd-9a8a-2ded8148268d": {"__data__": {"id_": "29c89dbd-6326-4cbd-9a8a-2ded8148268d", "embedding": null, "metadata": {"page_label": "340", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "633c1e8d-115b-4768-ade8-e94550ad85ce", "node_type": "4", "metadata": {"page_label": "340", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9f1f8387ec1ad992e3adf2f1c9ddeeb062aacca213208aba447dc3127521c772", "class_name": "RelatedNodeInfo"}}, "text": "340 Recurrent Neural Networks\nfreqs =[freq for token, freq invocab .token_freqs]\nd2l.plot(freqs, xlabel ='token: x ', ylabel ='frequency: n(x) ',\nxscale ='log', yscale ='log')\nAfterdealingwiththefirstfewwordsasexceptions,alltheremainingwordsroughlyfollow\na straight line on a log\u2013log plot. This phenomenon is captured by Zipf\u2019s law , which states\nthat the frequency \ud835\udc5b\ud835\udc56of the\ud835\udc56thmost frequent word is:\n\ud835\udc5b\ud835\udc56/1\n\ud835\udc56\ud835\udefc, (9.2.1)\nwhich is equivalent to\nlog\ud835\udc5b\ud835\udc56=\u0000\ud835\udefclog\ud835\udc56\u00b8\ud835\udc50, (9.2.2)\nwhere\ud835\udefcis the exponent that characterizes the distribution and \ud835\udc50is a constant. This should\nalready give us pause for thought if we want to model words by counting statistics. After\nall, we will significantly overestimate the frequency of the tail, also known as the infre-\nquent words. But what about the other word combinations, such as two consecutive words\n(bigrams), three consecutive words (trigrams), and beyond? Let\u2019s see whether the bigram\nfrequency behaves in the same manner as the single word (unigram) frequency.\nbigram_tokens =['--'.join(pair) for pair inzip(words[: -1], words[ 1:])]\nbigram_vocab =Vocab(bigram_tokens)\nbigram_vocab .token_freqs[: 10]\n[('of--the ',309),\n('in--the ',169),\n('i--had ',130),\n('i--was ',112),\n('and--the ',109),\n('the--time ',102),\n('it--was ',99),\n('to--the ',85),\n('as--i ',78),\n('of--a ',73)]\nOne thing is notable here. Out of the ten most frequent word pairs, nine are composed of\nbothstopwordsandonlyoneisrelevanttotheactualbook\u2014\u201cthetime\u201d. Furthermore, let\u2019s\nsee whether the trigram frequency behaves in the same manner.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18213439-c4fe-4178-b0f3-2c378c4d675a": {"__data__": {"id_": "18213439-c4fe-4178-b0f3-2c378c4d675a", "embedding": null, "metadata": {"page_label": "341", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "567e0dc5-5d02-4532-89fe-b97ba870ab92", "node_type": "4", "metadata": {"page_label": "341", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "256c4eefa62a4e810a793c0ce89b6f04e1ae51d683473efffdbef06ea41b0353", "class_name": "RelatedNodeInfo"}}, "text": "341 Converting Raw Text into Sequence Data\ntrigram_tokens =['--'.join(triple) for triple inzip(\nwords[: -2], words[ 1:-1], words[ 2:])]\ntrigram_vocab =Vocab(trigram_tokens)\ntrigram_vocab .token_freqs[: 10]\n[('the--time--traveller ',59),\n('the--time--machine ',30),\n('the--medical--man ',24),\n('it--seemed--to ',16),\n('it--was--a ',15),\n('here--and--there ',15),\n('seemed--to--me ',14),\n('i--did--not ',14),\n('i--saw--the ',13),\n('i--began--to ',13)]\nNow, let\u2019s visualize the token frequency among these three models: unigrams, bigrams,\nand trigrams.\nbigram_freqs =[freq for token, freq inbigram_vocab .token_freqs]\ntrigram_freqs =[freq for token, freq intrigram_vocab .token_freqs]\nd2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel ='token: x ',\nylabel ='frequency: n(x) ', xscale ='log', yscale ='log',\nlegend =['unigram ','bigram ','trigram '])\nThis figure is quite exciting. First, beyond unigram words, sequences of words also appear\nto be following Zipf\u2019s law, albeit with a smaller exponent \ud835\udefcin(9.2.1 ), depending on the\nsequence length. Second, the number of distinct \ud835\udc5b-grams is not that large. This gives us\nhope that there is quite a lot of structure in language. Third, many \ud835\udc5b-grams occur very\nrarely. This makes certain methods unsuitable for language modeling and motivates the\nuse of deep learning models. We will discuss this in the next section.\n9.2.6Summary\nText is among the most common forms of sequence data encountered in deep learning.\nCommon choices for what constitutes a token are characters, words, and word pieces. To\npreprocess text, we usually (i) split text into tokens; (ii) build a vocabulary to map token\nstrings to numerical indices; and (iii) convert text data into token indices for models to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1731, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f53c4e68-3d6a-425e-825d-473ae0ccb697": {"__data__": {"id_": "f53c4e68-3d6a-425e-825d-473ae0ccb697", "embedding": null, "metadata": {"page_label": "342", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58a8000e-a877-4b02-9119-46cc25b53a20", "node_type": "4", "metadata": {"page_label": "342", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "37865c4d0c524490d61f704fe625d483d6fecd9674159bce06206aa9e1ec48cf", "class_name": "RelatedNodeInfo"}}, "text": "342 Recurrent Neural Networks\n138manipulate. In practice, the frequency of words tends to follow Zipf\u2019s law. This is true not\njust for individual words (unigrams), but also for \ud835\udc5b-grams.\n9.2.7Exercises\n1.In the experiment of this section, tokenize text into words and vary the min_freq argu-\nment value of the Vocabinstance. Qualitatively characterize how changes in min_freq\nimpact the size of the resulting vocabulary.\n2.EstimatetheexponentofZipfiandistributionforunigrams,bigrams,andtrigramsinthis\ncorpus.\n3.Find some other sources of data (download a standard machine learning dataset, pick\nanother public domain book, scrape a website, etc). For each, tokenize the data at both\nthe word and character levels. How do the vocabulary sizes compare with The Time\nMachine corpusatequivalentvaluesof min_freq . EstimatetheexponentoftheZipfian\ndistribution corresponding to the unigram and bigram distributions for these corpora.\nHow do they compare with the values that you observed for TheTimeMachine corpus?\nDiscussions138.\n9.3LanguageModels\nInSection 9.2 , we saw how to map text sequences into tokens, where these tokens can be\nviewed as a sequence of discrete observations such as words or characters. Assume that\nthe tokens in a text sequence of length \ud835\udc47are in turn\ud835\udc651,\ud835\udc652,...,\ud835\udc65\ud835\udc47. The goal of language\nmodelsis to estimate the joint probability of the whole sequence:\n\ud835\udc43\u00b9\ud835\udc651,\ud835\udc652,...,\ud835\udc65\ud835\udc47\u00ba, (9.3.1)\nwhere statistical tools in Section 9.1 can be applied.\nLanguage models are incredibly useful. For instance, an ideal language model should\ngenerate natural text on its own, simply by drawing one token at a time \ud835\udc65\ud835\udc61\u0018\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\n\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1\u00ba. Quite unlike the monkey using a typewriter, all text emerging from such\na model would pass as natural language, e.g., English text. Furthermore, it would be suffi-\ncientforgeneratingameaningfuldialog,simplybyconditioningthetextonpreviousdialog\nfragments. Clearly we are still very far from designing such a system, since it would need\ntounderstand the text rather than just generate grammatically sensible content.\nNonetheless, language models are of great service even in their limited form. For instance,\nthephrases\u201ctorecognizespeech\u201dand\u201ctowreckanicebeach\u201dsoundverysimilar. Thiscan\ncause ambiguity in speech recognition, which is easily resolved through a language model\nthat rejects the second translation as outlandish. Likewise, in a document summarization\nalgorithm it is worthwhile knowing that \u201cdog bites man\u201d is much more frequent than \u201cman", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2479, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "430b26d6-616a-4488-926c-2ab730379230": {"__data__": {"id_": "430b26d6-616a-4488-926c-2ab730379230", "embedding": null, "metadata": {"page_label": "343", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3769e97-8951-465f-a04f-14d1296e1c02", "node_type": "4", "metadata": {"page_label": "343", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b5e7cfa73894b14ff2e3d8c86b4404a89647a3d637c0df2d0d0e6158d62b174c", "class_name": "RelatedNodeInfo"}}, "text": "343 Language Models\n139bitesdog\u201d,orthat\u201cIwanttoeatgrandma\u201disaratherdisturbingstatement,whereas\u201cIwant\nto eat, grandma\u201d is much more benign.\nimport torch\nfrom d2l import torch asd2l\n9.3.1Learning LanguageModels\nThe obvious question is how we should model a document, or even a sequence of tokens.\nSupposethatwetokenizetextdataatthewordlevel. Let\u2019sstartbyapplyingbasicprobability\nrules:\n\ud835\udc43\u00b9\ud835\udc651,\ud835\udc652,...,\ud835\udc65\ud835\udc47\u00ba=\ud835\udc47\u00d6\n\ud835\udc61=1\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc651,...,\ud835\udc65\ud835\udc61\u00001\u00ba. (9.3.2)\nFor example, the probability of a text sequence containing four words would be given\nas:\n\ud835\udc43\u00b9deep,learning,is,fun\u00ba\n=\ud835\udc43\u00b9deep\u00ba\ud835\udc43\u00b9learningjdeep\u00ba\ud835\udc43\u00b9isjdeep,learning\u00ba\ud835\udc43\u00b9funjdeep,learning,is\u00ba.(9.3.3)\nMarkovModels and \ud835\udc5b-grams\nAmong those sequence model analyses in Section 9.1 , let\u2019s apply Markov models to lan-\nguage modeling. A distribution over sequences satisfies the Markov property of first order\nif\ud835\udc43\u00b9\ud835\udc65\ud835\udc61\u00b81j\ud835\udc65\ud835\udc61,...,\ud835\udc65 1\u00ba=\ud835\udc43\u00b9\ud835\udc65\ud835\udc61\u00b81j\ud835\udc65\ud835\udc61\u00ba. Higher orders correspond to longer dependencies.\nThisleadstoanumberofapproximationsthatwecouldapplytomodelasequence:\n\ud835\udc43\u00b9\ud835\udc651,\ud835\udc652,\ud835\udc653,\ud835\udc654\u00ba=\ud835\udc43\u00b9\ud835\udc651\u00ba\ud835\udc43\u00b9\ud835\udc652\u00ba\ud835\udc43\u00b9\ud835\udc653\u00ba\ud835\udc43\u00b9\ud835\udc654\u00ba,\n\ud835\udc43\u00b9\ud835\udc651,\ud835\udc652,\ud835\udc653,\ud835\udc654\u00ba=\ud835\udc43\u00b9\ud835\udc651\u00ba\ud835\udc43\u00b9\ud835\udc652j\ud835\udc651\u00ba\ud835\udc43\u00b9\ud835\udc653j\ud835\udc652\u00ba\ud835\udc43\u00b9\ud835\udc654j\ud835\udc653\u00ba,\n\ud835\udc43\u00b9\ud835\udc651,\ud835\udc652,\ud835\udc653,\ud835\udc654\u00ba=\ud835\udc43\u00b9\ud835\udc651\u00ba\ud835\udc43\u00b9\ud835\udc652j\ud835\udc651\u00ba\ud835\udc43\u00b9\ud835\udc653j\ud835\udc651,\ud835\udc652\u00ba\ud835\udc43\u00b9\ud835\udc654j\ud835\udc652,\ud835\udc653\u00ba.(9.3.4)\nThe probability formulae that involve one, two, and three variables are typically referred\nto asunigram ,bigram, andtrigrammodels, respectively. In order to compute the language\nmodel, we need to calculate the probability of words and the conditional probability of\na word given the previous few words. Note that such probabilities are language model\nparameters.\nWordFrequency\nHere,weassumethatthetrainingdatasetisalargetextcorpus,suchasallWikipediaentries,\nProject Gutenberg139, and all text posted on the web. The probability of words can be\ncalculated from the relative word frequency of a given word in the training dataset. For\nexample, theestimate \u02c6\ud835\udc43\u00b9deep\u00bacanbecalculatedastheprobabilityofanysentencestarting\nwith the word \u201cdeep\u201d. A slightly less accurate approach would be to count all occurrences", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38deae6a-6df0-4085-828c-fb5ef4d9b841": {"__data__": {"id_": "38deae6a-6df0-4085-828c-fb5ef4d9b841", "embedding": null, "metadata": {"page_label": "344", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86f48024-32fe-41d4-992f-18d4fcfc9b94", "node_type": "4", "metadata": {"page_label": "344", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9f8e5d142d95158080c9f9341a47b8b11d67381ec4ce0324043183e15a3dfa94", "class_name": "RelatedNodeInfo"}}, "text": "344 Recurrent Neural Networks\nof the word \u201cdeep\u201d and divide it by the total number of words in the corpus. This works\nfairlywell,particularlyforfrequentwords. Movingon,wecouldattempttoestimate\n\u02c6\ud835\udc43\u00b9learningjdeep\u00ba=\ud835\udc5b\u00b9deep, learning\u00ba\n\ud835\udc5b\u00b9deep\u00ba, (9.3.5)\nwhere\ud835\udc5b\u00b9\ud835\udc65\u00baand\ud835\udc5b\u00b9\ud835\udc65,\ud835\udc650\u00baare the number of occurrences of singletons and consecutive word\npairs, respectively. Unfortunately, estimating the probability of a word pair is somewhat\nmoredifficult,sincetheoccurrencesof\u201cdeeplearning\u201darealotlessfrequent. Inparticular,\nfor some unusual word combinations it may be tricky to find enough occurrences to get\naccurate estimates. As suggested by the empirical results in Section 9.2.5 , things take a\nturn for the worse for three-word combinations and beyond. There will be many plausible\nthree-wordcombinationsthatwelikelywillnotseeinourdataset. Unlessweprovidesome\nsolutiontoassignsuchwordcombinationsanonzerocount,wewillnotbeabletousethem\nin a language model. If the dataset is small or if the words are very rare, we might not find\neven a single one of them.\nLaplaceSmoothing\nA common strategy is to perform some form of Laplacesmoothing . The solution is to add\nasmallconstanttoallcounts. Denoteby \ud835\udc5bthetotalnumberofwordsinthetrainingsetand\n\ud835\udc5athe number of unique words. This solution helps with singletons, e.g., via\n\u02c6\ud835\udc43\u00b9\ud835\udc65\u00ba=\ud835\udc5b\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf161\u009d\ud835\udc5a\n\ud835\udc5b\u00b8\ud835\udf161,\n\u02c6\ud835\udc43\u00b9\ud835\udc650j\ud835\udc65\u00ba=\ud835\udc5b\u00b9\ud835\udc65,\ud835\udc650\u00ba\u00b8\ud835\udf162\u02c6\ud835\udc43\u00b9\ud835\udc650\u00ba\n\ud835\udc5b\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf162,\n\u02c6\ud835\udc43\u00b9\ud835\udc6500j\ud835\udc65,\ud835\udc650\u00ba=\ud835\udc5b\u00b9\ud835\udc65,\ud835\udc650,\ud835\udc6500\u00ba\u00b8\ud835\udf163\u02c6\ud835\udc43\u00b9\ud835\udc6500\u00ba\n\ud835\udc5b\u00b9\ud835\udc65,\ud835\udc650\u00ba\u00b8\ud835\udf163.(9.3.6)\nHere\ud835\udf161,\ud835\udf162,and\ud835\udf163arehyperparameters. Take \ud835\udf161asanexample: when \ud835\udf161=0,nosmoothing\nis applied; when \ud835\udf161approaches positive infinity, \u02c6\ud835\udc43\u00b9\ud835\udc65\u00baapproaches the uniform probability\n1\u009d\ud835\udc5a. The above is a rather primitive variant of what other techniques can accomplish\n(Woodetal., 2011).\nUnfortunately, modelslikethisgetunwieldyratherquicklyforthefollowingreasons. First,\nas discussed in Section 9.2.5 , many\ud835\udc5b-grams occur very rarely, making Laplace smoothing\nrather unsuitable for language modeling. Second, we need to store all counts. Third, this\nentirely ignores the meaning of the words. For instance, \u201ccat\u201d and \u201cfeline\u201d should occur in\nrelated contexts. It is quite difficult to adjust such models to additional contexts, whereas,\ndeep learning based language models are well suited to take this into account. Last, long\nword sequences are almost certain to be novel, hence a model that simply counts the fre-\nquencyofpreviouslyseenwordsequencesisboundtoperformpoorlythere. Therefore,we\nfocus on using neural networks for language modeling in the rest of the chapter.\n9.3.2Perplexity", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73253272-c53b-4b63-83cf-119ef30bd504": {"__data__": {"id_": "73253272-c53b-4b63-83cf-119ef30bd504", "embedding": null, "metadata": {"page_label": "345", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8227386-cadc-4d1d-a2d0-721ddcfb0fe0", "node_type": "4", "metadata": {"page_label": "345", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "710d7e697f76422302fa9acefa88be281db783e2c922425a47430e672ffa5e2e", "class_name": "RelatedNodeInfo"}}, "text": "345 Language Models\nNext, let\u2019s discuss about how to measure the quality of the language model, which we\nwill then use to evaluate our models in the subsequent sections. One way is to check how\nsurprising the text is. A good language model is able to predict, with high accuracy, the\ntokens that come next. Consider the following continuations of the phrase \u201cIt is raining\u201d,\nas proposed by different language models:\n1.\u201cIt is raining outside\u201d\n2.\u201cIt is raining banana tree\u201d\n3.\u201cIt is raining piouw;kcj pwepoiut\u201d\nIn terms of quality, Example 1 is clearly the best. The words are sensible and logically co-\nherent. Whileitmightnotquiteaccuratelyreflectwhichwordfollowssemantically(\u201cinSan\nFrancisco\u201d and \u201cin winter\u201d would have been perfectly reasonable extensions), the model is\nabletocapturewhichkindofwordfollows. Example2isconsiderablyworsebyproducing\na nonsensical extension. Nonetheless, at least the model has learned how to spell words\nand some degree of correlation between words. Last, Example 3 indicates a poorly trained\nmodel that does not fit data properly.\nWe might measure the quality of the model by computing the likelihood of the sequence.\nUnfortunatelythisisanumberthatishardtounderstandanddifficulttocompare. Afterall,\nshorter sequencesare muchmore likelyto occur than the longerones, hence evaluatingthe\nmodel on Tolstoy\u2019s magnum opus War and Peace will inevitably produce a much smaller\nlikelihood than, say, on Saint-Exupery\u2019s novella The Little Prince . What is missing is the\nequivalent of an average.\nInformation theory comes handy here. We defined entropy, surprisal, and cross-entropy\nwhen we introduced the softmax regression ( Section 4.1.3 ). If we want to compress text,\nwecanaskaboutpredictingthenexttokengiventhecurrentsetoftokens. Abetterlanguage\nmodelshouldallowustopredictthenexttokenmoreaccurately. Thus,itshouldallowusto\nspend fewer bits in compressing the sequence. So we can measure it by the cross-entropy\nloss averaged over all the \ud835\udc5btokens of a sequence:\n1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc61=1\u0000log\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1\u00ba, (9.3.7)\nwhere\ud835\udc43isgivenbyalanguagemodeland \ud835\udc65\ud835\udc61istheactualtokenobservedattimestep \ud835\udc61from\nthe sequence. This makes the performance on documents of different lengths comparable.\nFor historical reasons, scientists in natural language processing prefer to use a quantity\ncalledperplexity . In a nutshell, it is the exponential of (9.3.7 ):\nexp \n\u00001\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc61=1log\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1\u00ba!\n. (9.3.8)\nPerplexitycanbebestunderstoodasthereciprocalofthegeometricmeanofthenumberof\nreal choices that we have when deciding which token to pick next. Let\u2019s look at a number\nof cases:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d825421e-7c53-4858-a460-4e2597330de4": {"__data__": {"id_": "d825421e-7c53-4858-a460-4e2597330de4", "embedding": null, "metadata": {"page_label": "346", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4cd4b3cd-2e73-487e-929e-abe9947605bb", "node_type": "4", "metadata": {"page_label": "346", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5b3a271921db8777ef810a24c3c259faac54869a1c955a7963813601ad2f6423", "class_name": "RelatedNodeInfo"}}, "text": "346 Recurrent Neural Networks\n\u000fIn the best case scenario, the model always perfectly estimates the probability of the\ntarget token as 1. In this case the perplexity of the model is 1.\n\u000fIn the worst case scenario, the model always predicts the probability of the target token\nas 0. In this situation, the perplexity is positive infinity.\n\u000fAtthe baseline, the model predicts a uniform distribution overall the availabletokensof\nthe vocabulary. In this case, the perplexity equals the number of unique tokens of the\nvocabulary. In fact, if we were to store the sequence without any compression, this\nwouldbe thebestwecould doforencodingit. Hence, this providesa nontrivialupper\nbound that any useful model must beat.\n9.3.3PartitioningSequences\nWe will design language models using neural networks and use perplexity to evaluate how\ngood the model is at predicting the next token given the current set of tokens in text se-\nquences. Before introducing the model, let\u2019s assume that it processes a minibatch of se-\nquences with predefined length at a time. Now the question is how to read minibatches of\ninput sequences and target sequences at random.\nSuppose that the dataset takes the form of a sequence of \ud835\udc47token indices in corpus. We\nwill partition it into subsequences, where each subsequence has \ud835\udc5btokens (time steps). To\niterateover(almost)allthetokensoftheentiredatasetforeachepochandobtainallpossible\nlength-\ud835\udc5bsubsequences, we can introduce randomness. More concretely, at the beginning\nof each epoch, discard the first \ud835\udc51tokens, where \ud835\udc512\u00bb0,\ud835\udc5b\u00bais uniformly sampled at random.\nTherestofthesequenceisthenpartitionedinto \ud835\udc5a=b\u00b9\ud835\udc47\u0000\ud835\udc51\u00ba\u009d\ud835\udc5bcsubsequences. Denoteby\nx\ud835\udc61=\u00bb\ud835\udc65\ud835\udc61,...,\ud835\udc65\ud835\udc61\u00b8\ud835\udc5b\u00001\u00bcthe length-\ud835\udc5bsubsequence starting from token \ud835\udc65\ud835\udc61at time step\ud835\udc61. The\nresulting\ud835\udc5apartitioned subsequences are x\ud835\udc51,x\ud835\udc51\u00b8\ud835\udc5b,...,x\ud835\udc51\u00b8\ud835\udc5b\u00b9\ud835\udc5a\u00001\u00ba.Each subsequence will\nbe used as an input sequence into the language model.\nFor language modeling, the goal is to predict the next token based on the tokens we have\nseen so far; hence the targets (labels) are the original sequence, shifted by one token. The\ntarget sequence for any input sequence x\ud835\udc61isx\ud835\udc61\u00b81with length\ud835\udc5b.\ntFig. 9.3.1 Obtaining \ufb01ve pairs of input sequences and target sequences from partitioned length-5\nsubsequences.\nFig.9.3.1 showsanexampleofobtainingfivepairsofinputsequencesandtargetsequences\nwith\ud835\udc5b=5and\ud835\udc51=2.\n@d2l .add_to_class(d2l .TimeMachine) #@save\ndef __init__ (self , batch_size, num_steps, num_train =10000 , num_val =5000 ):\nsuper (d2l .TimeMachine, self ).__init__ ()\nself .save_hyperparameters()\ncorpus, self .vocab =self .build( self ._download())\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd87714c-1909-4881-9083-d8b4255f975c": {"__data__": {"id_": "dd87714c-1909-4881-9083-d8b4255f975c", "embedding": null, "metadata": {"page_label": "347", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "390b40e6-70d1-49a7-800e-d2330f7716c7", "node_type": "4", "metadata": {"page_label": "347", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3fb3033d9b51c433961beb0cdc4c9ed222618f3f3a921b59152402c90bbaf785", "class_name": "RelatedNodeInfo"}}, "text": "347 Language Models\n(continued from previous page)\narray =torch .tensor([corpus[i:i +num_steps +1]\nfor iinrange (len(corpus) -num_steps)])\nself .X,self .Y=array[:,: -1], array[:, 1:]\nTo train language models, we will randomly sample pairs of input sequences and target\nsequences in minibatches. The followingdata loader randomlygeneratesa minibatchfrom\nthe dataset each time. The argument batch_size specifies the number of subsequence\nexamples in each minibatch and num_steps is the subsequence length in tokens.\n@d2l .add_to_class(d2l .TimeMachine) #@save\ndef get_dataloader (self , train):\nidx =slice (0,self .num_train) iftrain else slice (\nself .num_train, self .num_train +self .num_val)\nreturn self .get_tensorloader([ self .X,self .Y], train, idx)\nAs we can see in the following, a minibatch of target sequences can be obtained by shifting\nthe input sequences by one token.\ndata =d2l.TimeMachine(batch_size =2, num_steps =10)\nfor X, Y indata .train_dataloader():\nprint ('X:', X, '\\nY:', Y)\nbreak\nDownloading ../data /timemachine .txt from http ://d2l-data .s3-accelerate .\n\u21a9!amazonaws .com/timemachine .txt...\nX: tensor([[ 10,4,2,21,10,16,15,0,20,2],\n[21,9,6,19,0,24,2,26,0,16]])\nY: tensor([[ 4,2,21,10,16,15,0,20,2,10],\n[9,6,19,0,24,2,26,0,16,9]])\n9.3.4Summaryand Discussion\nLanguage models estimate the joint probability of a text sequence. For long sequences,\n\ud835\udc5b-grams provide a convenient model by truncating the dependence. However, there is a lot\nofstructurebutnotenoughfrequencytodealefficientlywithinfrequentwordcombinations\nvia Laplace smoothing. Thus, we will focus on neural language modeling in subsequent\nsections. To train language models, we can randomly sample pairs of input sequences\nand target sequences in minibatches. After training, we will use perplexity to measure the\nlanguage model quality.\nLanguage models can be scaled up with increased data size, model size, and amount in\ntraining compute. Large language models can perform desired tasks by predicting output\ntextgiveninputtextinstructions. Aswewilldiscusslater(e.g., Section11.9 ),atthepresent\nmoment large language models form the basis of state-of-the-art systems across diverse\ntasks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2175, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c1285ff-c1d9-4940-a2f3-94f1fdaabddd": {"__data__": {"id_": "5c1285ff-c1d9-4940-a2f3-94f1fdaabddd", "embedding": null, "metadata": {"page_label": "348", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "243aafdc-431b-47e3-bd00-1033a755c66d", "node_type": "4", "metadata": {"page_label": "348", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6d642c6bd8d4661e763d1e9b6e115cebc2e11e6a17db8d58b0f76e35943501c0", "class_name": "RelatedNodeInfo"}}, "text": "348 Recurrent Neural Networks\n1409.3.5Exercises\n1.Suppose there are 100,000 words in the training dataset. How much word frequency\nand multi-word adjacent frequency does a four-gram need to store?\n2.How would you model a dialogue?\n3.What other methods can you think of for reading long sequence data?\n4.Consider our method for discarding a uniformly random number of the first few tokens\nat the beginning of each epoch.\n1.Doesitreallyleadtoaperfectlyuniformdistributionoverthesequencesonthedocu-\nment?\n2.What would you have to do to make things even more uniform?\n5.If we want a sequence example to be a complete sentence, what kind of problem does\nthis introduce in minibatch sampling? How can we fix it?\nDiscussions140.\n9.4RecurrentNeuralNetworks\nInSection 9.3 we described Markov models and \ud835\udc5b-grams for language modeling, where\nthe conditional probability of token \ud835\udc65\ud835\udc61at time step \ud835\udc61only depends on the \ud835\udc5b\u00001previous\ntokens. If we want to incorporate the possible effect of tokens earlier than time step \ud835\udc61\u0000\n\u00b9\ud835\udc5b\u00001\u00baon\ud835\udc65\ud835\udc61, weneedtoincrease \ud835\udc5b. However, thenumberofmodelparameterswouldalso\nincrease exponentially with it, as we need to store jVj\ud835\udc5bnumbers for a vocabulary set V.\nHence,ratherthanmodeling \ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65\ud835\udc61\u0000\ud835\udc5b\u00b81\u00baitispreferabletousealatentvariable\nmodel,\n\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1\u00ba\u0019\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\u210e\ud835\udc61\u00001\u00ba, (9.4.1)\nwhere\u210e\ud835\udc61\u00001is ahidden state that stores the sequence information up to time step \ud835\udc61\u00001. In\ngeneral, the hidden state at any time step \ud835\udc61could be computed based on both the current\ninput\ud835\udc65\ud835\udc61and the previous hidden state \u210e\ud835\udc61\u00001:\n\u210e\ud835\udc61=\ud835\udc53\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61\u00001\u00ba. (9.4.2)\nForasufficientlypowerfulfunction \ud835\udc53in(9.4.2 ),thelatentvariablemodelisnotanapprox-\nimation. Afterall, \u210e\ud835\udc61maysimplystoreallthedataithasobservedsofar. However, itcould\npotentially make both computation and storage expensive.\nRecallthatwehavediscussedhiddenlayerswithhiddenunitsin Chapter5 . Itisnoteworthy\nthat hidden layers and hidden states refer to two very different concepts. Hidden layers are,\nas explained, layers that are hidden from view on the path from input to output. Hidden", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3972638c-89f1-4e3c-9793-becaa8ad912d": {"__data__": {"id_": "3972638c-89f1-4e3c-9793-becaa8ad912d", "embedding": null, "metadata": {"page_label": "349", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "765a9e98-524c-48ed-aae1-2548635bd96a", "node_type": "4", "metadata": {"page_label": "349", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "466b85d44958b1419fefc9d3b82284b4d98b91ef727cc880d41b6f5f4ddd7ff3", "class_name": "RelatedNodeInfo"}}, "text": "349 Recurrent Neural Networks\nstates are technically speaking inputsto whatever we do at a given step, and they can only\nbe computed by looking at data at previous time steps.\nRecurrent neural networks (RNNs) are neural networks with hidden states. Before intro-\nducing the RNN model, we first revisit the MLP model introduced in Section 5.1 .\nimport torch\nfrom d2l import torch asd2l\n9.4.1NeuralNetworkswithout Hidden States\nLet\u2019s take a look at an MLP with a single hidden layer. Let the hidden layer\u2019s activation\nfunction be\ud835\udf19. Given a minibatch of examples X2R\ud835\udc5b\u0002\ud835\udc51with batch size \ud835\udc5band\ud835\udc51inputs,\nthe hidden layer output H2R\ud835\udc5b\u0002\u210eis calculated as\nH=\ud835\udf19\u00b9XWxh\u00b8bh\u00ba. (9.4.3)\nIn(9.4.3 ), we have the weight parameter Wxh2R\ud835\udc51\u0002\u210e, the bias parameter bh2R1\u0002\u210e, and\nthe number of hidden units \u210e, for the hidden layer. So armed, we apply broadcasting (see\nSection 2.1.4 ) during the summation. Next, the hidden layer output His used as input of\nthe output layer, which is given by\nO=HWhq\u00b8bq, (9.4.4)\nwhere O2R\ud835\udc5b\u0002\ud835\udc5eis the output variable, Whq2R\u210e\u0002\ud835\udc5eis the weight parameter, and bq2\nR1\u0002\ud835\udc5eis the bias parameter of the output layer. If it is a classification problem, we can use\nsoftmax\u00b9O\u00bato compute the probability distribution of the output categories.\nThis is entirely analogous to the regression problem we solved previously in Section 9.1 ,\nhence we omit details. Suffice it to say that we can pick feature-label pairs at random and\nlearn the parameters of our network via automatic differentiation and stochastic gradient\ndescent.\n9.4.2RecurrentNeuralNetworkswith Hidden States\nMatters are entirely different when we have hidden states. Let\u2019s look at the structure in\nsome more detail.\nAssume that we have a minibatch of inputs X\ud835\udc612R\ud835\udc5b\u0002\ud835\udc51at time step\ud835\udc61. In other words, for\na minibatch of \ud835\udc5bsequence examples, each row of X\ud835\udc61corresponds to one example at time\nstep\ud835\udc61from the sequence. Next, denote by H\ud835\udc612R\ud835\udc5b\u0002\u210ethe hidden layer output of time step\n\ud835\udc61. UnlikewithMLP,herewesavethehiddenlayeroutput H\ud835\udc61\u00001fromtheprevioustimestep\nandintroduceanewweightparameter Whh2R\u210e\u0002\u210etodescribehowtousethehiddenlayer\noutput of the previous time step in the current time step. Specifically, the calculation of the\nhidden layer output of the current time step is determined by the input of the current time\nstep together with the hidden layer output of the previous time step:\nH\ud835\udc61=\ud835\udf19\u00b9X\ud835\udc61Wxh\u00b8H\ud835\udc61\u00001Whh\u00b8bh\u00ba. (9.4.5)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbd28fdb-8a33-44dc-a6a1-14d79327f14f": {"__data__": {"id_": "dbd28fdb-8a33-44dc-a6a1-14d79327f14f", "embedding": null, "metadata": {"page_label": "350", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1c14f6f-9881-4305-8e47-fffafdf986a7", "node_type": "4", "metadata": {"page_label": "350", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "06410292f70743add8f7e5e1a5a672d933654b49362c46f4767e33033e281532", "class_name": "RelatedNodeInfo"}}, "text": "350 Recurrent Neural Networks\nComparedwith (9.4.3 ),(9.4.5 )addsonemoreterm H\ud835\udc61\u00001Whhandthusinstantiates (9.4.2 ).\nFrom the relationship between hidden layer outputs H\ud835\udc61andH\ud835\udc61\u00001of adjacent time steps,\nwe know that these variables captured and retained the sequence\u2019s historical information\nup to their current time step, just like the state or memory of the neural network\u2019s current\ntime step. Therefore, such a hidden layer output is called a hidden state . Since the hidden\nstate uses the same definition of the previous time step in the current time step, the compu-\ntation of (9.4.5 )isrecurrent . Hence, as we said, neural networks with hidden states based\non recurrent computation are named recurrent neural networks . Layers that perform the\ncomputation of (9.4.5 )in RNNs are called recurrentlayers .\nThere are many different ways for constructing RNNs. Those with a hidden state defined\nby(9.4.5 )are very common. For time step \ud835\udc61, the output of the output layer is similar to the\ncomputation in the MLP:\nO\ud835\udc61=H\ud835\udc61Whq\u00b8bq. (9.4.6)\nParameters of the RNN include the weights Wxh2R\ud835\udc51\u0002\u210e,Whh2R\u210e\u0002\u210e, and the bias bh2\nR1\u0002\u210eof the hidden layer, together with the weights Whq2R\u210e\u0002\ud835\udc5eand the bias bq2R1\u0002\ud835\udc5e\nof the output layer. It is worth mentioning that even at different time steps, RNNs always\nuse these model parameters. Therefore, the parametrization cost of an RNN does not grow\nas the number of time steps increases.\nFig. 9.4.1 illustrates the computational logic of an RNN at three adjacent time steps. At\nany time step \ud835\udc61, the computation of the hidden state can be treated as: (i) concatenating the\ninputX\ud835\udc61at the current time step \ud835\udc61and the hidden state H\ud835\udc61\u00001at the previous time step \ud835\udc61\u00001;\n(ii)feedingtheconcatenationresultintoafullyconnectedlayerwiththeactivationfunction\n\ud835\udf19. Theoutputofsuchafullyconnectedlayeristhehiddenstate H\ud835\udc61ofthecurrenttimestep\n\ud835\udc61. In this case, the model parameters are the concatenation of WxhandWhh, and a bias\nofbh, all from (9.4.5 ). The hidden state of the current time step \ud835\udc61,H\ud835\udc61, will participate in\ncomputing the hidden state H\ud835\udc61\u00b81of the next time step \ud835\udc61\u00b81. What is more, H\ud835\udc61will also be\nfed into the fully connected output layer to compute the output O\ud835\udc61of the current time step\n\ud835\udc61.\ntFig. 9.4.1 An RNN with a hidden state.\nWejustmentionedthatthecalculationof X\ud835\udc61Wxh\u00b8H\ud835\udc61\u00001Whhforthehiddenstateisequiv-\nalent to matrix multiplication of the concatenation of X\ud835\udc61andH\ud835\udc61\u00001and the concatenation\nofWxhandWhh. Though this can be proven mathematically, in the following we just use", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1c5711d-70d8-47fc-a118-21cd5572200a": {"__data__": {"id_": "a1c5711d-70d8-47fc-a118-21cd5572200a", "embedding": null, "metadata": {"page_label": "351", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "907c869d-6af4-400b-a7e2-8902a75233ef", "node_type": "4", "metadata": {"page_label": "351", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4178e5a562f76249d5a1e5464c30676f5a02fe18c6ad7d092da43eb1c9d2ab69", "class_name": "RelatedNodeInfo"}}, "text": "351 Recurrent Neural Networks\na simple code snippet as a demonstration. To begin with, we define matrices X,W_xh,H,\nandW_hh, whose shapes are (3, 1), (1, 4), (3, 4), and (4, 4), respectively. Multiplying Xby\nW_xh, and HbyW_hh, and then adding these two products, we obtain a matrix of shape (3,\n4).\nX, W_xh =torch .randn( 3,1), torch .randn( 1,4)\nH, W_hh =torch .randn( 3,4), torch .randn( 4,4)\ntorch .matmul(X, W_xh) +torch .matmul(H, W_hh)\ntensor([[ 1.2526 ,0.0580 ,-3.3460 ,-0.2519 ],\n[-1.3064 ,1.4132 ,-0.1435 ,0.3482 ],\n[3.1495 ,0.8172 ,1.5167 ,-0.9038 ]])\nNow we concatenate the matrices XandHalong columns (axis 1), and the matrices W_xh\nandW_hhalong rows (axis 0). These two concatenations result in matrices of shape (3, 5)\nand of shape (5, 4), respectively. Multiplying these two concatenated matrices, we obtain\nthe same output matrix of shape (3, 4) as above.\ntorch .matmul(torch .cat((X, H), 1), torch .cat((W_xh, W_hh), 0))\ntensor([[ 1.2526 ,0.0580 ,-3.3460 ,-0.2519 ],\n[-1.3064 ,1.4132 ,-0.1435 ,0.3482 ],\n[3.1495 ,0.8172 ,1.5167 ,-0.9038 ]])\n9.4.3RNN-Based Character-LevelLanguageModels\nRecallthatforlanguagemodelingin Section9.3 , weaimtopredictthenexttokenbasedon\nthe current and past tokens; thus we shift the original sequence by one token as the targets\n(labels). Bengio etal.(2003)firstproposedtouseaneuralnetworkforlanguagemodeling.\nIn the following we illustrate how RNNs can be used to build a language model. Let the\nminibatch size be one, and the sequence of the text be \u201cmachine\u201d. To simplify training\nin subsequent sections, we tokenize text into characters rather than words and consider a\ncharacter-level language model .Fig. 9.4.2 demonstrates how to predict the next charac-\nter based on the current and previous characters via an RNN for character-level language\nmodeling.\nDuringthetrainingprocess,werunasoftmaxoperationontheoutputfromtheoutputlayer\nfor each time step, and then use the cross-entropy loss to compute the error between the\nmodeloutputandthetarget. Becauseoftherecurrentcomputationofthehiddenstateinthe\nhidden layer, the output, O3, of time step 3 in Fig. 9.4.2 is determined by the text sequence\n\u201cm\u201d, \u201ca\u201d, and \u201cc\u201d. Since the next character of the sequence in the training data is \u201ch\u201d, the\nlossoftimestep3willdependontheprobabilitydistributionofthenextcharactergenerated\nbased on the feature sequence \u201cm\u201d, \u201ca\u201d, \u201cc\u201d and the target \u201ch\u201d of this time step.\nIn practice, each token is represented by a \ud835\udc51-dimensional vector, and we use a batch size", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3c439cf-7e0c-4eb4-aff4-e0c7f5f0a1a0": {"__data__": {"id_": "e3c439cf-7e0c-4eb4-aff4-e0c7f5f0a1a0", "embedding": null, "metadata": {"page_label": "352", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "76565ab8-6625-49bc-b791-b8217bd0dde6", "node_type": "4", "metadata": {"page_label": "352", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a67f7146b6c22dbebd0d332d30e65fd79043cca08fb4c8a5677271c1ee21ff8b", "class_name": "RelatedNodeInfo"}}, "text": "352 Recurrent Neural Networks\ntFig. 9.4.2 A character-level language model based on the RNN. The input and target sequences are\n\u201cmachin\u201d and \u201cachine\u201d, respectively.\n141\ud835\udc5b> 1. Therefore, the input X\ud835\udc61at time step\ud835\udc61will be an\ud835\udc5b\u0002\ud835\udc51matrix, which is identical to\nwhat we discussed in Section 9.4.2 .\nIn the following sections, we will implement RNNs for character-level language mod-\nels.\n9.4.4Summary\nA neural network that uses recurrent computation for hidden states is called a recurrent\nneural network (RNN). The hidden state of an RNN can capture historical information of\nthe sequence up to the current time step. With recurrent computation, the number of RNN\nmodelparametersdoesnotgrowasthenumberoftimestepsincreases. Asforapplications,\nan RNN can be used to create character-level language models.\n9.4.5Exercises\n1.If we use an RNN to predict the next character in a text sequence, what is the required\ndimension for any output?\n2.Why can RNNs express the conditional probability of a token at some time step based\non all the previous tokens in the text sequence?\n3.What happens to the gradient if you backpropagate through a long sequence?\n4.What are some of the problems associated with the language model described in this\nsection?\nDiscussions141.\n9.5RecurrentNeuralNetworkImplementation\nfromScratch\nWe are now ready to implement an RNN from scratch. In particular, we will train this\nRNN to function as a character-level language model (see Section 9.4 ) and train it on a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b46c5485-e571-4c1f-9831-720b5c478a20": {"__data__": {"id_": "b46c5485-e571-4c1f-9831-720b5c478a20", "embedding": null, "metadata": {"page_label": "353", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e5ee014-1bc0-4736-a3bc-09a7810bc635", "node_type": "4", "metadata": {"page_label": "353", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "489f6ffc0983e4530be78081dc0b79905fd9b8a42a53256f2751f5e736e29c38", "class_name": "RelatedNodeInfo"}}, "text": "353 Recurrent Neural Network Implementation from Scratch\ncorpus consisting of the entire text of H. G. Wells\u2019 The Time Machine , following the data\nprocessing steps outlined in Section 9.2 . We start by loading the dataset.\n%matplotlib inline\nimport math\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n9.5.1RNNModel\nWe begin by defining a class to implement the RNN model ( Section 9.4.2 ). Note that the\nnumber of hidden units num_hiddens is a tunable hyperparameter.\nclass RNNScratch (d2l .Module): #@save\n\"\"\"The RNN model implemented from scratch.\"\"\"\ndef __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .W_xh =nn.Parameter(\ntorch .randn(num_inputs, num_hiddens) *sigma)\nself .W_hh =nn.Parameter(\ntorch .randn(num_hiddens, num_hiddens) *sigma)\nself .b_h =nn.Parameter(torch .zeros(num_hiddens))\nTheforward methodbelowdefineshowtocomputetheoutputandhiddenstateatanytime\nstep, given the current input and the state of the model at the previous time step. Note that\nthe RNN model loops through the outermost dimension of inputs, updating the hidden\nstate one time step at a time. The model here uses a tanhactivation function ( Section\n5.1.2).\n@d2l .add_to_class(RNNScratch) #@save\ndef forward (self , inputs, state =None ):\nifstate isNone :\n# Initial state with shape: (batch_size, num_hiddens)\nstate =torch .zeros((inputs .shape[ 1],self .num_hiddens),\ndevice =inputs .device)\nelse :\nstate, =state\noutputs =[]\nfor Xininputs: # Shape of inputs: (num_steps, batch_size, num_inputs)\nstate =torch .tanh(torch .matmul(X, self .W_xh) +\ntorch .matmul(state, self .W_hh) +self .b_h)\noutputs .append(state)\nreturn outputs, state\nWe can feed a minibatch of input sequences into an RNN model as follows.\nbatch_size, num_inputs, num_hiddens, num_steps =2,16,32,100\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd47e34d-7af4-4db6-ad16-cc7a182f4179": {"__data__": {"id_": "dd47e34d-7af4-4db6-ad16-cc7a182f4179", "embedding": null, "metadata": {"page_label": "354", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "683f59d0-02c0-4b64-9db2-44bf0799aaf6", "node_type": "4", "metadata": {"page_label": "354", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "78252e63c3248017ce2978400764f152793cb4bf435743057bbed051559a525b", "class_name": "RelatedNodeInfo"}}, "text": "354 Recurrent Neural Networks\n(continued from previous page)\nrnn =RNNScratch(num_inputs, num_hiddens)\nX=torch .ones((num_steps, batch_size, num_inputs))\noutputs, state =rnn(X)\nLet\u2019s check whether the RNN model produces results of the correct shapes to ensure that\nthe dimensionality of the hidden state remains unchanged.\ndef check_len (a, n): #@save\n\"\"\"Check the length of a list.\"\"\"\nassert len(a) ==n,f'list \\'s length {len(a)}!= expected length {n}'\ndef check_shape (a, shape): #@save\n\"\"\"Check the shape of a tensor.\"\"\"\nassert a.shape ==shape, \\\nf'tensor \\'s shape {a.shape }!= expected shape {shape }'\ncheck_len(outputs, num_steps)\ncheck_shape(outputs[ 0], (batch_size, num_hiddens))\ncheck_shape(state, (batch_size, num_hiddens))\n9.5.2RNN-BasedLanguageModel\nThe following RNNLMScratch class defines an RNN-based language model, where wepass\nin our RNN via the rnnargument of the __init__ method. When training language mod-\nels, the inputs and outputs are from the same vocabulary. Hence, they have the same di-\nmension, which is equal to the vocabulary size. Note that we use perplexity to evaluate the\nmodel. As discussed in Section 9.3.2 , this ensures that sequences of different length are\ncomparable.\nclass RNNLMScratch (d2l .Classifier): #@save\n\"\"\"The RNN-based language model implemented from scratch.\"\"\"\ndef __init__ (self , rnn, vocab_size, lr =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .init_params()\ndef init_params (self ):\nself .W_hq =nn.Parameter(\ntorch .randn(\nself .rnn.num_hiddens, self .vocab_size) *self .rnn.sigma)\nself .b_q =nn.Parameter(torch .zeros( self .vocab_size))\ndef training_step (self , batch):\nl=self .loss( self (*batch[: -1]), batch[ -1])\nself .plot( 'ppl', torch .exp(l), train =True )\nreturn l\ndef validation_step (self , batch):\nl=self .loss( self (*batch[: -1]), batch[ -1])\nself .plot( 'ppl', torch .exp(l), train =False )", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1887, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bb4ab3e-31fb-42f7-94d0-f6ecc748cbde": {"__data__": {"id_": "1bb4ab3e-31fb-42f7-94d0-f6ecc748cbde", "embedding": null, "metadata": {"page_label": "355", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6f7a0035-bd79-4070-8cbe-58855794f9d7", "node_type": "4", "metadata": {"page_label": "355", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3a9bf8d31faa8a5709b2d4569fff2b270a3ac5b4dd85d3eeec37f08a09f9109c", "class_name": "RelatedNodeInfo"}}, "text": "355 Recurrent Neural Network Implementation from Scratch\nOne-HotEncoding\nRecall that each token is represented by a numerical index indicating the position in the\nvocabularyofthecorrespondingword/character/wordpiece. Youmightbetemptedtobuild\na neural network with a single input node (at each time step), where the index could be fed\nin as a scalar value. This works when we are dealing with numerical inputs like price or\ntemperature, where any two values sufficiently close together should be treated similarly.\nBut this does not quite make sense. The 45thand46thwords in our vocabulary happen to\nbe \u201ctheir\u201d and \u201csaid\u201d, whose meanings are not remotely similar.\nWhen dealing with such categorical data, the most common strategy is to represent each\nitem by a one-hot encoding (recall from Section 4.1.1 ). A one-hot encoding is a vector\nwhose length is given by the size of the vocabulary \ud835\udc41, where all entries are set to 0, except\nfor the entry corresponding to our token, which is set to 1. For example, if the vocabulary\nhad five elements, then the one-hot vectors corresponding to indices 0 and 2 would be the\nfollowing.\nF.one_hot(torch .tensor([ 0,2]), 5)\ntensor([[ 1,0,0,0,0],\n[0,0,1,0,0]])\nThe minibatches that we sample at each iteration will take the shape (batch size, number\nof time steps). Once representing each input as a one-hot vector, we can think of each\nminibatch as a three-dimensional tensor, where the length along the third axis is given by\nthe vocabulary size ( len(vocab) ). We often transpose the input so that we will obtain an\noutput of shape (number of time steps, batch size, vocabulary size). This will allow us to\nloop more conveniently through the outermost dimension for updating hidden states of a\nminibatch, time step by time step (e.g., in the above forward method).\n@d2l .add_to_class(RNNLMScratch) #@save\ndef one_hot (self , X):\n# Output shape: (num_steps, batch_size, vocab_size)\nreturn F.one_hot(X .T,self .vocab_size) .type(torch .float32)\nTransformingRNN Outputs\nThe language model uses a fully connected output layer to transform RNN outputs into\ntoken predictions at each time step.\n@d2l .add_to_class(RNNLMScratch) #@save\ndef output_layer (self , rnn_outputs):\noutputs =[torch .matmul(H, self .W_hq) +self .b_q for Hinrnn_outputs]\nreturn torch .stack(outputs, 1)\n@d2l .add_to_class(RNNLMScratch) #@save\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "505c9f14-bf19-4050-b9bc-caf55feec1a8": {"__data__": {"id_": "505c9f14-bf19-4050-b9bc-caf55feec1a8", "embedding": null, "metadata": {"page_label": "356", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9aadcb44-f588-4a6f-b607-186acb4c376d", "node_type": "4", "metadata": {"page_label": "356", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "15f714b5c421463c1d97f2612c0fd8fabc5ad9d7611d973ddd96707a00a2d505", "class_name": "RelatedNodeInfo"}}, "text": "356 Recurrent Neural Networks\n(continued from previous page)\ndef forward (self , X, state =None ):\nembs =self .one_hot(X)\nrnn_outputs, _ =self .rnn(embs, state)\nreturn self .output_layer(rnn_outputs)\nLet\u2019scheckwhethertheforwardcomputationproducesoutputswiththecorrectshape.\nmodel =RNNLMScratch(rnn, num_inputs)\noutputs =model(torch .ones((batch_size, num_steps), dtype =torch .int64))\ncheck_shape(outputs, (batch_size, num_steps, num_inputs))\n9.5.3Gradient Clipping\nWhileyouarealreadyusedtothinkingofneuralnetworksas\u201cdeep\u201dinthesensethatmany\nlayers separate the input and output even within a single time step, the length of the se-\nquence introduces a new notion of depth. In addition to the passing through the network\nin the input-to-output direction, inputs at the first time step must pass through a chain of \ud835\udc47\nlayers along the time steps in order to influence the output of the model at the final time\nstep. Taking the backwards view, in each iteration, we backpropagate gradients through\ntime, resulting in a chain of matrix-products of length O\u00b9\ud835\udc47\u00ba. As mentioned in Section 5.4 ,\nthis can result in numerical instability, causing the gradients either to explode or vanish,\ndepending on the properties of the weight matrices.\nDealing with vanishing and exploding gradients is a fundamental problem when designing\nRNNs and has inspired some of the biggest advances in modern neural network architec-\ntures. Inthenextchapter,wewilltalkaboutspecializedarchitecturesthatweredesignedin\nhopes of mitigating the vanishing gradient problem. However, even modern RNNs often\nsufferfromexplodinggradients. Oneinelegantbutubiquitoussolutionistosimplyclipthe\ngradients forcing the resulting \u201cclipped\u201d gradients to take smaller values.\nGenerally speaking, when optimizing some objective by gradient descent, we iteratively\nupdatetheparameterofinterest,sayavector x,butpushingitinthedirectionofthenegative\ngradient g(instochasticgradientdescent,wecalculatethisgradientonarandomlysampled\nminibatch). Forexample,withlearningrate \ud835\udf02> 0,eachupdatetakestheform x x\u0000\ud835\udf02g.\nLet\u2019s further assume that the objective function \ud835\udc53is sufficiently smooth. Formally, we say\nthat the objective is Lipschitz continuous with constant \ud835\udc3f, meaning that for any xandy,\nwe have\nj\ud835\udc53\u00b9x\u00ba\u0000\ud835\udc53\u00b9y\u00baj\u0014\ud835\udc3fkx\u0000yk. (9.5.1)\nAs you can see, when we update the parameter vector by subtracting \ud835\udf02g, the change in\nthe value of the objective depends on the learning rate, the norm of the gradient and \ud835\udc3fas\nfollows:\nj\ud835\udc53\u00b9x\u00ba\u0000\ud835\udc53\u00b9x\u0000\ud835\udf02g\u00baj\u0014\ud835\udc3f\ud835\udf02kgk. (9.5.2)\nIn other words, the objective cannot change by more than \ud835\udc3f\ud835\udf02kgk. Having a small value for", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5530e02b-449e-4a6e-a1e6-6f43ff70fcdb": {"__data__": {"id_": "5530e02b-449e-4a6e-a1e6-6f43ff70fcdb", "embedding": null, "metadata": {"page_label": "357", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd890bb9-9be8-40fc-a6b8-f335e77bfe44", "node_type": "4", "metadata": {"page_label": "357", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b7c2a181e5c47d8f890aeb184cd4677e7271b8bf8d227ecb23a1d8331292e0c1", "class_name": "RelatedNodeInfo"}}, "text": "357 Recurrent Neural Network Implementation from Scratch\nthis upper bound might be viewed as good or bad. On the downside, we are limiting the\nspeed at which we can reduce the value of the objective. On the bright side, this limits by\njust how much we can go wrong in any one gradient step.\nWhen we say that gradients explode, we mean that kgkbecomes excessively large. In this\nworst case, we might do so much damage in a single gradient step that we could undo all\nof the progress made over the course of thousands of training iterations. When gradients\ncan be so large, neural network training often diverges, failing to reduce the value of the\nobjective. At other times, training eventually converges but is unstable owing to massive\nspikes in the loss.\nOne way to limit the size of \ud835\udc3f\ud835\udf02kgkis to shrink the learning rate \ud835\udf02to tiny values. This\nhas the advantage that we do not bias the updates. But what if we only rarelyget large\ngradients? This drastic move slows down our progress at all steps, just to deal with the rare\nexploding gradient events. A popular alternative is to adopt a gradient clipping heuristic\nprojecting the gradients gonto a ball of some given radius \ud835\udf03as follows:\ng min\u0012\n1,\ud835\udf03\nkgk\u0013\ng. (9.5.3)\nThisensuresthatthegradientnormneverexceeds \ud835\udf03andthattheupdatedgradientisentirely\naligned with the original direction of g. It also has the desirable side-effect of limiting the\ninfluence any given minibatch (and within it any given sample) can exert on the parameter\nvector. This bestows a certain degree of robustness to the model. To be clear, it is a hack.\nGradient clipping means that we are not always following the true gradient and it is hard to\nreason analytically about the possible side effects. However, it is a very useful hack, and is\nwidely adopted in RNN implementations in most deep learning frameworks.\nBelow we define a method to clip gradients, which is invoked by the fit_epoch method\nof the d2l.Trainer class (see Section 3.4 ). Note that when computing the gradient norm,\nwe are concatenating all model parameters, treating them as a single giant parameter vec-\ntor.\n@d2l .add_to_class(d2l .Trainer) #@save\ndef clip_gradients (self , grad_clip_val, model):\nparams =[pfor pinmodel .parameters() ifp.requires_grad]\nnorm =torch .sqrt( sum(torch .sum((p .grad **2))for pinparams))\nifnorm >grad_clip_val:\nfor param inparams:\nparam .grad[:] *=grad_clip_val /norm\n9.5.4Training\nUsingTheTimeMachine dataset( data),wetrainacharacter-levellanguagemodel( model)\nbased on the RNN ( rnn) implemented from scratch. Note that we first calculate the gra-\ndients, then clip them, and finally update the model parameters using the clipped gradi-\nents.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdc29e24-77ca-4049-ba5b-ea8d771a1e10": {"__data__": {"id_": "cdc29e24-77ca-4049-ba5b-ea8d771a1e10", "embedding": null, "metadata": {"page_label": "358", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a353861-0601-4f06-b39c-57dabd68afa0", "node_type": "4", "metadata": {"page_label": "358", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7310f7f6e9c3ddbe0b327cc47322deaf140b3f1d82c06f23425176e7fe534f5b", "class_name": "RelatedNodeInfo"}}, "text": "358 Recurrent Neural Networks\ndata =d2l.TimeMachine(batch_size =1024 , num_steps =32)\nrnn =RNNScratch(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =RNNLMScratch(rnn, vocab_size =len(data .vocab), lr =1)\ntrainer =d2l.Trainer(max_epochs =100, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\n9.5.5Decoding\nOnce a language model has been learned, we can use it not only to predict the next token\nbut to continue predicting each subsequent one, treating the previously predicted token as\nthoughitwerethenextintheinput. Sometimeswewilljustwanttogeneratetextasthough\nwewerestartingatthebeginningofadocument. However,itisoftenusefultoconditionthe\nlanguagemodelonauser-suppliedprefix. Forexample,ifweweredevelopinganautocom-\nplete feature for a search engine or to assist users in writing emails, we would want to feed\nin what they had written so far (the prefix), and then generate a likely continuation.\nThe following predict method generates a continuation, one character at a time, after\ningesting a user-provided prefix. When looping through the characters in prefix, we\nkeep passing the hidden state to the next time step but do not generate any output. This is\ncalled the warm-up period. After ingesting the prefix, we are now ready to begin emitting\nthe subsequent characters, each of which will be fed back into the model as the input at the\nnext time step.\n@d2l .add_to_class(RNNLMScratch) #@save\ndef predict (self , prefix, num_preds, vocab, device =None ):\nstate, outputs =None , [vocab[prefix[ 0]]]\nfor iinrange (len(prefix) +num_preds -1):\nX=torch .tensor([[outputs[ -1]]], device =device)\nembs =self .one_hot(X)\nrnn_outputs, state =self .rnn(embs, state)\nifi<len(prefix) -1:# Warm-up period\noutputs .append(vocab[prefix[i +1]])\nelse :# Predict num_preds steps\nY=self .output_layer(rnn_outputs)\noutputs .append( int(Y.argmax(axis =2).reshape( 1)))\nreturn ''.join([vocab .idx_to_token[i] for iinoutputs])\nInthefollowing,wespecifytheprefixandhaveitgenerate20additionalcharacters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "311d43ce-e70a-4574-a47e-cd4828caac9f": {"__data__": {"id_": "311d43ce-e70a-4574-a47e-cd4828caac9f", "embedding": null, "metadata": {"page_label": "359", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44cbd1a7-7346-4ff7-a99d-920665f9c0c4", "node_type": "4", "metadata": {"page_label": "359", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7eb06ab85d1a478346942b81e46366040401adb7105d301a25bd324e292f0673", "class_name": "RelatedNodeInfo"}}, "text": "359 Recurrent Neural Network Implementation from Scratch\nmodel .predict( 'it has ',20, data .vocab, d2l .try_gpu())\n'it has in the the the the '\nWhileimplementingtheaboveRNNmodelfromscratchisinstructive,itisnotconvenient.\nInthenextsection,wewillseehowtoleveragedeeplearningframeworkstowhipupRNNs\nusingstandardarchitectures, andtoreapperformancegainsbyrelyingonhighlyoptimized\nlibrary functions.\n9.5.6Summary\nWecantrainRNN-basedlanguagemodelstogeneratetextfollowingtheuser-providedtext\nprefix. A simple RNN language model consists of input encoding, RNN modeling, and\noutput generation. During training, gradient clipping can mitigate the problem of explod-\ning gradients but does not address the problem of vanishing gradients. In the experiment,\nweimplementedasimpleRNNlanguagemodelandtraineditwithgradientclippingonse-\nquences of text, tokenized at the character level. By conditioning on a prefix, we can use a\nlanguagemodeltogeneratelikelycontinuations,whichprovesusefulinmanyapplications,\ne.g., autocomplete features.\n9.5.7Exercises\n1.Doestheimplementedlanguagemodelpredictthenexttokenbasedonallthepasttokens\nup to the very first token in TheTimeMachine ?\n2.Which hyperparameter controls the length of history used for prediction?\n3.Show that one-hot encoding is equivalent to picking a different embedding for each\nobject.\n4.Adjust the hyperparameters (e.g., number of epochs, number of hidden units, number\nof time steps in a minibatch, and learning rate) to improve the perplexity. How low can\nyou go while sticking with this simple architecture?\n5.Replace one-hot encoding with learnable embeddings. Does this lead to better perfor-\nmance?\n6.Conductanexperimenttodeterminehowwellthislanguagemodeltrainedon TheTime\nMachine works on other books by H. G. Wells, e.g., The Warof theWorlds .\n7.Conduct another experiment to evaluate the perplexity of this model on books written\nby other authors.\n8.Modify the prediction method so as to use sampling rather than picking the most likely\nnext character.\n\u000fWhat happens?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0724728-ca01-49b6-8f87-17154bb6a9aa": {"__data__": {"id_": "a0724728-ca01-49b6-8f87-17154bb6a9aa", "embedding": null, "metadata": {"page_label": "360", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1dd2a4d5-7dfd-4099-bc2d-7415cc52d2ec", "node_type": "4", "metadata": {"page_label": "360", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9474bc3502d312baf82addca02327eedceca6e648277a3af3d629415c3f7903b", "class_name": "RelatedNodeInfo"}}, "text": "360 Recurrent Neural Networks\n142\u000fBiasthemodeltowardsmorelikelyoutputs,e.g.,bysamplingfrom \ud835\udc5e\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1\u00ba/\n\ud835\udc43\u00b9\ud835\udc65\ud835\udc61j\ud835\udc65\ud835\udc61\u00001,...,\ud835\udc65 1\u00ba\ud835\udefcfor\ud835\udefc> 1.\n9.Run the code in this section without clipping the gradient. What happens?\n10.Replace the activation function used in this section with ReLU and repeat the experi-\nments in this section. Do we still need gradient clipping? Why?\nDiscussions142.\n9.6ConciseImplementation of RecurrentNeural\nNetworks\nLike most of our from-scratch implementations, Section 9.5 was designed to provide in-\nsight into how each component works. But when you are using RNNs every day or writing\nproduction code, you will want to rely more on libraries that cut down on both implemen-\ntationtime(bysupplyinglibrarycodeforcommonmodelsandfunctions)andcomputation\ntime (by optimizing the heck out of these library implementations). This section will show\nyou how to implement the same language model more efficiently using the high-level API\nprovided by your deep learning framework. We begin, as before, by loading The Time\nMachine dataset.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n9.6.1Definingthe Model\nWe define the following class using the RNN implemented by high-level APIs.\nclass RNN(d2l .Module): #@save\n\"\"\"The RNN model implemented with high-level APIs.\"\"\"\ndef __init__ (self , num_inputs, num_hiddens):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .rnn =nn.RNN(num_inputs, num_hiddens)\ndef forward (self , inputs, H =None ):\nreturn self .rnn(inputs, H)\nInheriting from the RNNLMScratch class inSection 9.5 , the following RNNLMclass defines\na complete RNN-based language model. Note that we need to create a separate fully con-\nnected output layer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "99b9c3b0-666c-425f-9a85-60bb66f2b182": {"__data__": {"id_": "99b9c3b0-666c-425f-9a85-60bb66f2b182", "embedding": null, "metadata": {"page_label": "361", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "515e522d-cb4d-461b-8716-ebc1f88fc1fc", "node_type": "4", "metadata": {"page_label": "361", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0463a71f6bd6803e5a09a1b1c34a715fdc59cec21a2fd5e53fd1f0bf0975fd50", "class_name": "RelatedNodeInfo"}}, "text": "361 Concise Implementation of Recurrent Neural Networks\nclass RNNLM (d2l .RNNLMScratch): #@save\n\"\"\"The RNN-based language model implemented with high-level APIs.\"\"\"\ndef init_params (self ):\nself .linear =nn.LazyLinear( self .vocab_size)\ndef output_layer (self , hiddens):\nreturn self .linear(hiddens) .swapaxes( 0,1)\n9.6.2Trainingand Predicting\nBefore training the model, let\u2019s make a prediction with a model initialized with random\nweights. Given that we have not trained the network, it will generate nonsensical predic-\ntions.\ndata =d2l.TimeMachine(batch_size =1024 , num_steps =32)\nrnn =RNN(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =RNNLM(rnn, vocab_size =len(data .vocab), lr =1)\nmodel .predict( 'it has ',20, data .vocab)\n'it hasoadd dd dd dd dd dd '\nNext, we train our model, leveraging the high-level API.\ntrainer =d2l.Trainer(max_epochs =100, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\nComparedwith Section9.5 , thismodelachievescomparableperplexity, butrunsfasterdue\nto the optimized implementations. As before, we can generate predicted tokens following\nthe specified prefix string.\nmodel .predict( 'it has ',20, data .vocab, d2l .try_gpu())\n'it has and the trave the t '\n9.6.3Summary", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1225, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b2fc0d0-9a91-49f8-b668-f040b6869c91": {"__data__": {"id_": "9b2fc0d0-9a91-49f8-b668-f040b6869c91", "embedding": null, "metadata": {"page_label": "362", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7d9801e-b41b-4bb0-b55c-3b6f41645198", "node_type": "4", "metadata": {"page_label": "362", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3b67aa1848c5ba202535e8da12cc5d40e82d616ff1ac4de069a0e140b16c1acf", "class_name": "RelatedNodeInfo"}}, "text": "362 Recurrent Neural Networks\n143High-level APIs in deep learning frameworks provide implementations of standard RNNs.\nTheselibrarieshelpyoutoavoidwastingtimereimplementingstandardmodels. Moreover,\nframework implementations are often highly optimized, leading to significant (computa-\ntional) performance gains when compared with implementations from scratch.\n9.6.4Exercises\n1.Can you make the RNN model overfit using the high-level APIs?\n2.Implement the autoregressive model of Section 9.1 using an RNN.\nDiscussions143.\n9.7BackpropagationThroughTime\nIfyoucompletedtheexercisesin Section9.5 , youwouldhaveseenthatgradientclippingis\nvitalforpreventingtheoccasionalmassivegradientsfromdestabilizingtraining. Wehinted\nthat the exploding gradients stem from backpropagating across long sequences. Before in-\ntroducing a slew of modern RNN architectures, let\u2019s take a closer look at how backprop-\nagationworks in sequence models in mathematical detail. Hopefully, this discussion will\nbring some precision to the notion of vanishing andexploding gradients. If you recall our\ndiscussion of forward and backward propagation through computational graphs when we\nintroduced MLPs in Section 5.3 , then forward propagation in RNNs should be relatively\nstraightforward. Applying backpropagation in RNNs is called backpropagation through\ntime(Werbos, 1990 ). This procedure requires us to expand (or unroll) the computational\ngraph of an RNN one time step at a time. The unrolled RNN is essentially a feedforward\nneural network with the special property that the same parameters are repeated throughout\nthe unrolled network, appearing at each time step. Then, just as in any feedforward neural\nnetwork, we can apply the chain rule, backpropagating gradients through the unrolled net.\nThe gradient with respect to each parameter must be summed across all places that the pa-\nrameteroccursintheunrollednet. Handlingsuchweighttyingshouldbefamiliarfromour\nchapters on convolutional neural networks.\nComplications arise because sequences can be rather long. It is not unusual to work with\ntext sequences consisting of over a thousand tokens. Note that this poses problems both\nfrom a computational (too much memory) and optimization (numerical instability) stand-\npoint. Input from the first step passes through over 1000 matrix products before arriving\nat the output, and another 1000 matrix products are required to compute the gradient. We\nnow analyze what can go wrong and how to address it in practice.\n9.7.1Analysisof Gradients in RNNs\nWe start with a simplified model of how an RNN works. This model ignores details about\nthespecificsofthehiddenstateandhowitisupdated. Themathematicalnotationheredoes", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1c82237-8bc1-4c64-ba2f-b52ec7c3c68c": {"__data__": {"id_": "d1c82237-8bc1-4c64-ba2f-b52ec7c3c68c", "embedding": null, "metadata": {"page_label": "363", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad9de203-421b-4667-8507-5dbde850858c", "node_type": "4", "metadata": {"page_label": "363", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9e6e1ba7882d331f561cb612c9ebbdb3747c0dde270c26aef033aa66c62584ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "24f2c4bf-d5f7-4c72-817d-9ac5d97ac74b", "node_type": "1", "metadata": {}, "hash": "3725b4c82ab9c0b796f6355b82187d1b983e16e466f6e6487896b82644408114", "class_name": "RelatedNodeInfo"}}, "text": "363 Backpropagation Through Time\nnotexplicitlydistinguishscalars, vectors, andmatrices. Wearejusttryingtodevelopsome\nintuition. In this simplified model, we denote \u210e\ud835\udc61as the hidden state, \ud835\udc65\ud835\udc61as input, and \ud835\udc5c\ud835\udc61as\noutput at time step \ud835\udc61. Recall our discussions in Section 9.4.2 that the input and the hidden\nstatecanbeconcatenatedbeforebeingmultipliedbyoneweightvariableinthehiddenlayer.\nThus, we use \ud835\udc64hand\ud835\udc64oto indicate the weights of the hidden layer and the output layer,\nrespectively. As a result, the hidden states and outputs at each time step are\n\u210e\ud835\udc61=\ud835\udc53\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61\u00001,\ud835\udc64h\u00ba,\n\ud835\udc5c\ud835\udc61=\ud835\udc54\u00b9\u210e\ud835\udc61,\ud835\udc64o\u00ba,(9.7.1)\nwhere\ud835\udc53and\ud835\udc54are transformations of the hidden layer and the output layer, respectively.\nHence, we have a chain of values f...,\u00b9\ud835\udc65\ud835\udc61\u00001,\u210e\ud835\udc61\u00001,\ud835\udc5c\ud835\udc61\u00001\u00ba,\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61,\ud835\udc5c\ud835\udc61\u00ba,...gthat depend on\neach other via recurrent computation. The forward propagation is fairly straightforward.\nAll we need is to loop through the \u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61,\ud835\udc5c\ud835\udc61\u00batriples one time step at a time. The discrep-\nancy between output \ud835\udc5c\ud835\udc61and the desired target \ud835\udc66\ud835\udc61is then evaluated by an objective function\nacross all the \ud835\udc47time steps as\n\ud835\udc3f\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc47,\ud835\udc661,...,\ud835\udc66\ud835\udc47,\ud835\udc64h,\ud835\udc64o\u00ba=1\n\ud835\udc47\ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udc59\u00b9\ud835\udc66\ud835\udc61,\ud835\udc5c\ud835\udc61\u00ba. (9.7.2)\nFor backpropagation, matters are a bit trickier, especially when we compute the gradients\nwith regard to the parameters \ud835\udc64hof the objective function \ud835\udc3f. To be specific, by the chain\nrule,\n\ud835\udf15\ud835\udc3f\n\ud835\udf15\ud835\udc64h=1\n\ud835\udc47\ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udf15\ud835\udc59\u00b9\ud835\udc66\ud835\udc61,\ud835\udc5c\ud835\udc61\u00ba\n\ud835\udf15\ud835\udc64h\n=1\n\ud835\udc47\ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udf15\ud835\udc59\u00b9\ud835\udc66\ud835\udc61,\ud835\udc5c\ud835\udc61\u00ba\n\ud835\udf15\ud835\udc5c\ud835\udc61\ud835\udf15\ud835\udc54\u00b9\u210e\ud835\udc61,\ud835\udc64o\u00ba\n\ud835\udf15\u210e\ud835\udc61\ud835\udf15\u210e\ud835\udc61\n\ud835\udf15\ud835\udc64h.(9.7.3)\nThe first and the second factors of the product in (9.7.3 )are easy to compute. The third\nfactor\ud835\udf15\u210e\ud835\udc61\u009d\ud835\udf15\ud835\udc64his where things get tricky, since we need to recurrently compute the effect\nof the parameter \ud835\udc64hon\u210e\ud835\udc61. According to the recurrent computation in (9.7.1 ),\u210e\ud835\udc61depends\non both\u210e\ud835\udc61\u00001and\ud835\udc64h, where computation of \u210e\ud835\udc61\u00001also depends on \ud835\udc64h. Thus, evaluating the\ntotal derivate of \u210e\ud835\udc61with respect to \ud835\udc64husing the chain rule yields\n\ud835\udf15\u210e\ud835\udc61\n\ud835\udf15\ud835\udc64h=\ud835\udf15\ud835\udc53\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61\u00001,\ud835\udc64h\u00ba\n\ud835\udf15\ud835\udc64h\u00b8\ud835\udf15\ud835\udc53\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61\u00001,\ud835\udc64h\u00ba\n\ud835\udf15\u210e\ud835\udc61\u00001\ud835\udf15\u210e\ud835\udc61\u00001\n\ud835\udf15\ud835\udc64h.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1851, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24f2c4bf-d5f7-4c72-817d-9ac5d97ac74b": {"__data__": {"id_": "24f2c4bf-d5f7-4c72-817d-9ac5d97ac74b", "embedding": null, "metadata": {"page_label": "363", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad9de203-421b-4667-8507-5dbde850858c", "node_type": "4", "metadata": {"page_label": "363", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9e6e1ba7882d331f561cb612c9ebbdb3747c0dde270c26aef033aa66c62584ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1c82237-8bc1-4c64-ba2f-b52ec7c3c68c", "node_type": "1", "metadata": {"page_label": "363", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "048335e0499ee64bacbff993d274c0db3bb3588d116a36fc2c9859c28e37af15", "class_name": "RelatedNodeInfo"}}, "text": "According to the recurrent computation in (9.7.1 ),\u210e\ud835\udc61depends\non both\u210e\ud835\udc61\u00001and\ud835\udc64h, where computation of \u210e\ud835\udc61\u00001also depends on \ud835\udc64h. Thus, evaluating the\ntotal derivate of \u210e\ud835\udc61with respect to \ud835\udc64husing the chain rule yields\n\ud835\udf15\u210e\ud835\udc61\n\ud835\udf15\ud835\udc64h=\ud835\udf15\ud835\udc53\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61\u00001,\ud835\udc64h\u00ba\n\ud835\udf15\ud835\udc64h\u00b8\ud835\udf15\ud835\udc53\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61\u00001,\ud835\udc64h\u00ba\n\ud835\udf15\u210e\ud835\udc61\u00001\ud835\udf15\u210e\ud835\udc61\u00001\n\ud835\udf15\ud835\udc64h. (9.7.4)\nTo derive the above gradient, assume that we have three sequences f\ud835\udc4e\ud835\udc61g,f\ud835\udc4f\ud835\udc61g,f\ud835\udc50\ud835\udc61gsatisfy-\ning\ud835\udc4e0=0and\ud835\udc4e\ud835\udc61=\ud835\udc4f\ud835\udc61\u00b8\ud835\udc50\ud835\udc61\ud835\udc4e\ud835\udc61\u00001for\ud835\udc61=1,2,.... Then for\ud835\udc61\u00151, it is easy to show\n\ud835\udc4e\ud835\udc61=\ud835\udc4f\ud835\udc61\u00b8\ud835\udc61\u00001\u00d5\n\ud835\udc56=1\u00a9\u00ad\n\u00ab\ud835\udc61\u00d6\n\ud835\udc57=\ud835\udc56\u00b81\ud835\udc50\ud835\udc57\u00aa\u00ae\n\u00ac\ud835\udc4f\ud835\udc56. (9.7.5)", "mimetype": "text/plain", "start_char_idx": 1583, "end_char_idx": 2060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2c410c9-0a82-4fda-a12b-0ef0595bf471": {"__data__": {"id_": "a2c410c9-0a82-4fda-a12b-0ef0595bf471", "embedding": null, "metadata": {"page_label": "364", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed82fd56-ca7e-43e4-9ab3-9c44657e6ea2", "node_type": "4", "metadata": {"page_label": "364", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "752ee09434f0b71f314c7fc4b1b13c84b7acd1b6c9be1e08027da44eaac61b06", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8284f43f-fb3d-453c-9bea-c7b78cb9064a", "node_type": "1", "metadata": {}, "hash": "ea6ed4be73503d09dda219a28754a495f6925157145a26716c2f59a06e130e3b", "class_name": "RelatedNodeInfo"}}, "text": "364 Recurrent Neural Networks\nBy substituting \ud835\udc4e\ud835\udc61,\ud835\udc4f\ud835\udc61, and\ud835\udc50\ud835\udc61according to\n\ud835\udc4e\ud835\udc61=\ud835\udf15\u210e\ud835\udc61\n\ud835\udf15\ud835\udc64h,\n\ud835\udc4f\ud835\udc61=\ud835\udf15\ud835\udc53\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61\u00001,\ud835\udc64h\u00ba\n\ud835\udf15\ud835\udc64h,\n\ud835\udc50\ud835\udc61=\ud835\udf15\ud835\udc53\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61\u00001,\ud835\udc64h\u00ba\n\ud835\udf15\u210e\ud835\udc61\u00001,(9.7.6)\nthe gradient computation in (9.7.4 )satisfies\ud835\udc4e\ud835\udc61=\ud835\udc4f\ud835\udc61\u00b8\ud835\udc50\ud835\udc61\ud835\udc4e\ud835\udc61\u00001. Thus, per (9.7.5 ), we can\nremove the recurrent computation in (9.7.4 )with\n\ud835\udf15\u210e\ud835\udc61\n\ud835\udf15\ud835\udc64h=\ud835\udf15\ud835\udc53\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61\u00001,\ud835\udc64h\u00ba\n\ud835\udf15\ud835\udc64h\u00b8\ud835\udc61\u00001\u00d5\n\ud835\udc56=1\u00a9\u00ad\n\u00ab\ud835\udc61\u00d6\n\ud835\udc57=\ud835\udc56\u00b81\ud835\udf15\ud835\udc53\u00b9\ud835\udc65\ud835\udc57,\u210e\ud835\udc57\u00001,\ud835\udc64h\u00ba\n\ud835\udf15\u210e\ud835\udc57\u00001\u00aa\u00ae\n\u00ac\ud835\udf15\ud835\udc53\u00b9\ud835\udc65\ud835\udc56,\u210e\ud835\udc56\u00001,\ud835\udc64h\u00ba\n\ud835\udf15\ud835\udc64h.(9.7.7)\nWhile we can use the chain rule to compute \ud835\udf15\u210e\ud835\udc61\u009d\ud835\udf15\ud835\udc64hrecursively, this chain can get very\nlong whenever \ud835\udc61is large. Let\u2019s discuss a number of strategies for dealing with this prob-\nlem.\nFull Computation\nOne idea might be to compute the full sum in (9.7.7 ). However, this is very slow and\ngradients can blow up, since subtle changes in the initial conditions can potentially affect\ntheoutcomealot. Thatis,wecouldseethingssimilartothebutterflyeffect,whereminimal\nchanges in the initial conditions lead to disproportionate changes in the outcome. This is\ngenerally undesirable. After all, we are looking for robust estimators that generalize well.\nHence this strategy is almost never used in practice.\nTruncatingTime Steps\nAlternatively, we can truncate the sum in (9.7.7 )after\ud835\udf0fsteps. This is what we have been\ndiscussingsofar. Thisleadstoan approximation ofthetruegradient,simplybyterminating\nthe sum at\ud835\udf15\u210e\ud835\udc61\u0000\ud835\udf0f\u009d\ud835\udf15\ud835\udc64h. In practice this works quite well. It is what is commonly referred\nto as truncated backpropgation through time ( Jaeger, 2002 ). One of the consequences of\nthis is that the model focuses primarily on short-term influence rather than long-term con-\nsequences. Thisisactually desirable ,sinceitbiasestheestimatetowardssimplerandmore\nstable models.\nRandomizedTruncation\nLast, we can replace \ud835\udf15\u210e\ud835\udc61\u009d\ud835\udf15\ud835\udc64hby a random variable which is correct in expectation but\ntruncates the sequence. This is achieved by using a sequence of \ud835\udf09\ud835\udc61with predefined 0\u0014\n\ud835\udf0b\ud835\udc61\u00141, where\ud835\udc43\u00b9\ud835\udf09\ud835\udc61=0\u00ba=1\u0000\ud835\udf0b\ud835\udc61and\ud835\udc43\u00b9\ud835\udf09\ud835\udc61=\ud835\udf0b\u00001\n\ud835\udc61\u00ba=\ud835\udf0b\ud835\udc61, thus\ud835\udc38\u00bb\ud835\udf09\ud835\udc61\u00bc=1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1872, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8284f43f-fb3d-453c-9bea-c7b78cb9064a": {"__data__": {"id_": "8284f43f-fb3d-453c-9bea-c7b78cb9064a", "embedding": null, "metadata": {"page_label": "364", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed82fd56-ca7e-43e4-9ab3-9c44657e6ea2", "node_type": "4", "metadata": {"page_label": "364", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "752ee09434f0b71f314c7fc4b1b13c84b7acd1b6c9be1e08027da44eaac61b06", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2c410c9-0a82-4fda-a12b-0ef0595bf471", "node_type": "1", "metadata": {"page_label": "364", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "54066395b17736f57785e348c15e37dd3874d0c5ade5848bcbdf429a8b80f560", "class_name": "RelatedNodeInfo"}}, "text": "One of the consequences of\nthis is that the model focuses primarily on short-term influence rather than long-term con-\nsequences. Thisisactually desirable ,sinceitbiasestheestimatetowardssimplerandmore\nstable models.\nRandomizedTruncation\nLast, we can replace \ud835\udf15\u210e\ud835\udc61\u009d\ud835\udf15\ud835\udc64hby a random variable which is correct in expectation but\ntruncates the sequence. This is achieved by using a sequence of \ud835\udf09\ud835\udc61with predefined 0\u0014\n\ud835\udf0b\ud835\udc61\u00141, where\ud835\udc43\u00b9\ud835\udf09\ud835\udc61=0\u00ba=1\u0000\ud835\udf0b\ud835\udc61and\ud835\udc43\u00b9\ud835\udf09\ud835\udc61=\ud835\udf0b\u00001\n\ud835\udc61\u00ba=\ud835\udf0b\ud835\udc61, thus\ud835\udc38\u00bb\ud835\udf09\ud835\udc61\u00bc=1. We use this to\nreplace the gradient \ud835\udf15\u210e\ud835\udc61\u009d\ud835\udf15\ud835\udc64hin(9.7.4 )with\n\ud835\udc67\ud835\udc61=\ud835\udf15\ud835\udc53\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61\u00001,\ud835\udc64h\u00ba\n\ud835\udf15\ud835\udc64h\u00b8\ud835\udf09\ud835\udc61\ud835\udf15\ud835\udc53\u00b9\ud835\udc65\ud835\udc61,\u210e\ud835\udc61\u00001,\ud835\udc64h\u00ba\n\ud835\udf15\u210e\ud835\udc61\u00001\ud835\udf15\u210e\ud835\udc61\u00001\n\ud835\udf15\ud835\udc64h. (9.7.8)", "mimetype": "text/plain", "start_char_idx": 1410, "end_char_idx": 1993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54467f4d-d8be-46a1-b2be-a628f373b618": {"__data__": {"id_": "54467f4d-d8be-46a1-b2be-a628f373b618", "embedding": null, "metadata": {"page_label": "365", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0491b36f-d847-4dd7-a9c3-58a8f2f61a89", "node_type": "4", "metadata": {"page_label": "365", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "be769dec39e5f8467245ba3bf2580f27cd03255f3fc2e782866425edd1301797", "class_name": "RelatedNodeInfo"}}, "text": "365 Backpropagation Through Time\nIt follows from the definition of \ud835\udf09\ud835\udc61that\ud835\udc38\u00bb\ud835\udc67\ud835\udc61\u00bc=\ud835\udf15\u210e\ud835\udc61\u009d\ud835\udf15\ud835\udc64h. Whenever\ud835\udf09\ud835\udc61=0the recurrent\ncomputation terminates at that time step \ud835\udc61. This leads to a weighted sum of sequences of\nvarying lengths, where long sequences are rare but appropriately overweighted. This idea\nwas proposed by Tallec and Ollivier ( 2017).\nComparingStrategies\ntFig. 9.7.1 Comparing strategies for computing gradients in RNNs. From top to bottom: randomized\ntruncation, regular truncation, and full computation.\nFig.9.7.1 illustratesthethreestrategieswhenanalyzingthefirstfewcharactersof TheTime\nMachine using backpropagation through time for RNNs:\n\u000fThefirstrowistherandomizedtruncationthatpartitionsthetextintosegmentsofvarying\nlengths.\n\u000fThe second row is the regular truncation that breaks the text into subsequences of the\nsame length. This is what we have been doing in RNN experiments.\n\u000fThe third row is the full backpropagation through time that leads to a computationally\ninfeasible expression.\nUnfortunately, while appealing in theory, randomized truncation does not work much bet-\nter than regular truncation, most likely due to a number of factors. First, the effect of an\nobservation after a number of backpropagation steps into the past is quite sufficient to cap-\nture dependencies in practice. Second, the increased variance counteracts the fact that the\ngradient is more accurate with more steps. Third, we actually wantmodels that have only\nashortrangeofinteractions. Hence, regularlytruncatedbackpropagationthroughtimehas\na slight regularizing effect that can be desirable.\n9.7.2BackpropagationThroughTimein Detail\nAfterdiscussingthegeneralprinciple,let\u2019sdiscussbackpropagationthroughtimeindetail.\nIncontrasttotheanalysisin Section9.7.1 ,inthefollowingwewillshowhowtocomputethe\ngradients of the objective function with respect to all the decomposed model parameters.\nTo keep things simple, we consider an RNN without bias parameters, whose activation\nfunction in the hidden layer uses the identity mapping ( \ud835\udf19\u00b9\ud835\udc65\u00ba=\ud835\udc65). For time step \ud835\udc61, let\nthe single example input and the target be x\ud835\udc612R\ud835\udc51and\ud835\udc66\ud835\udc61, respectively. The hidden state\nh\ud835\udc612R\u210eand the output o\ud835\udc612R\ud835\udc5eare computed as\nh\ud835\udc61=Whxx\ud835\udc61\u00b8Whhh\ud835\udc61\u00001,\no\ud835\udc61=Wqhh\ud835\udc61,(9.7.9)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e11baa39-e7b3-48f9-a577-a30b95ff3bb6": {"__data__": {"id_": "e11baa39-e7b3-48f9-a577-a30b95ff3bb6", "embedding": null, "metadata": {"page_label": "366", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51d5fa42-8f2c-491d-8e22-1735a0e5455e", "node_type": "4", "metadata": {"page_label": "366", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0ca08223fec45706eefcc607d600630952cfeb11c038424bfc6d7bf3097530b6", "class_name": "RelatedNodeInfo"}}, "text": "366 Recurrent Neural Networks\nwhere Whx2R\u210e\u0002\ud835\udc51,Whh2R\u210e\u0002\u210e, andWqh2R\ud835\udc5e\u0002\u210eare the weight parameters. Denote\nby\ud835\udc59\u00b9o\ud835\udc61,\ud835\udc66\ud835\udc61\u00bathe loss at time step \ud835\udc61. Our objective function, the loss over \ud835\udc47time steps from\nthe beginning of the sequence is thus\n\ud835\udc3f=1\n\ud835\udc47\ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udc59\u00b9o\ud835\udc61,\ud835\udc66\ud835\udc61\u00ba. (9.7.10)\nIn order to visualize the dependencies among model variables and parameters during com-\nputation of the RNN, we can draw a computational graph for the model, as shown in Fig.\n9.7.2. For example, the computation of the hidden states of time step 3, h3, depends on\nthe model parameters WhxandWhh, the hidden state of the previous time step h2, and the\ninput of the current time step x3.\ntFig. 9.7.2 Computational graph showing dependencies for an RNN model with three time steps.\nBoxes represent variables (not shaded) or parameters (shaded) and circles represent\noperators.\nAs just mentioned, the model parameters in Fig. 9.7.2 areWhx,Whh, andWqh. Gen-\nerally, training this model requires gradient computation with respect to these parameters\n\ud835\udf15\ud835\udc3f\u009d\ud835\udf15Whx,\ud835\udf15\ud835\udc3f\u009d\ud835\udf15Whh,and\ud835\udf15\ud835\udc3f\u009d\ud835\udf15Wqh. Accordingtothedependenciesin Fig.9.7.2 ,wecan\ntraverse in the opposite direction of the arrows to calculate and store the gradients in turn.\nTo flexibly express the multiplication of matrices, vectors, and scalars of different shapes\nin the chain rule, we continue to use the prod operator as described in Section 5.3 .\nFirst of all, differentiating the objective function with respect to the model output at any\ntime step\ud835\udc61is fairly straightforward:\n\ud835\udf15\ud835\udc3f\n\ud835\udf15o\ud835\udc61=\ud835\udf15\ud835\udc59\u00b9o\ud835\udc61,\ud835\udc66\ud835\udc61\u00ba\n\ud835\udc47\u0001\ud835\udf15o\ud835\udc612R\ud835\udc5e. (9.7.11)\nNow we can calculate the gradient of the objective with respect to the parameter Wqhin\nthe output layer: \ud835\udf15\ud835\udc3f\u009d\ud835\udf15Wqh2R\ud835\udc5e\u0002\u210e. Based on Fig. 9.7.2 , the objective \ud835\udc3fdepends on Wqh\nviao1,...,o\ud835\udc47. Using the chain rule yields\n\ud835\udf15\ud835\udc3f\n\ud835\udf15Wqh=\ud835\udc47\u00d5\n\ud835\udc61=1prod\u0012\ud835\udf15\ud835\udc3f\n\ud835\udf15o\ud835\udc61,\ud835\udf15o\ud835\udc61\n\ud835\udf15Wqh\u0013\n=\ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udf15\ud835\udc3f\n\ud835\udf15o\ud835\udc61h>\n\ud835\udc61, (9.7.12)\nwhere\ud835\udf15\ud835\udc3f\u009d\ud835\udf15o\ud835\udc61is given by (9.7.11 ).\nNext, as shown in Fig. 9.7.2 , at the final time step \ud835\udc47, the objective function \ud835\udc3fdepends on\nthe hidden state h\ud835\udc47only via o\ud835\udc47. Therefore, we can easily find the gradient \ud835\udf15\ud835\udc3f\u009d\ud835\udf15h\ud835\udc472R\u210e", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2002, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6b76ca3-b943-4945-bf82-02c63b3a4e45": {"__data__": {"id_": "c6b76ca3-b943-4945-bf82-02c63b3a4e45", "embedding": null, "metadata": {"page_label": "367", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eae95560-c74c-4697-9694-e3ff2e326153", "node_type": "4", "metadata": {"page_label": "367", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ecf21ba94ffc040e22e7b710b31e7d67e2216bd81435d76621e44512362c75c0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e227e0c7-af64-45f5-94bc-65031b194d35", "node_type": "1", "metadata": {}, "hash": "37e98a7f7b2f4dbad8a559754b4312db0e2cf8e8ea9acf52eb3585aab8102705", "class_name": "RelatedNodeInfo"}}, "text": "367 Backpropagation Through Time\nusing the chain rule:\n\ud835\udf15\ud835\udc3f\n\ud835\udf15h\ud835\udc47=prod\u0012\ud835\udf15\ud835\udc3f\n\ud835\udf15o\ud835\udc47,\ud835\udf15o\ud835\udc47\n\ud835\udf15h\ud835\udc47\u0013\n=W>\nqh\ud835\udf15\ud835\udc3f\n\ud835\udf15o\ud835\udc47. (9.7.13)\nIt gets trickier for any time step \ud835\udc61 < \ud835\udc47, where the objective function \ud835\udc3fdepends on h\ud835\udc61via\nh\ud835\udc61\u00b81ando\ud835\udc61. According to the chain rule, the gradient of the hidden state \ud835\udf15\ud835\udc3f\u009d\ud835\udf15h\ud835\udc612R\u210eat\nany time step \ud835\udc61 <\ud835\udc47can be recurrently computed as:\n\ud835\udf15\ud835\udc3f\n\ud835\udf15h\ud835\udc61=prod\u0012\ud835\udf15\ud835\udc3f\n\ud835\udf15h\ud835\udc61\u00b81,\ud835\udf15h\ud835\udc61\u00b81\n\ud835\udf15h\ud835\udc61\u0013\n\u00b8prod\u0012\ud835\udf15\ud835\udc3f\n\ud835\udf15o\ud835\udc61,\ud835\udf15o\ud835\udc61\n\ud835\udf15h\ud835\udc61\u0013\n=W>\nhh\ud835\udf15\ud835\udc3f\n\ud835\udf15h\ud835\udc61\u00b81\u00b8W>\nqh\ud835\udf15\ud835\udc3f\n\ud835\udf15o\ud835\udc61.(9.7.14)\nForanalysis,expandingtherecurrentcomputationforanytimestep 1\u0014\ud835\udc61\u0014\ud835\udc47gives\n\ud835\udf15\ud835\udc3f\n\ud835\udf15h\ud835\udc61=\ud835\udc47\u00d5\n\ud835\udc56=\ud835\udc61\u0000W>\nhh\u0001\ud835\udc47\u0000\ud835\udc56W>\nqh\ud835\udf15\ud835\udc3f\n\ud835\udf15o\ud835\udc47\u00b8\ud835\udc61\u0000\ud835\udc56. (9.7.15)\nWe can see from (9.7.15 )that this simple linear example already exhibits some key prob-\nlems of long sequence models: it involves potentially very large powers of W>\nhh. In it,\neigenvalues smaller than 1 vanish and eigenvalues larger than 1 diverge. This is numer-\nically unstable, which manifests itself in the form of vanishing and exploding gradients.\nOnewaytoaddressthisistotruncatethetimestepsatacomputationallyconvenientsizeas\ndiscussedin Section9.7.1 . Inpractice,thistruncationcanalsobeeffectedbydetachingthe\ngradient after a given number of time steps. Later on, we will see how more sophisticated\nsequence models such as long short-term memory can alleviate this further.\nFinally,Fig. 9.7.2 shows that the objective function \ud835\udc3fdepends on model parameters Whx\nandWhhin the hidden layer via hidden states h1,...,h\ud835\udc47. To compute gradients with\nrespect to such parameters \ud835\udf15\ud835\udc3f\u009d\ud835\udf15Whx2R\u210e\u0002\ud835\udc51and\ud835\udf15\ud835\udc3f\u009d\ud835\udf15Whh2R\u210e\u0002\u210e, we apply the chain\nrule giving\n\ud835\udf15\ud835\udc3f\n\ud835\udf15Whx=\ud835\udc47\u00d5\n\ud835\udc61=1prod\u0012\ud835\udf15\ud835\udc3f\n\ud835\udf15h\ud835\udc61,\ud835\udf15h\ud835\udc61\n\ud835\udf15Whx\u0013\n=\ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udf15\ud835\udc3f\n\ud835\udf15h\ud835\udc61x>\n\ud835\udc61,\n\ud835\udf15\ud835\udc3f\n\ud835\udf15Whh=\ud835\udc47\u00d5\n\ud835\udc61=1prod\u0012\ud835\udf15\ud835\udc3f\n\ud835\udf15h\ud835\udc61,\ud835\udf15h\ud835\udc61\n\ud835\udf15Whh\u0013\n=\ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udf15\ud835\udc3f\n\ud835\udf15h\ud835\udc61h>\n\ud835\udc61\u00001,(9.7.16)\nwhere\ud835\udf15\ud835\udc3f\u009d\ud835\udf15h\ud835\udc61which is recurrently computed by (9.7.13 )and(9.7.14 )is the key quantity\nthat affects the numerical stability.\nSince backpropagation through time is the application of backpropagation in RNNs, as we\nhave explained in Section 5.3 , training RNNs alternates forward propagation with back-\npropagation through time. Moreover, backpropagation through time computes and stores\nthe above gradients in turn.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2008, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e227e0c7-af64-45f5-94bc-65031b194d35": {"__data__": {"id_": "e227e0c7-af64-45f5-94bc-65031b194d35", "embedding": null, "metadata": {"page_label": "367", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eae95560-c74c-4697-9694-e3ff2e326153", "node_type": "4", "metadata": {"page_label": "367", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ecf21ba94ffc040e22e7b710b31e7d67e2216bd81435d76621e44512362c75c0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6b76ca3-b943-4945-bf82-02c63b3a4e45", "node_type": "1", "metadata": {"page_label": "367", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2a2b96111e41b50ba8dfbb465ae454bb41d02b3ca48216cee92b8940c1e0143e", "class_name": "RelatedNodeInfo"}}, "text": "Since backpropagation through time is the application of backpropagation in RNNs, as we\nhave explained in Section 5.3 , training RNNs alternates forward propagation with back-\npropagation through time. Moreover, backpropagation through time computes and stores\nthe above gradients in turn. Specifically, stored intermediate values are reused to avoid du-\nplicate calculations, such as storing \ud835\udf15\ud835\udc3f\u009d\ud835\udf15h\ud835\udc61to be used in computation of both \ud835\udf15\ud835\udc3f\u009d\ud835\udf15Whx\nand\ud835\udf15\ud835\udc3f\u009d\ud835\udf15Whh.\n9.7.3Summary", "mimetype": "text/plain", "start_char_idx": 1719, "end_char_idx": 2184, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4663428-a7ad-41cc-be84-05daf9649fa5": {"__data__": {"id_": "c4663428-a7ad-41cc-be84-05daf9649fa5", "embedding": null, "metadata": {"page_label": "368", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ea044575-f8a7-4b4b-829d-9477ae6e577f", "node_type": "4", "metadata": {"page_label": "368", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2e358fe2a2fa4f6d0156520b0082ade824612fb36a7a406dd7eb96b93e9c5f79", "class_name": "RelatedNodeInfo"}}, "text": "368 Recurrent Neural Networks\n144Backpropagation through time is merely an application of backpropagation to sequence\nmodels with a hidden state. Truncation, such as regular or randomized, is needed for com-\nputational convenience and numerical stability. High powers of matrices can lead to diver-\ngent or vanishing eigenvalues. This manifests itself in the form of exploding or vanishing\ngradients. For efficient computation, intermediate values are cached during backpropaga-\ntion through time.\n9.7.4Exercises\n1.Assume that we have a symmetric matrix M2R\ud835\udc5b\u0002\ud835\udc5bwith eigenvalues \ud835\udf06\ud835\udc56whose cor-\nresponding eigenvectors are v\ud835\udc56(\ud835\udc56=1,...,\ud835\udc5b). Without loss of generality, assume that\nthey are ordered in the order j\ud835\udf06\ud835\udc56j\u0015j\ud835\udf06\ud835\udc56\u00b81j.\n1.Show that M\ud835\udc58has eigenvalues \ud835\udf06\ud835\udc58\n\ud835\udc56.\n2.Provethatforarandomvector x2R\ud835\udc5b,withhighprobability M\ud835\udc58xwillbeverymuch\naligned with the eigenvector v1ofM. Formalize this statement.\n3.What does the above result mean for gradients in RNNs?\n2.Besides gradient clipping, can you think of any other methods to cope with gradient\nexplosion in recurrent neural networks?\nDiscussions144.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f41c9bf9-fa20-41c6-b2e2-add1d5852427": {"__data__": {"id_": "f41c9bf9-fa20-41c6-b2e2-add1d5852427", "embedding": null, "metadata": {"page_label": "369", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15f7c676-5147-48af-aec6-20fad312d9d1", "node_type": "4", "metadata": {"page_label": "369", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ceb8eb9571fc5459bcaa1cf4941fc1a1418ffa4be54b5c6f2ae7835c7389da83", "class_name": "RelatedNodeInfo"}}, "text": "10 Modern Recurrent Neural Networks\nThe previous chapter introduced the key ideas behind recurrent neural networks (RNNs).\nHowever, just as with convolutional neural networks, there has been a tremendous amount\nof innovation in RNN architectures, culminating in several complex designs that have\nproven successful in practice. In particular, the most popular designs feature mechanisms\nfor mitigating the notorious numerical instability faced by RNNs, as typified by vanishing\nand exploding gradients. Recall that in Chapter 9 we dealt with exploding gradients by ap-\nplying a blunt gradient clipping heuristic. Despite the efficacy of this hack, it leaves open\nthe problem of vanishing gradients.\nInthischapter,weintroducethekeyideasbehindthemostsuccessfulRNNarchitecturesfor\nsequences, which stem from two papers. The first, Long Short-Term Memory (Hochreiter\nand Schmidhuber, 1997 ), introduces the memory cell , a unit of computation that replaces\ntraditional nodes in the hidden layer of a network. With these memory cells, networks\nare able to overcome difficulties with training encountered by earlier recurrent networks.\nIntuitively, the memory cell avoids the vanishing gradient problem by keeping values in\neach memory cell\u2019s internal state cascading along a recurrent edge with weight 1 across\nmanysuccessivetimesteps. Asetofmultiplicativegateshelpthenetworktodeterminenot\nonly the inputs to allow into the memory state, but when the content of the memory state\nshould influence the model\u2019s output.\nThe second paper, Bidirectional Recurrent Neural Networks (Schuster and Paliwal, 1997 ),\nintroducesanarchitectureinwhichinformationfromboththefuture(subsequenttimesteps)\nand the past (preceding time steps) are used to determine the output at any point in the se-\nquence. This is in contrast to previous networks, in which only past input can affect the\noutput. Bidirectional RNNs have become a mainstay for sequence labeling tasks in natu-\nral language processing, among a myriad of other tasks. Fortunately, the two innovations\narenotmutuallyexclusive,andhavebeensuccessfullycombinedforphonemeclassification\n(GravesandSchmidhuber,2005 )andhandwritingrecognition( Gravesetal.,2008).\nThefirstsectionsinthischapterwillexplaintheLSTMarchitecture,alighter-weightversion\ncalled the gated recurrent unit (GRU), the key ideas behind bidirectional RNNs and a brief\nexplanation of how RNN layers are stacked together to form deep RNNs. Subsequently,\nwe will explore the application of RNNs in sequence-to-sequence tasks, introducing ma-\nchine translation along with key ideas such as encoder\u2013decoder architectures and beam\nsearch.\n369", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84332a99-4369-4945-9569-4b3c5a4d9af5": {"__data__": {"id_": "84332a99-4369-4945-9569-4b3c5a4d9af5", "embedding": null, "metadata": {"page_label": "370", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84f5c419-ad8a-4b08-8346-075f0c1a199f", "node_type": "4", "metadata": {"page_label": "370", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2d97892ecd7e7ba0b26c289068589a080c00d0205435c977203be63c113b543a", "class_name": "RelatedNodeInfo"}}, "text": "370 Modern Recurrent Neural Networks\n10.1Long Short-TermMemory (LSTM)\nShortlyafterthefirstElman-styleRNNsweretrainedusingbackpropagation( Elman,1990 ),\nthe problems of learning long-term dependencies (owing to vanishing and exploding gra-\ndients) became salient, with Bengio and Hochreiter discussing the problem ( Bengioetal.,\n1994,Hochreiter et al., 2001). Hochreiter had articulated this problem as early as 1991 in\nhisMaster\u2019sthesis,althoughtheresultswerenotwidelyknownbecausethethesiswaswrit-\nteninGerman. Whilegradientclippinghelpswithexplodinggradients,handlingvanishing\ngradientsappearstorequireamoreelaboratesolution. Oneofthefirstandmostsuccessful\ntechniquesforaddressingvanishinggradientscameintheformofthelongshort-termmem-\nory (LSTM) model due to Hochreiter and Schmidhuber ( 1997). LSTMs resemble standard\nrecurrent neural networks but here each ordinary recurrent node is replaced by a memory\ncell. Each memory cell contains an internal state , i.e., a node with a self-connected re-\ncurrent edge of fixed weight 1, ensuring that the gradient can pass across many time steps\nwithout vanishing or exploding.\nThe term \u201clong short-term memory\u201d comes from the following intuition. Simple recurrent\nneuralnetworkshave long-termmemory intheformofweights. Theweightschangeslowly\nduring training, encoding general knowledge about the data. They also have short-term\nmemory in the form of ephemeral activations, which pass from each node to successive\nnodes. The LSTM model introduces an intermediate type of storage via the memory cell.\nA memory cell is a composite unit, built from simpler nodes in a specific connectivity\npattern, with the novel inclusion of multiplicative nodes.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n10.1.1Gated Memory Cell\nEach memory cell is equipped with an internal state and a number of multiplicative gates\nthat determine whether (i) a given input should impact the internal state (the input gate ),\n(ii) the internal state should be flushed to 0(theforgetgate ), and (iii) the internal state of a\ngiven neuron should be allowed to impact the cell\u2019s output (the outputgate).\nGatedHidden State\nThe key distinction between vanilla RNNs and LSTMs is that the latter support gating of\nthe hidden state. This means that we have dedicated mechanisms for when a hidden state\nshould be updated and also for when it should be reset. These mechanisms are learned and\ntheyaddresstheconcernslistedabove. Forinstance,ifthefirsttokenisofgreatimportance\nwe will learn not to update the hidden state after the first observation. Likewise, we will\nlearn to skip irrelevant temporary observations. Last, we will learn to reset the latent state\nwhenever needed. We discuss this in detail below.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d506a5b-812c-41c1-a911-662568a3c373": {"__data__": {"id_": "7d506a5b-812c-41c1-a911-662568a3c373", "embedding": null, "metadata": {"page_label": "371", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2cca7d5d-5e82-4eb3-99d4-b6e8293cb28e", "node_type": "4", "metadata": {"page_label": "371", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ad3a44ddf290d21219de8501c4e1d9b406a0aa155dbda9f2b16325d12fd32665", "class_name": "RelatedNodeInfo"}}, "text": "371 Long Short-Term Memory (LSTM)\nInputGate, ForgetGate, and Output Gate\nThe data feeding into the LSTM gates are the input at the current time step and the hidden\nstate of the previous time step, as illustrated in Fig. 10.1.1 . Three fully connected layers\nwithsigmoidactivationfunctionscomputethevaluesoftheinput, forget, andoutputgates.\nAs a result of the sigmoid activation, all values of the three gates are in the range of \u00b90,1\u00ba.\nAdditionally, we require an input node , typically computed with a tanhactivation func-\ntion. Intuitively, the input gate determines how much of the input node\u2019s value should be\naddedtothecurrentmemorycellinternalstate. The forgetgate determineswhethertokeep\nthe current value of the memory or flush it. And the output gate determines whether the\nmemory cell should influence the output at the current time step.\ntFig. 10.1.1 Computing the input gate, the forget gate, and the output gate in an LSTM model.\nMathematically, suppose that there are \u210ehidden units, the batch size is \ud835\udc5b, and the number\nof inputs is\ud835\udc51. Thus, the input is X\ud835\udc612R\ud835\udc5b\u0002\ud835\udc51and the hidden state of the previous time step\nisH\ud835\udc61\u000012R\ud835\udc5b\u0002\u210e. Correspondingly, the gates at time step \ud835\udc61are defined as follows: the input\ngate is I\ud835\udc612R\ud835\udc5b\u0002\u210e, the forget gate is F\ud835\udc612R\ud835\udc5b\u0002\u210e, and the output gate is O\ud835\udc612R\ud835\udc5b\u0002\u210e. They\nare calculated as follows:\nI\ud835\udc61=\ud835\udf0e\u00b9X\ud835\udc61Wxi\u00b8H\ud835\udc61\u00001Whi\u00b8bi\u00ba,\nF\ud835\udc61=\ud835\udf0e\u00b9X\ud835\udc61Wxf\u00b8H\ud835\udc61\u00001Whf\u00b8bf\u00ba,\nO\ud835\udc61=\ud835\udf0e\u00b9X\ud835\udc61Wxo\u00b8H\ud835\udc61\u00001Who\u00b8bo\u00ba,(10.1.1)\nwhere Wxi,Wxf,Wxo2R\ud835\udc51\u0002\u210eandWhi,Whf,Who2R\u210e\u0002\u210eare weight parameters and\nbi,bf,bo2R1\u0002\u210eare bias parameters. Note that broadcasting (see Section 2.1.4 ) is trig-\ngered during the summation. We use sigmoid functions (as introduced in Section 5.1 ) to\nmap the input values to the interval \u00b90,1\u00ba.\nInputNode\nNextwedesignthememorycell. Sincewehavenotspecifiedtheactionofthevariousgates\nyet, we first introduce the input node \u02dcC\ud835\udc612R\ud835\udc5b\u0002\u210e. Its computation is similar to that of the\nthree gates described above, but uses a tanhfunction with a value range for \u00b9\u00001,1\u00baas the\nactivation function. This leads to the following equation at time step \ud835\udc61:\n\u02dcC\ud835\udc61=tanh\u00b9X\ud835\udc61Wxc\u00b8H\ud835\udc61\u00001Whc\u00b8bc\u00ba, (10.1.2)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29f277f3-6e38-4630-b838-b4325e8c189a": {"__data__": {"id_": "29f277f3-6e38-4630-b838-b4325e8c189a", "embedding": null, "metadata": {"page_label": "372", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2b8b953-f39b-4755-bfd3-4368039bedc9", "node_type": "4", "metadata": {"page_label": "372", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "344325ba2401ef946f3b57b8163cbeb07becc295bb13b6eba222b775bd378129", "class_name": "RelatedNodeInfo"}}, "text": "372 Modern Recurrent Neural Networks\nwhere Wxc2R\ud835\udc51\u0002\u210eandWhc2R\u210e\u0002\u210eare weight parameters and bc2R1\u0002\u210eis a bias\nparameter.\nA quick illustration of the input node is shown in Fig. 10.1.2 .\ntFig. 10.1.2 Computing the input node in an LSTM model.\nMemory Cell Internal State\nIn LSTMs, the input gate I\ud835\udc61governs how much we take new data into account via \u02dcC\ud835\udc61and\ntheforgetgate F\ud835\udc61addresseshowmuchoftheoldcellinternalstate C\ud835\udc61\u000012R\ud835\udc5b\u0002\u210eweretain.\nUsing the Hadamard (elementwise) product operator \fwe arrive at the following update\nequation:\nC\ud835\udc61=F\ud835\udc61\fC\ud835\udc61\u00001\u00b8I\ud835\udc61\f\u02dcC\ud835\udc61. (10.1.3)\nIf the forget gate is always 1 and the input gate is always 0, the memory cell internal state\nC\ud835\udc61\u00001will remain constant forever, passing unchanged to each subsequent time step. How-\never, input gates and forget gates give the model the flexibility of being able to learn when\nto keep this value unchanged and when to perturb it in response to subsequent inputs. In\npractice, this design alleviates the vanishing gradient problem, resulting in models that are\nmuch easier to train, especially when facing datasets with long sequence lengths.\nWe thus arrive at the flow diagram in Fig. 10.1.3 .\nHidden State\nLast,weneedtodefinehowtocomputetheoutputofthememorycell, i.e., thehiddenstate\nH\ud835\udc612R\ud835\udc5b\u0002\u210e, as seen by other layers. This is where the output gate comes into play. In\nLSTMs, we first apply tanhto the memory cell internal state and then apply another point-\nwise multiplication, this time with the output gate. This ensures that the values of H\ud835\udc61are\nalways in the interval \u00b9\u00001,1\u00ba:\nH\ud835\udc61=O\ud835\udc61\ftanh\u00b9C\ud835\udc61\u00ba. (10.1.4)\nWhenever the output gate is close to 1, we allow the memory cell internal state to impact\nthesubsequentlayersuninhibited, whereasforoutputgatevaluescloseto0, wepreventthe", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17ecd3c8-3de2-4018-be42-5316f92aedf9": {"__data__": {"id_": "17ecd3c8-3de2-4018-be42-5316f92aedf9", "embedding": null, "metadata": {"page_label": "373", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1ec923d1-3aae-400a-8605-4f388253f05c", "node_type": "4", "metadata": {"page_label": "373", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "02087869016e38ad98b8daa782d7b022234783be6e282b44d418acaf67781bdf", "class_name": "RelatedNodeInfo"}}, "text": "373 Long Short-Term Memory (LSTM)\ntFig. 10.1.3 Computing the memory cell internal state in an LSTM model.\ncurrent memory from impacting other layers of the network at the current time step. Note\nthat a memory cell can accrue information across many time steps without impacting the\nrest of the network (as long as the output gate takes values close to 0), and then suddenly\nimpact the network at a subsequent time step as soon as the output gate flips from values\ncloseto0tovaluescloseto1. Fig.10.1.4 hasagraphicalillustrationofthedataflow.\ntFig. 10.1.4 Computing the hidden state in an LSTM model.\n10.1.2Implementation fromScratch\nNow let\u2019s implement an LSTM from scratch. As same as the experiments in Section 9.5 ,\nwe first load The TimeMachine dataset.\nInitializingModel Parameters\nNext, we need to define and initialize the model parameters. As previously, the hyperpa-\nrameter num_hiddens dictates the number of hidden units. We initialize weights following\na Gaussian distribution with 0.01 standard deviation, and we set the biases to 0.\nclass LSTMScratch (d2l .Module):\ndef __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd5ce960-0a2d-46ab-a3ad-94a279875110": {"__data__": {"id_": "fd5ce960-0a2d-46ab-a3ad-94a279875110", "embedding": null, "metadata": {"page_label": "374", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d09f6409-d387-4d39-a3d0-a5338ac8566e", "node_type": "4", "metadata": {"page_label": "374", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b281e93e86930d84cc86ff704062a9516cde58c18665a22f2c631b7ab9297c91", "class_name": "RelatedNodeInfo"}}, "text": "374 Modern Recurrent Neural Networks\n(continued from previous page)\nsuper ().__init__ ()\nself .save_hyperparameters()\ninit_weight =lambda *shape: nn .Parameter(torch .randn( *shape) *sigma)\ntriple =lambda : (init_weight(num_inputs, num_hiddens),\ninit_weight(num_hiddens, num_hiddens),\nnn.Parameter(torch .zeros(num_hiddens)))\nself .W_xi, self .W_hi, self .b_i =triple() # Input gate\nself .W_xf, self .W_hf, self .b_f =triple() # Forget gate\nself .W_xo, self .W_ho, self .b_o =triple() # Output gate\nself .W_xc, self .W_hc, self .b_c =triple() # Input node\nTheactualmodelisdefinedasdescribedabove,consistingofthreegatesandaninputnode.\nNote that only the hidden state is passed to the output layer.\n@d2l .add_to_class(LSTMScratch)\ndef forward (self , inputs, H_C =None ):\nifH_C isNone :\n# Initial state with shape: (batch_size, num_hiddens)\nH=torch .zeros((inputs .shape[ 1],self .num_hiddens),\ndevice =inputs .device)\nC=torch .zeros((inputs .shape[ 1],self .num_hiddens),\ndevice =inputs .device)\nelse :\nH, C =H_C\noutputs =[]\nfor Xininputs:\nI=torch .sigmoid(torch .matmul(X, self .W_xi) +\ntorch .matmul(H, self .W_hi) +self .b_i)\nF=torch .sigmoid(torch .matmul(X, self .W_xf) +\ntorch .matmul(H, self .W_hf) +self .b_f)\nO=torch .sigmoid(torch .matmul(X, self .W_xo) +\ntorch .matmul(H, self .W_ho) +self .b_o)\nC_tilde =torch .tanh(torch .matmul(X, self .W_xc) +\ntorch .matmul(H, self .W_hc) +self .b_c)\nC=F*C+I*C_tilde\nH=O*torch .tanh(C)\noutputs .append(H)\nreturn outputs, (H, C)\nTrainingand Prediction\nLet\u2019strainanLSTMmodelbyinstantiatingthe RNNLMScratch classfrom Section9.5 .\ndata =d2l.TimeMachine(batch_size =1024 , num_steps =32)\nlstm =LSTMScratch(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =d2l.RNNLMScratch(lstm, vocab_size =len(data .vocab), lr =4)\ntrainer =d2l.Trainer(max_epochs =50, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\n10.1.3ConciseImplementation", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7a7287a-7979-492c-8a11-7dc51f0ce9c4": {"__data__": {"id_": "d7a7287a-7979-492c-8a11-7dc51f0ce9c4", "embedding": null, "metadata": {"page_label": "375", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "119ca212-8a8e-478e-b555-3ac829c68b3d", "node_type": "4", "metadata": {"page_label": "375", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2b8452a68809bb6cc9924a94e801337501cc67df308053f09d679809b90e7aa8", "class_name": "RelatedNodeInfo"}}, "text": "375 Long Short-Term Memory (LSTM)\nUsing high-level APIs, we can directly instantiate an LSTM model. This encapsulates\nall the configuration details that we made explicit above. The code is significantly faster\nas it uses compiled operators rather than Python for many details that we spelled out be-\nfore.\nclass LSTM (d2l .RNN):\ndef __init__ (self , num_inputs, num_hiddens):\nd2l.Module .__init__ (self )\nself .save_hyperparameters()\nself .rnn =nn.LSTM(num_inputs, num_hiddens)\ndef forward (self , inputs, H_C =None ):\nreturn self .rnn(inputs, H_C)\nlstm =LSTM(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =d2l.RNNLM(lstm, vocab_size =len(data .vocab), lr =4)\ntrainer .fit(model, data)\nmodel .predict( 'it has ',20, data .vocab, d2l .try_gpu())\n'it has a the time travelly '\nLSTMs are the prototypical latent variable autoregressive model with nontrivial state con-\ntrol. Many variants thereof have been proposed over the years, e.g., multiple layers, resid-\nual connections, different types of regularization. However, training LSTMs and other", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9effaa8-b527-4685-b88a-48f7155fbe82": {"__data__": {"id_": "d9effaa8-b527-4685-b88a-48f7155fbe82", "embedding": null, "metadata": {"page_label": "376", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "430adb4b-bec2-4541-bfac-186e84f3dcc5", "node_type": "4", "metadata": {"page_label": "376", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c9a5a8d6b5ccd6cdbde5badca5d5a54e8cd60bc105ec39fb1caec3f1329fda54", "class_name": "RelatedNodeInfo"}}, "text": "376 Modern Recurrent Neural Networks\n145sequence models (such as GRUs) is quite costly because of the long range dependency of\nthe sequence. Later we will encounter alternative models such as Transformers that can be\nused in some cases.\n10.1.4Summary\nWhileLSTMswerepublishedin1997,theyrosetogreatprominencewithsomevictoriesin\nprediction competitions in the mid-2000s, and became the dominant models for sequence\nlearning from 2011 until the rise of Transformer models, starting in 2017. Even Tran-\nformers owe some of their key ideas to architecture design innovations introduced by the\nLSTM.\nLSTMshavethreetypesofgates: inputgates,forgetgates,andoutputgatesthatcontrolthe\nflow of information. The hidden layer output of LSTM includes the hidden state and the\nmemory cell internal state. Only the hidden state is passed into the output layer while the\nmemory cell internal state remains entirely internal. LSTMs can alleviate vanishing and\nexploding gradients.\n10.1.5Exercises\n1.Adjustthehyperparametersandanalyzetheirinfluenceonrunningtime,perplexity,and\nthe output sequence.\n2.How would you need to change the model to generate proper words rather than just\nsequences of characters?\n3.Compare the computational cost for GRUs, LSTMs, and regular RNNs for a given hid-\nden dimension. Pay special attention to the training and inference cost.\n4.Since the candidate memory cell ensures that the value range is between \u00001and1by\nusing the tanhfunction, why does the hidden state need to use the tanhfunction again\nto ensure that the output value range is between \u00001and1?\n5.Implement an LSTM model for time series prediction rather than character sequence\nprediction.\nDiscussions145.\n10.2GatedRecurrentUnits(GRU)\nAs RNNs and particularly the LSTM architecture ( Section 10.1 ) rapidly gained popularity\nduring the 2010s, a number of researchers began to experiment with simplified architec-\ntures in hopes of retaining the key idea of incorporating an internal state and multiplicative\ngating mechanisms but with the aim of speeding up computation. The gated recurrent unit", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2068, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92e8fddd-9c33-4897-b94c-36d2d2eb357b": {"__data__": {"id_": "92e8fddd-9c33-4897-b94c-36d2d2eb357b", "embedding": null, "metadata": {"page_label": "377", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc830c54-edde-4c6c-9f3a-cfeb0515e07b", "node_type": "4", "metadata": {"page_label": "377", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a5086d481162956d3351d524a8b3badbbc4bb3e7a2ad3299beb370ef14284b76", "class_name": "RelatedNodeInfo"}}, "text": "377 Gated Recurrent Units (GRU)\n(GRU) (Choet al., 2014) offered a streamlined version of the LSTM memory cell that of-\nten achieves comparable performance but with the advantage of being faster to compute\n(Chungetal., 2014).\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n10.2.1ResetGate and UpdateGate\nHere, the LSTM\u2019s three gates are replaced by two: the reset gate and theupdate gate . As\nwith LSTMs, these gates are given sigmoid activations, forcing their values to lie in the\ninterval\u00b90,1\u00ba. Intuitively, the reset gate controls how much of the previous state we might\nstill want to remember. Likewise, an update gate would allow us to control how much of\nthenewstateisjustacopyoftheoldone. Fig.10.2.1 illustratestheinputsforboththereset\nand update gates in a GRU, given the input of the current time step and the hidden state\nof the previous time step. The outputs of the gates are given by two fully connected layers\nwith a sigmoid activation function.\ntFig. 10.2.1 Computing the reset gate and the update gate in a GRU model.\nMathematically, for a given time step \ud835\udc61, suppose that the input is a minibatch X\ud835\udc612R\ud835\udc5b\u0002\ud835\udc51\n(number of examples =\ud835\udc5b; number of inputs =\ud835\udc51) and the hidden state of the previous time\nstep is H\ud835\udc61\u000012R\ud835\udc5b\u0002\u210e(number of hidden units =\u210e). Then the reset gate R\ud835\udc612R\ud835\udc5b\u0002\u210eand\nupdate gate Z\ud835\udc612R\ud835\udc5b\u0002\u210eare computed as follows:\nR\ud835\udc61=\ud835\udf0e\u00b9X\ud835\udc61Wxr\u00b8H\ud835\udc61\u00001Whr\u00b8br\u00ba,\nZ\ud835\udc61=\ud835\udf0e\u00b9X\ud835\udc61Wxz\u00b8H\ud835\udc61\u00001Whz\u00b8bz\u00ba,(10.2.1)\nwhereWxr,Wxz2R\ud835\udc51\u0002\u210eandWhr,Whz2R\u210e\u0002\u210eareweightparametersand br,bz2R1\u0002\u210e\nare bias parameters.\n10.2.2Candidate Hidden State", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1515, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8029e1f-e2b8-4eeb-95fc-49a88e87a1de": {"__data__": {"id_": "a8029e1f-e2b8-4eeb-95fc-49a88e87a1de", "embedding": null, "metadata": {"page_label": "378", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6c05785-88af-499d-bba4-5818ebced598", "node_type": "4", "metadata": {"page_label": "378", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "73e89c72fb19c7d91bbd9d5a779a075de3da446efdf3c75117d884bdc0fe62ed", "class_name": "RelatedNodeInfo"}}, "text": "378 Modern Recurrent Neural Networks\nNext,weintegratetheresetgate R\ud835\udc61withtheregularupdatingmechanismin (9.4.5 ),leading\nto the following candidatehidden state \u02dcH\ud835\udc612R\ud835\udc5b\u0002\u210eat time step\ud835\udc61:\n\u02dcH\ud835\udc61=tanh\u00b9X\ud835\udc61Wxh\u00b8\u00b9R\ud835\udc61\fH\ud835\udc61\u00001\u00baWhh\u00b8bh\u00ba, (10.2.2)\nwhereWxh2R\ud835\udc51\u0002\u210eandWhh2R\u210e\u0002\u210eareweightparameters, bh2R1\u0002\u210eisthebias,andthe\nsymbol\fis the Hadamard (elementwise) product operator. Here we use a tanh activation\nfunction.\nThe result is a candidate , since we still need to incorporate the action of the update gate.\nComparing with (9.4.5 ), the influence of the previous states can now be reduced with the\nelementwise multiplication of R\ud835\udc61andH\ud835\udc61\u00001in(10.2.2 ). Whenever the entries in the reset\ngateR\ud835\udc61are close to 1, we recover a vanilla RNN such as that in (9.4.5 ). For all entries of\nthe reset gate R\ud835\udc61that are close to 0, the candidate hidden state is the result of an MLP with\nX\ud835\udc61as input. Any pre-existing hidden state is thus resetto defaults.\nFig. 10.2.2 illustrates the computational flow after applying the reset gate.\ntFig. 10.2.2 Computing the candidate hidden state in a GRU model.\n10.2.3HiddenState\nFinally, we need to incorporate the effect of the update gate Z\ud835\udc61. This determines the extent\nto which the new hidden state H\ud835\udc612R\ud835\udc5b\u0002\u210ematches the old state H\ud835\udc61\u00001compared with how\nmuch it resembles the new candidate state \u02dcH\ud835\udc61. The update gate Z\ud835\udc61can be used for this\npurpose, simply by taking elementwise convex combinations of H\ud835\udc61\u00001and \u02dcH\ud835\udc61. This leads\nto the final update equation for the GRU:\nH\ud835\udc61=Z\ud835\udc61\fH\ud835\udc61\u00001\u00b8\u00b91\u0000Z\ud835\udc61\u00ba\f \u02dcH\ud835\udc61. (10.2.3)\nWhenever the update gate Z\ud835\udc61is close to 1, we simply retain the old state. In this case\nthe information from X\ud835\udc61is ignored, effectively skipping time step \ud835\udc61in the dependency\nchain. By contrast, whenever Z\ud835\udc61is close to 0, the new latent state H\ud835\udc61approaches the\ncandidate latent state \u02dcH\ud835\udc61.Fig. 10.2.3 shows the computational flow after the update gate is\nin action.\nIn summary, GRUs have the following two distinguishing features:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ac38a36-8697-4c0c-bf90-e6a55e32edd8": {"__data__": {"id_": "8ac38a36-8697-4c0c-bf90-e6a55e32edd8", "embedding": null, "metadata": {"page_label": "379", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1506c16-4cac-401a-8980-028809d6ebdd", "node_type": "4", "metadata": {"page_label": "379", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "31a690b6ae2db765293ea2ebf8c803d19672b23918ab711cc75c068913e46a74", "class_name": "RelatedNodeInfo"}}, "text": "379 Gated Recurrent Units (GRU)\ntFig. 10.2.3 Computing the hidden state in a GRU model.\n\u000fReset gates help capture short-term dependencies in sequences.\n\u000fUpdate gates help capture long-term dependencies in sequences.\n10.2.4Implementation fromScratch\nTo gain a better understanding of the GRU model, let\u2019s implement it from scratch.\nInitializingModel Parameters\nThe first step is to initialize the model parameters. We draw the weights from a Gaussian\ndistribution with standard deviation to be sigmaand set the bias to 0. The hyperparameter\nnum_hiddens defines the number of hidden units. We instantiate all weights and biases\nrelating to the update gate, the reset gate, and the candidate hidden state.\nclass GRUScratch (d2l .Module):\ndef __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\ninit_weight =lambda *shape: nn .Parameter(torch .randn( *shape) *sigma)\ntriple =lambda : (init_weight(num_inputs, num_hiddens),\ninit_weight(num_hiddens, num_hiddens),\nnn.Parameter(torch .zeros(num_hiddens)))\nself .W_xz, self .W_hz, self .b_z =triple() # Update gate\nself .W_xr, self .W_hr, self .b_r =triple() # Reset gate\nself .W_xh, self .W_hh, self .b_h =triple() # Candidate hidden state\nDefiningthe Model\nNow we are ready to define the GRU forward computation. Its structure is the same as that\nof the basic RNN cell, except that the update equations are more complex.\n@d2l .add_to_class(GRUScratch)\ndef forward (self , inputs, H =None ):\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b129ed1-9ec2-4684-93b5-a1152b40ff69": {"__data__": {"id_": "7b129ed1-9ec2-4684-93b5-a1152b40ff69", "embedding": null, "metadata": {"page_label": "380", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c4159930-e9bc-4684-9f2d-df448e8b3142", "node_type": "4", "metadata": {"page_label": "380", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0fbe3db782f19d78db73064425b61b6d7e2ce3d81ff6475a5129e2232948c868", "class_name": "RelatedNodeInfo"}}, "text": "380 Modern Recurrent Neural Networks\n(continued from previous page)\nifHisNone :\n# Initial state with shape: (batch_size, num_hiddens)\nH=torch .zeros((inputs .shape[ 1],self .num_hiddens),\ndevice =inputs .device)\noutputs =[]\nfor Xininputs:\nZ=torch .sigmoid(torch .matmul(X, self .W_xz) +\ntorch .matmul(H, self .W_hz) +self .b_z)\nR=torch .sigmoid(torch .matmul(X, self .W_xr) +\ntorch .matmul(H, self .W_hr) +self .b_r)\nH_tilde =torch .tanh(torch .matmul(X, self .W_xh) +\ntorch .matmul(R *H,self .W_hh) +self .b_h)\nH=Z*H+(1-Z)*H_tilde\noutputs .append(H)\nreturn outputs, H\nTraining\nTrainingalanguagemodelon TheTimeMachine datasetworksinexactlythesamemanner\nas inSection 9.5 .\ndata =d2l.TimeMachine(batch_size =1024 , num_steps =32)\ngru =GRUScratch(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =d2l.RNNLMScratch(gru, vocab_size =len(data .vocab), lr =4)\ntrainer =d2l.Trainer(max_epochs =50, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\n10.2.5ConciseImplementation\nIn high-level APIs, we can directly instantiate a GRU model. This encapsulates all the\nconfiguration detail that we made explicit above.\nclass GRU(d2l .RNN):\ndef __init__ (self , num_inputs, num_hiddens):\nd2l.Module .__init__ (self )\nself .save_hyperparameters()\nself .rnn =nn.GRU(num_inputs, num_hiddens)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7a5f071-1bdb-4504-8e69-ca8a5f1aba3d": {"__data__": {"id_": "e7a5f071-1bdb-4504-8e69-ca8a5f1aba3d", "embedding": null, "metadata": {"page_label": "381", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "744c407b-0a6c-467c-bbda-66f0f68ea438", "node_type": "4", "metadata": {"page_label": "381", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "930330cf86177acde292f0c0975e0ebab295e88c829444307609d7137ecba4d1", "class_name": "RelatedNodeInfo"}}, "text": "381 Gated Recurrent Units (GRU)\n146ThecodeissignificantlyfasterintrainingasitusescompiledoperatorsratherthanPython.\ngru =GRU(num_inputs =len(data .vocab), num_hiddens =32)\nmodel =d2l.RNNLM(gru, vocab_size =len(data .vocab), lr =4)\ntrainer .fit(model, data)\nAfter training, we print out the perplexity on the training set and the predicted sequence\nfollowing the provided prefix.\nmodel .predict( 'it has ',20, data .vocab, d2l .try_gpu())\n'it has so it and the time '\n10.2.6Summary\nCompared with LSTMs, GRUs achieve similar performance but tend to be lighter com-\nputationally. Generally, compared with simple RNNs, gated RNNS, just like LSTMs and\nGRUs,canbettercapturedependenciesforsequenceswithlargetimestepdistances. GRUs\ncontainbasicRNNsastheirextremecasewhenevertheresetgateisswitchedon. Theycan\nalso skip subsequences by turning on the update gate.\n10.2.7Exercises\n1.Assume that we only want to use the input at time step \ud835\udc610to predict the output at time\nstep\ud835\udc61 >\ud835\udc610. What are the best values for the reset and update gates for each time step?\n2.Adjustthehyperparametersandanalyzetheirinfluenceonrunningtime,perplexity,and\nthe output sequence.\n3.Compare runtime, perplexity, and the output strings for rnn.RNN andrnn.GRU imple-\nmentations with each other.\n4.Whathappensifyouimplementonlyparts ofaGRU,e.g., withonlyareset gateoronly\nan update gate?\nDiscussions146.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1366, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4229dfe0-21a6-4872-9514-e719255e5e1e": {"__data__": {"id_": "4229dfe0-21a6-4872-9514-e719255e5e1e", "embedding": null, "metadata": {"page_label": "382", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15893b6e-9588-4d95-8e60-3c0ade826740", "node_type": "4", "metadata": {"page_label": "382", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8ced53a9bac64621f48d36f00c11cf61f4a93c55f199b3aefb7bfd665d69b747", "class_name": "RelatedNodeInfo"}}, "text": "382 Modern Recurrent Neural Networks\n10.3Deep RecurrentNeuralNetworks\nUpuntilnow,wehavefocusedondefiningnetworksconsistingofasequenceinput,asingle\nhidden RNN layer, and an output layer. Despite having just one hidden layer between the\ninputatanytimestepandthecorrespondingoutput,thereisasenseinwhichthesenetworks\nare deep. Inputs from the first time step can influence the outputs at the final time step\n\ud835\udc47(often 100s or 1000s of steps later). These inputs pass through \ud835\udc47applications of the\nrecurrent layer before reaching the final output. However, we often also wish to retain the\nability to express complex relationships between the inputs at a given time step and the\noutputs at that same time step. Thus we often construct RNNs that are deep not only in the\ntimedirectionbutalsointheinput-to-outputdirection. Thisispreciselythenotionofdepth\nthat we have already encountered in our development of MLPs and deep CNNs.\nThe standard method for building this sort of deep RNN is strikingly simple: we stack\nthe RNNs on top of each other. Given a sequence of length \ud835\udc47, the first RNN produces a\nsequence of outputs, also of length \ud835\udc47. These, in turn, constitute the inputs to the next RNN\nlayer. Inthisshortsection,weillustratethisdesignpatternandpresentasimpleexamplefor\nhowtocodeupsuchstackedRNNs. Below,in Fig.10.3.1 ,weillustrateadeepRNNwith \ud835\udc3f\nhidden layers. Each hidden state operates on a sequential input and produces a sequential\noutput. Moreover, any RNN cell (white box in Fig. 10.3.1 ) at each time step depends on\nboth the same layer\u2019s value at the previous time step and the previous layer\u2019s value at the\nsame time step.\ntFig. 10.3.1 Architecture of a deep RNN.\nFormally, suppose that we have a minibatch input X\ud835\udc612R\ud835\udc5b\u0002\ud835\udc51(number of examples =\ud835\udc5b;\nnumber of inputs in each example =\ud835\udc51) at time step \ud835\udc61. At the same time step, let the hidden\nstate of the\ud835\udc59thhidden layer ( \ud835\udc59=1,...,\ud835\udc3f) beH\u00b9\ud835\udc59\u00ba\n\ud835\udc612R\ud835\udc5b\u0002\u210e(number of hidden units =\u210e)\nand the output layer variable be O\ud835\udc612R\ud835\udc5b\u0002\ud835\udc5e(number of outputs: \ud835\udc5e). Setting H\u00b90\u00ba\n\ud835\udc61=X\ud835\udc61,\nthe hidden state of the \ud835\udc59thhidden layer that uses the activation function \ud835\udf19\ud835\udc59is calculated as\nfollows:\nH\u00b9\ud835\udc59\u00ba\n\ud835\udc61=\ud835\udf19\ud835\udc59\u00b9H\u00b9\ud835\udc59\u00001\u00ba\n\ud835\udc61W\u00b9\ud835\udc59\u00ba\nxh\u00b8H\u00b9\ud835\udc59\u00ba\n\ud835\udc61\u00001W\u00b9\ud835\udc59\u00ba\nhh\u00b8b\u00b9\ud835\udc59\u00ba\nh\u00ba, (10.3.1)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a73dee91-9a96-46fc-8b00-f3a6e862ebe3": {"__data__": {"id_": "a73dee91-9a96-46fc-8b00-f3a6e862ebe3", "embedding": null, "metadata": {"page_label": "383", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45c18173-4b74-4433-abd3-d5342778adab", "node_type": "4", "metadata": {"page_label": "383", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "77a1d3d1e98e53f17be1eb22c5006bbaff12d539a70cb00af376f34c149a7a27", "class_name": "RelatedNodeInfo"}}, "text": "383 Deep Recurrent Neural Networks\nwheretheweights W\u00b9\ud835\udc59\u00ba\nxh2R\u210e\u0002\u210eandW\u00b9\ud835\udc59\u00ba\nhh2R\u210e\u0002\u210e, togetherwiththebias b\u00b9\ud835\udc59\u00ba\nh2R1\u0002\u210e, are\nthe model parameters of the \ud835\udc59thhidden layer.\nAt the end, the calculation of the output layer is only based on the hidden state of the final\n\ud835\udc3fthhidden layer:\nO\ud835\udc61=H\u00b9\ud835\udc3f\u00ba\n\ud835\udc61Whq\u00b8bq, (10.3.2)\nwhere the weight Whq2R\u210e\u0002\ud835\udc5eand the bias bq2R1\u0002\ud835\udc5eare the model parameters of the\noutput layer.\nJustaswithMLPs, thenumberofhiddenlayers \ud835\udc3fandthenumberofhiddenunits \u210earehy-\nperparametersthatwecantune. CommonRNNlayerwidths( \u210e)areintherange\u00b964,2056\u00ba,\nand common depths ( \ud835\udc3f) are in the range\u00b91,8\u00ba. In addition, we can easily get a deep-gated\nRNN by replacing the hidden state computation in (10.3.1 )with that from an LSTM or a\nGRU.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n10.3.1Implementation fromScratch\nTo implement a multilayer RNN from scratch, we can treat each layer as an RNNScratch\ninstance with its own learnable parameters.\nclass StackedRNNScratch (d2l .Module):\ndef __init__ (self , num_inputs, num_hiddens, num_layers, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .rnns =nn.Sequential( *[d2l .RNNScratch(\nnum_inputs ifi==0else num_hiddens, num_hiddens, sigma)\nfor iinrange (num_layers)])\nThemultilayerforwardcomputationsimplyperformsforwardcomputationlayerbylayer.\n@d2l .add_to_class(StackedRNNScratch)\ndef forward (self , inputs, Hs =None ):\noutputs =inputs\nifHsisNone : Hs =[None ]*self .num_layers\nfor iinrange (self .num_layers):\noutputs, Hs[i] =self .rnns[i](outputs, Hs[i])\noutputs =torch .stack(outputs, 0)\nreturn outputs, Hs\nAs an example, we train a deep GRU model on The Time Machine dataset (same as in\nSection 9.5 ). To keep things simple we set the number of layers to 2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c18eecdb-b125-425f-aa93-883c579ce57c": {"__data__": {"id_": "c18eecdb-b125-425f-aa93-883c579ce57c", "embedding": null, "metadata": {"page_label": "384", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05bbdacd-b517-4cba-8e22-b263bfb39b04", "node_type": "4", "metadata": {"page_label": "384", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c545a9a3d6ac09e259d789b36e9e82d1de04e6ed475d5ca596d31592d2c0b4e6", "class_name": "RelatedNodeInfo"}}, "text": "384 Modern Recurrent Neural Networks\ndata =d2l.TimeMachine(batch_size =1024 , num_steps =32)\nrnn_block =StackedRNNScratch(num_inputs =len(data .vocab),\nnum_hiddens =32, num_layers =2)\nmodel =d2l.RNNLMScratch(rnn_block, vocab_size =len(data .vocab), lr =2)\ntrainer =d2l.Trainer(max_epochs =100, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\n10.3.2ConciseImplementation\nFortunatelymanyof thelogisticaldetails requiredtoimplement multiplelayersofanRNN\nare readily available in high-level APIs. Our concise implementation will use such built-\nin functionalities. The code generalizes the one we used previously in Section 10.2 , let-\nting us specify the number of layers explicitly rather than picking the default of only one\nlayer.\nclass GRU(d2l .RNN): #@save\n\"\"\"The multilayer GRU model.\"\"\"\ndef __init__ (self , num_inputs, num_hiddens, num_layers, dropout =0):\nd2l.Module .__init__ (self )\nself .save_hyperparameters()\nself .rnn =nn.GRU(num_inputs, num_hiddens, num_layers,\ndropout =dropout)\nThe architectural decisions such as choosing hyperparameters are very similar to those of\nSection 10.2 . We pick the same number of inputs and outputs as we have distinct tokens,\ni.e.,vocab_size . Thenumberofhiddenunitsisstill32. Theonlydifferenceisthatwenow\nselect a nontrivial number of hidden layers by specifying the value of num_layers .\ngru =GRU(num_inputs =len(data .vocab), num_hiddens =32, num_layers =2)\nmodel =d2l.RNNLM(gru, vocab_size =len(data .vocab), lr =2)\ntrainer .fit(model, data)\nmodel .predict( 'it has ',20, data .vocab, d2l .try_gpu())\n'it has for and the time th '", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "309ef138-456e-4867-814e-eb7aba692d0d": {"__data__": {"id_": "309ef138-456e-4867-814e-eb7aba692d0d", "embedding": null, "metadata": {"page_label": "385", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9353f800-dccf-4804-81c6-4a7e0b728082", "node_type": "4", "metadata": {"page_label": "385", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1889c6494967c161696d4023d124904905a359481d5afddc4f5042f56e3f5651", "class_name": "RelatedNodeInfo"}}, "text": "385 Bidirectional Recurrent Neural Networks\n14710.3.3Summary\nIn deep RNNs, the hidden state information is passed to the next time step of the current\nlayer and the current time step of the next layer. There exist many different flavors of\ndeep RNNs, such as LSTMs, GRUs, or vanilla RNNs. Conveniently, these models are\nall available as parts of the high-level APIs of deep learning frameworks. Initialization of\nmodels requires care. Overall, deep RNNs require considerable amount of work (such as\nlearning rate and clipping) to ensure proper convergence.\n10.3.4Exercises\n1.Replace the GRU by an LSTM and compare the accuracy and training speed.\n2.Increase the training data to include multiple books. How low can you go on the per-\nplexity scale?\n3.Would you want to combine sources of different authors when modeling text? Why is\nthis a good idea? What could go wrong?\nDiscussions147.\n10.4BidirectionalRecurrentNeuralNetworks\nSofar,ourworkingexampleofasequencelearningtaskhasbeenlanguagemodeling,where\nwe aim to predict the next token given all previous tokens in a sequence. In this scenario,\nwewishonlytoconditionupontheleftwardcontext,andthustheunidirectionalchainingof\nastandardRNNseemsappropriate. However,therearemanyothersequencelearningtasks\ncontexts where it is perfectly fine to condition the prediction at every time step on both the\nleftward and the rightward context. Consider, for example, part of speech detection. Why\nshouldn\u2019t we take the context in both directions into account when assessing the part of\nspeech associated with a given word?\nAnother common task\u2014often useful as a pretraining exercise prior to fine-tuning a model\non an actual task of interest\u2014is to mask out random tokens in a text document and then", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffa10416-ee53-4eda-81a3-d4df7c123079": {"__data__": {"id_": "ffa10416-ee53-4eda-81a3-d4df7c123079", "embedding": null, "metadata": {"page_label": "386", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3cf4579f-765a-48d9-b24f-84257403da46", "node_type": "4", "metadata": {"page_label": "386", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2559f8b41ea7c4c42addedc8b72e0e90f36e127011943a60e1a9ae1982da9069", "class_name": "RelatedNodeInfo"}}, "text": "386 Modern Recurrent Neural Networks\nto train a sequence model to predict the values of the missing tokens. Note that depend-\ning on what comes after the blank, the likely value of the missing token changes dramati-\ncally:\n\u000fI am ___.\n\u000fI am ___hungry.\n\u000fI am ___hungry, and I can eat half a pig.\nIn the first sentence \u201chappy\u201d seems to be a likely candidate. The words \u201cnot\u201d and \u201cvery\u201d\nseem plausible in the second sentence, but \u201cnot\u201d seems incompatible with the third sen-\ntences.\nFortunately, a simple technique transforms any unidirectional RNN into a bidirectional\nRNN (Schuster and Paliwal, 1997 ). We simply implement two unidirectional RNN layers\nchained together in opposite directions and acting on the same input ( Fig. 10.4.1 ). For\nthe first RNN layer, the first input is x1and the last input is x\ud835\udc47, but for the second RNN\nlayer,thefirstinputis x\ud835\udc47andthelastinputis x1. Toproducetheoutputofthisbidirectional\nRNNlayer,wesimplyconcatenatetogetherthecorrespondingoutputsofthetwounderlying\nunidirectional RNN layers.\ntFig. 10.4.1 Architecture of a bidirectional RNN.\nFormally for any time step \ud835\udc61, we consider a minibatch input X\ud835\udc612R\ud835\udc5b\u0002\ud835\udc51(number of exam-\nples=\ud835\udc5b;numberofinputsineachexample =\ud835\udc51)andletthehiddenlayeractivationfunction\nbe\ud835\udf19. Inthebidirectionalarchitecture, theforwardandbackwardhiddenstatesforthistime\nstep are\u0000 !H\ud835\udc612R\ud835\udc5b\u0002\u210eand \u0000H\ud835\udc612R\ud835\udc5b\u0002\u210e, respectively, where \u210eis the number of hidden units.\nThe forward and backward hidden state updates are as follows:\n\u0000 !H\ud835\udc61=\ud835\udf19\u00b9X\ud835\udc61W\u00b9\ud835\udc53\u00ba\nxh\u00b8\u0000 !H\ud835\udc61\u00001W\u00b9\ud835\udc53\u00ba\nhh\u00b8b\u00b9\ud835\udc53\u00ba\nh\u00ba,\n \u0000H\ud835\udc61=\ud835\udf19\u00b9X\ud835\udc61W\u00b9\ud835\udc4f\u00ba\nxh\u00b8 \u0000H\ud835\udc61\u00b81W\u00b9\ud835\udc4f\u00ba\nhh\u00b8b\u00b9\ud835\udc4f\u00ba\nh\u00ba,(10.4.1)\nwhere the weights W\u00b9\ud835\udc53\u00ba\nxh2R\ud835\udc51\u0002\u210e,W\u00b9\ud835\udc53\u00ba\nhh2R\u210e\u0002\u210e,W\u00b9\ud835\udc4f\u00ba\nxh2R\ud835\udc51\u0002\u210e,andW\u00b9\ud835\udc4f\u00ba\nhh2R\u210e\u0002\u210e, and\nthe biases b\u00b9\ud835\udc53\u00ba\nh2R1\u0002\u210eandb\u00b9\ud835\udc4f\u00ba\nh2R1\u0002\u210eare all the model parameters.\nNext, we concatenate the forward and backward hidden states\u0000 !H\ud835\udc61and \u0000H\ud835\udc61to obtain the\nhiddenstate H\ud835\udc612R\ud835\udc5b\u00022\u210eforfeedingintotheoutputlayer. IndeepbidirectionalRNNswith\nmultiplehiddenlayers,suchinformationispassedonas inputtothenextbidirectionallayer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9632a7f2-9afd-480d-90ac-24c788d055f3": {"__data__": {"id_": "9632a7f2-9afd-480d-90ac-24c788d055f3", "embedding": null, "metadata": {"page_label": "387", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "890d97e6-77c5-4b56-93b5-8dc5c23be368", "node_type": "4", "metadata": {"page_label": "387", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "219e4250a5a02d8e190012e214b90221502cd0526362242340ba989c6c6f0b98", "class_name": "RelatedNodeInfo"}}, "text": "387 Bidirectional Recurrent Neural Networks\nLast, the output layer computes the output O\ud835\udc612R\ud835\udc5b\u0002\ud835\udc5e(number of outputs =\ud835\udc5e):\nO\ud835\udc61=H\ud835\udc61Whq\u00b8bq. (10.4.2)\nHere, the weight matrix Whq2R2\u210e\u0002\ud835\udc5eand the bias bq2R1\u0002\ud835\udc5eare the model parameters\nof the output layer. While technically, the two directions can have different numbers of\nhidden units, this design choice is seldom made in practice. We now demonstrate a simple\nimplementation of a bidirectional RNN.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n10.4.1Implementation fromScratch\nToimplementabidirectionalRNNfromscratch,wecanincludetwounidirectional RNNScratch\ninstances with separate learnable parameters.\nclass BiRNNScratch (d2l .Module):\ndef __init__ (self , num_inputs, num_hiddens, sigma =0.01 ):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .f_rnn =d2l.RNNScratch(num_inputs, num_hiddens, sigma)\nself .b_rnn =d2l.RNNScratch(num_inputs, num_hiddens, sigma)\nself .num_hiddens *=2# The output dimension will be doubled\nStates of forward and backward RNNs are updated separately, while outputs of these two\nRNNs are concatenated.\n@d2l .add_to_class(BiRNNScratch)\ndef forward (self , inputs, Hs =None ):\nf_H, b_H =HsifHsisnot None else (None ,None )\nf_outputs, f_H =self .f_rnn(inputs, f_H)\nb_outputs, b_H =self .b_rnn( reversed (inputs), b_H)\noutputs =[torch .cat((f, b), -1)for f, b inzip(\nf_outputs, reversed (b_outputs))]\nreturn outputs, (f_H, b_H)\n10.4.2ConciseImplementation\nUsingthehigh-levelAPIs,wecanimplementbidirectionalRNNsmoreconcisely. Herewe\ntake a GRU model as an example.\nclass BiGRU (d2l .RNN):\ndef __init__ (self , num_inputs, num_hiddens):\nd2l.Module .__init__ (self )\nself .save_hyperparameters()\nself .rnn =nn.GRU(num_inputs, num_hiddens, bidirectional =True )\nself .num_hiddens *=2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6e67135-e211-4cd5-a81d-fc57dbe3182d": {"__data__": {"id_": "b6e67135-e211-4cd5-a81d-fc57dbe3182d", "embedding": null, "metadata": {"page_label": "388", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "587c35bf-d49e-453b-bd1d-101813bc3803", "node_type": "4", "metadata": {"page_label": "388", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "36448b7ff669af5d9178d6019c7cc2d30eeb7b4a9750ac6d29306156b654bc46", "class_name": "RelatedNodeInfo"}}, "text": "388 Modern Recurrent Neural Networks\n14810.4.3Summary\nIn bidirectional RNNs, the hidden state for each time step is simultaneously determined\nby the data prior to and after the current time step. Bidirectional RNNs are mostly use-\nful for sequence encoding and the estimation of observations given bidirectional context.\nBidirectional RNNs are very costly to train due to long gradient chains.\n10.4.4Exercises\n1.If the different directions use a different number of hidden units, how will the shape of\nH\ud835\udc61change?\n2.Design a bidirectional RNN with multiple hidden layers.\n3.Polysemy is common in natural languages. For example, the word \u201cbank\u201d has different\nmeanings in contexts \u201ci went to the bank to deposit cash\u201d and \u201ci went to the bank to sit\ndown\u201d. How can we design a neural network model such that given a context sequence\nand a word, a vector representation of the word in the correct context will be returned?\nWhat type of neural architectures is preferred for handling polysemy?\nDiscussions148.\n10.5MachineTranslationand the Dataset\nAmong the major breakthroughs that prompted widespread interest in modern RNNs was\na major advance in the applied field of statistical machine translation . Here, the model is\npresented with a sentence in one language and must predict the corresponding sentence in\nanother. Note that here the sentences may be of different lengths, and that corresponding\nwords in the two sentences may not occur in the same order, owing to differences in the\ntwo language\u2019s grammatical structure.\nMany problems have this flavor of mapping between two such \u201cunaligned\u201d sequences.\nExamples include mapping from dialog prompts to replies or from questions to answers.\nBroadly, such problems are called sequence-to-sequence (seq2seq) problems and they are\nour focus for both the remainder of this chapter and much of Chapter 11 .\nIn this section, we introduce the machine translation problem and an example dataset that\nwe will use in the subsequent examples. For decades, statistical formulations of translation\nbetween languages had been popular ( Brownetal., 1990,Brownetal., 1988), even before\nresearchers got neural network approaches working (methods were often lumped together\nunder the term neuralmachinetranslation ).\nFirst we will need some new code to process our data. Unlike the language modeling that\nwesawin Section9.3 ,hereeachexampleconsistsoftwoseparatetextsequences,oneinthe\nsource language and another (the translation) in the target language. The following code\nsnippets will show how to load the preprocessed data into minibatches for training.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17c21e18-50cd-45da-9068-9d9e67c31ad5": {"__data__": {"id_": "17c21e18-50cd-45da-9068-9d9e67c31ad5", "embedding": null, "metadata": {"page_label": "389", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65ba714e-d0b1-40a8-b862-70dada25177e", "node_type": "4", "metadata": {"page_label": "389", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e9100f30748797b3bd434deefbcc9c8257362f1b6bdc544139e45269e5d4d9e3", "class_name": "RelatedNodeInfo"}}, "text": "389 Machine Translation and the Dataset\n149import os\nimport torch\nfrom d2l import torch asd2l\n10.5.1Downloadingand Preprocessingthe Dataset\nTo begin, we download an English\u2013French dataset that consists of bilingual sentence pairs\nfrom the Tatoeba Project149. Each line in the dataset is a tab-delimited pair consisting\nof an English text sequence (the source) and the translated French text sequence (the tar-\nget). Note that each text sequence can be just one sentence, or a paragraph of multiple\nsentences.\nclass MTFraEng (d2l .DataModule): #@save\n\"\"\"The English-French dataset.\"\"\"\ndef _download (self ):\nd2l.extract(d2l .download(\nd2l.DATA_URL +'fra-eng.zip ',self .root,\n'94646ad1522d915e7b0f9296181140edcf86a4f5 '))\nwith open (self .root +'/fra-eng/fra.txt ', encoding ='utf-8 ')asf:\nreturn f.read()\ndata =MTFraEng()\nraw_text =data ._download()\nprint (raw_text[: 75])\nDownloading ../data/fra-eng.zip from http://d2l-data.s3-accelerate.amazonaws.\n\u21a9!com/fra-eng.zip...\nGo. Va !\nHi. Salut !\nRun! Cours !\nRun! Courez !\nWho? Qui ?\nWow! \u00c7a alors !\nAfterdownloadingthedataset,weproceedwithseveralpreprocessingstepsfortherawtext\ndata. For instance, we replace non-breaking space with space, convert uppercase letters to\nlowercase ones, and insert space between words and punctuation marks.\n@d2l .add_to_class(MTFraEng) #@save\ndef _preprocess (self , text):\n# Replace non-breaking space with space\ntext =text .replace( '\\u202f ','').replace( '\\xa0 ','')\n# Insert space between words and punctuation marks\nno_space =lambda char, prev_char: char in',.!? 'and prev_char !=''\nout =[''+char ifi>0and no_space(char, text[i -1])else char\nfor i, char inenumerate (text .lower())]\nreturn ''.join(out)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27110594-a5b8-45ab-9ef4-655d0b145c33": {"__data__": {"id_": "27110594-a5b8-45ab-9ef4-655d0b145c33", "embedding": null, "metadata": {"page_label": "390", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7780c01d-32d4-4b9e-b885-cde44be1fe18", "node_type": "4", "metadata": {"page_label": "390", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e9c93e89b933fbd7870e1c6624d5ffd8ef83122e292f947cbe02129dc310e59a", "class_name": "RelatedNodeInfo"}}, "text": "390 Modern Recurrent Neural Networks\ntext =data ._preprocess(raw_text)\nprint (text[: 80])\ngo . va !\nhi . salut !\nrun ! cours !\nrun ! courez !\nwho ? qui ?\nwow ! \u00e7a alors !\n10.5.2Tokenization\nUnlike the character-level tokenization in Section 9.3 , for machine translation we prefer\nword-level tokenization here (today\u2019s state-of-the-art models use more complex tokeniza-\ntion techniques). The following _tokenize method tokenizes the first max_examples text\nsequence pairs, where each token is either a word or a punctuation mark. We append the\nspecial \u201c<eos>\u201d token to the end of every sequence to indicate the end of the sequence.\nWhen a model is predicting by generating a sequence token after token, the generation of\nthe\u201c<eos>\u201dtokencansuggestthattheoutputsequenceiscomplete. Intheend,themethod\nbelow returns two lists of token lists: srcandtgt. Specifically, src[i]is a list of tokens\nfrom the\ud835\udc56thtext sequence in the source language (English here) and tgt[i]is that in the\ntarget language (French here).\n@d2l .add_to_class(MTFraEng) #@save\ndef _tokenize (self , text, max_examples =None ):\nsrc, tgt =[], []\nfor i, line inenumerate (text .split( '\\n')):\nifmax_examples and i>max_examples: break\nparts =line .split( '\\t')\niflen(parts) ==2:\n# Skip empty tokens\nsrc.append([t for tinf'{parts[ 0]}<eos> '.split( '')ift])\ntgt.append([t for tinf'{parts[ 1]}<eos> '.split( '')ift])\nreturn src, tgt\nsrc, tgt =data ._tokenize(text)\nsrc[: 6], tgt[: 6]\n([['go','.','<eos> '],\n['hi','.','<eos> '],\n['run','!','<eos> '],\n['run','!','<eos> '],\n['who','?','<eos> '],\n['wow','!','<eos> ']],\n[['va','!','<eos> '],\n['salut ','!','<eos> '],\n['cours ','!','<eos> '],\n['courez ','!','<eos> '],\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2dde1704-9ba0-40ae-ba21-54c27005be6d": {"__data__": {"id_": "2dde1704-9ba0-40ae-ba21-54c27005be6d", "embedding": null, "metadata": {"page_label": "391", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16e17059-437f-4dc2-86a6-e56c5bec37e6", "node_type": "4", "metadata": {"page_label": "391", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e8260b4b806f56eb064dd36d1b9a977dfabaf2d66c26deb33f282b0b88354e01", "class_name": "RelatedNodeInfo"}}, "text": "391 Machine Translation and the Dataset\n(continued from previous page)\n['qui','?','<eos> '],\n['\u00e7a','alors ','!','<eos> ']])\nLet\u2019s plot the histogram of the number of tokens per text sequence. In this simple English\u2013\nFrench dataset, most of the text sequences have fewer than 20 tokens.\n#@save\ndef show_list_len_pair_hist (legend, xlabel, ylabel, xlist, ylist):\n\"\"\"Plot the histogram for list length pairs.\"\"\"\nd2l.set_figsize()\n_, _, patches =d2l.plt.hist(\n[[len(l) for linxlist], [ len(l) for linylist]])\nd2l.plt.xlabel(xlabel)\nd2l.plt.ylabel(ylabel)\nfor patch inpatches[ 1].patches:\npatch .set_hatch( '/')\nd2l.plt.legend(legend)\nshow_list_len_pair_hist([ 'source ','target '],'# tokens per sequence ',\n'count ', src, tgt);\n10.5.3LoadingSequencesof FixedLength\nRecallthat in languagemodeling eachexamplesequence, either a segment of one sentence\nor a span over multiple sentences, had a fixed length. This was specified by the num_steps\n(number of time steps or tokens) argument from Section 9.3 . In machine translation, each\nexample is a pair of source and target text sequences, where the two text sequences may\nhave different lengths.\nForcomputationalefficiency,wecanstillprocessaminibatchoftextsequencesatonetime\nbytruncation andpadding. Supposethateverysequenceinthesameminibatchshouldhave\nthe same length num_steps . If a text sequence has fewer than num_steps tokens, we will\nkeep appending the special \u201c<pad>\u201d token to its end until its length reaches num_steps .\nOtherwise, wewilltruncatethetextsequencebyonlytakingitsfirst num_steps tokensand\ndiscarding the remaining. In this way, every text sequence will have the same length to be\nloaded in minibatches of the same shape. Furthermore, we also record length of the source", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f39c8907-953f-48d6-ba36-2c7c6582b4c8": {"__data__": {"id_": "f39c8907-953f-48d6-ba36-2c7c6582b4c8", "embedding": null, "metadata": {"page_label": "392", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "364ea9bd-3a81-4c52-9a02-8d00ad590cb0", "node_type": "4", "metadata": {"page_label": "392", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b3211600978d55645c5f2562c21c8f0db047d385fed0ebcd5c1b5e0fc73174b8", "class_name": "RelatedNodeInfo"}}, "text": "392 Modern Recurrent Neural Networks\nsequence excluding padding tokens. This information will be needed by some models that\nwe will cover later.\nSince the machine translation dataset consists of pairs of languages, we can build two vo-\ncabulariesforboththesourcelanguageandthetargetlanguageseparately. Withword-level\ntokenization, the vocabulary size will be significantly larger than that using character-level\ntokenization. To alleviate this, here we treat infrequent tokens that appear less than twice\nas the same unknown (\u201c<unk>\u201d) token. As we will explain later ( Fig. 10.7.1 ), when train-\ning with target sequences, the decoder output (label tokens) can be the same decoder input\n(targettokens),shiftedbyonetoken; andthespecialbeginning-of-sequence\u201c<bos>\u201dtoken\nwill be used as the first input token for predicting the target sequence ( Fig. 10.7.3 ).\n@d2l .add_to_class(MTFraEng) #@save\ndef __init__ (self , batch_size, num_steps =9, num_train =512, num_val =128):\nsuper (MTFraEng, self ).__init__ ()\nself .save_hyperparameters()\nself .arrays, self .src_vocab, self .tgt_vocab =self ._build_arrays(\nself ._download())\n@d2l .add_to_class(MTFraEng) #@save\ndef _build_arrays (self , raw_text, src_vocab =None , tgt_vocab =None ):\ndef _build_array (sentences, vocab, is_tgt =False ):\npad_or_trim =lambda seq, t: (\nseq[:t] iflen(seq) >telse seq +['<pad> ']*(t-len(seq)))\nsentences =[pad_or_trim(s, self .num_steps) for sinsentences]\nifis_tgt:\nsentences =[['<bos> ']+sfor sinsentences]\nifvocab isNone :\nvocab =d2l.Vocab(sentences, min_freq =2)\narray =torch .tensor([vocab[s] for sinsentences])\nvalid_len =(array !=vocab[ '<pad> ']).type(torch .int32) .sum( 1)\nreturn array, vocab, valid_len\nsrc, tgt =self ._tokenize( self ._preprocess(raw_text),\nself .num_train +self .num_val)\nsrc_array, src_vocab, src_valid_len =_build_array(src, src_vocab)\ntgt_array, tgt_vocab, _ =_build_array(tgt, tgt_vocab, True )\nreturn ((src_array, tgt_array[:,: -1], src_valid_len, tgt_array[:, 1:]),\nsrc_vocab, tgt_vocab)\n10.5.4Readingthe Dataset\nFinally, we define the get_dataloader method to return the data iterator.\n@d2l .add_to_class(MTFraEng) #@save\ndef get_dataloader (self , train):\nidx =slice (0,self .num_train) iftrain else slice (self .num_train, None )\nreturn self .get_tensorloader( self .arrays, train, idx)\nLet\u2019s read the first minibatch from the English\u2013French dataset.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2e4feef-306b-49a7-a3c7-b3971169bb83": {"__data__": {"id_": "b2e4feef-306b-49a7-a3c7-b3971169bb83", "embedding": null, "metadata": {"page_label": "393", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24e42174-6850-4ffa-880e-2838eeede7c7", "node_type": "4", "metadata": {"page_label": "393", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0e0143ba6c0ca40599f73ce6aa6dc6487727bf44b31a6808abba279674c7c0a5", "class_name": "RelatedNodeInfo"}}, "text": "393 Machine Translation and the Dataset\ndata =MTFraEng(batch_size =3)\nsrc, tgt, src_valid_len, label =next (iter (data .train_dataloader()))\nprint ('source: ', src .type(torch .int32))\nprint ('decoder input: ', tgt .type(torch .int32))\nprint ('source len excluding pad: ', src_valid_len .type(torch .int32))\nprint ('label: ', label .type(torch .int32))\nsource: tensor([[ 117,182, 0, 3, 4, 4, 4, 4, 4],\n[62,72, 2, 3, 4, 4, 4, 4, 4],\n[57,124, 0, 3, 4, 4, 4, 4, 4]], dtype =torch .int32)\ndecoder input : tensor([[ 3,37,100,58,160, 0, 4, 5, 5],\n[3, 6, 2, 4, 5, 5, 5, 5, 5],\n[3,180, 0, 4, 5, 5, 5, 5, 5]], dtype =torch .int32)\nsource len excluding pad: tensor([ 4,4,4], dtype =torch .int32)\nlabel: tensor([[ 37,100,58,160, 0, 4, 5, 5, 5],\n[6, 2, 4, 5, 5, 5, 5, 5, 5],\n[180, 0, 4, 5, 5, 5, 5, 5, 5]], dtype =torch .int32)\nWe show a pair of source and target sequences processed by the above _build_arrays\nmethod (in the string format).\n@d2l .add_to_class(MTFraEng) #@save\ndef build (self , src_sentences, tgt_sentences):\nraw_text ='\\n'.join([src +'\\t'+tgt for src, tgt inzip(\nsrc_sentences, tgt_sentences)])\narrays, _, _ =self ._build_arrays(\nraw_text, self .src_vocab, self .tgt_vocab)\nreturn arrays\nsrc, tgt, _, _ =data .build([ 'hi . '], [ 'salut . '])\nprint ('source: ', data .src_vocab .to_tokens(src[ 0].type(torch .int32)))\nprint ('target: ', data .tgt_vocab .to_tokens(tgt[ 0].type(torch .int32)))\nsource: [ 'hi','.','<eos> ','<pad> ','<pad> ','<pad> ','<pad> ','<pad> ','\n\u21a9!<pad> ']\ntarget: [ '<bos> ','salut ','.','<eos> ','<pad> ','<pad> ','<pad> ','<pad> ','\n\u21a9!<pad> ']\n10.5.5Summary\nInnaturallanguageprocessing, machinetranslation referstothetaskofautomaticallymap-\npingfromasequencerepresentingastringoftextina sourcelanguagetoastringrepresent-\ning a plausible translation in a targetlanguage. Using word-level tokenization, the vocab-\nulary size will be significantly larger than that using character-level tokenization, but the\nsequence lengths will be much shorter. To mitigate the large vocabulary size, we can treat\ninfrequent tokens as some \u201cunknown\u201d token. We can truncate and pad text sequences so\nthat all of them will have the same length to be loaded in minibatches. Modern implemen-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61425244-fbf8-4090-a9b6-57355549d5e3": {"__data__": {"id_": "61425244-fbf8-4090-a9b6-57355549d5e3", "embedding": null, "metadata": {"page_label": "394", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8850da52-324e-4616-9b75-40111487b6e4", "node_type": "4", "metadata": {"page_label": "394", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "82e942ed212b7fbd7ada956d1f3e9d574a56db3dcec02d349a317497813f0d59", "class_name": "RelatedNodeInfo"}}, "text": "394 Modern Recurrent Neural Networks\n150tationsoftenbucketsequenceswithsimilarlengthstoavoidwastingexcessivecomputation\non padding.\n10.5.6Exercises\n1.Try different values of the max_examples argument in the _tokenize method. How\ndoes this affect the vocabulary sizes of the source language and the target language?\n2.Text in some languages such as Chinese and Japanese does not have word boundary\nindicators(e.g.,space). Isword-leveltokenizationstillagoodideaforsuchcases? Why\nor why not?\nDiscussions150.\n10.6TheEncoder\u0000Decoder Architecture\nIn general sequence-to-sequence problems like machine translation ( Section 10.5 ), inputs\nand outputs are of varying lengths that are unaligned. The standard approach to handling\nthis sort of data is to design an encoder\u2013decoder architecture ( Fig. 10.6.1 ) consisting of\ntwo major components: an encoder that takes a variable-length sequence as input, and a\ndecoder that acts as a conditional language model, taking in the encoded input and the\nleftwards context of the target sequence and predicting the subsequent token in the target\nsequence.\ntFig. 10.6.1 The encoder\u2013decoder architecture.\nLet\u2019s take machine translation from English to French as an example. Given an input\nsequence in English: \u201cThey\u201d, \u201care\u201d, \u201cwatching\u201d, \u201c.\u201d, this encoder\u2013decoder architecture\nfirst encodes the variable-length input into a state, then decodes the state to generate the\ntranslated sequence, token by token, as output: \u201cIls\u201d, \u201cregardent\u201d, \u201c.\u201d. Since the encoder\u2013\ndecoder architecture forms the basis of different sequence-to-sequence models in subse-\nquent sections, this section will convert this architecture into an interface that will be im-\nplemented later.\nfrom torch import nn\nfrom d2l import torch asd2l\n10.6.1Encoder\nIntheencoderinterface,wejustspecifythattheencodertakesvariable-lengthsequencesas\ninput X. The implementation will be provided by any model that inherits this base Encoder\nclass.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4fca6872-3f4d-4b2f-b7b2-457c5407f36a": {"__data__": {"id_": "4fca6872-3f4d-4b2f-b7b2-457c5407f36a", "embedding": null, "metadata": {"page_label": "395", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67a22e51-2b6d-442b-be97-c90f437a1f3f", "node_type": "4", "metadata": {"page_label": "395", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "37b0b67987af508afad6832b3f58009cc91276ccaa59785540b8a89bbe060cd5", "class_name": "RelatedNodeInfo"}}, "text": "395 The Encoder\u0000Decoder Architecture\nclass Encoder (nn.Module): #@save\n\"\"\"The base encoder interface for the encoder--decoder architecture.\"\"\"\ndef __init__ (self ):\nsuper ().__init__ ()\n# Later there can be additional arguments (e.g., length excluding padding)\ndef forward (self , X, *args):\nraise NotImplementedError\n10.6.2Decoder\nInthefollowingdecoderinterface,weaddanadditional init_state methodtoconvertthe\nencoder output ( enc_all_outputs ) into the encoded state. Note that this step may require\nextra inputs, such as the valid length of the input, which was explained in Section 10.5 .\nTo generate a variable-length sequence token by token, every time the decoder may map\nan input (e.g., the generated token at the previous time step) and the encoded state into an\noutput token at the current time step.\nclass Decoder (nn.Module): #@save\n\"\"\"The base decoder interface for the encoder--decoder architecture.\"\"\"\ndef __init__ (self ):\nsuper ().__init__ ()\n# Later there can be additional arguments (e.g., length excluding padding)\ndef init_state (self , enc_all_outputs, *args):\nraise NotImplementedError\ndef forward (self , X, state):\nraise NotImplementedError\n10.6.3Puttingthe Encoder and Decoder Together\nIn the forward propagation, the output of the encoder is used to produce the encoded state,\nand this state will be further used by the decoder as one of its input.\nclass EncoderDecoder (d2l .Classifier): #@save\n\"\"\"The base class for the encoder--decoder architecture.\"\"\"\ndef __init__ (self , encoder, decoder):\nsuper ().__init__ ()\nself .encoder =encoder\nself .decoder =decoder\ndef forward (self , enc_X, dec_X, *args):\nenc_all_outputs =self .encoder(enc_X, *args)\ndec_state =self .decoder .init_state(enc_all_outputs, *args)\n# Return decoder output only\nreturn self .decoder(dec_X, dec_state)[ 0]\nInthenextsection,wewillseehowtoapplyRNNstodesignsequence-to-sequencemodels\nbased on this encoder\u2013decoder architecture.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bdada0f-7628-4949-aafe-e7957356e4f5": {"__data__": {"id_": "5bdada0f-7628-4949-aafe-e7957356e4f5", "embedding": null, "metadata": {"page_label": "396", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "620bc0bb-832c-483a-99ec-7851e6154b33", "node_type": "4", "metadata": {"page_label": "396", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9e6adc02c10b99eae78290ba9c595b674b3ac7f6c1b66ed7ea26817d9c14195b", "class_name": "RelatedNodeInfo"}}, "text": "396 Modern Recurrent Neural Networks\n15110.6.4Summary\nEncoder-decoder architectures can handle inputs and outputs that both consist of variable-\nlengthsequencesandthusaresuitableforsequence-to-sequenceproblemssuchasmachine\ntranslation. The encoder takes a variable-length sequence as input and transforms it into a\nstatewitha fixedshape. Thedecoder mapsthe encodedstateofa fixedshapeto avariable-\nlength sequence.\n10.6.5Exercises\n1.Suppose that we use neural networks to implement the encoder\u2013decoder architecture.\nDo the encoder and the decoder have to be the same type of neural network?\n2.Besides machine translation, can you think of another application where the encoder\u2013\ndecoder architecture can be applied?\nDiscussions151.\n10.7Sequence-to-SequenceLearning forMachine\nTranslation\nIn so-called sequence-to-sequence problems such as machine translation (as discussed in\nSection10.5 ),whereinputsandoutputseachconsistofvariable-lengthunalignedsequences,\nwe generally rely on encoder\u2013decoder architectures ( Section 10.6 ). In this section, we will\ndemonstrate the application of an encoder\u2013decoder architecture, where both the encoder\nand decoder are implemented as RNNs, to the task of machine translation ( Choet al.,\n2014,Sutskever etal., 2014).\nHere, the encoder RNN will take a variable-length sequence as input and transform it into\na fixed-shape hidden state. Later, in Chapter 11 , we will introduce attention mechanisms,\nwhich allow us to access encoded inputs without having to compress the entire input into a\nsingle fixed-length representation.\nThen to generate the output sequence, one token at a time, the decoder model, consisting\nof a separate RNN, will predict each successive target token given both the input sequence\nand the preceding tokens in the output. During training, the decoder will typically be con-\nditioned upon the preceding tokens in the official \u201cground truth\u201d label. However, at test\ntime, we will want to condition each output of the decoder on the tokens already predicted.\nNote that if we ignore the encoder, the decoder in a sequence-to-sequence architecture be-\nhaves just like a normal language model. Fig. 10.7.1 illustrates how to use two RNNs for\nsequence-to-sequence learning in machine translation.\nInFig. 10.7.1 , the special \u201c<eos>\u201d token marks the end of the sequence. Our model can\nstop making predictions once this token is generated. At the initial time step of the RNN\ndecoder, there are two special design decisions to be aware of: First, we begin every input", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2516, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4923efa5-3aa3-4cb8-9c22-b3a5ddc7ee96": {"__data__": {"id_": "4923efa5-3aa3-4cb8-9c22-b3a5ddc7ee96", "embedding": null, "metadata": {"page_label": "397", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f1341be-80db-4de4-bff7-636d6c7cd6fc", "node_type": "4", "metadata": {"page_label": "397", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a2390b30a776989674d37c75dffcd2d5e885e628846ec04b91346203d47e8888", "class_name": "RelatedNodeInfo"}}, "text": "397 Sequence-to-Sequence Learning for Machine Translation\ntFig. 10.7.1 Sequence-to-sequence learning with an RNN encoder and an RNN decoder.\nwithaspecialbeginning-of-sequence\u201c<bos>\u201dtoken. Second,wemayfeedthefinalhidden\nstate of the encoder into the decoder at every single decoding time step ( Choet al., 2014).\nIn some other designs, such as that of Sutskever et al.(2014), the final hidden state of the\nRNN encoder is used to initiate the hidden state of the decoder only at the first decoding\nstep.\nimport collections\nimport math\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n10.7.1TeacherForcing\nWhile running the encoder on the input sequence is relatively straightforward, handling\nthe input and output of the decoder requires more care. The most common approach is\nsometimes called teacher forcing . Here, the original target sequence (token labels) is fed\ninto the decoder as input. More concretely, the special beginning-of-sequence token and\nthe original target sequence, excluding the final token, are concatenated as input to the\ndecoder,whilethedecoderoutput(labelsfortraining)istheoriginaltargetsequence,shifted\nby one token: \u201c<bos>\u201d, \u201cIls\u201d, \u201cregardent\u201d, \u201c.\u201d !\u201cIls\u201d, \u201cregardent\u201d, \u201c.\u201d, \u201c<eos>\u201d ( Fig.\n10.7.1).\nOur implementation in Section 10.5.3 prepared training data for teacher forcing, where\nshifting tokens for self-supervised learning is similar to the training of language models in\nSection 9.3 . An alternative approach is to feed the predicted token from the previous time\nstep as the current input to the decoder.\nIn the following, we explain the design depicted in Fig. 10.7.1 in greater detail. We will\ntrain this model for machine translation on the English\u2013French dataset as introduced in\nSection 10.5 .\n10.7.2Encoder\nRecall that the encoder transforms an input sequence of variable length into a fixed-shape\ncontextvariable c(seeFig. 10.7.1 ).\nConsiderasinglesequenceexample(batchsize1). Supposetheinputsequenceis \ud835\udc651,...,\ud835\udc65\ud835\udc47,\nsuch that\ud835\udc65\ud835\udc61is the\ud835\udc61thtoken. At time step \ud835\udc61, the RNN transforms the input feature vector x\ud835\udc61", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2095, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d035c769-9d32-478c-a883-ce501cd61314": {"__data__": {"id_": "d035c769-9d32-478c-a883-ce501cd61314", "embedding": null, "metadata": {"page_label": "398", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f1a841b7-bd18-40ad-9be6-310d9c74470a", "node_type": "4", "metadata": {"page_label": "398", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6a3f53ee722267863b984905ad88ef2be9d163fd5907a318449eac748a988c27", "class_name": "RelatedNodeInfo"}}, "text": "398 Modern Recurrent Neural Networks\nfor\ud835\udc65\ud835\udc61andthehiddenstate h\ud835\udc61\u00001fromtheprevioustimestepintothecurrenthiddenstate h\ud835\udc61.\nWecanuseafunction \ud835\udc53toexpressthetransformationoftheRNN\u2019srecurrentlayer:\nh\ud835\udc61=\ud835\udc53\u00b9x\ud835\udc61,h\ud835\udc61\u00001\u00ba. (10.7.1)\nIn general, the encoder transforms the hidden states at all time steps into a context variable\nthrough a customized function \ud835\udc5e:\nc=\ud835\udc5e\u00b9h1,...,h\ud835\udc47\u00ba. (10.7.2)\nFor example, in Fig. 10.7.1 , the context variable is just the hidden state h\ud835\udc47correspond-\ning to the encoder RNN\u2019s representation after processing the final token of the input se-\nquence.\nInthisexample,wehaveusedaunidirectionalRNNtodesigntheencoder,wherethehidden\nstate only depends on the input subsequence at and before the time step of the hidden state.\nWe can also construct encoders using bidirectional RNNs. In this case, a hidden state\ndependsonthesubsequencebeforeandafterthetimestep(includingtheinputatthecurrent\ntime step), which encodes the information of the entire sequence.\nNow let\u2019s implement the RNN encoder. Note that we use an embedding layer to obtain\nthe feature vector for each token in the input sequence. The weight of an embedding\nlayer is a matrix, where the number of rows corresponds to the size of the input vocab-\nulary ( vocab_size ) and number of columns corresponds to the feature vector\u2019s dimension\n(embed_size ). Foranyinputtokenindex \ud835\udc56,theembeddinglayerfetchesthe \ud835\udc56throw(starting\nfrom 0) of the weight matrix to return its feature vector. Here we implement the encoder\nwith a multilayer GRU.\ndef init_seq2seq (module): #@save\n\"\"\"Initialize weights for sequence-to-sequence learning.\"\"\"\niftype (module) ==nn.Linear:\nnn.init .xavier_uniform_(module .weight)\niftype (module) ==nn.GRU:\nfor param inmodule ._flat_weights_names:\nif\"weight \"inparam:\nnn.init .xavier_uniform_(module ._parameters[param])\nclass Seq2SeqEncoder (d2l .Encoder): #@save\n\"\"\"The RNN encoder for sequence-to-sequence learning.\"\"\"\ndef __init__ (self , vocab_size, embed_size, num_hiddens, num_layers,\ndropout =0):\nsuper ().__init__ ()\nself .embedding =nn.Embedding(vocab_size, embed_size)\nself .rnn =d2l.GRU(embed_size, num_hiddens, num_layers, dropout)\nself .apply(init_seq2seq)\ndef forward (self , X, *args):\n# X shape: (batch_size, num_steps)\nembs =self .embedding(X .t().type(torch .int64))\n# embs shape: (num_steps, batch_size, embed_size)\noutputs, state =self .rnn(embs)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2367, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aae1a597-6ae8-4090-9a1f-de46b6135c13": {"__data__": {"id_": "aae1a597-6ae8-4090-9a1f-de46b6135c13", "embedding": null, "metadata": {"page_label": "399", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d51c34ff-d274-434d-a6b6-109da4fff6ae", "node_type": "4", "metadata": {"page_label": "399", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "29c532059e53dd4e02d42d66d449c3b904c78403d26687211643d80c04c1fa2a", "class_name": "RelatedNodeInfo"}}, "text": "399 Sequence-to-Sequence Learning for Machine Translation\n(continued from previous page)\n# outputs shape: (num_steps, batch_size, num_hiddens)\n# state shape: (num_layers, batch_size, num_hiddens)\nreturn outputs, state\nLet\u2019s use a concrete example to illustrate the above encoder implementation. Below, we\ninstantiateatwo-layerGRUencoderwhosenumberofhiddenunitsis16. Givenaminibatch\nof sequence inputs X(batch size =4; number of time steps =9), the hidden states of the\nfinal layer at all the time steps ( enc_outputs returned by the encoder\u2019s recurrent layers)\nare a tensor of shape (number of time steps, batch size, number of hidden units).\nvocab_size, embed_size, num_hiddens, num_layers =10,8,16,2\nbatch_size, num_steps =4,9\nencoder =Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\nX=torch .zeros((batch_size, num_steps))\nenc_outputs, enc_state =encoder(X)\nd2l.check_shape(enc_outputs, (num_steps, batch_size, num_hiddens))\nSince we are using a GRU here, the shape of the multilayer hidden states at the final time\nstep is (number of hidden layers, batch size, number of hidden units).\nd2l.check_shape(enc_state, (num_layers, batch_size, num_hiddens))\n10.7.3Decoder\nGivenatargetoutputsequence \ud835\udc661,\ud835\udc662,...,\ud835\udc66\ud835\udc470foreachtimestep \ud835\udc610(weuse\ud835\udc610todifferentiate\nfrom the input sequence time steps), the decoder assigns a predicted probability to each\npossible token occurring at step \ud835\udc66\ud835\udc610\u00b81conditioned upon the previous tokens in the target\n\ud835\udc661,...,\ud835\udc66\ud835\udc610and the context variable c, i.e.,\ud835\udc43\u00b9\ud835\udc66\ud835\udc610\u00b81j\ud835\udc661,...,\ud835\udc66\ud835\udc610,c\u00ba.\nTo predict the subsequent token \ud835\udc610\u00b81in the target sequence, the RNN decoder takes the\nprevious step\u2019s target token \ud835\udc66\ud835\udc610, the hidden RNN state from the previous time step s\ud835\udc610\u00001,\nand the context variable cas its input, and transforms them into the hidden state s\ud835\udc610at the\ncurrent time step. We can use a function \ud835\udc54to express the transformation of the decoder\u2019s\nhidden layer:\ns\ud835\udc610=\ud835\udc54\u00b9\ud835\udc66\ud835\udc610\u00001,c,s\ud835\udc610\u00001\u00ba. (10.7.3)\nAfter obtaining the hidden state of the decoder, we can use an output layer and the softmax\noperation to compute the predictive distribution \ud835\udc5d\u00b9\ud835\udc66\ud835\udc610\u00b81j\ud835\udc661,...,\ud835\udc66\ud835\udc610,c\u00baover the subse-\nquent output token \ud835\udc610\u00b81.\nFollowing Fig. 10.7.1 , when implementing the decoder as follows, we directly use the hid-\nden state at the final time step of the encoder to initialize the hidden state of the decoder.\nThis requires that the RNN encoder and the RNN decoder have the same number of lay-\ners and hidden units. To further incorporate the encoded input sequence information, the\ncontext variable is concatenated with the decoder input at all the time steps. To predict the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21508e82-51ca-4f85-b375-af0aa6674294": {"__data__": {"id_": "21508e82-51ca-4f85-b375-af0aa6674294", "embedding": null, "metadata": {"page_label": "400", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2ef67385-f61d-47e6-87bd-29fc4c775555", "node_type": "4", "metadata": {"page_label": "400", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "53ba486ef092b753f627c5aec960617b13eb94f0688c804f87f9f5f80240fc9e", "class_name": "RelatedNodeInfo"}}, "text": "400 Modern Recurrent Neural Networks\nprobability distribution of the output token, we use a fully connected layer to transform the\nhidden state at the final layer of the RNN decoder.\nclass Seq2SeqDecoder (d2l .Decoder):\n\"\"\"The RNN decoder for sequence to sequence learning.\"\"\"\ndef __init__ (self , vocab_size, embed_size, num_hiddens, num_layers,\ndropout =0):\nsuper ().__init__ ()\nself .embedding =nn.Embedding(vocab_size, embed_size)\nself .rnn =d2l.GRU(embed_size +num_hiddens, num_hiddens,\nnum_layers, dropout)\nself .dense =nn.LazyLinear(vocab_size)\nself .apply(init_seq2seq)\ndef init_state (self , enc_all_outputs, *args):\nreturn enc_all_outputs\ndef forward (self , X, state):\n# X shape: (batch_size, num_steps)\n# embs shape: (num_steps, batch_size, embed_size)\nembs =self .embedding(X .t().type(torch .int32))\nenc_output, hidden_state =state\n# context shape: (batch_size, num_hiddens)\ncontext =enc_output[ -1]\n# Broadcast context to (num_steps, batch_size, num_hiddens)\ncontext =context .repeat(embs .shape[ 0],1,1)\n# Concat at the feature dimension\nembs_and_context =torch .cat((embs, context), -1)\noutputs, hidden_state =self .rnn(embs_and_context, hidden_state)\noutputs =self .dense(outputs) .swapaxes( 0,1)\n# outputs shape: (batch_size, num_steps, vocab_size)\n# hidden_state shape: (num_layers, batch_size, num_hiddens)\nreturn outputs, [enc_output, hidden_state]\nTo illustrate the implemented decoder, below we instantiate it with the same hyperparam-\neters from the aforementioned encoder. As we can see, the output shape of the decoder\nbecomes (batch size, number of time steps, vocabulary size), where the final dimension of\nthe tensor stores the predicted token distribution.\ndecoder =Seq2SeqDecoder(vocab_size, embed_size, num_hiddens, num_layers)\nstate =decoder .init_state(encoder(X))\ndec_outputs, state =decoder(X, state)\nd2l.check_shape(dec_outputs, (batch_size, num_steps, vocab_size))\nd2l.check_shape(state[ 1], (num_layers, batch_size, num_hiddens))\nThelayersintheaboveRNNencoder\u2013decodermodelaresummarizedin Fig.10.7.2 .\n10.7.4Encoder\u2013DecoderforSequence-to-SequenceLearning\nPutting it all together in code yields the following:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "966e7a9a-f5bb-4755-93d7-96307ca32cb8": {"__data__": {"id_": "966e7a9a-f5bb-4755-93d7-96307ca32cb8", "embedding": null, "metadata": {"page_label": "401", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27f77c74-98cd-4841-8e75-0482ac84a378", "node_type": "4", "metadata": {"page_label": "401", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b05578733aa2f8d6cff4a68975b6097a049985b54a4d4425402fa326347da4ec", "class_name": "RelatedNodeInfo"}}, "text": "401 Sequence-to-Sequence Learning for Machine Translation\ntFig. 10.7.2 Layers in an RNN encoder\u2013decoder model.\nclass Seq2Seq (d2l .EncoderDecoder): #@save\n\"\"\"The RNN encoder--decoder for sequence to sequence learning.\"\"\"\ndef __init__ (self , encoder, decoder, tgt_pad, lr):\nsuper ().__init__ (encoder, decoder)\nself .save_hyperparameters()\ndef validation_step (self , batch):\nY_hat =self (*batch[: -1])\nself .plot( 'loss ',self .loss(Y_hat, batch[ -1]), train =False )\ndef configure_optimizers (self ):\n# Adam optimizer is used here\nreturn torch .optim .Adam( self .parameters(), lr =self .lr)\n10.7.5LossFunction with Masking\nAt each time step, the decoder predicts a probability distribution for the output tokens.\nAs with language modeling, we can apply softmax to obtain the distribution and calculate\nthe cross-entropy loss for optimization. Recall from Section 10.5 that the special padding\ntokens are appended to the end of sequences and so sequences of varying lengths can be\nefficientlyloadedinminibatchesofthesameshape. However,predictionofpaddingtokens\nshould be excluded from loss calculations. To this end, we can mask irrelevant entries\nwith zero values so that multiplication of any irrelevant prediction with zero equates to\nzero.\n@d2l .add_to_class(Seq2Seq)\ndef loss (self , Y_hat, Y):\nl=super (Seq2Seq, self ).loss(Y_hat, Y, averaged =False )\nmask =(Y.reshape( -1)!=self .tgt_pad) .type(torch .float32)\nreturn (l*mask) .sum() /mask .sum()\n10.7.6Training\nNow we can create and train an RNN encoder\u2013decoder model for sequence-to-sequence\nlearning on the machine translation dataset.\ndata =d2l.MTFraEng(batch_size =128)\nembed_size, num_hiddens, num_layers, dropout =256,256,2,0.2\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1718, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac52f110-579a-43f1-8d50-0f658cd0f73a": {"__data__": {"id_": "ac52f110-579a-43f1-8d50-0f658cd0f73a", "embedding": null, "metadata": {"page_label": "402", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26394e48-a8b2-469d-bd93-f350850ce6e4", "node_type": "4", "metadata": {"page_label": "402", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "df5d470a0e9510d407a9799f113bdb1407d66049ff1cd37014e581a2d0927b7c", "class_name": "RelatedNodeInfo"}}, "text": "402 Modern Recurrent Neural Networks\n(continued from previous page)\nencoder =Seq2SeqEncoder(\nlen(data .src_vocab), embed_size, num_hiddens, num_layers, dropout)\ndecoder =Seq2SeqDecoder(\nlen(data .tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nmodel =Seq2Seq(encoder, decoder, tgt_pad =data .tgt_vocab[ '<pad> '],\nlr=0.005 )\ntrainer =d2l.Trainer(max_epochs =30, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\n10.7.7Prediction\nTopredicttheoutputsequenceateachstep,thepredictedtokenfromtheprevioustimestep\nis fed into the decoder as an input. One simple strategy is to sample whichever token that\nhas been assigned by the decoder the highest probability when predicting at each step. As\nintraining,attheinitialtimestepthebeginning-of-sequence(\u201c<bos>\u201d)tokenisfedintothe\ndecoder. This prediction process is illustrated in Fig. 10.7.3 . When the end-of-sequence\n(\u201c<eos>\u201d) token is predicted, the prediction of the output sequence is complete.\ntFig. 10.7.3 Predicting the output sequence token by token using an RNN encoder\u2013decoder.\nIn the next section, we will introduce more sophisticated strategies based on beam search\n(Section 10.8 ).\n@d2l .add_to_class(d2l .EncoderDecoder) #@save\ndef predict_step (self , batch, device, num_steps,\nsave_attention_weights =False ):\nbatch =[a.to(device) for ainbatch]\nsrc, tgt, src_valid_len, _ =batch\nenc_all_outputs =self .encoder(src, src_valid_len)\ndec_state =self .decoder .init_state(enc_all_outputs, src_valid_len)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1502, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1783f64d-549a-48a8-aa95-dc3c228ff8a1": {"__data__": {"id_": "1783f64d-549a-48a8-aa95-dc3c228ff8a1", "embedding": null, "metadata": {"page_label": "403", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc93ac9b-5e8f-40fb-8223-a1bfcc3478e7", "node_type": "4", "metadata": {"page_label": "403", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d26b125b13734501be95db882989bd18d310c9ba4743be5ff8e4e7dba5f414a9", "class_name": "RelatedNodeInfo"}}, "text": "403 Sequence-to-Sequence Learning for Machine Translation\n(continued from previous page)\noutputs, attention_weights =[tgt[:, ( 0)].unsqueeze( 1), ], []\nfor _inrange (num_steps):\nY, dec_state =self .decoder(outputs[ -1], dec_state)\noutputs .append(Y .argmax( 2))\n# Save attention weights (to be covered later)\nifsave_attention_weights:\nattention_weights .append( self .decoder .attention_weights)\nreturn torch .cat(outputs[ 1:], 1), attention_weights\n10.7.8Evaluationof PredictedSequences\nWecanevaluateapredictedsequencebycomparingitwiththetargetsequence(theground\ntruth). Butwhatpreciselyistheappropriatemeasureforcomparingsimilaritybetweentwo\nsequences?\nBilingual Evaluation Understudy (BLEU), though originally proposed for evaluating ma-\nchinetranslationresults( Papinenietal., 2002), hasbeenextensivelyusedinmeasuringthe\nqualityofoutputsequencesfordifferentapplications. Inprinciple,forany \ud835\udc5b-gram(Section\n9.3.1)inthepredictedsequence,BLEUevaluateswhetherthis \ud835\udc5b-gramappearsinthetarget\nsequence.\nDenote by\ud835\udc5d\ud835\udc5bthe precision of an \ud835\udc5b-gram, defined as the ratio of the number of matched\n\ud835\udc5b-grams in the predicted and target sequences to the number of \ud835\udc5b-grams in the predicted\nsequence. To explain, given a target sequence \ud835\udc34,\ud835\udc35,\ud835\udc36,\ud835\udc37,\ud835\udc38,\ud835\udc39, and a predicted sequence\n\ud835\udc34,\ud835\udc35,\ud835\udc35,\ud835\udc36,\ud835\udc37, we have\ud835\udc5d1=4\u009d5,\ud835\udc5d2=3\u009d4,\ud835\udc5d3=1\u009d3, and\ud835\udc5d4=0. Now let len label\nand len predbe the numbers of tokens in the target sequence and the predicted sequence,\nrespectively. Then, BLEU is defined as\nexp\u0012\nmin\u0012\n0,1\u0000lenlabel\nlenpred\u0013\u0013\ud835\udc58\u00d6\n\ud835\udc5b=1\ud835\udc5d1\u009d2\ud835\udc5b\n\ud835\udc5b, (10.7.4)\nwhere\ud835\udc58is the longest \ud835\udc5b-gram for matching.\nBased on the definition of BLEU in (10.7.4 ), whenever the predicted sequence is the same\nas the target sequence, BLEU is 1. Moreover, since matching longer \ud835\udc5b-grams is more diffi-\ncult,BLEUassignsagreaterweightwhenalonger \ud835\udc5b-gramhashighprecision. Specifically,\nwhen\ud835\udc5d\ud835\udc5bis fixed,\ud835\udc5d1\u009d2\ud835\udc5b\n\ud835\udc5bincreases as \ud835\udc5bgrows (the original paper uses \ud835\udc5d1\u009d\ud835\udc5b\n\ud835\udc5b). Furthermore,\nsince predicting shorter sequences tends to yield a higher \ud835\udc5d\ud835\udc5bvalue, the coefficient before\nthe multiplication term in (10.7.4 )penalizes shorter predicted sequences. For example,\nwhen\ud835\udc58=2, given the target sequence \ud835\udc34,\ud835\udc35,\ud835\udc36,\ud835\udc37,\ud835\udc38,\ud835\udc39and the predicted sequence \ud835\udc34,\ud835\udc35,\nalthough\ud835\udc5d1=\ud835\udc5d2=1, the penalty factor exp\u00b91\u00006\u009d2\u00ba\u00190.14lowers the BLEU.\nWe implement the BLEU measure as follows.\ndef bleu (pred_seq, label_seq, k): #@save\n\"\"\"Compute the BLEU.\"\"\"\npred_tokens, label_tokens =pred_seq .split( ''), label_seq .split( '')\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b7c6cdf-3d00-44e0-bd62-21a922bb2a37": {"__data__": {"id_": "6b7c6cdf-3d00-44e0-bd62-21a922bb2a37", "embedding": null, "metadata": {"page_label": "404", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20522f53-d63a-488f-9e79-8b40bfc00b25", "node_type": "4", "metadata": {"page_label": "404", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a27d97b23b06eeb94c6685ed12c5024fc61fc0dacb0c75a813b6c317f6803b24", "class_name": "RelatedNodeInfo"}}, "text": "404 Modern Recurrent Neural Networks\n(continued from previous page)\nlen_pred, len_label =len(pred_tokens), len(label_tokens)\nscore =math .exp( min(0,1-len_label /len_pred))\nfor ninrange (1,min(k, len_pred) +1):\nnum_matches, label_subs =0, collections .defaultdict( int)\nfor iinrange (len_label -n+1):\nlabel_subs[ ''.join(label_tokens[i: i +n])] +=1\nfor iinrange (len_pred -n+1):\niflabel_subs[ ''.join(pred_tokens[i: i +n])] >0:\nnum_matches +=1\nlabel_subs[ ''.join(pred_tokens[i: i +n])] -=1\nscore *=math .pow(num_matches /(len_pred -n+1), math .pow( 0.5, n))\nreturn score\nIn the end, we use the trained RNN encoder\u2013decoder to translate a few English sentences\ninto French and compute the BLEU of the results.\nengs =['go . ','i lost . ','he\\'s calm . ','i\\'m home . ']\nfras =['va ! ','j\\'ai perdu . ','il est calme . ','je suis chez moi . ']\npreds, _ =model .predict_step(\ndata .build(engs, fras), d2l .try_gpu(), data .num_steps)\nfor en, fr, p inzip(engs, fras, preds):\ntranslation =[]\nfor token indata .tgt_vocab .to_tokens(p):\niftoken =='<eos> ':\nbreak\ntranslation .append(token)\nprint (f'{en}=>{translation }, bleu, '\nf'{bleu( \"\".join(translation), fr, k=2):.3f}')\ngo.=>['va','!'], bleu, 1.000\ni lost .=>[\"j'ai\",'perdu ','.'], bleu, 1.000\nhe's calm . => [ 'elle ','court ','.'], bleu,0.000\ni'm home . => [ 'je','suis ','chez ','moi','.'], bleu,1.000\n10.7.9Summary\nFollowing the design of the encoder\u2013decoder architecture, we can use two RNNs to design\na model for sequence-to-sequence learning. In encoder\u2013decoder training, the teacher forc-\ning approach feeds original output sequences (in contrast to predictions) into the decoder.\nWhen implementing the encoder and the decoder, we can use multilayer RNNs. We can\nusemaskstofilteroutirrelevantcomputations, suchaswhencalculatingtheloss. Foreval-\nuating output sequences, BLEU is a popular measure that matches \ud835\udc5b-grams between the\npredicted sequence and the target sequence.\n10.7.10Exercises\n1.Can you adjust the hyperparameters to improve the translation results?\n2.Rerun the experiment without using masks in the loss calculation. What results do you\nobserve? Why?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2120, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f89070ba-8c07-420c-b843-0a1fa8ca5efb": {"__data__": {"id_": "f89070ba-8c07-420c-b843-0a1fa8ca5efb", "embedding": null, "metadata": {"page_label": "405", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "835f5ae4-f5d1-48f3-a9d5-fd1464fcfca9", "node_type": "4", "metadata": {"page_label": "405", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "953336e4f1b623710b9ab0c8ec92ad1771cd56ff0143c265a16e3f4c1ad9d223", "class_name": "RelatedNodeInfo"}}, "text": "405 Beam Search\n1523.If the encoder and the decoder differ in the number of layers or the number of hidden\nunits, how can we initialize the hidden state of the decoder?\n4.In training, replace teacher forcing with feeding the prediction at the previous time step\ninto the decoder. How does this influence the performance?\n5.Rerun the experiment by replacing GRU with LSTM.\n6.Are there any other ways to design the output layer of the decoder?\nDiscussions152.\n10.8Beam Search\nInSection 10.7 , we introduced the encoder\u2013decoder architecture, and the standard tech-\nniques for training them end-to-end. However, when it came to test-time prediction, we\nmentioned only the greedystrategy, where we select at each time step the token given the\nhighest predicted probability of coming next, until, at some time step, we find that we have\npredicted the special end-of-sequence \u201c<eos>\u201d token. In this section, we will begin by for-\nmalizing this greedysearch strategy and identifying some problems that practitioners tend\ntoruninto. Subsequently,wecomparethisstrategywithtwoalternatives: exhaustivesearch\n(illustrative but not practical) and beamsearch (the standard method in practice).\nLet\u2019s begin by setting up our mathematical notation, borrowing conventions from Section\n10.7. At any time step \ud835\udc610, the decoder outputs predictions representing the probability of\neach token in the vocabulary coming next in the sequence (the likely value of \ud835\udc66\ud835\udc610\u00b81), con-\nditioned on the previous tokens \ud835\udc661,...,\ud835\udc66\ud835\udc610and the context variable c, produced by the\nencoder to represent the input sequence. To quantify computational cost, denote by Ythe\noutput vocabulary (including the special end-of-sequence token \u201c<eos>\u201d). Let\u2019s also spec-\nify the maximum number of tokens of an output sequence as \ud835\udc470. Our goal is to search for\nan ideal output from all O\u00b9jYj\ud835\udc470\u00bapossible output sequences. Note that this slightly over-\nestimates the number of distinct outputs because there are no subsequent tokens once the\n\u201c<eos>\u201d token occurs. However, for our purposes, this number roughly captures the size\nof the search space.\n10.8.1GreedySearch\nConsider the simple greedy search strategy from Section 10.7 . Here, at any time step \ud835\udc610,\nwe simply select the token with the highest conditional probability from Y, i.e.,\n\ud835\udc66\ud835\udc610=argmax\n\ud835\udc662Y\ud835\udc43\u00b9\ud835\udc66j\ud835\udc661,...,\ud835\udc66\ud835\udc610\u00001,c\u00ba.(10.8.1)\nOnceourmodeloutputs\u201c<eos>\u201d(orwereachthemaximumlength \ud835\udc470)theoutputsequence\nis completed.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2cf00503-d809-43be-a4c6-0abca0d633ae": {"__data__": {"id_": "2cf00503-d809-43be-a4c6-0abca0d633ae", "embedding": null, "metadata": {"page_label": "406", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d6cabf4f-e2cf-4834-8c91-54fba7ddb801", "node_type": "4", "metadata": {"page_label": "406", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cb54dfbaad0f868094dbda5a274014bcdfeb6b1fb0c5ed05f5b623b207347505", "class_name": "RelatedNodeInfo"}}, "text": "406 Modern Recurrent Neural Networks\nThisstrategymightlookreasonable,andinfactitisnotsobad! Consideringhowcomputa-\ntionallyundemandingitis,you\u2019dbehardpressedtogetmorebangforyourbuck. However,\nifweputasideefficiencyforaminute,itmightseemmorereasonabletosearchforthe most\nlikelysequence , not the sequence of (greedily selected) mostlikelytokens . It turns out that\nthesetwoobjectscanbequitedifferent. Themostlikelysequenceistheonethatmaximizes\nthe expression\u00ce\ud835\udc470\n\ud835\udc610=1\ud835\udc43\u00b9\ud835\udc66\ud835\udc610j\ud835\udc661,...,\ud835\udc66\ud835\udc610\u00001,c\u00ba. In our machine translation example, if the\ndecoder truly recovered the probabilities of the underlying generative process, then this\nwould give us the most likely translation. Unfortunately, there is no guarantee that greedy\nsearch will give us this sequence.\nLet\u2019s illustrate it with an example. Suppose that there are four tokens \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, and\n\u201c<eos>\u201dintheoutputdictionary. In Fig.10.8.1 ,thefournumbersundereachtimesteprep-\nresent the conditional probabilities of generating \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, and \u201c<eos>\u201d respectively,\nat that time step.\ntFig. 10.8.1 At each time step, greedy search selects the token with the highest conditional probability.\nAt each time step, greedy search selects the token with the highest conditional probability.\nTherefore, the output sequence \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, and \u201c<eos>\u201d will be predicted ( Fig. 10.8.1 ).\nTheconditionalprobabilityofthisoutputsequenceis 0.5\u00020.4\u00020.4\u00020.6=0.048.\nNext, let\u2019s look at another example in Fig. 10.8.2 . Unlike in Fig. 10.8.1 , at time step 2 we\nselect the token \u201cC\u201d, which has the secondhighest conditional probability.\ntFig. 10.8.2 The four numbers under each time step represent the conditional probabilities of\ngenerating \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, and \u201c<eos>\u201d at that time step. At time step 2, the token \u201cC\u201d,\nwhich has the second highest conditional probability, is selected.\nSince the output subsequences at time steps 1 and 2, on which time step 3 is based, have\nchanged from \u201cA\u201d and \u201cB\u201d in Fig. 10.8.1 to \u201cA\u201d and \u201cC\u201d in Fig. 10.8.2 , the conditional\nprobability of each token at time step 3 has also changed in Fig. 10.8.2 . Suppose that\nwe choose the token \u201cB\u201d at time step 3. Now time step 4 is conditional on the output\nsubsequence at the first three time steps \u201cA\u201d, \u201cC\u201d, and \u201cB\u201d, which has changed from \u201cA\u201d,\n\u201cB\u201d,and\u201cC\u201din Fig.10.8.1 . Therefore,theconditionalprobabilityofgeneratingeachtoken\nat time step 4 in Fig. 10.8.2 is also different from that in Fig. 10.8.1 . As a result, the\nconditional probability of the output sequence \u201cA\u201d, \u201cC\u201d, \u201cB\u201d, and \u201c<eos>\u201d in Fig. 10.8.2\nis0.5\u00020.3\u00020.6\u00020.6=0.054, which is greater than that of greedy search in Fig. 10.8.1 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10efd3e2-9b49-4348-ad77-a7caee0e0d9a": {"__data__": {"id_": "10efd3e2-9b49-4348-ad77-a7caee0e0d9a", "embedding": null, "metadata": {"page_label": "407", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78c4bf7a-6919-42ae-ade7-f5478d491901", "node_type": "4", "metadata": {"page_label": "407", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4e5c26aea978c0865a1398c4c0754a97382d44056abff23c9cfb26ea7b7d6b73", "class_name": "RelatedNodeInfo"}}, "text": "407 Beam Search\nIn this example, the output sequence \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, and \u201c<eos>\u201d obtained by the greedy\nsearch is not optimal.\n10.8.2ExhaustiveSearch\nIf the goal is to obtain the most likely sequence, we may consider using exhaustivesearch :\nenumerate all the possible output sequences with their conditional probabilities, and then\noutput the one that scores the highest predicted probability.\nWhile this would certainly give us what we desire, it would come at a prohibitive com-\nputational cost ofO\u00b9jYj\ud835\udc470\u00ba, exponential in the sequence length and with an enormous\nbase given by the vocabulary size. For example, when jYj=10000and\ud835\udc470=10, both\nsmall numbers when compared with ones in real applications, we will need to evaluate\n1000010=1040sequences, which is already beyond the capabilities of any foreseeable\ncomputers. Ontheotherhand,thecomputationalcostofgreedysearchis O\u00b9jYj\ud835\udc470\u00ba: mirac-\nulously cheap but far from optimal. For example, when jYj=10000and\ud835\udc470=10, we only\nneed to evaluate 10000\u000210=105sequences.\n10.8.3Beam Search\nYou could view sequence decoding strategies as lying on a spectrum, with beam search\nstriking a compromise between the efficiency of greedy search and the optimality of ex-\nhaustive search. The most straightforward version of beam search is characterized by a\nsingle hyperparameter, the beam size ,\ud835\udc58. Let\u2019s explain this terminology. At time step 1,\nwe select the \ud835\udc58tokens with the highest predicted probabilities. Each of them will be the\nfirst token of \ud835\udc58candidate output sequences, respectively. At each subsequent time step,\nbased on the \ud835\udc58candidate output sequences at the previous time step, we continue to select\n\ud835\udc58candidate output sequences with the highest predicted probabilities from \ud835\udc58jYjpossible\nchoices.\ntFig. 10.8.3 The process of beam search (beam size =2; maximum length of an output sequence =3).\nThe candidate output sequences are A,C,AB,CE,ABD , and CED .\nFig. 10.8.3 demonstrates the process of beam search with an example. Suppose that the\noutput vocabulary contains only five elements: Y=f\ud835\udc34,\ud835\udc35,\ud835\udc36,\ud835\udc37,\ud835\udc38g, where one of them is", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2067, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "995d4a4c-de56-440e-bb54-10e1f423ec88": {"__data__": {"id_": "995d4a4c-de56-440e-bb54-10e1f423ec88", "embedding": null, "metadata": {"page_label": "408", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38af870f-8f73-4f26-b2b3-a615dcac6c2b", "node_type": "4", "metadata": {"page_label": "408", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4a1a9813ca6fd88c92cd10f5042551f2b3ea87aa2c592f33e0f2001c38ef3b9d", "class_name": "RelatedNodeInfo"}}, "text": "408 Modern Recurrent Neural Networks\n153\u201c<eos>\u201d. Letthebeamsizebetwoandthemaximumlengthofanoutputsequencebethree.\nAt time step 1, suppose that the tokens with the highest conditional probabilities \ud835\udc43\u00b9\ud835\udc661jc\u00ba\nare\ud835\udc34and\ud835\udc36. At time step 2, for all \ud835\udc6622Y,we compute\n\ud835\udc43\u00b9\ud835\udc34,\ud835\udc66 2jc\u00ba=\ud835\udc43\u00b9\ud835\udc34jc\u00ba\ud835\udc43\u00b9\ud835\udc662j\ud835\udc34,c\u00ba,\n\ud835\udc43\u00b9\ud835\udc36,\ud835\udc66 2jc\u00ba=\ud835\udc43\u00b9\ud835\udc36jc\u00ba\ud835\udc43\u00b9\ud835\udc662j\ud835\udc36,c\u00ba,(10.8.2)\nand pick the largest two among these ten values, say \ud835\udc43\u00b9\ud835\udc34,\ud835\udc35jc\u00baand\ud835\udc43\u00b9\ud835\udc36,\ud835\udc38jc\u00ba. Then at\ntime step 3, for all \ud835\udc6632Y, we compute\n\ud835\udc43\u00b9\ud835\udc34,\ud835\udc35,\ud835\udc66 3jc\u00ba=\ud835\udc43\u00b9\ud835\udc34,\ud835\udc35jc\u00ba\ud835\udc43\u00b9\ud835\udc663j\ud835\udc34,\ud835\udc35,c\u00ba,\n\ud835\udc43\u00b9\ud835\udc36,\ud835\udc38,\ud835\udc66 3jc\u00ba=\ud835\udc43\u00b9\ud835\udc36,\ud835\udc38jc\u00ba\ud835\udc43\u00b9\ud835\udc663j\ud835\udc36,\ud835\udc38,c\u00ba,(10.8.3)\nand pick the largest two among these ten values, say \ud835\udc43\u00b9\ud835\udc34,\ud835\udc35,\ud835\udc37jc\u00baand\ud835\udc43\u00b9\ud835\udc36,\ud835\udc38,\ud835\udc37jc\u00ba.\nAs a result, we get six candidates output sequences: (i) \ud835\udc34; (ii)\ud835\udc36; (iii)\ud835\udc34,\ud835\udc35; (iv)\ud835\udc36,\ud835\udc38; (v)\n\ud835\udc34,\ud835\udc35,\ud835\udc37; and (vi)\ud835\udc36,\ud835\udc38,\ud835\udc37.\nIn the end, we obtain the set of final candidate output sequences based on these six se-\nquences (e.g., discard portions including and after \u201c<eos>\u201d). Then we choose the output\nsequence which maximizes the following score:\n1\n\ud835\udc3f\ud835\udefclog\ud835\udc43\u00b9\ud835\udc661,...,\ud835\udc66\ud835\udc3fjc\u00ba=1\n\ud835\udc3f\ud835\udefc\ud835\udc3f\u00d5\n\ud835\udc610=1log\ud835\udc43\u00b9\ud835\udc66\ud835\udc610j\ud835\udc661,...,\ud835\udc66\ud835\udc610\u00001,c\u00ba; (10.8.4)\nhere\ud835\udc3fis the length of the final candidate sequence and \ud835\udefcis usually set to 0.75. Since a\nlonger sequence has more logarithmic terms in the summation of (10.8.4 ), the term\ud835\udc3f\ud835\udefcin\nthe denominator penalizes long sequences.\nThe computational cost of beam search is O\u00b9\ud835\udc58jYj\ud835\udc470\u00ba. This result is in between that of\ngreedy search and that of exhaustive search. Greedy search can be treated as a special case\nof beam search arising when the beam size is set to 1.\n10.8.4Summary\nSequence searching strategies include greedy search, exhaustive search, and beam search.\nBeam search provides a trade-off between accuracy and computational cost via the flexible\nchoice of the beam size.\n10.8.5Exercises\n1.Can we treat exhaustive search as a special type of beam search? Why or why not?\n2.Apply beam search in the machine translation problem in Section 10.7 . How does the\nbeam size affect the translation results and the prediction speed?\n3.Weusedlanguagemodelingforgeneratingtextfollowinguser-providedprefixesin Sec-\ntion 9.5. Which kind of search strategy does it use? Can you improve it?\nDiscussions153.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e379d22a-c04d-4adc-9af5-a3e2604cc116": {"__data__": {"id_": "e379d22a-c04d-4adc-9af5-a3e2604cc116", "embedding": null, "metadata": {"page_label": "409", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26ba1eb2-5253-4e94-aace-aef0d559431b", "node_type": "4", "metadata": {"page_label": "409", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0ff10e25b11950ee8220c4ffd7e723f156e54c73165770db5a611378010cf854", "class_name": "RelatedNodeInfo"}}, "text": "11 Attention Mechanisms and Transformers\nThe earliest years of the deep learning boom were driven primarily by results produced us-\ning the multilayer perceptron, convolutional network, and recurrent network architectures.\nRemarkably, the model architectures that underpinned many of deep learning\u2019s break-\nthroughsinthe2010shadchangedremarkablylittlerelativetotheirantecedentsdespitethe\nlapse of nearly 30 years. While plenty of new methodological innovations made their way\ninto most practitioner\u2019s toolkits\u2014ReLU activations, residual layers, batch normalization,\ndropout, and adaptive learning rate schedules come to mind\u2014the core underlying archi-\ntectures were clearly recognizable as scaled-up implementations of classic ideas. Despite\nthousandsofpapersproposingalternativeideas,modelsresemblingclassicalconvolutional\nneural networks ( Chapter 7 ) retained state-of-the-art status in computer vision and models\nresemblingSeppHochreiter\u2019soriginaldesignfortheLSTMrecurrentneuralnetwork( Sec-\ntion 10.1 ), dominated most applications in natural language processing. Arguably, to that\npoint, the rapid emergence of deep learning appeared to be primarily attributable to shifts\nin the available computational resources (thanks to innovations in parallel computing with\nGPUs) and the availability of massive data resources (thanks to cheap storage and Internet\nservices). While these factors may indeed remain the primary drivers behind this technol-\nogy\u2019s increasing power we are also witnessing, at long last, a sea change in the landscape\nof dominant architectures.\nAt the present moment, the dominant models for nearly all natural language processing\ntasks are based on the Transformer architecture. Given any new task in natural language\nprocessing, the default first-pass approach is to grab a large Transformer-based pretrained\nmodel, (e.g., BERT ( Devlinet al., 2018), ELECTRA ( Clarket al., 2020), RoBERTa ( Liu\net al., 2019), or Longformer ( Beltagyet al., 2020)) adapting the output layers as neces-\nsary, and fine-tuning the model on the available data for the downstream task. If you have\nbeen paying attention to the last few years of breathless news coverage centered on Ope-\nnAI\u2019s large language models, then you have been tracking a conversation centered on the\nGPT-2 and GPT-3 Transformer-based models ( Brownet al., 2020,Radfordet al., 2019).\nMeanwhile,thevisionTransformerhasemergedasadefaultmodelfordiversevisiontasks,\nincludingimagerecognition, objectdetection, semanticsegmentation, andsuperresolution\n(Dosovitskiy et al., 2021,Liuet al., 2021). Transformers also showed up as competitive\nmethods for speech recognition ( Gulatiet al., 2020), reinforcement learning ( Chenet al.,\n2021), and graph neural networks ( Dwivedi and Bresson, 2020 ).\nThe core idea behind the Transformer model is the attention mechanism , an innovation\nthat was originally envisioned as an enhancement for encoder\u2013decoder RNNs applied to\n409", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db63c922-ae5a-4c1f-b5e5-81db89bab57f": {"__data__": {"id_": "db63c922-ae5a-4c1f-b5e5-81db89bab57f", "embedding": null, "metadata": {"page_label": "410", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fd6f813-a6fa-4055-9eee-c05395734085", "node_type": "4", "metadata": {"page_label": "410", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f114c21a514d68f17b74dffe4fa8f8d5c0742b514b02d7dd0724214e2e8c7a89", "class_name": "RelatedNodeInfo"}}, "text": "410 Attention Mechanisms and Transformers\nsequence-to-sequence applications, such as machine translations ( Bahdanau et al., 2014).\nYou might recall that in the first sequence-to-sequence models for machine translation\n(Sutskever etal., 2014), theentireinputwascompressedbytheencoderintoasinglefixed-\nlength vector to be fed into the decoder. The intuition behind attention is that rather than\ncompressing the input, it might be better for the decoder to revisit the input sequence at\nevery step. Moreover, rather than always seeing the same representation of the input, one\nmight imagine that the decoder should selectively focus on particular parts of the input se-\nquence at particular decoding steps. Bahdanau\u2019s attention mechanism provided a simple\nmeansbywhichthedecodercoulddynamically attendtodifferentpartsoftheinputateach\ndecoding step. The high-level idea is that the encoder could produce a representation of\nlength equal to the original input sequence. Then, at decoding time, the decoder can (via\nsome control mechanism) receive as input a context vector consisting of a weighted sum\nof the representations on the input at each time step. Intuitively, the weights determine the\nextent to which each step\u2019s context \u201cfocuses\u201d on each input token, and the key is to make\nthis process for assigning the weights differentiable so that it can be learned along with all\nof the other neural network parameters.\nInitially, the idea was a remarkably successful enhancement to the recurrent neural net-\nworks that already dominated machine translation applications. The models performed\nbetterthantheoriginalencoder\u2013decodersequence-to-sequencearchitectures. Furthermore,\nresearchers noted that some nice qualitative insights sometimes emerged from inspecting\nthe pattern of attention weights. In translation tasks, attention models often assigned high\nattention weights to cross-lingual synonyms when generating the corresponding words in\nthe target language. For example, when translating the sentence \u201cmy feet hurt\u201d to \u201cj\u2019ai mal\nau pieds\u201d, the neural network might assign high attention weights to the representation of\n\u201cfeet\u201d when generating the corresponding French word \u201cpieds\u201d. These insights spurred\nclaims that attention models confer \u201cinterpretability\u201d although what precisely the atten-\ntion weights mean\u2014i.e., how, if at all, they should be interpreted remains a hazy research\ntopic.\nHowever, attention mechanisms soon emerged as more significant concerns, beyond their\nusefulnessasanenhancementforencoder\u2013decoderrecurrentneuralnetworksandtheirpu-\ntative usefulness for picking out salient inputs. Vaswani et al.(2017) proposed the Trans-\nformer architecture for machine translation, dispensing with recurrent connections alto-\ngether, and instead relying on cleverly arranged attention mechanisms to capture all rela-\ntionshipsamonginputandoutputtokens. Thearchitectureperformedremarkablywell,and\nby 2018 the Transformer began showing up in the majority of state-of-the-art natural lan-\nguageprocessingsystems. Moreover,atthesametime,thedominantpracticeinnaturallan-\nguage processing became to pretrain large-scale models on enormous generic background\ncorpora to optimize some self-supervised pretraining objective, and then to fine-tune these\nmodelsusingtheavailabledownstreamdata. ThegapbetweenTransformersandtraditional\narchitectures grew especially wide when applied in this pretraining paradigm, and thus the\nascendance of Transformers coincided with the ascendence of such large-scale pretrained\nmodels, now sometimes called foundationmodels (Bommasani etal., 2021).\nIn this chapter, we introduce attention models, starting with the most basic intuitions and", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3509d3de-ec8a-4ee4-b173-969e021decc3": {"__data__": {"id_": "3509d3de-ec8a-4ee4-b173-969e021decc3", "embedding": null, "metadata": {"page_label": "411", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9eb559bb-ca03-4dc1-9501-f02b7c9c169d", "node_type": "4", "metadata": {"page_label": "411", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3a42bf95e727a3a8a6b4636a36556fde79c8010926c428ca76c590809e58b5bc", "class_name": "RelatedNodeInfo"}}, "text": "411 Queries, Keys, and Values\nthe simplest instantiations of the idea. We then work our way up to the Transformer archi-\ntecture,thevisionTransformer,andthelandscapeofmodernTransformer-basedpretrained\nmodels.\n11.1Queries, Keys,and Values\nSo far all the networks we have reviewed crucially relied on the input being of a well-\ndefined size. For instance, the images in ImageNet are of size 224\u0002224pixels and CNNs\nare specifically tuned to this size. Even in natural language processing the input size for\nRNNs is well defined and fixed. Variable size is addressed by sequentially processing one\ntoken at a time, or by specially designed convolution kernels ( Kalchbrenner et al., 2014).\nThis approach can lead to significant problems when the input is truly of varying size with\nvaryinginformationcontent,suchasin Section10.7 inthetransformationoftext( Sutskever\net al., 2014). In particular, for long sequences it becomes quite difficult to keep track of\neverything that has already been generated or even viewed by the network. Even explicit\ntrackingheuristicssuchasproposedbyYang etal.(2016)onlyofferlimitedbenefit.\nComparethistodatabases. Intheirsimplestformtheyarecollectionsofkeys( \ud835\udc58)andvalues\n(\ud835\udc63). For instance, our database Dmight consist of tuples {(\u201cZhang\u201d, \u201cAston\u201d), (\u201cLipton\u201d,\n\u201cZachary\u201d), (\u201cLi\u201d, \u201cMu\u201d), (\u201cSmola\u201d, \u201cAlex\u201d), (\u201cHu\u201d, \u201cRachel\u201d), (\u201cWerness\u201d, \u201cBrent\u201d)}\nwith the last name being the key and the first name being the value. We can operate on\nD, for instance with the exact query ( \ud835\udc5e) for \u201cLi\u201d which would return the value \u201cMu\u201d. If\n(\u201cLi\u201d, \u201cMu\u201d) was not a record in D, there would be no valid answer. If we also allowed for\napproximate matches, we would retrieve (\u201cLipton\u201d, \u201cZachary\u201d) instead. This quite simple\nand trivial example nonetheless teaches us a number of useful things:\n\u000fWe can design queries \ud835\udc5ethat operate on ( \ud835\udc58,\ud835\udc63) pairs in such a manner as to be valid\nregardless of the database size.\n\u000fThe same query can receive different answers, according to the contents of the database.\n\u000fThe\u201ccode\u201dbeingexecutedforoperatingonalargestatespace(thedatabase)canbequite\nsimple (e.g., exact match, approximate match, top- \ud835\udc58).\n\u000fThere is no need to compress or simplify the database to make the operations effective.\nClearly we would not have introduced a simple database here if it wasn\u2019t for the purpose of\nexplainingdeeplearning. Indeed,thisleadstooneofthemostexcitingconceptsintroduced\nin deep learning in the past decade: the attention mechanism (Bahdanau et al., 2014). We\nwill cover the specifics of its application to machine translation later. For now, simply\nconsider the following: denote by Ddef=f\u00b9k1,v1\u00ba,...\u00b9k\ud835\udc5a,v\ud835\udc5a\u00baga database of \ud835\udc5atuples of\nkeysandvalues. Moreover, denote by qaquery. Then we can define the attention overD", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2745, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b8fff3a-0596-4258-91b5-c07f19dd94f4": {"__data__": {"id_": "8b8fff3a-0596-4258-91b5-c07f19dd94f4", "embedding": null, "metadata": {"page_label": "412", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1732a6f8-01c1-4218-b4f3-99d11cbe957e", "node_type": "4", "metadata": {"page_label": "412", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f191277827ce79d895ff56a5268e59a8d6df6d12385a4895ee9b83e5de277559", "class_name": "RelatedNodeInfo"}}, "text": "412 Attention Mechanisms and Transformers\nas\nAttention\u00b9q,D\u00badef=\ud835\udc5a\u00d5\n\ud835\udc56=1\ud835\udefc\u00b9q,k\ud835\udc56\u00bav\ud835\udc56, (11.1.1)\nwhere\ud835\udefc\u00b9q,k\ud835\udc56\u00ba 2R(\ud835\udc56=1,...,\ud835\udc5a) are scalar attention weights. The operation itself is\ntypically referred to as attentionpooling . The name attention derives from the fact that the\noperation pays particular attention to the terms for which the weight \ud835\udefcis significant (i.e.,\nlarge). As such, the attention over Dgenerates a linear combination of values contained in\nthe database. In fact, this contains the above example as a special case where all but one\nweight is zero. We have a number of special cases:\n\u000fTheweights \ud835\udefc\u00b9q,k\ud835\udc56\u00baarenonnegative. Inthiscasetheoutputoftheattentionmechanism\nis contained in the convex cone spanned by the values v\ud835\udc56.\n\u000fTheweights \ud835\udefc\u00b9q,k\ud835\udc56\u00baformaconvexcombination,i.e.,\u00cd\n\ud835\udc56\ud835\udefc\u00b9q,k\ud835\udc56\u00ba=1and\ud835\udefc\u00b9q,k\ud835\udc56\u00ba\u00150\nfor all\ud835\udc56. This is the most common setting in deep learning.\n\u000fExactlyoneoftheweights \ud835\udefc\u00b9q,k\ud835\udc56\u00bais1,whileallothersare 0. Thisisakintoatraditional\ndatabase query.\n\u000fAll weights are equal, i.e., \ud835\udefc\u00b9q,k\ud835\udc56\u00ba=1\n\ud835\udc5afor all\ud835\udc56. This amounts to averaging across the\nentire database, also called average pooling in deep learning.\nAcommonstrategyforensuringthattheweightssumupto 1istonormalizethemvia\n\ud835\udefc\u00b9q,k\ud835\udc56\u00ba=\ud835\udefc\u00b9q,k\ud835\udc56\u00ba\u00cd\n\ud835\udc57\ud835\udefc\u00b9q,k\ud835\udc57\u00ba. (11.1.2)\nIn particular, to ensure that the weights are also nonnegative, one can resort to exponenti-\nation. This means that we can now pick anyfunction\ud835\udc4e\u00b9q,k\u00baand then apply the softmax\noperation used for multinomial models to it via\n\ud835\udefc\u00b9q,k\ud835\udc56\u00ba=exp\u00b9\ud835\udc4e\u00b9q,k\ud835\udc56\u00ba\u00ba\u00cd\n\ud835\udc57exp\u00b9\ud835\udc4e\u00b9q,k\ud835\udc57\u00ba\u00ba. (11.1.3)\nThis operation is readily available in all deep learning frameworks. It is differentiable and\nits gradient never vanishes, all of which are desirable properties in a model. Note though,\nthe attention mechanism introduced above is not the only option. For instance, we can\ndesignanon-differentiableattentionmodelthatcanbetrainedusingreinforcementlearning\nmethods( Mnihetal., 2014). Asonewouldexpect, trainingsuchamodelisquitecomplex.\nConsequentlythebulkofmodernattentionresearchfollowstheframeworkoutlinedin Fig.\n11.1.1. We thus focus our exposition on this family of differentiable mechanisms.\nWhatisquiteremarkableisthattheactual\u201ccode\u201dforexecutingonthesetofkeysandvalues,\nnamely the query, can be quite concise, even though the space to operate on is significant.\nThisisadesirablepropertyforanetworklayerasitdoesnotrequiretoomanyparametersto\nlearn. Just as convenient is the fact that attention can operate on arbitrarily large databases\nwithout the need to change the way the attention pooling operation is performed.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a52e5ef7-7393-4e6c-a358-3645ce95d1da": {"__data__": {"id_": "a52e5ef7-7393-4e6c-a358-3645ce95d1da", "embedding": null, "metadata": {"page_label": "413", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "507ccc98-5fd9-47ca-ac07-25d22df75645", "node_type": "4", "metadata": {"page_label": "413", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7530dfceb7fdef2215125c723908b28d8ca3cf25df36cebc958d0fb100b04c1a", "class_name": "RelatedNodeInfo"}}, "text": "413 Queries, Keys, and Values\ntFig. 11.1.1 The attention mechanism computes a linear combination over values vivia attention\npooling, where weights are derived according to the compatibility between a query qand\nkeyski.\nimport torch\nfrom d2l import torch asd2l\n11.1.1Visualization\nOne of the benefits of the attention mechanism is that it can be quite intuitive, particularly\nwhen the weights are nonnegative and sum to 1. In this case we might interpret large\nweights as a way for the model to select components of relevance. While this is a good\nintuition, it is important to remember that it is just that, an intuition . Regardless, we may\nwant to visualize its effect on the given set of keys when applying a variety of different\nqueries. This function will come in handy later.\nWethusdefinethe show_heatmaps function. Notethatitdoesnottakeamatrix(ofattention\nweights) as its input but rather a tensor with four axes, allowing for an array of different\nqueries and weights. Consequently the input matrices has the shape (number of rows\nfor display, number of columns for display, number of queries, number of keys). This\nwill come in handy later on when we want to visualize the workings that are to design\nTransformers.\n#@save\ndef show_heatmaps (matrices, xlabel, ylabel, titles =None , figsize =(2.5,2.5),\ncmap ='Reds '):\n\"\"\"Show heatmaps of matrices.\"\"\"\nd2l.use_svg_display()\nnum_rows, num_cols, _, _ =matrices .shape\nfig, axes =d2l.plt.subplots(num_rows, num_cols, figsize =figsize,\nsharex =True , sharey =True , squeeze =False )\nfor i, (row_axes, row_matrices) inenumerate (zip(axes, matrices)):\nfor j, (ax, matrix) inenumerate (zip(row_axes, row_matrices)):\npcm =ax.imshow(matrix .detach() .numpy(), cmap =cmap)\nifi==num_rows -1:\nax.set_xlabel(xlabel)\nifj==0:\nax.set_ylabel(ylabel)\niftitles:\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78fbc8c8-c8e4-4ad4-9ac2-0919d814fd44": {"__data__": {"id_": "78fbc8c8-c8e4-4ad4-9ac2-0919d814fd44", "embedding": null, "metadata": {"page_label": "414", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af1b8f7a-4bdf-4406-b185-4b44a6ccb98a", "node_type": "4", "metadata": {"page_label": "414", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fd7f8d6716aab2c6fe79e798b292435c2f6f940ee2e969fba716d0d4fe00eb2d", "class_name": "RelatedNodeInfo"}}, "text": "414 Attention Mechanisms and Transformers\n(continued from previous page)\nax.set_title(titles[j])\nfig.colorbar(pcm, ax =axes, shrink =0.6);\nAs a quick sanity check let\u2019s visualize the identity matrix, representing a case where the\nattention weight is 1only when the query and the key are the same.\nattention_weights =torch .eye( 10).reshape(( 1,1,10,10))\nshow_heatmaps(attention_weights, xlabel ='Keys ', ylabel ='Queries ')\n11.1.2Summary\nThe attention mechanism allows us to aggregate data from many (key, value) pairs. So\nfar our discussion was quite abstract, simply describing a way to pool data. We have not\nexplained yet where those mysterious queries, keys, and values might arise from. Some\nintuition might help here: for instance, in a regression setting, the query might correspond\nto the location where the regression should be carried out. The keys are the locations\nwhere past data was observed and the values are the (regression) values themselves. This\nis the so-called Nadaraya\u2013Watson estimator ( Nadaraya, 1964 ,Watson, 1964 ) that we will\nbe studying in the next section.\nBy design, the attention mechanism provides a differentiable means of control by which a\nneural network can select elements from a set and to construct an associated weighted sum\nover representations.\n11.1.3Exercises\n1.Suppose that you wanted to reimplement approximate (key, query) matches as used in\nclassical databases, which attention function would you pick?\n2.Suppose that the attention function is given by \ud835\udc4e\u00b9q,k\ud835\udc56\u00ba=q>k\ud835\udc56and that k\ud835\udc56=v\ud835\udc56for\n\ud835\udc56=1,...,\ud835\udc5a. Denote by \ud835\udc5d\u00b9k\ud835\udc56;q\u00bathe probability distribution over keys when using the\nsoftmax normalization in (11.1.3 ). Prove thatrqAttention\u00b9q,D\u00ba=Cov\ud835\udc5d\u00b9k\ud835\udc56;q\u00ba\u00bbk\ud835\udc56\u00bc.\n3.Design a differentiable search engine using the attention mechanism.\n4.ReviewthedesignoftheSqueezeandExcitationNetworks( Huetal.,2018)andinterpret\nthem through the lens of the attention mechanism.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1892, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7c0f173-10c3-4700-bdac-279913bfbc8c": {"__data__": {"id_": "b7c0f173-10c3-4700-bdac-279913bfbc8c", "embedding": null, "metadata": {"page_label": "415", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9f3c7a6-ddd1-47dc-8859-a70e4a4b2be9", "node_type": "4", "metadata": {"page_label": "415", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8155a34c3b71da35e238b867a9507311f952777733d262fbe5d37543647ab40f", "class_name": "RelatedNodeInfo"}}, "text": "415 Attention Pooling by Similarity\n154\n155Discussions154.\n11.2Attention PoolingbySimilarity\nNow that we have introduced the primary components of the attention mechanism, let\u2019s\nuse them in a rather classical setting, namely regression and classification via kernel den-\nsity estimation ( Nadaraya, 1964 ,Watson, 1964 ). This detour simply provides additional\nbackground: it is entirely optional and can be skipped if needed. At their core, Nadaraya\u2013\nWatsonestimatorsrelyonsomesimilaritykernel \ud835\udefc\u00b9q,k\u00barelatingqueries qtokeys k. Some\ncommon kernels are\n\ud835\udefc\u00b9q,k\u00ba=exp\u0012\n\u00001\n2kq\u0000kk2\u0013\nGaussian;\n\ud835\udefc\u00b9q,k\u00ba=1ifkq\u0000kk\u00141 Boxcar;\n\ud835\udefc\u00b9q,k\u00ba=max\u00b90,1\u0000kq\u0000kk\u00baEpanechikov.(11.2.1)\nThere are many more choices that we could pick. See a Wikipedia article155for a more\nextensivereviewandhowthechoiceofkernelsisrelatedtokerneldensityestimation,some-\ntimes also called Parzen Windows (Parzen, 1957 ). All of the kernels are heuristic and can\nbe tuned. For instance, we can adjust the width, not only on a global basis but even on a\nper-coordinate basis. Regardless, all of them lead to the following equation for regression\nand classification alike:\n\ud835\udc53\u00b9q\u00ba=\u00d5\n\ud835\udc56v\ud835\udc56\ud835\udefc\u00b9q,k\ud835\udc56\u00ba\u00cd\n\ud835\udc57\ud835\udefc\u00b9q,k\ud835\udc57\u00ba. (11.2.2)\nInthecaseofa(scalar)regressionwithobservations \u00b9x\ud835\udc56,\ud835\udc66\ud835\udc56\u00baforfeaturesandlabelsrespec-\ntively, v\ud835\udc56=\ud835\udc66\ud835\udc56are scalars, k\ud835\udc56=x\ud835\udc56are vectors, and the query qdenotes the new location\nwhere\ud835\udc53should be evaluated. In the case of (multiclass) classification, we use one-hot-\nencoding of \ud835\udc66\ud835\udc56to obtain v\ud835\udc56. One of the convenient properties of this estimator is that it re-\nquiresnotraining. Evenmoreso, ifwesuitablynarrowthekernelwithincreasingamounts\nof data, the approach is consistent ( Mack and Silverman, 1982 ), i.e., it will converge to\nsome statistically optimal solution. Let\u2019s start by inspecting some kernels.\nimport numpy asnp\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\nd2l.use_svg_display()\n11.2.1Kernelsand Data\nAll the kernels \ud835\udefc\u00b9k,q\u00badefined in this section are translation and rotation invariant ; that\nis, if we shift and rotate kandqin the same manner, the value of \ud835\udefcremains unchanged.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2086, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b96e79c-9da4-4909-ab88-d34788aede01": {"__data__": {"id_": "1b96e79c-9da4-4909-ab88-d34788aede01", "embedding": null, "metadata": {"page_label": "416", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4dda1b94-0c0d-4778-98a9-201045855444", "node_type": "4", "metadata": {"page_label": "416", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b6185536542cb3501e59e944a305dae0631342008b7c7e1207c8cb8b0e25681d", "class_name": "RelatedNodeInfo"}}, "text": "416 Attention Mechanisms and Transformers\nFor simplicity we thus pick scalar arguments \ud835\udc58,\ud835\udc5e2Rand pick the key \ud835\udc58=0as the origin.\nThis yields:\n# Define some kernels\ndef gaussian (x):\nreturn torch .exp( -x**2/2)\ndef boxcar (x):\nreturn torch .abs(x) <1.0\ndef constant (x):\nreturn 1.0 +0*x\ndef epanechikov (x):\nreturn torch .max( 1-torch .abs(x), torch .zeros_like(x))\nfig, axes =d2l.plt.subplots( 1,4, sharey =True , figsize =(12,3))\nkernels =(gaussian, boxcar, constant, epanechikov)\nnames =('Gaussian ','Boxcar ','Constant ','Epanechikov ')\nx=torch .arange( -2.5,2.5,0.1)\nfor kernel, name, ax inzip(kernels, names, axes):\nax.plot(x .detach() .numpy(), kernel(x) .detach() .numpy())\nax.set_xlabel(name)\nd2l.plt.show()\nDifferent kernels correspond to different notions of range and smoothness. For instance,\nthe boxcar kernel only attends to observations within a distance of 1(or some otherwise\ndefined hyperparameter) and does so indiscriminately.\nTo see Nadaraya\u2013Watson estimation in action, let\u2019s define some training data. In the fol-\nlowing we use the dependency\n\ud835\udc66\ud835\udc56=2 sin\u00b9\ud835\udc65\ud835\udc56\u00ba\u00b8\ud835\udc65\ud835\udc56\u00b8\ud835\udf16, (11.2.3)\nwhere\ud835\udf16is drawn from a normal distribution with zero mean and unit variance. We draw\n40 training examples.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bdc039b-61e1-4663-886d-ef40d5e482ba": {"__data__": {"id_": "6bdc039b-61e1-4663-886d-ef40d5e482ba", "embedding": null, "metadata": {"page_label": "417", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2469be29-7572-4342-bb35-7cd3db8c8fbf", "node_type": "4", "metadata": {"page_label": "417", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "02a31395e8be123834d9b55cf0512aa39a92e31f9e0754cc032364ba7693ced7", "class_name": "RelatedNodeInfo"}}, "text": "417 Attention Pooling by Similarity\ndef f(x):\nreturn 2*torch .sin(x) +x\nn=40\nx_train, _ =torch .sort(torch .rand(n) *5)\ny_train =f(x_train) +torch .randn(n)\nx_val =torch .arange( 0,5,0.1)\ny_val =f(x_val)\n11.2.2AttentionPoolingvia Nadaraya\u2013WatsonRegression\nNow that we have data and kernels, all we need is a function that computes the kernel\nregression estimates. Note that we also want to obtain the relative kernel weights in order\ntoperformsomeminordiagnostics. Hencewefirstcomputethekernelbetweenalltraining\nfeatures(covariates) x_train andallvalidationfeatures x_val. Thisyieldsamatrix,which\nwe subsequently normalize. When multiplied with the training labels y_train we obtain\nthe estimates.\nRecall attention pooling in (11.1.1 ). Let each validation feature be a query, and each\ntraining feature\u2013label pair be a key\u2013value pair. As a result, the normalized relative ker-\nnel weights ( attention_w below) are the attentionweights .\ndef nadaraya_watson (x_train, y_train, x_val, kernel):\ndists =x_train .reshape(( -1,1))-x_val .reshape(( 1,-1))\n# Each column/row corresponds to each query/key\nk=kernel(dists) .type(torch .float32)\n# Normalization over keys for each query\nattention_w =k/k.sum( 0)\ny_hat =y_train @attention_w\nreturn y_hat, attention_w\nLet\u2019s have a look at the kind of estimates that the different kernels produce.\ndef plot (x_train, y_train, x_val, y_val, kernels, names, attention =False ):\nfig, axes =d2l.plt.subplots( 1,4, sharey =True , figsize =(12,3))\nfor kernel, name, ax inzip(kernels, names, axes):\ny_hat, attention_w =nadaraya_watson(x_train, y_train, x_val, kernel)\nifattention:\npcm =ax.imshow(attention_w .detach() .numpy(), cmap ='Reds ')\nelse :\nax.plot(x_val, y_hat)\nax.plot(x_val, y_val, 'm--')\nax.plot(x_train, y_train, 'o', alpha =0.5);\nax.set_xlabel(name)\nifnot attention:\nax.legend([ 'y_hat ','y'])\nifattention:\nfig.colorbar(pcm, ax =axes, shrink =0.7)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47def66b-b3a9-4292-94a8-f1be0f473deb": {"__data__": {"id_": "47def66b-b3a9-4292-94a8-f1be0f473deb", "embedding": null, "metadata": {"page_label": "418", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d072a0b-ba58-4673-8370-1fed05b9976b", "node_type": "4", "metadata": {"page_label": "418", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d62eb83c3c54c51669a612f1ab150c4c44f8b85a64fc2509196b8aa4143bbf2f", "class_name": "RelatedNodeInfo"}}, "text": "418 Attention Mechanisms and Transformers\nplot(x_train, y_train, x_val, y_val, kernels, names)\nThe first thing that stands out is that all three nontrivial kernels (Gaussian, Boxcar, and\nEpanechikov) produce fairly workable estimates that are not too far from the true function.\nOnly the constant kernel that leads to the trivial estimate \ud835\udc53\u00b9\ud835\udc65\u00ba=1\n\ud835\udc5b\u00cd\n\ud835\udc56\ud835\udc66\ud835\udc56produces a rather\nunrealistic result. Let\u2019s inspect the attention weighting a bit more closely:\nplot(x_train, y_train, x_val, y_val, kernels, names, attention =True )\nThe visualization clearly shows why the estimates for Gaussian, Boxcar, and Epanechikov\nare very similar: after all, they are derived from very similar attention weights, despite the\ndifferent functional form of the kernel. This raises the question as to whether this is always\nthe case.\n11.2.3AdaptingAttentionPooling\nWe could replace the Gaussian kernel with one of a different width. That is, we could use\n\ud835\udefc\u00b9q,k\u00ba=exp\u0010\n\u00001\n2\ud835\udf0e2kq\u0000kk2\u0011\nwhere\ud835\udf0e2determines the width of the kernel. Let\u2019s see\nwhether this affects the outcomes.\nsigmas =(0.1,0.2,0.5,1)\nnames =['Sigma '+str(sigma) for sigma insigmas]\ndef gaussian_with_width (sigma):\nreturn (lambda x: torch .exp( -x**2/(2*sigma **2)))\nkernels =[gaussian_with_width(sigma) for sigma insigmas]\nplot(x_train, y_train, x_val, y_val, kernels, names)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd4caa67-7854-493f-a245-c62e4816be1c": {"__data__": {"id_": "fd4caa67-7854-493f-a245-c62e4816be1c", "embedding": null, "metadata": {"page_label": "419", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0edd96fd-f414-486e-a804-a331afee648c", "node_type": "4", "metadata": {"page_label": "419", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5b419963e2cf42ea10ba7dd65976e654071a7db5cf100279a5dd827f264e64ad", "class_name": "RelatedNodeInfo"}}, "text": "419 Attention Pooling by Similarity\nClearly, the narrower the kernel, the less smooth the estimate. At the same time, it adapts\nbetter to the local variations. Let\u2019s look at the corresponding attention weights.\nplot(x_train, y_train, x_val, y_val, kernels, names, attention =True )\nAs we would expect, the narrower the kernel, the narrower the range of large attention\nweights. It is also clear that picking the same width might not be ideal. In fact, Silverman\n(1986) proposed a heuristic that depends on the local density. Many more such \u201ctricks\u201d\nhave been proposed. For instance, Norelli et al.(2022) used a similar nearest-neighbor\ninterpolation technique for designing cross-modal image and text representations.\nTheastutereadermightwonderwhyweareprovidingthisdeepdiveforamethodthatisover\nhalfacenturyold. First,itisoneoftheearliestprecursorsofmodernattentionmechanisms.\nSecond,itisgreatforvisualization. Third,andjustasimportantly,itdemonstratesthelimits\nof hand-crafted attention mechanisms. A much better strategy is to learnthe mechanism,\nby learning the representations for queries and keys. This is what we will embark on in the\nfollowing sections.\n11.2.4Summary\nNadaraya\u2013Watson kernel regression is an early precursor of the current attention mecha-\nnisms. It can be used directly with little to no training or tuning, either for classification or\nregression. The attention weight is assigned according to the similarity (or distance) be-\ntweenqueryandkey,andaccordingtohowmanysimilarobservationsareavailable.\n11.2.5Exercises", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40638989-acf5-4d36-bbf4-3bec6c732300": {"__data__": {"id_": "40638989-acf5-4d36-bbf4-3bec6c732300", "embedding": null, "metadata": {"page_label": "420", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a56e6206-a570-4259-b7be-2abce519d5b0", "node_type": "4", "metadata": {"page_label": "420", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b306db5ca0e28e77203b70b6ed5b744ee8a9bc6c0fbd553ef9850c9995646003", "class_name": "RelatedNodeInfo"}}, "text": "420 Attention Mechanisms and Transformers\n1561.Parzen windows density estimates are given by \u02c6\ud835\udc5d\u00b9x\u00ba=1\n\ud835\udc5b\u00cd\n\ud835\udc56\ud835\udc58\u00b9x,x\ud835\udc56\u00ba. Prove that for\nbinary classification the function \u02c6\ud835\udc5d\u00b9x,\ud835\udc66=1\u00ba\u0000 \u02c6\ud835\udc5d\u00b9x,\ud835\udc66=\u00001\u00ba, as obtained by Parzen\nwindows is equivalent to Nadaraya\u2013Watson classification.\n2.ImplementstochasticgradientdescenttolearnagoodvalueforkernelwidthsinNadaraya\u2013\nWatson regression.\n1.Whathappensifyoujustusetheaboveestimatestominimize \u00b9\ud835\udc53\u00b9xi\u00ba\u0000\ud835\udc66\ud835\udc56\u00ba2directly?\nHint:\ud835\udc66\ud835\udc56is part of the terms used to compute \ud835\udc53.\n2.Remove\u00b9x\ud835\udc56,\ud835\udc66\ud835\udc56\u00bafrom the estimate for \ud835\udc53\u00b9x\ud835\udc56\u00baand optimize over the kernel widths.\nDo you still observe overfitting?\n3.Assume that all xlie on the unit sphere, i.e., all satisfy kxk=1. Can you simplify the\nkx\u0000x\ud835\udc56k2termintheexponential? Hint: wewilllaterseethatthisisverycloselyrelated\nto dot product attention.\n4.RecallthatMackandSilverman( 1982)provedthatNadaraya\u2013Watsonestimationiscon-\nsistent. Howquicklyshouldyoureducethescalefortheattentionmechanismasyouget\nmore data? Provide some intuition for your answer. Does it depend on the dimension-\nality of the data? How?\nDiscussions156.\n11.3AttentionScoring Functions\nInSection11.2 ,weusedanumberofdifferentdistance-basedkernels,includingaGaussian\nkernel to model interactions between queries and keys. As it turns out, distance functions\nare slightly more expensive to compute than dot products. As such, with the softmax op-\neration to ensure nonnegative attention weights, much of the work has gone into attention\nscoringfunctions \ud835\udc4ein(11.1.3 )andFig. 11.3.1 that are simpler to compute.\ntFig. 11.3.1 Computing the output of attention pooling as a weighted average of values, where weights\nare computed with the attention scoring function aand the softmax operation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2087d7ba-b1fd-4b9b-a293-f1394a74db7d": {"__data__": {"id_": "2087d7ba-b1fd-4b9b-a293-f1394a74db7d", "embedding": null, "metadata": {"page_label": "421", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b063e1e-4251-447a-99b0-7879a71304ac", "node_type": "4", "metadata": {"page_label": "421", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "709499ac380a95190d6f9b431e183070b5bbcbd0727c2af9ea51c79e0e636b6b", "class_name": "RelatedNodeInfo"}}, "text": "421 Attention Scoring Functions\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n11.3.1DotProductAttention\nLet\u2019s review the attention function (without exponentiation) from the Gaussian kernel for\na moment:\n\ud835\udc4e\u00b9q,k\ud835\udc56\u00ba=\u00001\n2kq\u0000k\ud835\udc56k2=q>k\ud835\udc56\u00001\n2kk\ud835\udc56k2\u00001\n2kqk2. (11.3.1)\nFirst, note that the final term depends on qonly. As such it is identical for all \u00b9q,k\ud835\udc56\u00ba\npairs. Normalizing the attention weights to 1, as is done in (11.1.3 ), ensures that this term\ndisappears entirely. Second, note that both batch and layer normalization (to be discussed\nlater) lead to activations that have well-bounded, and often constant, norms kk\ud835\udc56k. This is\nthe case, for instance, whenever the keys k\ud835\udc56were generated by a layer norm. As such, we\ncan drop it from the definition of \ud835\udc4ewithout any major change in the outcome.\nLast, we need to keep the order of magnitude of the arguments in the exponential function\nunder control. Assume that all the elements of the query q2R\ud835\udc51and the key k\ud835\udc562R\ud835\udc51\nare independent and identically drawn random variables with zero mean and unit variance.\nThe dot product between both vectors has zero mean and a variance of \ud835\udc51. To ensure that\nthevarianceofthedotproductstillremains 1regardlessofvectorlength, weusethe scaled\ndot product attention scoring function. That is, we rescale the dot product by 1\u009dp\n\ud835\udc51. We\nthus arrive at the first commonly used attention function that is used, e.g., in Transformers\n(Vaswanietal., 2017):\n\ud835\udc4e\u00b9q,k\ud835\udc56\u00ba=q>k\ud835\udc56\u009dp\n\ud835\udc51. (11.3.2)\nNote that attention weights \ud835\udefcstill need normalizing. We can simplify this further via\n(11.1.3 )by using the softmax operation:\n\ud835\udefc\u00b9q,k\ud835\udc56\u00ba=softmax\u00b9\ud835\udc4e\u00b9q,k\ud835\udc56\u00ba\u00ba=exp\u00b9q>k\ud835\udc56\u009dp\n\ud835\udc51\u00ba\n\u00cd\n\ud835\udc57=1exp\u00b9q>k\ud835\udc57\u009dp\n\ud835\udc51\u00ba. (11.3.3)\nAs it turns out, all popular attention mechanisms use the softmax, hence we will limit\nourselves to that in the remainder of this chapter.\n11.3.2ConvenienceFunctions\nWeneedafewfunctionstomaketheattentionmechanismefficienttodeploy. Thisincludes\ntoolsfordealingwithstringsofvariablelengths(commonfornaturallanguageprocessing)\nand tools for efficient evaluation on minibatches (batch matrix multiplication).\nMaskedSoftmax Operation\nOne of the most popular applications of the attention mechanism is to sequence models.\nHence we need to be able to deal with sequences of different lengths. In some cases, such", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2274, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "acc0ead4-870d-413d-839a-277d7691c735": {"__data__": {"id_": "acc0ead4-870d-413d-839a-277d7691c735", "embedding": null, "metadata": {"page_label": "422", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0832e99-ac1e-4648-a4ef-e3c891af5858", "node_type": "4", "metadata": {"page_label": "422", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6c6da734f670852292000e5e2dbcfc509fa4bb7d09b9800110fcabd4834c0341", "class_name": "RelatedNodeInfo"}}, "text": "422 Attention Mechanisms and Transformers\nsequences may end up in the same minibatch, necessitating padding with dummy tokens\nfor shorter sequences (see Section 10.5 for an example). These special tokens do not carry\nmeaning. For instance, assume that we have the following three sentences:\nDive into Deep Learning\nLearn to code <blank >\nHello world <blank ><blank >\nSincewedonotwantblanksinourattentionmodelwesimplyneedtolimit\u00cd\ud835\udc5b\n\ud835\udc56=1\ud835\udefc\u00b9q,k\ud835\udc56\u00bav\ud835\udc56\nto\u00cd\ud835\udc59\n\ud835\udc56=1\ud835\udefc\u00b9q,k\ud835\udc56\u00bav\ud835\udc56forhoweverlong, \ud835\udc59\u0014\ud835\udc5b,theactualsentenceis. Sinceitissuchacommon\nproblem, it has a name: the maskedsoftmax operation .\nLet\u2019simplementit. Actually,theimplementationcheatseversoslightlybysettingthevalues\nofv\ud835\udc56, for\ud835\udc56 >\ud835\udc59, to zero. Moreover, it sets the attention weights to a large negative number,\nsuchas\u0000106,inordertomaketheircontributiontogradientsandvaluesvanishinpractice.\nThis is done since linear algebra kernels and operators are heavily optimized for GPUs and\nit is faster to be slightly wasteful in computation rather than to have code with conditional\n(if then else) statements.\ndef masked_softmax (X, valid_lens): #@save\n\"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n# X: 3D tensor, valid_lens: 1D or 2D tensor\ndef _sequence_mask (X, valid_len, value =0):\nmaxlen =X.size( 1)\nmask =torch .arange((maxlen), dtype =torch .float32,\ndevice =X.device)[ None , :] <valid_len[:, None ]\nX[~mask] =value\nreturn X\nifvalid_lens isNone :\nreturn nn.functional .softmax(X, dim =-1)\nelse :\nshape =X.shape\nifvalid_lens .dim() ==1:\nvalid_lens =torch .repeat_interleave(valid_lens, shape[ 1])\nelse :\nvalid_lens =valid_lens .reshape( -1)\n# On the last axis, replace masked elements with a very large negative\n# value, whose exponentiation outputs 0\nX=_sequence_mask(X .reshape( -1, shape[ -1]), valid_lens, value =-1e6)\nreturn nn.functional .softmax(X .reshape(shape), dim =-1)\nTo illustrate how this function works, consider a minibatch of two examples of size 2\u00024,\nwhere their valid lengths are 2and3, respectively. As a result of the masked softmax oper-\nation,valuesbeyondthevalidlengthsforeachpairofvectorsareallmaskedaszero.\nmasked_softmax(torch .rand( 2,2,4), torch .tensor([ 2,3]))\ntensor([[[ 0.4448 ,0.5552 ,0.0000 ,0.0000 ],\n[0.4032 ,0.5968 ,0.0000 ,0.0000 ]],\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0189eb7-b573-4bdd-8020-c1343ea9deaa": {"__data__": {"id_": "a0189eb7-b573-4bdd-8020-c1343ea9deaa", "embedding": null, "metadata": {"page_label": "423", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19804c2d-938c-4f07-abdc-f027568795fb", "node_type": "4", "metadata": {"page_label": "423", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "95bd616d25e84a1f736b735a6c34a1f9c68fa4c5cfd1a7cbc922a26ce489ebfa", "class_name": "RelatedNodeInfo"}}, "text": "423 Attention Scoring Functions\n(continued from previous page)\n[[0.2795 ,0.2805 ,0.4400 ,0.0000 ],\n[0.2798 ,0.3092 ,0.4110 ,0.0000 ]]])\nIf we need more fine-grained control to specify the valid length for each of the two vec-\ntors of every example, we simply use a two-dimensional tensor of valid lengths. This\nyields:\nmasked_softmax(torch .rand( 2,2,4), torch .tensor([[ 1,3], [ 2,4]]))\ntensor([[[ 1.0000 ,0.0000 ,0.0000 ,0.0000 ],\n[0.4109 ,0.2794 ,0.3097 ,0.0000 ]],\n[[0.3960 ,0.6040 ,0.0000 ,0.0000 ],\n[0.2557 ,0.1833 ,0.2420 ,0.3190 ]]])\nBatchMatrix Multiplication\nAnother commonly used operation is to multiply batches of matrices by one another. This\ncomes in handy when we have minibatches of queries, keys, and values. More specifically,\nassume that\nQ=\u00bbQ1,Q2,...,Q\ud835\udc5b\u00bc2R\ud835\udc5b\u0002\ud835\udc4e\u0002\ud835\udc4f,\nK=\u00bbK1,K2,...,K\ud835\udc5b\u00bc2R\ud835\udc5b\u0002\ud835\udc4f\u0002\ud835\udc50.(11.3.4)\nThen the batch matrix multiplication (BMM) computes the elementwise product\nBMM\u00b9Q,K\u00ba=\u00bbQ1K1,Q2K2,...,Q\ud835\udc5bK\ud835\udc5b\u00bc2R\ud835\udc5b\u0002\ud835\udc4e\u0002\ud835\udc50. (11.3.5)\nLet\u2019s see this in action in a deep learning framework.\nQ=torch .ones(( 2,3,4))\nK=torch .ones(( 2,4,6))\nd2l.check_shape(torch .bmm(Q, K), ( 2,3,6))\n11.3.3Scaled DotProductAttention\nLet\u2019s return to the dot product attention introduced in (11.3.2 ). In general, it requires that\nboth the query and the key have the same vector length, say \ud835\udc51, even though this can be\naddressed easily by replacing q>kwithq>Mkwhere Mis a matrix suitably chosen for\ntranslating between both spaces. For now assume that the dimensions match.\nIn practice, we often think of minibatches for efficiency, such as computing attention for\n\ud835\udc5bqueries and\ud835\udc5akey-value pairs, where queries and keys are of length \ud835\udc51and values are of\nlength\ud835\udc63. The scaled dot product attention of queries Q2R\ud835\udc5b\u0002\ud835\udc51, keysK2R\ud835\udc5a\u0002\ud835\udc51, and", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "856adfe6-5ce1-47ef-9e74-6a4ddd3e4ae9": {"__data__": {"id_": "856adfe6-5ce1-47ef-9e74-6a4ddd3e4ae9", "embedding": null, "metadata": {"page_label": "424", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "26880900-60fc-497e-ba27-1ea178ebade4", "node_type": "4", "metadata": {"page_label": "424", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dedd4c983d527801110d15911635467a1b07bd339f640a0d153102d8923b13ba", "class_name": "RelatedNodeInfo"}}, "text": "424 Attention Mechanisms and Transformers\nvalues V2R\ud835\udc5a\u0002\ud835\udc63thus can be written as\nsoftmax\u0012QK>\np\n\ud835\udc51\u0013\nV2R\ud835\udc5b\u0002\ud835\udc63. (11.3.6)\nNotethatwhenapplyingthistoaminibatch, weneedthebatchmatrixmultiplicationintro-\nduced in (11.3.5 ). In the following implementation of the scaled dot product attention, we\nuse dropout for model regularization.\nclass DotProductAttention (nn.Module): #@save\n\"\"\"Scaled dot product attention.\"\"\"\ndef __init__ (self , dropout):\nsuper ().__init__ ()\nself .dropout =nn.Dropout(dropout)\n# Shape of queries: (batch_size, no. of queries, d)\n# Shape of keys: (batch_size, no. of key-value pairs, d)\n# Shape of values: (batch_size, no. of key-value pairs, value dimension)\n# Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\ndef forward (self , queries, keys, values, valid_lens =None ):\nd=queries .shape[ -1]\n# Swap the last two dimensions of keys with keys.transpose(1, 2)\nscores =torch .bmm(queries, keys .transpose( 1,2))/math .sqrt(d)\nself .attention_weights =masked_softmax(scores, valid_lens)\nreturn torch .bmm( self .dropout( self .attention_weights), values)\nTo illustrate how the DotProductAttention class works, we use the same keys, values,\nand valid lengths from the earlier toy example for additive attention. For the purpose of\nour example we assume that we have a minibatch size of 2, a total of 10keys and values,\nand that the dimensionality of the values is 4. Lastly, we assume that the valid length per\nobservationis 2and6respectively. Giventhat,weexpecttheoutputtobea 2\u00021\u00024tensor,\ni.e., one row per example of the minibatch.\nqueries =torch .normal( 0,1, (2,1,2))\nkeys =torch .normal( 0,1, (2,10,2))\nvalues =torch .normal( 0,1, (2,10,4))\nvalid_lens =torch .tensor([ 2,6])\nattention =DotProductAttention(dropout =0.5)\nattention .eval()\nd2l.check_shape(attention(queries, keys, values, valid_lens), ( 2,1,4))\nLet\u2019s check whether the attention weights actually vanish for anything beyond the second\nand sixth column respectively (because of setting the valid length to 2and6).\nd2l.show_heatmaps(attention .attention_weights .reshape(( 1,1,2,10)),\nxlabel ='Keys ', ylabel ='Queries ')\n11.3.4AdditiveAttention\nWhenqueries qandkeys karevectorsofdifferentdimension,wecaneitheruseamatrixto\naddress the mismatch via q>Mk, or we can use additive attention as the scoring function.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2306, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a05252e-9a9b-458e-a0d2-0d15b06de29e": {"__data__": {"id_": "2a05252e-9a9b-458e-a0d2-0d15b06de29e", "embedding": null, "metadata": {"page_label": "425", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9df74982-31d5-4727-a3a2-d2727fc481e9", "node_type": "4", "metadata": {"page_label": "425", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "80eb87c2a89158353844e3381c7f3f422ffb356900c19c9975fad06ba6ce3f09", "class_name": "RelatedNodeInfo"}}, "text": "425 Attention Scoring Functions\nAnother benefit is that, as its name indicates, the attention is additive. This can lead to\nsome minor computational savings. Given a query q2R\ud835\udc5eand a key k2R\ud835\udc58, theadditive\nattention scoring function ( Bahdanau etal., 2014) is given by\n\ud835\udc4e\u00b9q,k\u00ba=w>\n\ud835\udc63tanh\u00b9W\ud835\udc5eq\u00b8W\ud835\udc58k\u00ba2R, (11.3.7)\nwhere W\ud835\udc5e2R\u210e\u0002\ud835\udc5e,W\ud835\udc582R\u210e\u0002\ud835\udc58, andw\ud835\udc632R\u210eare the learnable parameters. This term\nis then fed into a softmax to ensure both nonnegativity and normalization. An equivalent\ninterpretation of (11.3.7 )is that the query and key are concatenated and fed into an MLP\nwith a single hidden layer. Using tanhas the activation function and disabling bias terms,\nwe implement additive attention as follows:\nclass AdditiveAttention (nn.Module): #@save\n\"\"\"Additive attention.\"\"\"\ndef __init__ (self , num_hiddens, dropout, **kwargs):\nsuper (AdditiveAttention, self ).__init__ (**kwargs)\nself .W_k =nn.LazyLinear(num_hiddens, bias =False )\nself .W_q =nn.LazyLinear(num_hiddens, bias =False )\nself .w_v =nn.LazyLinear( 1, bias =False )\nself .dropout =nn.Dropout(dropout)\ndef forward (self , queries, keys, values, valid_lens):\nqueries, keys =self .W_q(queries), self .W_k(keys)\n# After dimension expansion, shape of queries: (batch_size, no. of\n# queries, 1, num_hiddens) and shape of keys: (batch_size, 1, no. of\n# key-value pairs, num_hiddens). Sum them up with broadcasting\nfeatures =queries .unsqueeze( 2)+keys .unsqueeze( 1)\nfeatures =torch .tanh(features)\n# There is only one output of self.w_v, so we remove the last\n# one-dimensional entry from the shape. Shape of scores: (batch_size,\n# no. of queries, no. of key-value pairs)\nscores =self .w_v(features) .squeeze( -1)\nself .attention_weights =masked_softmax(scores, valid_lens)\n# Shape of values: (batch_size, no. of key-value pairs, value\n# dimension)\nreturn torch .bmm( self .dropout( self .attention_weights), values)\nLet\u2019s see how AdditiveAttention works. In our toy example we pick queries, keys and\nvaluesofsize\u00b92,1,20\u00ba,\u00b92,10,2\u00baand\u00b92,10,4\u00ba,respectively. Thisisidenticaltoourchoice\nforDotProductAttention , except that now the queries are 20-dimensional. Likewise, we\npick\u00b92,6\u00baas the valid lengths for the sequences in the minibatch.\nqueries =torch .normal( 0,1, (2,1,20))\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2239, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f915f536-434b-42d4-b662-a46cbd1591f1": {"__data__": {"id_": "f915f536-434b-42d4-b662-a46cbd1591f1", "embedding": null, "metadata": {"page_label": "426", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f63fe20-0c8e-4263-902f-dfbe65a01215", "node_type": "4", "metadata": {"page_label": "426", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "04d6e253c0a7584fb5db38946bc42938d6c581876ee9f7705dc13e229b209e62", "class_name": "RelatedNodeInfo"}}, "text": "426 Attention Mechanisms and Transformers\n157\n158(continued from previous page)\nattention =AdditiveAttention(num_hiddens =8, dropout =0.1)\nattention .eval()\nd2l.check_shape(attention(queries, keys, values, valid_lens), ( 2,1,4))\nWhen reviewing the attention function we see a behavior that is qualitatively quite similar\nto that of DotProductAttention . That is, onlyterms within the chosenvalid length \u00b92,6\u00ba\nare nonzero.\nd2l.show_heatmaps(attention .attention_weights .reshape(( 1,1,2,10)),\nxlabel ='Keys ', ylabel ='Queries ')\n11.3.5Summary\nInthissectionweintroducedthetwokeyattentionscoringfunctions: dotproductandaddi-\ntive attention. They are effective tools for aggregating across sequences of variable length.\nInparticular,thedotproductattentionisthemainstayofmodernTransformerarchitectures.\nWhen queries and keys are vectors of different lengths, we can use the additive attention\nscoring function instead. Optimizing these layers is one of the key areas of advance in re-\ncent years. For instance, NVIDIA\u2019s Transformer Library157and Megatron ( Shoeybietal.,\n2019) crucially rely on efficient variants of the attention mechanism. We will dive into this\nin quite a bit more detail as we review Transformers in later sections.\n11.3.6Exercises\n1.Implementdistance-basedattentionbymodifyingthe DotProductAttention code. Note\nthat you only need the squared norms of the keys kk\ud835\udc56k2for an efficient implementation.\n2.Modify the dot product attention to allow for queries and keys of different dimension-\nalities by employing a matrix to adjust dimensions.\n3.How does the computational cost scale with the dimensionality of the keys, queries,\nvalues, and their number? What about the memory bandwidth requirements?\nDiscussions158.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "75f54cb3-443c-4d0b-9980-71258992fc3a": {"__data__": {"id_": "75f54cb3-443c-4d0b-9980-71258992fc3a", "embedding": null, "metadata": {"page_label": "427", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e579ddb6-8214-4a51-9332-5ef11f1d8a63", "node_type": "4", "metadata": {"page_label": "427", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d457641f58b764e81a40088b92422d41e299686def12ce1bf88452f1b8302a90", "class_name": "RelatedNodeInfo"}}, "text": "427 The Bahdanau Attention Mechanism\n11.4TheBahdanau AttentionMechanism\nWhenweencounteredmachinetranslationin Section10.7 ,wedesignedanencoder\u2013decoder\narchitectureforsequence-to-sequencelearningbasedontwoRNNs( Sutskever etal.,2014).\nSpecifically, the RNN encoder transforms a variable-length sequence into a fixed-shape\ncontext variable. Then, the RNN decoder generates the output (target) sequence token by\ntoken based on the generated tokens and the context variable.\nRecallFig. 10.7.2 which we repeat ( Fig. 11.4.1 ) with some additional detail. Convention-\nally, in an RNN all relevant information about a source sequence is translated into some\ninternalfixed-dimensional state representation by the encoder. It is this very state that is\nused by the decoder as the complete and exclusive source of information for generating the\ntranslated sequence. In other words, the sequence-to-sequence mechanism treats the inter-\nmediate state as a sufficient statistic of whatever string might have served as input.\ntFig. 11.4.1 Sequence-to-sequence model. The state, as generated by the encoder, is the only piece of\ninformation shared between the encoder and the decoder.\nWhilethisisquitereasonableforshortsequences,itisclearthatitisinfeasibleforlongones,\nsuchasabookchapterorevenjustaverylongsentence. Afterall,beforetoolongtherewill\nsimplynotbeenough\u201cspace\u201dintheintermediaterepresentationtostoreallthatisimportant\nin the source sequence. Consequently the decoder will fail to translate long and complex\nsentences. One of the first to encounter this was Graves ( 2013) who tried to design an\nRNNtogeneratehandwrittentext. Sincethesourcetexthasarbitrarylengththeydesigneda\ndifferentiableattentionmodeltoaligntextcharacterswiththemuchlongerpentrace,where\nthe alignment moves only in one direction. This, in turn, draws on decoding algorithms in\nspeech recognition, e.g., hidden Markov models ( Rabiner and Juang, 1993 ).\nInspired by the idea of learning to align, Bahdanau et al.(2014) proposed a differentiable\nattention model withoutthe unidirectional alignment limitation. When predicting a token,\nifnotalltheinputtokensarerelevant,themodelaligns(orattends)onlytopartsoftheinput\nsequencethataredeemedrelevanttothecurrentprediction. Thisisthenusedtoupdatethe\ncurrentstatebeforegeneratingthenexttoken. Whilequiteinnocuousinitsdescription,this\nBahdanau attention mechanism has arguably turned into one of the most influential ideas\nof the past decade in deep learning, giving rise to Transformers ( Vaswanietal., 2017) and\nmany related new architectures.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2550, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b184368-4eb3-40c4-996f-6d8cee2e63f0": {"__data__": {"id_": "8b184368-4eb3-40c4-996f-6d8cee2e63f0", "embedding": null, "metadata": {"page_label": "428", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "212411d6-20b1-464b-9afd-f7eca501b16d", "node_type": "4", "metadata": {"page_label": "428", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "db93ae28129b2ad2fc004df57680dd4e07bd1c5fa99dae17000923f1c4b5d933", "class_name": "RelatedNodeInfo"}}, "text": "428 Attention Mechanisms and Transformers\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n11.4.1Model\nWe follow the notation introduced by the sequence-to-sequence architecture of Section\n10.7, in particular (10.7.3 ). The key idea is that instead of keeping the state, i.e., the con-\ntext variable csummarizing the source sentence, as fixed, we dynamically update it, as a\nfunction of both the original text (encoder hidden states h\ud835\udc61) and the text that was already\ngenerated (decoder hidden states s\ud835\udc610\u00001). This yields c\ud835\udc610, which is updated after any decod-\ning time step \ud835\udc610. Suppose that the input sequence is of length \ud835\udc47. In this case the context\nvariable is the output of attention pooling:\nc\ud835\udc610=\ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udefc\u00b9s\ud835\udc610\u00001,h\ud835\udc61\u00bah\ud835\udc61. (11.4.1)\nWe used s\ud835\udc610\u00001as the query, and h\ud835\udc61as both the key and the value. Note that c\ud835\udc610is then\nused to generate the state s\ud835\udc610and to generate a new token: see (10.7.3 ). In particular, the\nattention weight \ud835\udefcis computed as in (11.3.3 )using the additive attention scoring function\ndefinedby (11.3.7 ). ThisRNNencoder\u2013decoderarchitectureusingattentionisdepictedin\nFig. 11.4.2 . Note that later this model was modified so as to include the already generated\ntokensinthedecoderasfurthercontext(i.e., theattentionsumdoesnotstopat \ud835\udc47butrather\nit proceeds up to \ud835\udc610\u00001). For instance, see Chan et al.(2015) for a description of this\nstrategy, as applied to speech recognition.\ntFig. 11.4.2 Layers in an RNN encoder\u2013decoder model with the Bahdanau attention mechanism.\n11.4.2Defining the Decoder with Attention\nTo implement the RNN encoder\u2013decoder with attention, we only need to redefine the de-\ncoder (omitting the generated symbols from the attention function simplifies the design).\nLet\u2019s begin with the base interface for decoders with attention by defining the quite unsur-\nprisingly named AttentionDecoder class.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7961dc4d-f139-442f-922c-42bbc4321cf6": {"__data__": {"id_": "7961dc4d-f139-442f-922c-42bbc4321cf6", "embedding": null, "metadata": {"page_label": "429", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24e9b62a-5dec-4e3d-8c08-de41e64ae5f5", "node_type": "4", "metadata": {"page_label": "429", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "65e087565dac6cff768bc8178fc8aeff08bc3e10b85c91b164b6985e1e29f663", "class_name": "RelatedNodeInfo"}}, "text": "429 The Bahdanau Attention Mechanism\nclass AttentionDecoder (d2l .Decoder): #@save\n\"\"\"The base attention-based decoder interface.\"\"\"\ndef __init__ (self ):\nsuper ().__init__ ()\n@property\ndef attention_weights (self ):\nraise NotImplementedError\nWe need to implement the RNN decoder in the Seq2SeqAttentionDecoder class. The\nstate of the decoder is initialized with (i) the hidden states of the last layer of the encoder\nat all time steps, used as keys and values for attention; (ii) the hidden state of the encoder\nat all layers at the final time step, which serves to initialize the hidden state of the decoder;\nand (iii) the valid length of the encoder, to exclude the padding tokens in attention pooling.\nAt each decoding time step, the hidden state of the final layer of the decoder, obtained at\nthe previous time step, is used as the query of the attention mechanism. Both the output of\nthe attention mechanism and the input embedding are concatenated to serve as the input of\nthe RNN decoder.\nclass Seq2SeqAttentionDecoder (AttentionDecoder):\ndef __init__ (self , vocab_size, embed_size, num_hiddens, num_layers,\ndropout =0):\nsuper ().__init__ ()\nself .attention =d2l.AdditiveAttention(num_hiddens, dropout)\nself .embedding =nn.Embedding(vocab_size, embed_size)\nself .rnn =nn.GRU(\nembed_size +num_hiddens, num_hiddens, num_layers,\ndropout =dropout)\nself .dense =nn.LazyLinear(vocab_size)\nself .apply(d2l .init_seq2seq)\ndef init_state (self , enc_outputs, enc_valid_lens):\n# Shape of outputs: (num_steps, batch_size, num_hiddens).\n# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\noutputs, hidden_state =enc_outputs\nreturn (outputs .permute( 1,0,2), hidden_state, enc_valid_lens)\ndef forward (self , X, state):\n# Shape of enc_outputs: (batch_size, num_steps, num_hiddens).\n# Shape of hidden_state: (num_layers, batch_size, num_hiddens)\nenc_outputs, hidden_state, enc_valid_lens =state\n# Shape of the output X: (num_steps, batch_size, embed_size)\nX=self .embedding(X) .permute( 1,0,2)\noutputs, self ._attention_weights =[], []\nfor xinX:\n# Shape of query: (batch_size, 1, num_hiddens)\nquery =torch .unsqueeze(hidden_state[ -1], dim =1)\n# Shape of context: (batch_size, 1, num_hiddens)\ncontext =self .attention(\nquery, enc_outputs, enc_outputs, enc_valid_lens)\n# Concatenate on the feature dimension\nx=torch .cat((context, torch .unsqueeze(x, dim =1)), dim =-1)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eed0536b-e2b4-4b6b-9d30-b868234ec025": {"__data__": {"id_": "eed0536b-e2b4-4b6b-9d30-b868234ec025", "embedding": null, "metadata": {"page_label": "430", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c83ff71-abea-42b8-a0f1-746e89ce16a1", "node_type": "4", "metadata": {"page_label": "430", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e3c94d1b55065368d4b88c47404a5cf73696a65080b4d65a3521af657e652a49", "class_name": "RelatedNodeInfo"}}, "text": "430 Attention Mechanisms and Transformers\n(continued from previous page)\n# Reshape x as (1, batch_size, embed_size + num_hiddens)\nout, hidden_state =self .rnn(x .permute( 1,0,2), hidden_state)\noutputs .append(out)\nself ._attention_weights .append( self .attention .attention_weights)\n# After fully connected layer transformation, shape of outputs:\n# (num_steps, batch_size, vocab_size)\noutputs =self .dense(torch .cat(outputs, dim =0))\nreturn outputs .permute( 1,0,2), [enc_outputs, hidden_state,\nenc_valid_lens]\n@property\ndef attention_weights (self ):\nreturn self ._attention_weights\nIn the following, we test the implemented decoder with attention using a minibatch of four\nsequences, each of which are seven time steps long.\nvocab_size, embed_size, num_hiddens, num_layers =10,8,16,2\nbatch_size, num_steps =4,7\nencoder =d2l.Seq2SeqEncoder(vocab_size, embed_size, num_hiddens, num_layers)\ndecoder =Seq2SeqAttentionDecoder(vocab_size, embed_size, num_hiddens,\nnum_layers)\nX=torch .zeros((batch_size, num_steps), dtype =torch .long)\nstate =decoder .init_state(encoder(X), None )\noutput, state =decoder(X, state)\nd2l.check_shape(output, (batch_size, num_steps, vocab_size))\nd2l.check_shape(state[ 0], (batch_size, num_steps, num_hiddens))\nd2l.check_shape(state[ 1][0], (batch_size, num_hiddens))\n11.4.3Training\nNow that we specified the new decoder we can proceed analogously to Section 10.7.6 :\nspecify the hyperparameters, instantiate a regular encoder and a decoder with attention,\nand train this model for machine translation.\ndata =d2l.MTFraEng(batch_size =128)\nembed_size, num_hiddens, num_layers, dropout =256,256,2,0.2\nencoder =d2l.Seq2SeqEncoder(\nlen(data .src_vocab), embed_size, num_hiddens, num_layers, dropout)\ndecoder =Seq2SeqAttentionDecoder(\nlen(data .tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\nmodel =d2l.Seq2Seq(encoder, decoder, tgt_pad =data .tgt_vocab[ '<pad> '],\nlr=0.005 )\ntrainer =d2l.Trainer(max_epochs =30, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\nAfter the model is trained, we use it to translate a few English sentences into French and\ncompute their BLEU scores.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cdd205f-5ca5-498d-82d4-ab8e5120c7f3": {"__data__": {"id_": "0cdd205f-5ca5-498d-82d4-ab8e5120c7f3", "embedding": null, "metadata": {"page_label": "431", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "87b341da-065a-483a-a356-e7f7c6a64394", "node_type": "4", "metadata": {"page_label": "431", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "589f7b99a29ddc1b7e3358e690fbeff9824aa86fb983ff43fa3bd24f820bb1d6", "class_name": "RelatedNodeInfo"}}, "text": "431 The Bahdanau Attention Mechanism\nengs =['go . ','i lost . ','he\\'s calm . ','i\\'m home . ']\nfras =['va ! ','j\\'ai perdu . ','il est calme . ','je suis chez moi . ']\npreds, _ =model .predict_step(\ndata .build(engs, fras), d2l .try_gpu(), data .num_steps)\nfor en, fr, p inzip(engs, fras, preds):\ntranslation =[]\nfor token indata .tgt_vocab .to_tokens(p):\niftoken =='<eos> ':\nbreak\ntranslation .append(token)\nprint (f'{en}=>{translation }, bleu, '\nf'{d2l.bleu( \"\".join(translation), fr, k=2):.3f}')\ngo.=>['va','!'], bleu, 1.000\ni lost .=>[\"j'ai\",'perdu ','.'], bleu, 1.000\nhe's calm . => [ 'il','court ','.'], bleu,0.000\ni'm home . => [ 'je','suis ','chez ','moi','.'], bleu,1.000\nLet\u2019svisualizetheattentionweightswhentranslatingthelastEnglishsentence. Weseethat\neach query assigns non-uniform weights over key\u2013value pairs. It shows that at each decod-\ning step, different parts of the input sequences are selectively aggregated in the attention\npooling.\n_, dec_attention_weights =model .predict_step(\ndata .build([engs[ -1]], [fras[ -1]]), d2l .try_gpu(), data .num_steps, True )\nattention_weights =torch .cat(\n[step[ 0][0][0]for step indec_attention_weights], 0)\nattention_weights =attention_weights .reshape(( 1,1,-1, data .num_steps))\n# Plus one to include the end-of-sequence token\nd2l.show_heatmaps(\nattention_weights[:, :, :, : len(engs[ -1].split()) +1].cpu(),\nxlabel ='Key positions ', ylabel ='Query positions ')\n11.4.4Summary\nWhenpredictingatoken,ifnotalltheinputtokensarerelevant,theRNNencoder\u2013decoder\nwith the Bahdanau attention mechanism selectively aggregates different parts of the input", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db9f8b9e-d41f-4f13-8040-9fb87cbbca33": {"__data__": {"id_": "db9f8b9e-d41f-4f13-8040-9fb87cbbca33", "embedding": null, "metadata": {"page_label": "432", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "544eee7c-ffc6-4d3b-8b7b-ad2d93fbd41b", "node_type": "4", "metadata": {"page_label": "432", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a74f3b46db2982241f6f389839d7271f8ae53e214bca7b71c270a220ed577074", "class_name": "RelatedNodeInfo"}}, "text": "432 Attention Mechanisms and Transformers\n159sequence. This is achieved by treating the state (context variable) as an output of additive\nattention pooling. In the RNN encoder\u2013decoder, the Bahdanau attention mechanism treats\nthe decoder hidden state at the previous time step as the query, and the encoder hidden\nstates at all the time steps as both the keys and values.\n11.4.5Exercises\n1.Replace GRU with LSTM in the experiment.\n2.Modifytheexperimenttoreplacetheadditiveattentionscoringfunctionwiththescaled\ndot-product. How does it influence the training efficiency?\nDiscussions159.\n11.5Multi-HeadAttention\nIn practice, given the same set of queries, keys, and values we may want our model to\ncombine knowledge from different behaviors of the same attention mechanism, such as\ncapturingdependenciesofvariousranges(e.g., shorter-rangevs.longer-range)withinase-\nquence. Thus,itmaybebeneficialtoallowourattentionmechanismtojointlyusedifferent\nrepresentation subspaces of queries, keys, and values.\nTo this end, instead of performing a single attention pooling, queries, keys, and values can\nbe transformed with \u210eindependently learned linear projections. Then these \u210eprojected\nqueries, keys, and values are fed into attention pooling in parallel. In the end, \u210eattention-\npoolingoutputsareconcatenatedandtransformedwithanotherlearnedlinearprojectionto\nproduce the final output. This design is called multi-head attention , where each of the \u210e\nattention pooling outputs is a head(Vaswaniet al., 2017). Using fully connected layers to\nperformlearnablelineartransformations, Fig.11.5.1 describesmulti-headattention.\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5199167-e9ff-44e1-8a43-003dbb9c7545": {"__data__": {"id_": "e5199167-e9ff-44e1-8a43-003dbb9c7545", "embedding": null, "metadata": {"page_label": "433", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2887db5d-bcf8-48e5-84c9-5a90aa2e0bad", "node_type": "4", "metadata": {"page_label": "433", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "eab545a9e0657dff0ad7e0577e398a57e42c4b6db4f937d5e7ae53ba1f957142", "class_name": "RelatedNodeInfo"}}, "text": "433 Multi-Head Attention\ntFig. 11.5.1 Multi-head attention, where multiple heads are concatenated then linearly transformed.\n11.5.1Model\nBefore providing the implementation of multi-head attention, let\u2019s formalize this model\nmathematically. Given a query q2R\ud835\udc51\ud835\udc5e, a key k2R\ud835\udc51\ud835\udc58, and a value v2R\ud835\udc51\ud835\udc63, each\nattention head h\ud835\udc56(\ud835\udc56=1,...,\u210e) is computed as\nh\ud835\udc56=\ud835\udc53\u00b9W\u00b9\ud835\udc5e\u00ba\n\ud835\udc56q,W\u00b9\ud835\udc58\u00ba\n\ud835\udc56k,W\u00b9\ud835\udc63\u00ba\n\ud835\udc56v\u00ba2R\ud835\udc5d\ud835\udc63, (11.5.1)\nwhere W\u00b9\ud835\udc5e\u00ba\n\ud835\udc562R\ud835\udc5d\ud835\udc5e\u0002\ud835\udc51\ud835\udc5e,W\u00b9\ud835\udc58\u00ba\n\ud835\udc562R\ud835\udc5d\ud835\udc58\u0002\ud835\udc51\ud835\udc58, andW\u00b9\ud835\udc63\u00ba\n\ud835\udc562R\ud835\udc5d\ud835\udc63\u0002\ud835\udc51\ud835\udc63are learnable parameters\nand\ud835\udc53is attention pooling, such as additive attention and scaled dot product attention in\nSection11.3 . Themulti-headattentionoutputisanotherlineartransformationvialearnable\nparameters W\ud835\udc5c2R\ud835\udc5d\ud835\udc5c\u0002\u210e\ud835\udc5d\ud835\udc63of the concatenation of \u210eheads:\nW\ud835\udc5c2666664h1\n...\nh\u210e37777752R\ud835\udc5d\ud835\udc5c. (11.5.2)\nBased on this design, each head may attend to different parts of the input. More sophisti-\ncated functions than the simple weighted average can be expressed.\n11.5.2Implementation\nIn our implementation, we choose the scaled dot product attention for each head of the\nmulti-head attention. To avoid significant growth of computational cost and parametriza-\ntion cost, we set \ud835\udc5d\ud835\udc5e=\ud835\udc5d\ud835\udc58=\ud835\udc5d\ud835\udc63=\ud835\udc5d\ud835\udc5c\u009d\u210e. Note that\u210eheads can be computed in parallel\nif we set the number of outputs of linear transformations for the query, key, and value to\n\ud835\udc5d\ud835\udc5e\u210e=\ud835\udc5d\ud835\udc58\u210e=\ud835\udc5d\ud835\udc63\u210e=\ud835\udc5d\ud835\udc5c. Inthefollowingimplementation, \ud835\udc5d\ud835\udc5cisspecifiedviatheargument\nnum_hiddens .\nclass MultiHeadAttention (d2l .Module): #@save\n\"\"\"Multi-head attention.\"\"\"\ndef __init__ (self , num_hiddens, num_heads, dropout, bias =False ,**kwargs):\nsuper ().__init__ ()\nself .num_heads =num_heads\nself .attention =d2l.DotProductAttention(dropout)\nself .W_q =nn.LazyLinear(num_hiddens, bias =bias)\nself .W_k =nn.LazyLinear(num_hiddens, bias =bias)\nself .W_v =nn.LazyLinear(num_hiddens, bias =bias)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ec5ef2e9-4726-4121-a503-6248dc13bc80": {"__data__": {"id_": "ec5ef2e9-4726-4121-a503-6248dc13bc80", "embedding": null, "metadata": {"page_label": "434", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c85362d-aad5-4e70-8c34-bf5ad5a43e05", "node_type": "4", "metadata": {"page_label": "434", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0c729256e51d101fb2ece7062d4a2d80ce79ccedd1c0acc5c46bb9c408cbe46a", "class_name": "RelatedNodeInfo"}}, "text": "434 Attention Mechanisms and Transformers\n(continued from previous page)\nself .W_o =nn.LazyLinear(num_hiddens, bias =bias)\ndef forward (self , queries, keys, values, valid_lens):\n# Shape of queries, keys, or values:\n# (batch_size, no. of queries or key-value pairs, num_hiddens)\n# Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n# After transposing, shape of output queries, keys, or values:\n# (batch_size * num_heads, no. of queries or key-value pairs,\n# num_hiddens / num_heads)\nqueries =self .transpose_qkv( self .W_q(queries))\nkeys =self .transpose_qkv( self .W_k(keys))\nvalues =self .transpose_qkv( self .W_v(values))\nifvalid_lens isnot None :\n# On axis 0, copy the first item (scalar or vector) for num_heads\n# times, then copy the next item, and so on\nvalid_lens =torch .repeat_interleave(\nvalid_lens, repeats =self .num_heads, dim =0)\n# Shape of output: (batch_size * num_heads, no. of queries,\n# num_hiddens / num_heads)\noutput =self .attention(queries, keys, values, valid_lens)\n# Shape of output_concat: (batch_size, no. of queries, num_hiddens)\noutput_concat =self .transpose_output(output)\nreturn self .W_o(output_concat)\nToallowforparallelcomputationofmultipleheads, theabove MultiHeadAttention class\nuses two transposition methods as defined below. Specifically, the transpose_output\nmethod reverses the operation of the transpose_qkv method.\n@d2l .add_to_class(MultiHeadAttention) #@save\ndef transpose_qkv (self , X):\n\"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n# Shape of input X: (batch_size, no. of queries or key-value pairs,\n# num_hiddens). Shape of output X: (batch_size, no. of queries or\n# key-value pairs, num_heads, num_hiddens / num_heads)\nX=X.reshape(X .shape[ 0], X .shape[ 1],self .num_heads, -1)\n# Shape of output X: (batch_size, num_heads, no. of queries or key-value\n# pairs, num_hiddens / num_heads)\nX=X.permute( 0,2,1,3)\n# Shape of output: (batch_size * num_heads, no. of queries or key-value\n# pairs, num_hiddens / num_heads)\nreturn X.reshape( -1, X.shape[ 2], X .shape[ 3])\n@d2l .add_to_class(MultiHeadAttention) #@save\ndef transpose_output (self , X):\n\"\"\"Reverse the operation of transpose_qkv.\"\"\"\nX=X.reshape( -1,self .num_heads, X .shape[ 1], X .shape[ 2])\nX=X.permute( 0,2,1,3)\nreturn X.reshape(X .shape[ 0], X .shape[ 1],-1)\nLet\u2019s test our implemented MultiHeadAttention class using a toy example where keys", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2398, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "834be78d-6808-4f9f-900c-355ab804dab6": {"__data__": {"id_": "834be78d-6808-4f9f-900c-355ab804dab6", "embedding": null, "metadata": {"page_label": "435", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9e3fceca-e435-43ec-93ed-9ae6e3f9f9f0", "node_type": "4", "metadata": {"page_label": "435", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e4f1b13a7b50d7ea99a45da03d0808c5f454479fb57d6e0f342af7d432f027c4", "class_name": "RelatedNodeInfo"}}, "text": "435 Self-Attention and Positional Encoding\n160and values are the same. As a result, the shape of the multi-head attention output is\n(batch_size ,num_queries ,num_hiddens ).\nnum_hiddens, num_heads =100,5\nattention =MultiHeadAttention(num_hiddens, num_heads, 0.5)\nbatch_size, num_queries, num_kvpairs =2,4,6\nvalid_lens =torch .tensor([ 3,2])\nX=torch .ones((batch_size, num_queries, num_hiddens))\nY=torch .ones((batch_size, num_kvpairs, num_hiddens))\nd2l.check_shape(attention(X, Y, Y, valid_lens),\n(batch_size, num_queries, num_hiddens))\n11.5.3Summary\nMulti-headattentioncombinesknowledgeofthesameattentionpoolingviadifferentrepre-\nsentation subspaces of queries, keys, and values. To compute multiple heads of multi-head\nattention in parallel, proper tensor manipulation is needed.\n11.5.4Exercises\n1.Visualize attention weights of multiple heads in this experiment.\n2.Suppose that we have a trained model based on multi-head attention and we want to\nprune less important attention heads to increase the prediction speed. How can we de-\nsign experiments to measure the importance of an attention head?\nDiscussions160.\n11.6Self-Attentionand PositionalEncoding\nIn deep learning, we often use CNNs or RNNs to encode sequences. Now with attention\nmechanisms in mind, imagine feeding a sequence of tokens into an attention mechanism\nsuchthatateverystep,eachtokenhasitsownquery,keys,andvalues. Here,whencomput-\ningthevalueofatoken\u2019srepresentationatthenextlayer,thetokencanattend(viaitsquery\nvector) to any other\u2019s token (matching based on their key vectors). Using the full set of\nquery-key compatibility scores, we can compute, for each token, a representation by build-\ning the appropriate weighted sum over the other tokens. Because every token is attending\nto each other token (unlike the case where decoder steps attend to encoder steps), such\narchitectures are typically described as self-attention models ( Linet al., 2017,Vaswaniet\nal., 2017), and elsewhere described as intra-attention model (Chenget al., 2016,Parikh\net al., 2016,Pauluset al., 2017). In this section, we will discuss sequence encoding using\nself-attention, including using additional information for the sequence order.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "666b28b6-6d43-4c28-8ef5-b8119b98f303": {"__data__": {"id_": "666b28b6-6d43-4c28-8ef5-b8119b98f303", "embedding": null, "metadata": {"page_label": "436", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "859c6b72-a850-4e6a-82c1-88f7c26b1ea0", "node_type": "4", "metadata": {"page_label": "436", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3d940f428aa96630602eee810fdf0746d25fd429fa9da9de5f233288685a693d", "class_name": "RelatedNodeInfo"}}, "text": "436 Attention Mechanisms and Transformers\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n11.6.1Self-Attention\nGiven a sequence of input tokens x1,...,x\ud835\udc5bwhere any x\ud835\udc562R\ud835\udc51(1\u0014\ud835\udc56\u0014\ud835\udc5b), its self-\nattention outputs a sequence of the same length y1,...,y\ud835\udc5b, where\ny\ud835\udc56=\ud835\udc53\u00b9x\ud835\udc56,\u00b9x1,x1\u00ba,...,\u00b9x\ud835\udc5b,x\ud835\udc5b\u00ba\u00ba2R\ud835\udc51(11.6.1)\naccording to the definition of attention pooling in (11.1.1 ). Using multi-head attention,\nthe following code snippet computes the self-attention of a tensor with shape (batch size,\nnumber of time steps or sequence length in tokens, \ud835\udc51). The output tensor has the same\nshape.\nnum_hiddens, num_heads =100,5\nattention =d2l.MultiHeadAttention(num_hiddens, num_heads, 0.5)\nbatch_size, num_queries, valid_lens =2,4, torch .tensor([ 3,2])\nX=torch .ones((batch_size, num_queries, num_hiddens))\nd2l.check_shape(attention(X, X, X, valid_lens),\n(batch_size, num_queries, num_hiddens))\n11.6.2ComparingCNNs, RNNs, and Self-Attention\nLet\u2019s compare architectures for mapping a sequence of \ud835\udc5btokens to another one of equal\nlength, where each input or output token is represented by a \ud835\udc51-dimensional vector. Specif-\nically, we will consider CNNs, RNNs, and self-attention. We will compare their computa-\ntional complexity, sequential operations, and maximum path lengths. Note that sequential\noperations prevent parallel computation, while a shorter path between any combination of\nsequence positions makes it easier to learn long-range dependencies within the sequence\n(Hochreiter etal., 2001).\nLet\u2019s regard any text sequence as a \u201cone-dimensional image\u201d. Similarly, one-dimensional\nCNNscanprocesslocalfeaturessuchas \ud835\udc5b-gramsintext. Givenasequenceoflength \ud835\udc5b,con-\nsider a convolutional layer whose kernel size is \ud835\udc58, and whose numbers of input and output\nchannels are both \ud835\udc51. The computational complexity of the convolutional layer is O\u00b9\ud835\udc58\ud835\udc5b\ud835\udc512\u00ba.\nAsFig. 11.6.1 shows, CNNs are hierarchical, so there are O\u00b91\u00basequential operations and\nthe maximum path length is O\u00b9\ud835\udc5b\u009d\ud835\udc58\u00ba. For example, x1andx5are within the receptive field\nof a two-layer CNN with kernel size 3 in Fig. 11.6.1 .\nWhenupdatingthehiddenstateofRNNs,multiplicationofthe \ud835\udc51\u0002\ud835\udc51weightmatrixandthe\n\ud835\udc51-dimensional hidden state has a computational complexity of O\u00b9\ud835\udc512\u00ba. Since the sequence\nlength is\ud835\udc5b, the computational complexity of the recurrent layer is O\u00b9\ud835\udc5b\ud835\udc512\u00ba. According\ntoFig. 11.6.1 , there areO\u00b9\ud835\udc5b\u00basequential operations that cannot be parallelized and the\nmaximum path length is also O\u00b9\ud835\udc5b\u00ba.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "982917d7-4f1f-4011-9870-fb119b56cd19": {"__data__": {"id_": "982917d7-4f1f-4011-9870-fb119b56cd19", "embedding": null, "metadata": {"page_label": "437", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "307c75f8-8385-40d1-83c1-ca74f5093f70", "node_type": "4", "metadata": {"page_label": "437", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c41ede649d9055a3139acb09e8980a686a1e6e36ddf2f80f6a4ffeac2d8f6fa4", "class_name": "RelatedNodeInfo"}}, "text": "437 Self-Attention and Positional Encoding\ntFig. 11.6.1 Comparing CNN (padding tokens are omitted), RNN, and self-attention architectures.\nIn self-attention, the queries, keys, and values are all \ud835\udc5b\u0002\ud835\udc51matrices. Consider the scaled\ndotproductattentionin (11.3.6 ),wherean\ud835\udc5b\u0002\ud835\udc51matrixismultipliedbya \ud835\udc51\u0002\ud835\udc5bmatrix,then\ntheoutput\ud835\udc5b\u0002\ud835\udc5bmatrixismultipliedbyan \ud835\udc5b\u0002\ud835\udc51matrix. Asaresult, theself-attentionhasa\nO\u00b9\ud835\udc5b2\ud835\udc51\u00bacomputational complexity. As we can see from Fig. 11.6.1 , each token is directly\nconnectedtoanyothertokenviaself-attention. Therefore,computationcanbeparallelwith\nO\u00b91\u00basequential operations and the maximum path length is also O\u00b91\u00ba.\nAll in all, both CNNs and self-attention enjoy parallel computation and self-attention has\nthe shortest maximum path length. However, the quadratic computational complexity with\nrespect to the sequence length makes self-attention prohibitively slow for very long se-\nquences.\n11.6.3Positional Encoding\nUnlike RNNs, which recurrently process tokens of a sequence one-by-one, self-attention\nditches sequential operations in favor of parallel computation. Note that self-attention by\nitself does not preserve the order of the sequence. What do we do if it really matters that\nthe model knows in which order the input sequence arrived?\nThedominantapproachforpreservinginformationabouttheorderoftokensistorepresent\nthis to the model as an additional input associated with each token. These inputs are called\npositional encodings , and they can either be learned or fixed a priori. We now describe a\nsimple scheme for fixed positional encodings based on sine and cosine functions ( Vaswani\netal., 2017).\nSuppose that the input representation X2R\ud835\udc5b\u0002\ud835\udc51contains the \ud835\udc51-dimensional embeddings\nfor\ud835\udc5btokens of a sequence. The positional encoding outputs X\u00b8Pusing a positional\nembedding matrix P2R\ud835\udc5b\u0002\ud835\udc51of the same shape, whose element on the \ud835\udc56throw and the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9451c91e-5cd2-4b07-8126-f86cde4ae9a0": {"__data__": {"id_": "9451c91e-5cd2-4b07-8126-f86cde4ae9a0", "embedding": null, "metadata": {"page_label": "438", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75e9e62b-1d76-48d5-9964-53d572d0924f", "node_type": "4", "metadata": {"page_label": "438", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "89da3ec63955c9290eec2b5c15c528b169f9642d402d179bba7cb612dfe2d150", "class_name": "RelatedNodeInfo"}}, "text": "438 Attention Mechanisms and Transformers\n\u00b92\ud835\udc57\u00bathor the\u00b92\ud835\udc57\u00b81\u00bathcolumn is\n\ud835\udc5d\ud835\udc56,2\ud835\udc57=sin\u0012\ud835\udc56\n100002\ud835\udc57\u009d\ud835\udc51\u0013\n,\n\ud835\udc5d\ud835\udc56,2\ud835\udc57\u00b81=cos\u0012\ud835\udc56\n100002\ud835\udc57\u009d\ud835\udc51\u0013\n.(11.6.2)\nAtfirstglance,thistrigonometricfunctiondesignlooksweird. Beforewegiveexplanations\nof this design, let\u2019s first implement it in the following PositionalEncoding class.\nclass PositionalEncoding (nn.Module): #@save\n\"\"\"Positional encoding.\"\"\"\ndef __init__ (self , num_hiddens, dropout, max_len =1000 ):\nsuper ().__init__ ()\nself .dropout =nn.Dropout(dropout)\n# Create a long enough P\nself .P=torch .zeros(( 1, max_len, num_hiddens))\nX=torch .arange(max_len, dtype =torch .float32) .reshape(\n-1,1)/torch .pow( 10000 , torch .arange(\n0, num_hiddens, 2, dtype =torch .float32) /num_hiddens)\nself .P[:, :, 0::2]=torch .sin(X)\nself .P[:, :, 1::2]=torch .cos(X)\ndef forward (self , X):\nX=X+self .P[:, :X .shape[ 1], :] .to(X .device)\nreturn self .dropout(X)\nIn the positional embedding matrix P, rows correspond to positions within a sequence and\ncolumns represent different positional encoding dimensions. In the example below, we\ncan see that the 6thand the 7thcolumns of the positional embedding matrix have a higher\nfrequencythanthe 8thandthe 9thcolumns. Theoffsetbetweenthe 6thandthe 7th(samefor\nthe8thand the 9th) columns is due to the alternation of sine and cosine functions.\nencoding_dim, num_steps =32,60\npos_encoding =PositionalEncoding(encoding_dim, 0)\nX=pos_encoding(torch .zeros(( 1, num_steps, encoding_dim)))\nP=pos_encoding .P[:, :X .shape[ 1], :]\nd2l.plot(torch .arange(num_steps), P[ 0, :, 6:10].T, xlabel ='Row (position) ',\nfigsize =(6,2.5), legend =[\"Col %d\"%dfor dintorch .arange( 6,10)])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f62182cc-4cbb-4f43-9149-617063909bf9": {"__data__": {"id_": "f62182cc-4cbb-4f43-9149-617063909bf9", "embedding": null, "metadata": {"page_label": "439", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c76e44ec-7081-45d2-bebd-c2e5317f9c34", "node_type": "4", "metadata": {"page_label": "439", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "703784217290c98de873511d8d0ce514d726d5c506149f2b7a00a5b3434fb6ef", "class_name": "RelatedNodeInfo"}}, "text": "439 Self-Attention and Positional Encoding\nAbsolutePositionalInformation\nTo see how the monotonically decreased frequency along the encoding dimension relates\nto absolute positional information, let\u2019s print out the binary representations of 0,1,..., 7.\nAs we can see, the lowest bit, the second-lowest bit, and the third-lowest bit alternate on\nevery number, every two numbers, and every four numbers, respectively.\nfor iinrange (8):\nprint (f'{i}in binary is {i:>03b }')\n0inbinary is000\n1inbinary is001\n2inbinary is010\n3inbinary is011\n4inbinary is100\n5inbinary is101\n6inbinary is110\n7inbinary is111\nIn binary representations, a higher bit has a lower frequency than a lower bit. Similarly,\nas demonstrated in the heat map below, the positional encoding decreases frequencies\nalongtheencodingdimensionbyusingtrigonometricfunctions. Sincetheoutputsarefloat\nnumbers, such continuous representations are more space-efficient than binary representa-\ntions.\nP=P[0, :, :] .unsqueeze( 0).unsqueeze( 0)\nd2l.show_heatmaps(P, xlabel ='Column (encoding dimension) ',\nylabel ='Row (position) ', figsize =(3.5,4), cmap ='Blues ')\nRelativePositionalInformation\nBesides capturing absolute positional information, the above positional encoding also al-\nlows a model to easily learn to attend by relative positions. This is because for any fixed", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d01244cc-69c4-42b9-ba91-c9c69a0d0042": {"__data__": {"id_": "d01244cc-69c4-42b9-ba91-c9c69a0d0042", "embedding": null, "metadata": {"page_label": "440", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d57d29f-acc1-4b6d-b012-d95f788a42b7", "node_type": "4", "metadata": {"page_label": "440", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "80099b66151c9a93c961fd7fbb538f7d72dbd0457efe0f14e8a86181b2cc8c8b", "class_name": "RelatedNodeInfo"}}, "text": "440 Attention Mechanisms and Transformers\n161position offset \ud835\udeff, the positional encoding at position \ud835\udc56\u00b8\ud835\udeffcan be represented by a linear\nprojection of that at position \ud835\udc56.\nThis projection can be explained mathematically. Denoting \ud835\udf14\ud835\udc57=1\u009d100002\ud835\udc57\u009d\ud835\udc51, any pair\nof\u00b9\ud835\udc5d\ud835\udc56,2\ud835\udc57,\ud835\udc5d\ud835\udc56,2\ud835\udc57\u00b81\u00bain(11.6.2 )can be linearly projected to \u00b9\ud835\udc5d\ud835\udc56\u00b8\ud835\udeff,2\ud835\udc57,\ud835\udc5d\ud835\udc56\u00b8\ud835\udeff,2\ud835\udc57\u00b81\u00bafor any fixed\noffset\ud835\udeff:\n\u0014cos\u00b9\ud835\udeff\ud835\udf14\ud835\udc57\u00ba sin\u00b9\ud835\udeff\ud835\udf14\ud835\udc57\u00ba\n\u0000sin\u00b9\ud835\udeff\ud835\udf14\ud835\udc57\u00bacos\u00b9\ud835\udeff\ud835\udf14\ud835\udc57\u00ba\u0015 \u0014\ud835\udc5d\ud835\udc56,2\ud835\udc57\n\ud835\udc5d\ud835\udc56,2\ud835\udc57\u00b81\u0015\n=\u0014cos\u00b9\ud835\udeff\ud835\udf14\ud835\udc57\u00basin\u00b9\ud835\udc56\ud835\udf14\ud835\udc57\u00ba\u00b8sin\u00b9\ud835\udeff\ud835\udf14\ud835\udc57\u00bacos\u00b9\ud835\udc56\ud835\udf14\ud835\udc57\u00ba\n\u0000sin\u00b9\ud835\udeff\ud835\udf14\ud835\udc57\u00basin\u00b9\ud835\udc56\ud835\udf14\ud835\udc57\u00ba\u00b8cos\u00b9\ud835\udeff\ud835\udf14\ud835\udc57\u00bacos\u00b9\ud835\udc56\ud835\udf14\ud835\udc57\u00ba\u0015\n=\u0014sin\u0000\u00b9\ud835\udc56\u00b8\ud835\udeff\u00ba\ud835\udf14\ud835\udc57\u0001\ncos\u0000\u00b9\ud835\udc56\u00b8\ud835\udeff\u00ba\ud835\udf14\ud835\udc57\u0001\u0015\n=\u0014\ud835\udc5d\ud835\udc56\u00b8\ud835\udeff,2\ud835\udc57\n\ud835\udc5d\ud835\udc56\u00b8\ud835\udeff,2\ud835\udc57\u00b81\u0015\n,\n(11.6.3)\nwhere the 2\u00022projection matrix does not depend on any position index \ud835\udc56.\n11.6.4Summary\nIn self-attention, the queries, keys, and values all come from the same place. Both CNNs\nand self-attention enjoy parallel computation and self-attention has the shortest maximum\npathlength. However,thequadraticcomputationalcomplexitywithrespecttothesequence\nlengthmakesself-attentionprohibitivelyslowforverylongsequences. Tousethesequence\norder information, we can inject absolute or relative positional information by adding po-\nsitional encoding to the input representations.\n11.6.5Exercises\n1.Suppose that we design a deep architecture to represent a sequence by stacking self-\nattention layers with positional encoding. What could the possible issues be?\n2.Can you design a learnable positional encoding method?\n3.Canweassigndifferentlearnedembeddingsaccordingtodifferentoffsetsbetweenqueries\nand keys that are compared in self-attention? Hint: you may refer to relative position\nembeddings ( Huangetal., 2018,Shawetal., 2018).\nDiscussions161.\n11.7The TransformerArchitecture\nWe have compared CNNs, RNNs, and self-attention in Section 11.6.2 . Notably, self-\nattention enjoys both parallel computation and the shortest maximum path length. There-\nfore, it is appealing to design deep architectures by using self-attention. Unlike earlier\nself-attention models that still rely on RNNs for input representations ( Chenget al., 2016,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0e7b778-9862-44a9-afc4-6a809e21a291": {"__data__": {"id_": "d0e7b778-9862-44a9-afc4-6a809e21a291", "embedding": null, "metadata": {"page_label": "441", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e4bd2813-2a5e-476b-940d-e368a7c7a507", "node_type": "4", "metadata": {"page_label": "441", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9a92ae2ed628d2e3fac6a9ac52cad76487cc28661835f09d0159df6e74152a0b", "class_name": "RelatedNodeInfo"}}, "text": "441 The Transformer Architecture\nLinet al., 2017,Pauluset al., 2017), the Transformer model is solely based on attention\nmechanisms without any convolutional or recurrent layer ( Vaswaniet al., 2017). Though\noriginallyproposedforsequence-to-sequencelearningontextdata,Transformershavebeen\npervasiveinawiderangeofmoderndeeplearningapplications, suchasinareastodowith\nlanguage, vision, speech, and reinforcement learning.\nimport math\nimport pandas aspd\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n11.7.1Model\nAs an instance of the encoder\u2013decoder architecture, the overall architecture of the Trans-\nformer is presented in Fig. 11.7.1 . As we can see, the Transformer is composed of an en-\ncoder and a decoder. In contrast to Bahdanau attention for sequence-to-sequence learning\ninFig. 11.4.2 , the input (source) and output (target) sequence embeddings are added with\npositional encoding before being fed into the encoder and the decoder that stack modules\nbased on self-attention.\nNowweprovideanoverviewoftheTransformerarchitecturein Fig.11.7.1 . Atahighlevel,\nthe Transformer encoder is a stack of multiple identical layers, where each layer has two\nsublayers (either is denoted as sublayer). The first is a multi-head self-attention pooling\nand the second is a positionwise feed-forward network. Specifically, in the encoder self-\nattention, queries, keys, and values are all from the outputs of the previous encoder layer.\nInspired by the ResNet design of Section 8.6 , a residual connection is employed around\nboth sublayers. In the Transformer, for any input x2R\ud835\udc51at any position of the sequence,\nwe require that sublayer \u00b9x\u00ba2R\ud835\udc51so that the residual connection x\u00b8sublayer\u00b9x\u00ba2R\ud835\udc51is\nfeasible. This addition from the residual connection is immediately followed by layer nor-\nmalization ( Baetal., 2016). As a result, the Transformer encoder outputs a \ud835\udc51-dimensional\nvector representation for each position of the input sequence.\nThe Transformer decoder is also a stack of multiple identical layers with residual connec-\ntions and layer normalizations. As well as the two sublayers described in the encoder, the\ndecoder inserts a third sublayer, known as the encoder\u2013decoder attention, between these\ntwo. In the encoder\u2013decoder attention, queries are from the outputs of the decoder\u2019s self-\nattention sublayer, and the keys and values are from the Transformer encoder outputs. In\nthedecoderself-attention, queries, keys, andvaluesareallfromtheoutputsoftheprevious\ndecoder layer. However, each position in the decoder is allowed only to attend to all posi-\ntions in the decoder up to that position. This maskedattention preserves the autoregressive\nproperty, ensuring that the prediction only depends on those output tokens that have been\ngenerated.\nWehavealreadydescribedandimplementedmulti-headattentionbasedonscaleddotprod-\nucts inSection 11.5 and positional encoding in Section 11.6.3 . In the following, we will\nimplement the rest of the Transformer model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0e977a7-942e-4647-ae6c-5d6a43848932": {"__data__": {"id_": "b0e977a7-942e-4647-ae6c-5d6a43848932", "embedding": null, "metadata": {"page_label": "442", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd36819f-b150-4935-8771-0538b9592b9b", "node_type": "4", "metadata": {"page_label": "442", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b48e2073667345b0199ec7a57ba5b6e99f0e61e98a52cca6faaf670ae6dddd1f", "class_name": "RelatedNodeInfo"}}, "text": "442 Attention Mechanisms and Transformers\ntFig. 11.7.1 The Transformer architecture.\n11.7.2PositionwiseFeed-ForwardNetworks\nThe positionwise feed-forward network transforms the representation at all the sequence\npositions using the same MLP. This is why we call it positionwise . In the implementation\nbelow,theinput Xwithshape(batchsize,numberoftimestepsorsequencelengthintokens,\nnumber of hidden units or feature dimension) will be transformed by a two-layer MLP into\nan output tensor of shape (batch size, number of time steps, ffn_num_outputs ).\nclass PositionWiseFFN (nn.Module): #@save\n\"\"\"The positionwise feed-forward network.\"\"\"\ndef __init__ (self , ffn_num_hiddens, ffn_num_outputs):\nsuper ().__init__ ()\nself .dense1 =nn.LazyLinear(ffn_num_hiddens)\nself .relu =nn.ReLU()\nself .dense2 =nn.LazyLinear(ffn_num_outputs)\ndef forward (self , X):\nreturn self .dense2( self .relu( self .dense1(X)))\nThefollowingexampleshowsthattheinnermostdimensionofatensorchangestothenum-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 975, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ed9b3d1-4698-4738-ac04-c1954ff3d573": {"__data__": {"id_": "1ed9b3d1-4698-4738-ac04-c1954ff3d573", "embedding": null, "metadata": {"page_label": "443", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1424a284-25c9-4e1e-9aca-046d0fa037e5", "node_type": "4", "metadata": {"page_label": "443", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e2b8f6fa0ae11eca3ada18d5d3558f55133d940004b62d79a8379c47567afa22", "class_name": "RelatedNodeInfo"}}, "text": "443 The Transformer Architecture\nber of outputs in the positionwise feed-forward network. Since the same MLP transforms\natallthepositions,whentheinputsatallthesepositionsarethesame,theiroutputsarealso\nidentical.\nffn =PositionWiseFFN( 4,8)\nffn.eval()\nffn(torch .ones(( 2,3,4)))[ 0]\ntensor([[ 0.6300 ,0.7739 ,0.0278 ,0.2508 ,-0.0519 ,0.4881 ,-0.4105 ,0.\n\u21a9!5163 ],\n[0.6300 ,0.7739 ,0.0278 ,0.2508 ,-0.0519 ,0.4881 ,-0.4105 ,0.\n\u21a9!5163 ],\n[0.6300 ,0.7739 ,0.0278 ,0.2508 ,-0.0519 ,0.4881 ,-0.4105 ,0.\n\u21a9!5163 ]],\ngrad_fn =<SelectBackward0 >)\n11.7.3Residual Connection and LayerNormalization\nNow let\u2019s focus on the \u201cadd & norm\u201d component in Fig. 11.7.1 . As we described at the\nbeginning of this section, this is a residual connection immediately followed by layer nor-\nmalization. Both are key to effective deep architectures.\nInSection 8.5 , we explained how batch normalization recenters and rescales across the\nexampleswithinaminibatch. Asdiscussedin Section8.5.2 ,layernormalizationisthesame\nasbatchnormalizationexceptthattheformernormalizesacrossthefeaturedimension,thus\nenjoyingbenefitsofscaleindependenceandbatchsizeindependence. Despiteitspervasive\napplications in computer vision, batch normalization is usually empirically less effective\nthan layer normalization in natural language processing tasks, where the inputs are often\nvariable-length sequences.\nThe following code snippet compares the normalization across different dimensions by\nlayer normalization and batch normalization.\nln=nn.LayerNorm( 2)\nbn=nn.LazyBatchNorm1d()\nX=torch .tensor([[ 1,2], [ 2,3]], dtype =torch .float32)\n# Compute mean and variance from X in the training mode\nprint ('layer norm: ', ln(X), '\\nbatch norm: ', bn(X))\nlayer norm: tensor([[ -1.0000 ,1.0000 ],\n[-1.0000 ,1.0000 ]], grad_fn =<NativeLayerNormBackward0 >)\nbatch norm: tensor([[ -1.0000 ,-1.0000 ],\n[1.0000 ,1.0000 ]], grad_fn =<NativeBatchNormBackward0 >)\nNow we can implement the AddNorm class using a residual connection followed by layer\nnormalization. Dropout is also applied for regularization.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ffbdfb1-5a77-4338-80f1-a436b1a728b4": {"__data__": {"id_": "2ffbdfb1-5a77-4338-80f1-a436b1a728b4", "embedding": null, "metadata": {"page_label": "444", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b450e992-5a88-4002-b156-6276ca33f482", "node_type": "4", "metadata": {"page_label": "444", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bd92b291f7f2f37390d87222289531e3349b1cc3e4f121c23985c297769d7c5c", "class_name": "RelatedNodeInfo"}}, "text": "444 Attention Mechanisms and Transformers\nclass AddNorm (nn.Module): #@save\n\"\"\"The residual connection followed by layer normalization.\"\"\"\ndef __init__ (self , norm_shape, dropout):\nsuper ().__init__ ()\nself .dropout =nn.Dropout(dropout)\nself .ln=nn.LayerNorm(norm_shape)\ndef forward (self , X, Y):\nreturn self .ln(self .dropout(Y) +X)\nTheresidualconnectionrequiresthatthetwoinputsareofthesameshapesothattheoutput\ntensor also has the same shape after the addition operation.\nadd_norm =AddNorm( 4,0.5)\nshape =(2,3,4)\nd2l.check_shape(add_norm(torch .ones(shape), torch .ones(shape)), shape)\n11.7.4Encoder\nWith all the essential components to assemble the Transformer encoder, let\u2019s start by im-\nplementing a single layer within the encoder. The following TransformerEncoderBlock\nclass contains two sublayers: multi-head self-attention and positionwise feed-forward net-\nworks, where a residual connection followed by layer normalization is employed around\nboth sublayers.\nclass TransformerEncoderBlock (nn.Module): #@save\n\"\"\"The Transformer encoder block.\"\"\"\ndef __init__ (self , num_hiddens, ffn_num_hiddens, num_heads, dropout,\nuse_bias =False ):\nsuper ().__init__ ()\nself .attention =d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout, use_bias)\nself .addnorm1 =AddNorm(num_hiddens, dropout)\nself .ffn =PositionWiseFFN(ffn_num_hiddens, num_hiddens)\nself .addnorm2 =AddNorm(num_hiddens, dropout)\ndef forward (self , X, valid_lens):\nY=self .addnorm1(X, self .attention(X, X, X, valid_lens))\nreturn self .addnorm2(Y, self .ffn(Y))\nAs we can see, no layer in the Transformer encoder changes the shape of its input.\nX=torch .ones(( 2,100,24))\nvalid_lens =torch .tensor([ 3,2])\nencoder_blk =TransformerEncoderBlock( 24,48,8,0.5)\nencoder_blk .eval()\nd2l.check_shape(encoder_blk(X, valid_lens), X .shape)\nIn the following Transformer encoder implementation, we stack num_blks instances of the\nabove TransformerEncoderBlock classes. Since we use the fixed positional encoding", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1971, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "859757f8-6754-4869-8e80-db04e9dd71dc": {"__data__": {"id_": "859757f8-6754-4869-8e80-db04e9dd71dc", "embedding": null, "metadata": {"page_label": "445", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17e872b1-0bcd-42e6-9180-c671fb9402ab", "node_type": "4", "metadata": {"page_label": "445", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c699fa6db2958797f275953e1b78622d5885fec8d56f6f0155120c02df31d409", "class_name": "RelatedNodeInfo"}}, "text": "445 The Transformer Architecture\nwhose values are always between \u00001and1, we multiply values of the learnable input em-\nbeddings by the square root of the embedding dimension to rescale before summing up the\ninput embedding and the positional encoding.\nclass TransformerEncoder (d2l .Encoder): #@save\n\"\"\"The Transformer encoder.\"\"\"\ndef __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout, use_bias =False ):\nsuper ().__init__ ()\nself .num_hiddens =num_hiddens\nself .embedding =nn.Embedding(vocab_size, num_hiddens)\nself .pos_encoding =d2l.PositionalEncoding(num_hiddens, dropout)\nself .blks =nn.Sequential()\nfor iinrange (num_blks):\nself .blks .add_module( \"block \"+str(i), TransformerEncoderBlock(\nnum_hiddens, ffn_num_hiddens, num_heads, dropout, use_bias))\ndef forward (self , X, valid_lens):\n# Since positional encoding values are between -1 and 1, the embedding\n# values are multiplied by the square root of the embedding dimension\n# to rescale before they are summed up\nX=self .pos_encoding( self .embedding(X) *math .sqrt( self .num_hiddens))\nself .attention_weights =[None ]*len(self .blks)\nfor i, blk inenumerate (self .blks):\nX=blk(X, valid_lens)\nself .attention_weights[\ni]=blk.attention .attention .attention_weights\nreturn X\nBelowwespecifyhyperparameterstocreateatwo-layerTransformerencoder. Theshapeof\ntheTransformerencoderoutputis(batchsize,numberoftimesteps, num_hiddens ).\nencoder =TransformerEncoder( 200,24,48,8,2,0.5)\nd2l.check_shape(encoder(torch .ones(( 2,100), dtype =torch .long), valid_lens),\n(2,100,24))\n11.7.5Decoder\nAs shown in Fig. 11.7.1 , the Transformer decoder is composed of multiple identical lay-\ners. Each layer is implemented in the following TransformerDecoderBlock class, which\ncontains three sublayers: decoder self-attention, encoder\u2013decoder attention, and position-\nwise feed-forward networks. These sublayers employ a residual connection around them\nfollowed by layer normalization.\nAs we described earlier in this section, in the masked multi-head decoder self-attention\n(the first sublayer), queries, keys, and values all come from the outputs of the previous\ndecoder layer. When training sequence-to-sequence models, tokens at all the positions\n(time steps) of the output sequence are known. However, during prediction the output\nsequence is generated token by token; thus, at any decoder time step only the generated\ntokenscanbeusedinthedecoderself-attention. Topreserveautoregressioninthedecoder,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d12898bc-f373-4fef-ae23-ebbba1419232": {"__data__": {"id_": "d12898bc-f373-4fef-ae23-ebbba1419232", "embedding": null, "metadata": {"page_label": "446", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f8cd6a3-733b-4aef-bf06-55d41d0ebb3d", "node_type": "4", "metadata": {"page_label": "446", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "954db1bd563429c3d4cc7c32d84915d0471238dd236953bc38678094a802f87a", "class_name": "RelatedNodeInfo"}}, "text": "446 Attention Mechanisms and Transformers\nits masked self-attention specifies dec_valid_lens so that any query only attends to all\npositions in the decoder up to the query position.\nclass TransformerDecoderBlock (nn.Module):\n# The i-th block in the Transformer decoder\ndef __init__ (self , num_hiddens, ffn_num_hiddens, num_heads, dropout, i):\nsuper ().__init__ ()\nself .i=i\nself .attention1 =d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout)\nself .addnorm1 =AddNorm(num_hiddens, dropout)\nself .attention2 =d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout)\nself .addnorm2 =AddNorm(num_hiddens, dropout)\nself .ffn =PositionWiseFFN(ffn_num_hiddens, num_hiddens)\nself .addnorm3 =AddNorm(num_hiddens, dropout)\ndef forward (self , X, state):\nenc_outputs, enc_valid_lens =state[ 0], state[ 1]\n# During training, all the tokens of any output sequence are processed\n# at the same time, so state[2][self.i] is None as initialized. When\n# decoding any output sequence token by token during prediction,\n# state[2][self.i] contains representations of the decoded output at\n# the i-th block up to the current time step\nifstate[ 2][self .i]isNone :\nkey_values =X\nelse :\nkey_values =torch .cat((state[ 2][self .i], X), dim =1)\nstate[ 2][self .i]=key_values\nifself .training:\nbatch_size, num_steps, _ =X.shape\n# Shape of dec_valid_lens: (batch_size, num_steps), where every\n# row is [1, 2, ..., num_steps]\ndec_valid_lens =torch .arange(\n1, num_steps +1, device =X.device) .repeat(batch_size, 1)\nelse :\ndec_valid_lens =None\n# Self-attention\nX2=self .attention1(X, key_values, key_values, dec_valid_lens)\nY=self .addnorm1(X, X2)\n# Encoder-decoder attention. Shape of enc_outputs:\n# (batch_size, num_steps, num_hiddens)\nY2=self .attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\nZ=self .addnorm2(Y, Y2)\nreturn self .addnorm3(Z, self .ffn(Z)), state\nTo facilitate scaled dot product operations in the encoder\u2013decoder attention and addition\noperationsintheresidualconnections,thefeaturedimension( num_hiddens )ofthedecoder\nis the same as that of the encoder.\ndecoder_blk =TransformerDecoderBlock( 24,48,8,0.5,0)\nX=torch .ones(( 2,100,24))\nstate =[encoder_blk(X, valid_lens), valid_lens, [ None ]]\nd2l.check_shape(decoder_blk(X, state)[ 0], X .shape)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bdbb2bd8-0c04-4c0e-a0dc-238291db6b1c": {"__data__": {"id_": "bdbb2bd8-0c04-4c0e-a0dc-238291db6b1c", "embedding": null, "metadata": {"page_label": "447", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a86abe0-878d-4dab-b123-24b3e4e41de4", "node_type": "4", "metadata": {"page_label": "447", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "feacc7255de591afbfc1e8564f3cd2a8c58941e96deea6d54ae5b038be7f8725", "class_name": "RelatedNodeInfo"}}, "text": "447 The Transformer Architecture\nNow we construct the entire Transformer decoder composed of num_blks instances of\nTransformerDecoderBlock . In the end, a fully connected layer computes the prediction\nfor all the vocab_size possible output tokens. Both of the decoder self-attention weights\nand the encoder\u2013decoder attention weights are stored for later visualization.\nclass TransformerDecoder (d2l .AttentionDecoder):\ndef __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout):\nsuper ().__init__ ()\nself .num_hiddens =num_hiddens\nself .num_blks =num_blks\nself .embedding =nn.Embedding(vocab_size, num_hiddens)\nself .pos_encoding =d2l.PositionalEncoding(num_hiddens, dropout)\nself .blks =nn.Sequential()\nfor iinrange (num_blks):\nself .blks .add_module( \"block \"+str(i), TransformerDecoderBlock(\nnum_hiddens, ffn_num_hiddens, num_heads, dropout, i))\nself .dense =nn.LazyLinear(vocab_size)\ndef init_state (self , enc_outputs, enc_valid_lens):\nreturn [enc_outputs, enc_valid_lens, [ None ]*self .num_blks]\ndef forward (self , X, state):\nX=self .pos_encoding( self .embedding(X) *math .sqrt( self .num_hiddens))\nself ._attention_weights =[[None ]*len(self .blks) for _inrange (2)]\nfor i, blk inenumerate (self .blks):\nX, state =blk(X, state)\n# Decoder self-attention weights\nself ._attention_weights[ 0][\ni]=blk.attention1 .attention .attention_weights\n# Encoder-decoder attention weights\nself ._attention_weights[ 1][\ni]=blk.attention2 .attention .attention_weights\nreturn self .dense(X), state\n@property\ndef attention_weights (self ):\nreturn self ._attention_weights\n11.7.6Training\nLet\u2019s instantiate an encoder\u2013decoder model by following the Transformer architecture.\nHere we specify that both the Transformer encoder and the Transformer decoder have two\nlayers using 4-head attention. As in Section 10.7.6 , we train the Transformer model for\nsequence-to-sequence learning on the English\u2013French machine translation dataset.\ndata =d2l.MTFraEng(batch_size =128)\nnum_hiddens, num_blks, dropout =256,2,0.2\nffn_num_hiddens, num_heads =64,4\nencoder =TransformerEncoder(\nlen(data .src_vocab), num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout)\ndecoder =TransformerDecoder(\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf32acb5-2a3b-4cd0-ad71-da4d36868d8c": {"__data__": {"id_": "cf32acb5-2a3b-4cd0-ad71-da4d36868d8c", "embedding": null, "metadata": {"page_label": "448", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8789bf31-3874-4aff-9f1d-e93894d2f417", "node_type": "4", "metadata": {"page_label": "448", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4b2e1567d73fe7b620ab93359c5e589e7e10634fba191de6a68858da94bb13e7", "class_name": "RelatedNodeInfo"}}, "text": "448 Attention Mechanisms and Transformers\n(continued from previous page)\nlen(data .tgt_vocab), num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout)\nmodel =d2l.Seq2Seq(encoder, decoder, tgt_pad =data .tgt_vocab[ '<pad> '],\nlr=0.001 )\ntrainer =d2l.Trainer(max_epochs =30, gradient_clip_val =1, num_gpus =1)\ntrainer .fit(model, data)\nAfter training, we use the Transformer model to translate a few English sentences into\nFrench and compute their BLEU scores.\nengs =['go . ','i lost . ','he\\'s calm . ','i\\'m home . ']\nfras =['va ! ','j\\'ai perdu . ','il est calme . ','je suis chez moi . ']\npreds, _ =model .predict_step(\ndata .build(engs, fras), d2l .try_gpu(), data .num_steps)\nfor en, fr, p inzip(engs, fras, preds):\ntranslation =[]\nfor token indata .tgt_vocab .to_tokens(p):\niftoken =='<eos> ':\nbreak\ntranslation .append(token)\nprint (f'{en}=>{translation }, bleu, '\nf'{d2l.bleu( \"\".join(translation), fr, k=2):.3f}')\ngo.=>['va','!'], bleu, 1.000\ni lost .=>['je','perdu ','.'], bleu, 0.687\nhe's calm . => [ 'il','est','mouill\u00e9 ','.'], bleu,0.658\ni'm home . => [ 'je','suis ','chez ','moi','.'], bleu,1.000\nLet\u2019s visualize the Transformer attention weights when translating the final English sen-\ntence into French. The shape of the encoder self-attention weights is (number of encoder\nlayers, numberofattentionheads, num_steps ornumberofqueries, num_steps ornumber\nof key-value pairs).\n_, dec_attention_weights =model .predict_step(\ndata .build([engs[ -1]], [fras[ -1]]), d2l .try_gpu(), data .num_steps, True )\nenc_attention_weights =torch .cat(model .encoder .attention_weights, 0)\nshape =(num_blks, num_heads, -1, data .num_steps)\nenc_attention_weights =enc_attention_weights .reshape(shape)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d6bdc77-291b-412a-82be-af198bfb31a3": {"__data__": {"id_": "1d6bdc77-291b-412a-82be-af198bfb31a3", "embedding": null, "metadata": {"page_label": "449", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f724f9a-261a-45dc-868f-4fb6ce8d4b12", "node_type": "4", "metadata": {"page_label": "449", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "34826069f1c363b35c30855d9a879d05f640419e7a39ea2929521a12a0e0aea9", "class_name": "RelatedNodeInfo"}}, "text": "449 The Transformer Architecture\n(continued from previous page)\nd2l.check_shape(enc_attention_weights,\n(num_blks, num_heads, data .num_steps, data .num_steps))\nIn the encoder self-attention, both queries and keys come from the same input sequence.\nSince padding tokens do not carry meaning, with specified valid length of the input se-\nquence no query attends to positions of padding tokens. In the following, two layers of\nmulti-head attention weights are presented row by row. Each head independently attends\nbased on a separate representation subspace of queries, keys, and values.\nd2l.show_heatmaps(\nenc_attention_weights .cpu(), xlabel ='Key positions ',\nylabel ='Query positions ', titles =['Head %d'%ifor iinrange (1,5)],\nfigsize =(7,3.5))\nTo visualize the decoder self-attention weights and the encoder\u2013decoder attention weights,\nwe need more data manipulations. For example, we fill the masked attention weights\nwith zero. Note that the decoder self-attention weights and the encoder\u2013decoder atten-\ntion weights both have the same queries: the beginning-of-sequence token followed by the\noutput tokens and possibly end-of-sequence tokens.\ndec_attention_weights_2d =[head[ 0].tolist()\nfor step indec_attention_weights\nfor attn instep for blk inattn for head inblk]\ndec_attention_weights_filled =torch .tensor(\npd.DataFrame(dec_attention_weights_2d) .fillna( 0.0).values)\nshape =(-1,2, num_blks, num_heads, data .num_steps)\ndec_attention_weights =dec_attention_weights_filled .reshape(shape)\ndec_self_attention_weights, dec_inter_attention_weights =\\\ndec_attention_weights .permute( 1,2,3,0,4)\nd2l.check_shape(dec_self_attention_weights,\n(num_blks, num_heads, data .num_steps, data .num_steps))\nd2l.check_shape(dec_inter_attention_weights,\n(num_blks, num_heads, data .num_steps, data .num_steps))", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b06afe50-386d-4829-8a31-fa8075b1293b": {"__data__": {"id_": "b06afe50-386d-4829-8a31-fa8075b1293b", "embedding": null, "metadata": {"page_label": "450", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6742c90-e8dd-4046-b535-899db847730e", "node_type": "4", "metadata": {"page_label": "450", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d5d45aa0ea8833c7c9b277f902b7ace8f5ec2ceb94fc60f673afc26279fe0e29", "class_name": "RelatedNodeInfo"}}, "text": "450 Attention Mechanisms and Transformers\nBecause of the autoregressive property of the decoder self-attention, no query attends to\nkey\u2013value pairs after the query position.\nd2l.show_heatmaps(\ndec_self_attention_weights[:, :, :, :],\nxlabel ='Key positions ', ylabel ='Query positions ',\ntitles =['Head %d'%ifor iinrange (1,5)], figsize =(7,3.5))\nSimilar to the case in the encoder self-attention, via the specified valid length of the input\nsequence,noqueryfromtheoutputsequenceattendstothosepaddingtokensfromtheinput\nsequence.\nd2l.show_heatmaps(\ndec_inter_attention_weights, xlabel ='Key positions ',\nylabel ='Query positions ', titles =['Head %d'%ifor iinrange (1,5)],\nfigsize =(7,3.5))\nAlthough the Transformer architecture was originally proposed for sequence-to-sequence\nlearning, aswewilldiscoverlaterinthebook, eithertheTransformerencoderortheTrans-\nformer decoder is often individually used for different deep learning tasks.\n11.7.7Summary", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3700ca56-d9a8-46b5-b814-b14a18fef5c5": {"__data__": {"id_": "3700ca56-d9a8-46b5-b814-b14a18fef5c5", "embedding": null, "metadata": {"page_label": "451", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a853a1be-9ead-4e6d-8faa-19baf84a27b4", "node_type": "4", "metadata": {"page_label": "451", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e8afeb2dd4a2cd542f35469f2836f1d89f60dfc289a6af0292d085fdb309268e", "class_name": "RelatedNodeInfo"}}, "text": "451 Transformers for Vision\n162The Transformer is an instance of the encoder\u2013decoder architecture, though either the en-\ncoder or the decoder can be used individually in practice. In the Transformer architec-\nture, multi-head self-attention is used for representing the input sequence and the output\nsequence, though the decoder has to preserve the autoregressive property via a masked\nversion. Both the residual connections and the layer normalization in the Transformer are\nimportant for training a very deep model. The positionwise feed-forward network in the\nTransformer model transforms the representation at all the sequence positions using the\nsame MLP.\n11.7.8Exercises\n1.Train a deeper Transformer in the experiments. How does it affect the training speed\nand the translation performance?\n2.Is it a good idea to replace scaled dot product attention with additive attention in the\nTransformer? Why?\n3.Forlanguagemodeling,shouldweusetheTransformerencoder,decoder,orboth? How\nwould you design this method?\n4.What challenges can Transformers face if input sequences are very long? Why?\n5.How would you improve the computational and memory efficiency of Transformers?\nHint: you may refer to the survey paper by Tay etal.(2020).\nDiscussions162.\n11.8TransformersforVision\nThe Transformer architecture was initially proposed for sequence-to-sequence learning,\nwith a focus on machine translation. Subsequently, Transformers emerged as the model\nof choice in various natural language processing tasks ( Brownet al., 2020,Devlinet al.,\n2018,Radfordetal.,2018,Radfordetal.,2019,Raffeletal.,2020). However,inthefieldof\ncomputer vision the dominant architecture has remained the CNN ( Chapter 8 ). Naturally,\nresearchers started to wonder if it might be possible to do better by adapting Transformer\nmodelstoimagedata. Thisquestionsparkedimmenseinterestinthecomputervisioncom-\nmunity. Recently, Ramachandran et al.(2019) proposed a scheme for replacing convolu-\ntion with self-attention. However, its use of specialized patterns in attention makes it hard\nto scale up models on hardware accelerators. Then, Cordonnier et al.(2020) theoretically\nproved that self-attention can learn to behave similarly to convolution. Empirically, 2\u00022\npatches were taken from images as inputs, but the small patch size makes the model only\napplicable to image data with low resolutions.\nWithout specific constraints on patch size, visionTransformers (ViTs) extract patches from\nimages and feed them into a Transformer encoder to obtain a global representation, which", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a851ed6-52ee-4a10-8339-62860712c3f5": {"__data__": {"id_": "7a851ed6-52ee-4a10-8339-62860712c3f5", "embedding": null, "metadata": {"page_label": "452", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14112a47-fb6f-4de7-b1ee-d396a86d0133", "node_type": "4", "metadata": {"page_label": "452", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5830f15703e2fad89c0b926cce5a545d92285b5102bcff6ba737518650cde315", "class_name": "RelatedNodeInfo"}}, "text": "452 Attention Mechanisms and Transformers\nwillfinallybetransformedforclassification( Dosovitskiy etal.,2021). Notably,Transform-\ners show better scalability than CNNs: and when training larger models on larger datasets,\nvision Transformers outperform ResNets by a significant margin. Similar to the landscape\nof network architecture design in natural language processing, Transformers have also be-\ncome a game-changer in computer vision.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n11.8.1Model\nFig.11.8.1 depictsthemodelarchitectureofvisionTransformers. Thisarchitectureconsists\nof a stem that patchifies images, a body based on the multilayer Transformer encoder, and\na head that transforms the global representation into the output label.\ntFig. 11.8.1 The vision Transformer architecture. In this example, an image is split into nine patches.\nA special \u201c<cls>\u201d token and the nine \ufb02attened image patches are transformed via patch\nembedding and nTransformer encoder blocks into ten representations, respectively. The\n\u201c<cls>\u201d representation is further transformed into the output label.\nConsider an input image with height \u210e, width\ud835\udc64, and\ud835\udc50channels. Specifying the patch\nheight and width both as \ud835\udc5d, the image is split into a sequence of \ud835\udc5a=\u210e\ud835\udc64\u009d\ud835\udc5d2patches,\nwhere each patch is flattened to a vector of length \ud835\udc50\ud835\udc5d2. In this way, image patches can be", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d960554-c8a0-45cd-8f05-dbe4ed527706": {"__data__": {"id_": "7d960554-c8a0-45cd-8f05-dbe4ed527706", "embedding": null, "metadata": {"page_label": "453", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66270d69-ecd3-4a9e-8ab0-b0be00805298", "node_type": "4", "metadata": {"page_label": "453", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4edeabfd16f68e76d9bc089d43e60daa2bb7a7bfdb25ddc7677edf2f3270ff45", "class_name": "RelatedNodeInfo"}}, "text": "453 Transformers for Vision\ntreated similarly to tokens in text sequences by Transformer encoders. A special \u201c<cls>\u201d\n(class) token and the \ud835\udc5aflattened image patches are linearly projected into a sequence of\n\ud835\udc5a\u00b81vectors, summed with learnable positional embeddings. The multilayer Transformer\nencoder transforms \ud835\udc5a\u00b81input vectors into the same number of output vector representa-\ntionsofthesamelength. ItworksexactlythesamewayastheoriginalTransformerencoder\ninFig. 11.7.1 , only differing in the position of normalization. Since the \u201c<cls>\u201d token at-\ntends to all the image patches via self-attention (see Fig. 11.6.1 ), its representation from\nthe Transformer encoder output will be further transformed into the output label.\n11.8.2PatchEmbedding\nTo implement a vision Transformer, let\u2019s start with patch embedding in Fig. 11.8.1 . Split-\nting an image into patches and linearly projecting these flattened patches can be simplified\nas a single convolution operation, where both the kernel size and the stride size are set to\nthe patch size.\nclass PatchEmbedding (nn.Module):\ndef __init__ (self , img_size =96, patch_size =16, num_hiddens =512):\nsuper ().__init__ ()\ndef _make_tuple (x):\nifnot isinstance (x, ( list ,tuple )):\nreturn (x, x)\nreturn x\nimg_size, patch_size =_make_tuple(img_size), _make_tuple(patch_size)\nself .num_patches =(img_size[ 0]//patch_size[ 0])*(\nimg_size[ 1]//patch_size[ 1])\nself .conv =nn.LazyConv2d(num_hiddens, kernel_size =patch_size,\nstride =patch_size)\ndef forward (self , X):\n# Output shape: (batch size, no. of patches, no. of channels)\nreturn self .conv(X) .flatten( 2).transpose( 1,2)\nIn the following example, taking images with height and width of img_size as inputs, the\npatchembeddingoutputs (img_size//patch_size)**2 patchesthatarelinearlyprojected\nto vectors of length num_hiddens .\nimg_size, patch_size, num_hiddens, batch_size =96,16,512,4\npatch_emb =PatchEmbedding(img_size, patch_size, num_hiddens)\nX=torch .zeros(batch_size, 3, img_size, img_size)\nd2l.check_shape(patch_emb(X),\n(batch_size, (img_size //patch_size) **2, num_hiddens))\n11.8.3VisionTransformerEncoder\nTheMLPofthevisionTransformerencoderisslightlydifferentfromthepositionwiseFFN\noftheoriginalTransformerencoder(see Section11.7.2 ). First,heretheactivationfunction\nusestheGaussianerrorlinearunit(GELU),whichcanbeconsideredasasmootherversion\nof the ReLU ( Hendrycks and Gimpel, 2016 ). Second, dropout is applied to the output of\neach fully connected layer in the MLP for regularization.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "703b8dd6-de68-4326-bb9a-b6350b57d03d": {"__data__": {"id_": "703b8dd6-de68-4326-bb9a-b6350b57d03d", "embedding": null, "metadata": {"page_label": "454", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "045d5eb6-8866-4894-9a1b-58923350de99", "node_type": "4", "metadata": {"page_label": "454", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "075040ba3acea3dd36f4290f213cf7194533525c270f98cda24813439906d8ba", "class_name": "RelatedNodeInfo"}}, "text": "454 Attention Mechanisms and Transformers\nclass ViTMLP (nn.Module):\ndef __init__ (self , mlp_num_hiddens, mlp_num_outputs, dropout =0.5):\nsuper ().__init__ ()\nself .dense1 =nn.LazyLinear(mlp_num_hiddens)\nself .gelu =nn.GELU()\nself .dropout1 =nn.Dropout(dropout)\nself .dense2 =nn.LazyLinear(mlp_num_outputs)\nself .dropout2 =nn.Dropout(dropout)\ndef forward (self , x):\nreturn self .dropout2( self .dense2( self .dropout1( self .gelu(\nself .dense1(x)))))\nThe vision Transformer encoder block implementation just follows the pre-normalization\ndesign in Fig. 11.8.1 , where normalization is applied right beforemulti-head attention or\nthe MLP. In contrast to post-normalization (\u201cadd & norm\u201d in Fig. 11.7.1 ), where normal-\nizationisplacedright afterresidualconnections, pre-normalizationleadstomoreeffective\nor efficient training for Transformers ( Baevski and Auli, 2018 ,Wangetal., 2019,Xionget\nal., 2020).\nclass ViTBlock (nn.Module):\ndef __init__ (self , num_hiddens, norm_shape, mlp_num_hiddens,\nnum_heads, dropout, use_bias =False ):\nsuper ().__init__ ()\nself .ln1 =nn.LayerNorm(norm_shape)\nself .attention =d2l.MultiHeadAttention(num_hiddens, num_heads,\ndropout, use_bias)\nself .ln2 =nn.LayerNorm(norm_shape)\nself .mlp =ViTMLP(mlp_num_hiddens, num_hiddens, dropout)\ndef forward (self , X, valid_lens =None ):\nX=X+self .attention( *([self .ln1(X)] *3), valid_lens)\nreturn X+self .mlp( self .ln2(X))\nJustasin Section11.7.4 ,novisionTransformerencoderblockchangesitsinputshape.\nX=torch .ones(( 2,100,24))\nencoder_blk =ViTBlock( 24,24,48,8,0.5)\nencoder_blk .eval()\nd2l.check_shape(encoder_blk(X), X .shape)\n11.8.4PuttingIt All Together\nThe forward pass of vision Transformers below is straightforward. First, input images are\nfedintoan PatchEmbedding instance,whoseoutputisconcatenatedwiththe\u201c<cls>\u201dtoken\nembedding. Theyaresummedwithlearnablepositionalembeddingsbeforedropout. Then\nthe output is fed into the Transformer encoder that stacks num_blks instances of the ViT-\nBlockclass. Finally, the representation of the \u201c<cls>\u201d token is projected by the network\nhead.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e7c6a1c-80c8-4f58-8dbb-ef96e4d09b1b": {"__data__": {"id_": "0e7c6a1c-80c8-4f58-8dbb-ef96e4d09b1b", "embedding": null, "metadata": {"page_label": "455", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "017e18ec-3413-4de4-b398-f372140c8c70", "node_type": "4", "metadata": {"page_label": "455", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7641c967001e2f25e6c0c31595cf641aa0035c31bfd58d861618b7b5d0a1d4c8", "class_name": "RelatedNodeInfo"}}, "text": "455 Transformers for Vision\nclass ViT(d2l .Classifier):\n\"\"\"Vision Transformer.\"\"\"\ndef __init__ (self , img_size, patch_size, num_hiddens, mlp_num_hiddens,\nnum_heads, num_blks, emb_dropout, blk_dropout, lr =0.1,\nuse_bias =False , num_classes =10):\nsuper ().__init__ ()\nself .save_hyperparameters()\nself .patch_embedding =PatchEmbedding(\nimg_size, patch_size, num_hiddens)\nself .cls_token =nn.Parameter(torch .zeros( 1,1, num_hiddens))\nnum_steps =self .patch_embedding .num_patches +1# Add the cls token\n# Positional embeddings are learnable\nself .pos_embedding =nn.Parameter(\ntorch .randn( 1, num_steps, num_hiddens))\nself .dropout =nn.Dropout(emb_dropout)\nself .blks =nn.Sequential()\nfor iinrange (num_blks):\nself .blks .add_module( f\"{i}\", ViTBlock(\nnum_hiddens, num_hiddens, mlp_num_hiddens,\nnum_heads, blk_dropout, use_bias))\nself .head =nn.Sequential(nn .LayerNorm(num_hiddens),\nnn.Linear(num_hiddens, num_classes))\ndef forward (self , X):\nX=self .patch_embedding(X)\nX=torch .cat(( self .cls_token .expand(X .shape[ 0],-1,-1), X), 1)\nX=self .dropout(X +self .pos_embedding)\nfor blk inself .blks:\nX=blk(X)\nreturn self .head(X[:, 0])\n11.8.5Training\nTraining a vision Transformer on the Fashion-MNIST dataset is just like how CNNs were\ntrained in Chapter 8 .\nimg_size, patch_size =96,16\nnum_hiddens, mlp_num_hiddens, num_heads, num_blks =512,2048 ,8,2\nemb_dropout, blk_dropout, lr =0.1,0.1,0.1\nmodel =ViT(img_size, patch_size, num_hiddens, mlp_num_hiddens, num_heads,\nnum_blks, emb_dropout, blk_dropout, lr)\ntrainer =d2l.Trainer(max_epochs =10, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =128, resize =(img_size, img_size))\ntrainer .fit(model, data)\n11.8.6Summary and Discussion\nYoumayhavenoticed thatforsmall datasetslikeFashion-MNIST,ourimplementedvision\nTransformer does not outperform the ResNet in Section 8.6 . Similar observations can be\nmadeevenontheImageNetdataset(1.2millionimages). ThisisbecauseTransformers lack\nthose useful principles in convolution, such as translation invariance and locality ( Section\n7.1). However, the picture changes when training larger models on larger datasets (e.g.,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22c72c54-55f9-4422-b0fb-ecc686336282": {"__data__": {"id_": "22c72c54-55f9-4422-b0fb-ecc686336282", "embedding": null, "metadata": {"page_label": "456", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70b0aac5-bae7-416a-a552-31ae317f7130", "node_type": "4", "metadata": {"page_label": "456", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b456560b558cebcf69d4340bb336e60f2695e9f9395783fd589764eaa48e64f3", "class_name": "RelatedNodeInfo"}}, "text": "456 Attention Mechanisms and Transformers\n163300 million images), where vision Transformers outperform ResNets by a large margin\nin image classification, demonstrating intrinsic superiority of Transformers in scalability\n(Dosovitskiy et al., 2021). The introduction of vision Transformers has changed the land-\nscapeofnetworkdesignformodelingimagedata. Theyweresoonshowntobeeffectiveon\nthe ImageNet dataset with data-efficient training strategies of DeiT ( Touvronet al., 2021).\nHowever, the quadratic complexity of self-attention ( Section 11.6 ) makes the Transformer\narchitecture less suitable for higher-resolution images. Towards a general-purpose back-\nbone network in computer vision, Swin Transformers addressed the quadratic computa-\ntional complexity with respect to image size ( Section 11.6.2 ) and reinstated convolution-\nlike priors, extending the applicability of Transformers to a range of computer vision tasks\nbeyond image classification with state-of-the-art results ( Liuetal., 2021).\n11.8.7Exercises\n1.How does the value of img_size affect training time?\n2.Instead of projecting the \u201c<cls>\u201d token representation to the output, how would you\nprojecttheaveragedpatchrepresentations? Implementthischangeandseehowitaffects\nthe accuracy.\n3.Can you modify hyperparameters to improve the accuracy of the vision Transformer?\nDiscussions163.\n11.9Large-ScalePretrainingwith Transformers\nSo far in our image classification and machine translation experiments, models have been\ntrained on datasets with input\u2013output examples fromscratch to perform specific tasks. For\nexample, a Transformer was trained with English\u2013French pairs ( Section 11.7 ) so that this\nmodel can translate input English text into French. As a result, each model becomes a\nspecific expert that is sensitive to even a slight shift in data distribution ( Section 4.7 ). For\nbetter generalized models, or even more competent generalists that can perform multiple\ntasks with or without adaptation, pretraining models on large data has been increasingly\ncommon.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2037, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bdf9977-0fc6-43a7-aeef-8ca0f6fa08cc": {"__data__": {"id_": "1bdf9977-0fc6-43a7-aeef-8ca0f6fa08cc", "embedding": null, "metadata": {"page_label": "457", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "66b6c0f4-7420-44e9-a216-4ee86ed8772a", "node_type": "4", "metadata": {"page_label": "457", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "85acde8f5ff8757afee09e425765def839415ec65161bcb5a9080b4c724beb0a", "class_name": "RelatedNodeInfo"}}, "text": "457 Large-Scale Pretraining with Transformers\nGiven larger data for pretraining, the Transformer architecture performs better with an in-\ncreasedmodelsizeandtrainingcompute,demonstratingsuperior scalingbehavior. Specif-\nically, performance of Transformer-based language models scales as a power law with the\namount of model parameters, training tokens, and training compute ( Kaplanet al., 2020).\nThe scalability of Transformers is also evidenced by the significantly boosted performance\nfrom larger vision Transformers trained on larger data (discussed in Section 11.8 ). More\nrecent success stories include Gato, a generalist model that can play Atari, caption im-\nages, chat, and act as a robot ( Reedet al., 2022). Gato is a single Transformer that scales\nwell when pretrained on diverse modalities, including text, images, joint torques, and but-\nton presses. Notably, all such multimodal data is serialized into a flat sequence of tokens,\nwhich can be processed akin to text tokens ( Section 11.7 ) or image patches ( Section 11.8 )\nby Transformers.\nPrior to the compelling success of pretraining Transformers for multimodal data, Trans-\nformerswereextensivelypretrainedwithawealthoftext. Originallyproposedformachine\ntranslation,theTransformerarchitecturein Fig.11.7.1 consistsofanencoderforrepresent-\ninginputsequencesandadecoderforgeneratingtargetsequences. Primarily,Transformers\ncan be used in three different modes: encoder-only ,encoder\u2013decoder , anddecoder-only .\nTo conclude this chapter, we will review these three modes and explain the scalability in\npretraining Transformers.\n11.9.1Encoder-Only\nWhenonlytheTransformerencoderisused,asequenceofinputtokensisconvertedintothe\nsame number of representations that can be further projected into output (e.g., classifica-\ntion). ATransformerencoderconsistsofself-attentionlayers,whereallinputtokensattend\nto each other. For example, vision Transformers depicted in Fig. 11.8.1 are encoder-only,\nconverting a sequence of input image patches into the representation of a special \u201c<cls>\u201d\ntoken. Since this representation depends on all input tokens, it is further projected into\nclassification labels. This design was inspired by an earlier encoder-only Transformer pre-\ntrainedontext: BERT(BidirectionalEncoderRepresentationsfromTransformers)( Devlin\netal., 2018).\nPretrainingBERT\nBERT is pretrained on text sequences using masked language modeling : input text with\nrandomly masked tokens is fed into a Transformer encoder to predict the masked tokens.\nAs illustrated in Fig. 11.9.1 , an original text sequence \u201cI\u201d, \u201clove\u201d, \u201cthis\u201d, \u201cred\u201d, \u201ccar\u201d is\nprepended with the \u201c<cls>\u201d token, and the \u201c<mask>\u201d token randomly replaces \u201clove\u201d;\nthen the cross-entropy loss between the masked token \u201clove\u201d and its prediction is to be\nminimized during pretraining. Note that there is no constraint in the attention pattern of\nTransformer encoders (right of Fig. 11.9.1 ) so all tokens can attend to each other. Thus,\nprediction of \u201clove\u201d depends on input tokens before and after it in the sequence. This is\nwhyBERTis a \u201cbidirectional encoder\u201d. Withoutneed formanual labeling, large-scaletext\ndata from books and Wikipedia can be used for pretraining BERT.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91f51b77-4c89-4056-9cd1-f44e96c4f8a6": {"__data__": {"id_": "91f51b77-4c89-4056-9cd1-f44e96c4f8a6", "embedding": null, "metadata": {"page_label": "458", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ed33ea0-a67f-4b43-ba36-9245588c1218", "node_type": "4", "metadata": {"page_label": "458", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "42a92bb82b80fe408e9226b8e709261e374dbd249bf16ce10c04502b761129de", "class_name": "RelatedNodeInfo"}}, "text": "458 Attention Mechanisms and Transformers\ntFig. 11.9.1 Left: Pretraining BERT with masked language modeling. Prediction of the masked \u201clove\u201d\ntoken depends on all input tokens before and after \u201clove\u201d. Right: Attention pattern in the\nTransformer encoder. Each token along the vertical axis attends to all input tokens along\nthe horizontal axis.\nFine-TuningBERT\nThepretrainedBERTcanbe fine-tuned todownstreamencodingtasksinvolvingsingletext\nor text pairs. During fine-tuning, additional layers can be added to BERT with randomized\nparameters: these parameters and those pretrained BERT parameters will be updated to fit\ntraining data of downstream tasks.\ntFig. 11.9.2 Fine-tuning BERT for sentiment analysis.\nFig.11.9.2 illustratesfine-tuningofBERTforsentimentanalysis. TheTransformerencoder\nis a pretrained BERT, which takes a text sequence as input and feeds the \u201c<cls>\u201d represen-\ntation(globalrepresentationoftheinput)intoanadditionalfullyconnectedlayertopredict\nthe sentiment. During fine-tuning, the cross-entropy loss between the prediction and the\nlabel on sentiment analysis data is minimized via gradient-based algorithms, where the\nadditional layer is trained from scratch while pretrained parameters of BERT are updated.\nBERT does more than sentiment analysis. The general language representations learned\nby the 350-million-parameter BERT from 250 billion training tokens advanced the state of\ntheartfornaturallanguagetaskssuchassingletextclassification, textpairclassificationor\nregression, text tagging, and question answering.\nYoumaynotethatthesedownstreamtasksincludetextpairunderstanding. BERTpretrain-\ning has another loss for predicting whether one sentence immediately follows the other.\nHowever, this loss was later found to be less useful when pretraining RoBERTa, a BERT\nvariant of the same size, on 2000 billion tokens ( Liuet al., 2019). Other derivatives of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71b0951f-fe18-4862-bb2e-806388d3b5e5": {"__data__": {"id_": "71b0951f-fe18-4862-bb2e-806388d3b5e5", "embedding": null, "metadata": {"page_label": "459", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fa3ffb3e-e43d-4cf6-86da-90269b57abe2", "node_type": "4", "metadata": {"page_label": "459", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7978978f98aefdfff6d432c3f3367859f38a6e9484e9b91a68ef8903c2820bb7", "class_name": "RelatedNodeInfo"}}, "text": "459 Large-Scale Pretraining with Transformers\nBERT improved model architectures or pretraining objectives, such as ALBERT (enforc-\ningparametersharing)( Lanetal.,2019),SpanBERT(representingandpredictingspansof\ntext) (Joshiet al., 2020), DistilBERT (lightweight via knowledge distillation) ( Sanhet al.,\n2019),andELECTRA(replacedtokendetection)( Clarketal.,2020). Moreover,BERTin-\nspiredTransformerpretrainingincomputervision,suchaswithvisionTransformers( Doso-\nvitskiyetal.,2021),SwinTransformers( Liuetal.,2021),andMAE(maskedautoencoders)\n(Heetal., 2022).\n11.9.2Encoder\u2013Decoder\nSince a Transformer encoder converts a sequence of input tokens into the same number\nof output representations, the encoder-only mode cannot generate a sequence of arbitrary\nlengthasinmachinetranslation. Asoriginallyproposedformachinetranslation,theTrans-\nformer architecture can be outfitted with a decoder that autoregressively predicts the tar-\nget sequence of arbitrary length, token by token, conditional on both encoder output and\ndecoder output: (i) for conditioning on encoder output, encoder\u2013decoder cross-attention\n(multi-head attention of decoder in Fig. 11.7.1 ) allows target tokens to attend to allinput\ntokens; (ii) conditioning on decoder output is achieved by a so-called causalattention (this\nname is common in the literature but is misleading as it has little connection to the proper\nstudy of causality) pattern (masked multi-head attention of decoder in Fig. 11.7.1 ), where\nany target token can only attend to pastandpresenttokens in the target sequence.\nTopretrainencoder\u2013decoderTransformersbeyondhuman-labeledmachinetranslationdata,\nBART (Lewiset al., 2019) and T5 ( Raffelet al., 2020) are two concurrently proposed\nencoder\u2013decoder Transformers pretrained on large-scale text corpora. Both attempt to re-\nconstruct original text in their pretraining objectives, while the former emphasizes noising\ninput(e.g.,masking,deletion,permutation,androtation)andthelatterhighlightsmultitask\nunification with comprehensive ablation studies.\nPretrainingT5\nAs an example of the pretrained Transformer encoder\u2013decoder, T5 (Text-to-Text Transfer\nTransformer) unifies many tasks as the same text-to-text problem: for any task, the input\nof the encoder is a task description (e.g., \u201cSummarize\u201d, \u201c:\u201d) followed by task input (e.g.,\na sequence of tokens from an article), and the decoder predicts the task output (e.g., a\nsequenceoftokenssummarizingtheinputarticle). Toperformastext-to-text, T5istrained\nto generate some target text conditional on input text.\nTo obtain input and output from any original text, T5 is pretrained to predict consecu-\ntive spans. Specifically, tokens from text are randomly replaced by special tokens where\neach consecutive span is replaced by the same special token. Consider the example in Fig.\n11.9.3, where the original text is \u201cI\u201d, \u201clove\u201d, \u201cthis\u201d, \u201cred\u201d, \u201ccar\u201d. Tokens \u201clove\u201d, \u201cred\u201d,\n\u201ccar\u201d are randomly replaced by special tokens. Since \u201cred\u201d and \u201ccar\u201d are a consecutive\nspan, they are replaced by the same special token. As a result, the input sequence is \u201cI\u201d,\n\u201c<X>\u201d, \u201cthis\u201d, \u201c<Y>\u201d, and the target sequence is \u201c<X>\u201d, \u201clove\u201d, \u201c<Y>\u201d, \u201cred\u201d, \u201ccar\u201d,\n\u201c<Z>\u201d, where \u201c<Z>\u201d is another special token marking the end. As shown in Fig. 11.9.3 ,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3253, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f722215-431d-4bc4-bbdf-346c1150cadc": {"__data__": {"id_": "0f722215-431d-4bc4-bbdf-346c1150cadc", "embedding": null, "metadata": {"page_label": "460", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45f33713-b6aa-4935-a278-d1a8f94b7a86", "node_type": "4", "metadata": {"page_label": "460", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "82cab7fdf35d7599b5ef19b869f97bc4d963143a96f786007b14895223b82dcd", "class_name": "RelatedNodeInfo"}}, "text": "460 Attention Mechanisms and Transformers\ntFig. 11.9.3 Left: Pretraining T5 by predicting consecutive spans. The original sentence is \u201cI\u201d, \u201clove\u201d,\n\u201cthis\u201d, \u201cred\u201d, \u201ccar\u201d, where \u201clove\u201d is replaced by a special \u201c<X>\u201d token, and consecutive\n\u201cred\u201d, \u201ccar\u201d are replaced by a special \u201c<Y>\u201d token. The target sequence ends with a\nspecial \u201c<Z>\u201d token. Right: Attention pattern in the Transformer encoder\u2013decoder. In the\nencoder self-attention (lower square), all input tokens attend to each other; In the\nencoder\u2013decoder cross-attention (upper rectangle), each target token attends to all input\ntokens; In the decoder self-attention (upper triangle), each target token attends to present\nand past target tokens only (causal).\nthe decoder has a causal attention pattern to prevent itself from attending to future tokens\nduring sequence prediction.\nIn T5, predicting consecutive span is also referred to as reconstructing corrupted text.\nWith this objective, T5 is pretrained with 1000 billion tokens from the C4 (Colossal Clean\nCrawled Corpus) data, which consists of clean English text from the web ( Raffelet al.,\n2020).\nFine-TuningT5\nSimilartoBERT,T5needstobefine-tuned(updatingT5parameters)ontask-specifictrain-\ning data to perform this task. Major differences from BERT fine-tuning include: (i) T5\ninput includes task descriptions; (ii) T5 can generate sequences with arbitrary length with\nits Transformer decoder; (iii) No additional layers are required.\nFig. 11.9.4 explains fine-tuning T5 using text summarization as an example. In this down-\nstream task, the task description tokens \u201cSummarize\u201d, \u201c:\u201d followed by the article tokens\nare input to the encoder.\nAfterfine-tuning,the11-billion-parameterT5(T5-11B)achievedstate-of-the-artresultson\nmultiple encoding (e.g., classification) and generation (e.g., summarization) benchmarks.\nSince released, T5 has been extensively used in later research. For example, switch Trans-\nformersaredesignedbasedonT5toactivateasubsetoftheparametersforbettercomputa-\ntional efficiency ( Fedusetal., 2022). In a text-to-image model called Imagen, text is input\nto a frozen T5 encoder (T5-XXL) with 4.6 billion parameters ( Sahariaet al., 2022). The", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2176, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca60b4d9-d605-4e42-a7b8-cd7f52694dd7": {"__data__": {"id_": "ca60b4d9-d605-4e42-a7b8-cd7f52694dd7", "embedding": null, "metadata": {"page_label": "461", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "468dbc05-4ddf-4c75-a967-3fd1d3c4a3b7", "node_type": "4", "metadata": {"page_label": "461", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "33e2eab26a0b822e303705f119ddf2f5f4553275485a6440427cd9230ff79179", "class_name": "RelatedNodeInfo"}}, "text": "461 Large-Scale Pretraining with Transformers\ntFig. 11.9.4 Fine-tuning T5 for text summarization. Both the task description and article tokens are\nfed into the Transformer encoder for predicting the summary.\nphotorealistic text-to-image examples in Fig. 11.9.5 suggest that the T5 encoder alone may\neffectively represent text even without fine-tuning.\ntFig. 11.9.5 Text-to-image examples by the Imagen model, whose text encoder is from T5 (\ufb01gures\ntaken from Saharia et al. ( 2022 )).\n11.9.3Decoder-Only\nWehavereviewedencoder-onlyandencoder\u2013decoderTransformers. Alternatively,decoder-\nonly Transformers remove the entire encoder and the decoder sublayer with the encoder\u2013\ndecoder cross-attention from the original encoder\u2013decoder architecture depicted in Fig.\n11.7.1. Nowadays, decoder-only Transformers have been the defacto architecture in large-\nscalelanguagemodeling( Section9.3 ),whichleveragestheworld\u2019sabundantunlabeledtext\ncorpora via self-supervised learning.\nGPT and GPT-2\nUsinglanguagemodelingasthetrainingobjective,theGPT(generativepre-training)model\nchooses a Transformer decoder as its backbone ( Radfordetal., 2018).\nFollowing the autoregressive language model training as described in Section 9.3.3 ,Fig.\n11.9.6illustratesGPTpretrainingwithaTransformerencoder,wherethetargetsequenceis\nthe input sequence shifted by one token. Note that the attention pattern in the Transformer", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9468516-50a6-4005-913a-2b8cf80b51c6": {"__data__": {"id_": "a9468516-50a6-4005-913a-2b8cf80b51c6", "embedding": null, "metadata": {"page_label": "462", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17ed5d61-6115-4619-ad62-d83f972d4364", "node_type": "4", "metadata": {"page_label": "462", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9c6ba6f27fecab8807f65a8f0ce2c2cf6b5a96da14537904d731cb4f1d1315b8", "class_name": "RelatedNodeInfo"}}, "text": "462 Attention Mechanisms and Transformers\ntFig. 11.9.6 Left: Pretraining GPT with language modeling. The target sequence is the input sequence\nshifted by one token. Both \u201c<bos>\u201d and \u201c<eos>\u201d are special tokens marking the\nbeginning and end of sequences, respectively. Right: Attention pattern in the Transformer\ndecoder. Each token along the vertical axis attends to only its past tokens along the\nhorizontal axis (causal).\ndecoder enforces that each token can only attend to its past tokens (future tokens cannot be\nattended to because they have not yet been chosen).\nGPT has 100 million parameters and needs to be fine-tuned for individual downstream\ntasks. A much larger Transformer-decoder language model, GPT-2, was introduced one\nyear later ( Radfordetal., 2019). Compared with the original Transformer decoder in GPT,\npre-normalization (discussed in Section 11.8.3 ) and improved initialization and weight-\nscalingwereadoptedinGPT-2. Pretrainedon40GBoftext,the1.5-billion-parameterGPT-\n2 obtained the state-of-the-art results on language modeling benchmarks and promising\nresults on multiple other tasks withoutupdating theparametersor architecture .\nGPT-3and Beyond\nGPT-2demonstratedpotentialofusingthesamelanguagemodelformultipletaskswithout\nupdatingthemodel. Thisismorecomputationallyefficientthanfine-tuning,whichrequires\nmodel updates via gradient computation.\nBefore explaining the more computationally efficient use of language models without pa-\nrameter update, recall Section 9.5 that a language model can be trained to generate a text\nsequenceconditionalonsomeprefixtextsequence. Thus,apretrainedlanguagemodelmay\ngenerate the task output as a sequence without parameter update , conditional on an input\nsequencewiththetaskdescription,task-specificinput\u2013outputexamples,andaprompt(task\ninput). This learning paradigm is called in-context learning (Brownet al., 2020), which\ncan be further categorized into zero-shot ,one-shot , andfew-shot , when there is no, one,\nand a few task-specific input\u2013output examples ( Fig. 11.9.7 ).\nThese three settings were tested in GPT-3 ( Brownetal., 2020), whose largest version uses\ndata and model size about two orders of magnitude larger than those in GPT-2. GPT-3\nusesthesameTransformerdecoderarchitectureasitsdirectpredecessorGPT-2exceptthat\nattention patterns (at the right in Fig. 11.9.6 ) are sparser at alternating layers. Pretrained", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd92cd31-278a-49a1-a353-fd997a042d06": {"__data__": {"id_": "bd92cd31-278a-49a1-a353-fd997a042d06", "embedding": null, "metadata": {"page_label": "463", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d478bcf-346a-4f4c-befd-66ccde15f541", "node_type": "4", "metadata": {"page_label": "463", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "470ea01bb19552c7d01c15fc1cac192979d99463ba0b3145647408f67ffd181c", "class_name": "RelatedNodeInfo"}}, "text": "463 Large-Scale Pretraining with Transformers\ntFig. 11.9.7 Zero-shot, one-shot, few-shot in-context learning with language models (Transformer\ndecoders). No parameter update is needed.\ntFig. 11.9.8 Aggregate performance of GPT-3 for all 42 accuracy-denominated benchmarks (caption\nadapted and \ufb01gure taken from Brown et al. ( 2020 )).\nwith 300 billion tokens, GPT-3 performs better with larger model size, where few-shot\nperformance increases most rapidly ( Fig. 11.9.8 ).\nThe subsequent GPT-4 model did not fully disclose technical details in its report ( OpenAI,\n2023). By contrast with its predecessors, GPT-4 is a large-scale, multimodal model that\ncan take both text and images as input and generate text output.\n11.9.4Scalability\nFig. 11.9.8 empirically demonstrates scalability of Transformers in the GPT-3 language\nmodel. For language modeling, more comprehensive empirical studies on the scalability", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3bfc543f-3e0d-4d8c-aa01-9bcb84f54407": {"__data__": {"id_": "3bfc543f-3e0d-4d8c-aa01-9bcb84f54407", "embedding": null, "metadata": {"page_label": "464", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "332edaba-6fc6-45f0-97cd-58fab188a06d", "node_type": "4", "metadata": {"page_label": "464", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "39c7bf034c8b28abc87eea50ede258592e3b671a8a0d64791cf5deae4914da72", "class_name": "RelatedNodeInfo"}}, "text": "464 Attention Mechanisms and Transformers\nof Transformers have led researchers to see promise in training larger Transformers with\nmore data and compute ( Kaplanetal., 2020).\ntFig. 11.9.9 Transformer language model performance improves smoothly as we increase the model\nsize, dataset size, and amount of compute used for training. For optimal performance all\nthree factors must be scaled up in tandem. Empirical performance has a power-law\nrelationship with each individual factor when not bottlenecked by the other two (caption\nadapted and \ufb01gure taken from Kaplan et al. ( 2020 )).\nAs shown in Fig. 11.9.9 ,power-law scaling can be observed in the performance with re-\nspect to the model size (number of parameters, excluding embedding layers), dataset size\n(numberoftrainingtokens), andamountoftrainingcompute(PetaFLOP/s-days, excluding\nembedding layers). In general, increasing all these three factors in tandem leads to better\nperformance. However, howto increase them in tandem still remains a matter of debate\n(Hoffmann etal., 2022).\ntFig. 11.9.10 Transformer language model training runs (\ufb01gure taken from Kaplan et al. ( 2020 )).\nAs well as increased performance, large models also enjoy better sample efficiency than\nsmall models. Fig. 11.9.10 shows that large models need fewer training samples (tokens\nprocessed) to perform at the same level achieved by small models, and performance is\nscaled smoothly with compute.\nThe empirical scaling behaviors in Kaplan et al.(2020) have been tested in subsequent\nlarge Transformer models. For example, GPT-3 supported this hypothesis with two more\norders of magnitude in Fig. 11.9.11 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b00b6d6-4faf-4e86-ad4e-543a67605569": {"__data__": {"id_": "6b00b6d6-4faf-4e86-ad4e-543a67605569", "embedding": null, "metadata": {"page_label": "465", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38e8a702-13cb-4772-b4dc-c87cf9faed3f", "node_type": "4", "metadata": {"page_label": "465", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "eec2a187b64f12552c29f10c7b2926c5705349a608724b3095ca812d4bcb23de", "class_name": "RelatedNodeInfo"}}, "text": "465 Large-Scale Pretraining with Transformers\ntFig. 11.9.11 GPT-3 performance (cross-entropy validation loss) follows a power-law trend with the\namount of compute used for training. The power-law behavior observed in Kaplan et al.\n(2020 ) continues for an additional two orders of magnitude with only small deviations\nfrom the predicted curve. Embedding parameters are excluded from compute and\nparameter counts (caption adapted and \ufb01gure taken from Brown et al. ( 2020 )).\n11.9.5LargeLanguageModels\nThe scalability of Transformers in the GPT series has inspired subsequent large language\nmodels. The GPT-2 Transformer decoder was used for training the 530-billion-parameter\nMegatron-Turing NLG ( Smithet al., 2022) with 270 billion training tokens. Following\ntheGPT-2design, the280-billion-parameterGopher( Raeetal., 2021)pretrainedwith300\nbillion tokens, performed competitively across diverse tasks. Inheriting the same architec-\nture and using the same compute budget of Gopher, Chinchilla ( Hoffmann et al., 2022)\nis a substantially smaller (70 billion parameters) model that trains for much longer (1.4\ntrillion training tokens), outperforming Gopher on many tasks and with more emphasis on\nthe number of tokens than on the number of parameters. To continue the scaling line of\nlanguage modeling, PaLM (Pathway Language Model) ( Chowdhery et al., 2022), a 540-\nbillion-parameter Transformer decoder with modified designs pretrained on 780 billion to-\nkens,outperformedaveragehumanperformanceontheBIG-Benchbenchmark( Srivastava\netal.,2022). Itslaterversion,PaLM2( Aniletal.,2023),scaleddataandmodelroughly1:1\nand improved multilingual and reasoning capabilities. Other large language models, such\nas Minerva ( Lewkowycz et al., 2022) that further trains a generalist (PaLM) and Galac-\ntica (Tayloret al., 2022) that is not trained on a general corpus, have shown promising\nquantitative and scientific reasoning capabilities.\nOpen-sourced releases, such as OPT (Open Pretrained Transformers) ( Zhangetal., 2022),\nBLOOM ( Scaoet al., 2022), and FALCON ( Penedoet al., 2023), democratized research\nand use of large language models. Focusing on computational efficiency at inference time,\nthe open-sourced Llama 1 ( Touvronet al., 2023a) outperformed much larger models by\ntraining on more tokens than had been typically used. The updated Llama 2 ( Touvronet\nal.,2023b)furtherincreasedthepretrainingcorpusby40%,leadingtoproductmodelsthat\nmay match the performance of competitive close-sourced models.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d495a40-0072-475f-9582-46f13a42f52e": {"__data__": {"id_": "0d495a40-0072-475f-9582-46f13a42f52e", "embedding": null, "metadata": {"page_label": "466", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc49a2ad-5616-4ea4-97de-2711e179ac7c", "node_type": "4", "metadata": {"page_label": "466", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5acf0ebd8ceabdb0e71f62106f0c8e75c71b309c40730016d9f0b86a65c2abeb", "class_name": "RelatedNodeInfo"}}, "text": "466 Attention Mechanisms and Transformers\n164Weietal.(2022) discussed emergent abilities of large language models that are present in\nlarger models, but not in smaller models. However, simply increasing model size does not\ninherently make models follow human instructions better. Sanh et al.(2021), Weiet al.\n(2021) have found that fine-tuning large language models on a range of datasets described\nviainstructions canimprovezero-shotperformanceonheld-outtasks. Using reinforcement\nlearningfromhumanfeedback , Ouyang etal.(2022) fine-tuned GPT-3 to follow a diverse\nsetofinstructions. FollowingtheresultantInstructGPTwhichalignslanguagemodelswith\nhuman intent via fine-tuning ( Ouyanget al., 2022),ChatGPT164can generate human-like\nresponses (e.g., code debugging and creative writing) based on conversations with humans\nandcanperformmanynaturallanguageprocessingtaskszero-shot( Qinetal.,2023). Baiet\nal.(2022)replacedhumaninputs(e.g.,human-labeleddata)withmodeloutputstopartially\nautomate the instruction tuning process, which is also known as reinforcement learning\nfromAI feedback .\nLargelanguagemodelsofferanexcitingprospectofformulatingtextinputtoinducemodels\nto perform desired tasks via in-context learning, which is also known as prompting . No-\ntably,chain-of-thought prompting (Weiet al., 2022), an in-context learning method with\nfew-shot \u201cquestion, intermediate reasoning steps, answer\u201d demonstrations, elicits the com-\nplex reasoning capabilities of large language models in order to solve mathematical, com-\nmonsense, and symbolic reasoning tasks. Sampling multiple reasoning paths ( Wangetal.,\n2023), diversifying few-shot demonstrations ( Zhanget al., 2023), and reducing complex\nproblems to sub-problems ( Zhouet al., 2023) can all improve the reasoning accuracy. In\nfact, with simple prompts like \u201cLet\u2019s think step by step\u201d just before each answer, large lan-\nguagemodelscanevenperform zero-shot chain-of-thoughtreasoningwithdecentaccuracy\n(Kojimaet al., 2022). Even for multimodal inputs consisting of both text and images, lan-\nguage models can perform multimodal chain-of-thought reasoning with higher accuracy\nthan using text input only ( Zhangetal., 2023).\n11.9.6Summary and Discussion\nTransformers have been pretrained as encoder-only (e.g., BERT), encoder\u2013decoder (e.g.,\nT5), and decoder-only (e.g., GPT series). Pretrained models may be adapted to perform\ndifferent tasks with model update (e.g., fine-tuning) or not (e.g., few-shot). Scalability of\nTransformers suggests that better performance benefits from larger models, more training\ndata, and more training compute. Since Transformers were first designed and pretrained\nfor text data, this section leans slightly towards natural language processing. Nonetheless,\nthose models discussed above can be often found in more recent models across multiple\nmodalities. For example, (i) Chinchilla ( Hoffmann et al., 2022) was further extended to\nFlamingo ( Alayracetal., 2022), a visual language model for few-shot learning; (ii) GPT-2\n(Radfordet al., 2019) and the vision Transformer encode text and images in CLIP (Con-\ntrastive Language-Image Pre-training) ( Radfordet al., 2021), whose image and text em-\nbeddings were later adopted in the DALL-E 2 text-to-image system ( Rameshetal., 2022).\nAlthough there have been no systematic studies on Transformer scalability in multimodal\npretrainingyet,anall-Transformertext-to-imagemodelcalledParti( Yuetal.,2022)shows\npotential of scalability across modalities: a larger Parti is more capable of high-fidelity\nimage generation and content-rich text understanding ( Fig. 11.9.12 ).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3602, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05933c88-bea1-4e6b-aab3-da0a4b47651a": {"__data__": {"id_": "05933c88-bea1-4e6b-aab3-da0a4b47651a", "embedding": null, "metadata": {"page_label": "467", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e757524d-4f28-45b0-9838-4cca885d8c50", "node_type": "4", "metadata": {"page_label": "467", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "363840265af16199dc228636ce0e631740094180f4d4cbc2bc7e42822777ea7a", "class_name": "RelatedNodeInfo"}}, "text": "467 Large-Scale Pretraining with Transformers\ntFig. 11.9.12 Image examples generated from the same text by the Parti model of increasing sizes\n(350M, 750M, 3B, 20B) (examples taken from Yu et al. ( 2022 )).\n16511.9.7Exercises\n1.Is it possible to fine-tune T5 using a minibatch consisting of different tasks? Why or\nwhy not? How about for GPT-2?\n2.Given a powerful language model, what applications can you think of?\n3.Say that you are asked to fine-tune a language model to perform text classification by\nadding additional layers. Where will you add them? Why?\n4.Consider sequence-to-sequence problems (e.g., machine translation) where the input\nsequence is always available throughout the target sequence prediction. What could be\nlimitations of modeling with decoder-only Transformers? Why?\nDiscussions165.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 808, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c14763a-b543-4dac-8aee-def227174b8b": {"__data__": {"id_": "2c14763a-b543-4dac-8aee-def227174b8b", "embedding": null, "metadata": {"page_label": "468", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ceb9deb8-479f-476b-ab84-1b940b71e966", "node_type": "4", "metadata": {"page_label": "468", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "23687d03d6688b0630874cc94b3ce341b9521a02f5443cbf532cd2c11622312a", "class_name": "RelatedNodeInfo"}}, "text": "12 Optimization Algorithms\nIfyoureadthebookinsequenceuptothispointyoualreadyusedanumberofoptimization\nalgorithms to train deep learning models. They were the tools that allowed us to continue\nupdating model parameters and to minimize the value of the loss function, as evaluated on\nthe training set. Indeed, anyone content with treating optimization as a black box device\nto minimize objective functions in a simple setting might well content oneself with the\nknowledge that there exists an array of incantations of such a procedure (with names such\nas \u201cSGD\u201d and \u201cAdam\u201d).\nTo do well, however, some deeper knowledge is required. Optimization algorithms are\nimportant for deep learning. On the one hand, training a complex deep learning model can\ntake hours, days, or even weeks. The performance of the optimization algorithm directly\naffects the model\u2019s training efficiency. On the other hand, understanding the principles\nof different optimization algorithms and the role of their hyperparameters will enable us to\ntunethehyperparametersinatargetedmannertoimprovetheperformanceofdeeplearning\nmodels.\nIn this chapter, we explore common deep learning optimization algorithms in depth. Al-\nmost all optimization problems arising in deep learning are nonconvex . Nonetheless, the\ndesignandanalysisofalgorithmsinthecontextof convexproblemshaveproventobevery\ninstructive. It is for that reason that this chapter includes a primer on convex optimization\nand the proof fora very simple stochastic gradient descent algorithm on a convexobjective\nfunction.\n12.1Optimizationand Deep Learning\nIn this section, we will discuss the relationship between optimization and deep learning as\nwellasthechallengesofusingoptimizationindeeplearning. Foradeeplearningproblem,\nwe will usually define a loss function first. Once we have the loss function, we can use an\noptimization algorithm in attempt to minimize the loss. In optimization, a loss function is\noftenreferredtoasthe objectivefunction oftheoptimizationproblem. Bytraditionandcon-\nvention most optimization algorithms are concerned with minimization . If we ever need to\nmaximize an objective there is a simple solution: just flip the sign on the objective.\n468", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2200, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1c6ac30-05cd-47b0-af37-b4a5dbc9a137": {"__data__": {"id_": "c1c6ac30-05cd-47b0-af37-b4a5dbc9a137", "embedding": null, "metadata": {"page_label": "469", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b57089e-e78f-464a-8192-4bb3fcec53ab", "node_type": "4", "metadata": {"page_label": "469", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b290c0883ca99427a7bce5f1adaeb69a7bb09b205f896e7ec53bdbf1bafb8dc2", "class_name": "RelatedNodeInfo"}}, "text": "469 Optimization and Deep Learning\n12.1.1Goal of Optimization\nAlthough optimization provides a way to minimize the loss function for deep learning,\nin essence, the goals of optimization and deep learning are fundamentally different. The\nformerisprimarilyconcernedwithminimizinganobjectivewhereasthelatterisconcerned\nwithfindingasuitablemodel,givenafiniteamountofdata. In Section3.6 ,wediscussedthe\ndifferencebetweenthesetwogoalsindetail. Forinstance,trainingerrorandgeneralization\nerrorgenerallydiffer: sincetheobjectivefunctionoftheoptimizationalgorithmisusuallya\nlossfunctionbasedonthetrainingdataset,thegoalofoptimizationistoreducethetraining\nerror. However,thegoalofdeeplearning(ormorebroadly,statisticalinference)istoreduce\nthe generalization error. To accomplish the latter we need to pay attention to overfitting in\naddition to using the optimization algorithm to reduce the training error.\n%matplotlib inline\nimport numpy asnp\nimport torch\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch asd2l\nTo illustrate the aforementioned different goals, let\u2019s consider the empirical risk and the\nrisk. As described in Section 4.7.3 , the empirical risk is an average loss on the training\ndatasetwhiletheriskistheexpectedlossontheentirepopulationofdata. Belowwedefine\ntwo functions: the risk function fand the empirical risk function g. Suppose that we have\nonly a finite amount of training data. As a result, here gis less smooth than f.\ndef f(x):\nreturn x*torch .cos(np .pi*x)\ndef g(x):\nreturn f(x) +0.2 *torch .cos( 5*np.pi*x)\nThe graph below illustrates that the minimum of the empirical risk on a training dataset\nmay be at a different location from the minimum of the risk (generalization error).\ndef annotate (text, xy, xytext): #@save\nd2l.plt.gca() .annotate(text, xy =xy, xytext =xytext,\narrowprops =dict (arrowstyle ='->'))\nx=torch .arange( 0.5,1.5,0.01 )\nd2l.set_figsize(( 4.5,2.5))\nd2l.plot(x, [f(x), g(x)], 'x','risk ')\nannotate( 'min of \\nempirical risk ', (1.0,-1.2), ( 0.5,-1.1))\nannotate( 'min of risk ', (1.1,-1.05 ), ( 0.95 ,-0.5))\n12.1.2OptimizationChallengesin Deep Learning\nIn this chapter, we are going to focus specifically on the performance of optimization algo-\nrithms in minimizing the objective function, rather than a model\u2019s generalization error. In\nSection 3.1 we distinguished between analytical solutions and numerical solutions in opti-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7171afe9-2651-4e5f-8670-ac1d6f7f1227": {"__data__": {"id_": "7171afe9-2651-4e5f-8670-ac1d6f7f1227", "embedding": null, "metadata": {"page_label": "470", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0fa3631d-408b-4c3e-b940-357df38437ef", "node_type": "4", "metadata": {"page_label": "470", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8a7cdac76912eb0f5c91a70568967361b1adc31058b22313df82aa40f3b8ee51", "class_name": "RelatedNodeInfo"}}, "text": "470 Optimization Algorithms\nmization problems. In deep learning, most objective functions are complicated and do not\nhave analytical solutions. Instead, we must use numerical optimization algorithms. The\noptimization algorithms in this chapter all fall into this category.\nThere are many challenges in deep learning optimization. Some of the most vexing ones\narelocalminima, saddlepoints, andvanishinggradients. Let\u2019shavealookatthem.\nLocalMinima\nFor any objective function \ud835\udc53\u00b9\ud835\udc65\u00ba, if the value of \ud835\udc53\u00b9\ud835\udc65\u00baat\ud835\udc65is smaller than the values of \ud835\udc53\u00b9\ud835\udc65\u00ba\nat any other points in the vicinity of \ud835\udc65, then\ud835\udc53\u00b9\ud835\udc65\u00bacould be a local minimum. If the value\nof\ud835\udc53\u00b9\ud835\udc65\u00baat\ud835\udc65is the minimum of the objective function over the entire domain, then \ud835\udc53\u00b9\ud835\udc65\u00bais\nthe global minimum.\nFor example, given the function\n\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc65\u0001cos\u00b9\ud835\udf0b\ud835\udc65\u00bafor\u00001.0\u0014\ud835\udc65\u00142.0, (12.1.1)\nwe can approximate the local minimum and global minimum of this function.\nx=torch .arange( -1.0,2.0,0.01 )\nd2l.plot(x, [f(x), ], 'x','f(x) ')\nannotate( 'local minimum ', (-0.3,-0.25 ), ( -0.77 ,-1.0))\nannotate( 'global minimum ', (1.1,-0.95 ), ( 0.6,0.8))\nThe objective function of deep learning models usually has many local optima. When the\nnumerical solution of an optimization problem is near the local optimum, the numerical", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1231, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e356fba-b60d-421d-ac4b-e161495ad5f5": {"__data__": {"id_": "6e356fba-b60d-421d-ac4b-e161495ad5f5", "embedding": null, "metadata": {"page_label": "471", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac183fc0-5bad-410d-9e2e-3a168f217883", "node_type": "4", "metadata": {"page_label": "471", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "576cbf5ee7ddb01f171640a37966839b33be94325b98371aa96c78bafd3d7134", "class_name": "RelatedNodeInfo"}}, "text": "471 Optimization and Deep Learning\nsolution obtained by the final iteration may only minimize the objective function locally,\nrather than globally, as the gradient of the objective function\u2019s solutions approaches or\nbecomes zero. Only some degree of noise might knock the parameter out of the local\nminimum. In fact, this is one of the beneficial properties of minibatch stochastic gradient\ndescent where the natural variation of gradients over minibatches is able to dislodge the\nparameters from local minima.\nSaddle Points\nBesides local minima, saddle points are another reason for gradients to vanish. A saddle\npointis any location where all gradients of a function vanish but which is neither a global\nnor a local minimum. Consider the function \ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc653. Its first and second derivative van-\nishfor\ud835\udc65=0. Optimizationmightstallatthispoint,eventhoughitisnotaminimum.\nx=torch .arange( -2.0,2.0,0.01 )\nd2l.plot(x, [x **3],'x','f(x) ')\nannotate( 'saddle point ', (0,-0.2), ( -0.52 ,-5.0))\nSaddle points in higher dimensions are even more insidious, as the example below shows.\nConsiderthefunction \ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba=\ud835\udc652\u0000\ud835\udc662. Ithasitssaddlepointat \u00b90,0\u00ba. Thisisamaximum\nwith respect to \ud835\udc66and a minimum with respect to \ud835\udc65. Moreover, it lookslike a saddle, which\nis where this mathematical property got its name.\nx, y =torch .meshgrid(\ntorch .linspace( -1.0,1.0,101), torch .linspace( -1.0,1.0,101))\nz=x**2-y**2\nax=d2l.plt.figure() .add_subplot( 111, projection ='3d')\nax.plot_wireframe(x, y, z, **{'rstride ':10,'cstride ':10})\nax.plot([ 0], [ 0], [ 0],'rx')\nticks =[-1,0,1]\nd2l.plt.xticks(ticks)\nd2l.plt.yticks(ticks)\nax.set_zticks(ticks)\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'y');\nWe assume that the input of a function is a \ud835\udc58-dimensional vector and its output is a scalar,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89270b80-2923-4580-ba20-afb22330f146": {"__data__": {"id_": "89270b80-2923-4580-ba20-afb22330f146", "embedding": null, "metadata": {"page_label": "472", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e2ec2e5-1475-4ce7-af5c-cf243f8e56b3", "node_type": "4", "metadata": {"page_label": "472", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "083eb8f42f8f3dd423c51f51bcb842e5428a118958d0d63bba086858c1ecd74e", "class_name": "RelatedNodeInfo"}}, "text": "472 Optimization Algorithms\nso its Hessian matrix will have \ud835\udc58eigenvalues. The solution of the function could be a local\nminimum, a local maximum, or a saddle point at a position where the function gradient is\nzero:\n\u000fWhen the eigenvalues of the function\u2019s Hessian matrix at the zero-gradient position are\nall positive, we have a local minimum for the function.\n\u000fWhen the eigenvalues of the function\u2019s Hessian matrix at the zero-gradient position are\nall negative, we have a local maximum for the function.\n\u000fWhen the eigenvalues of the function\u2019s Hessian matrix at the zero-gradient position are\nnegative and positive, we have a saddle point for the function.\nForhigh-dimensionalproblemsthelikelihoodthatatleast someoftheeigenvaluesareneg-\nativeisquitehigh. Thismakessaddlepointsmorelikelythanlocalminima. Wewilldiscuss\nsome exceptions to this situation in the next section when introducing convexity. In short,\nconvex functions are those where the eigenvalues of the Hessian are never negative. Sadly,\nthough,mostdeeplearningproblemsdonotfallintothiscategory. Nonethelessitisagreat\ntool to study optimization algorithms.\nVanishingGradients\nProbably the most insidious problem to encounter is the vanishing gradient. Recall our\ncommonly-used activation functions and their derivatives in Section 5.1.2 . For instance,\nassumethatwewanttominimizethefunction \ud835\udc53\u00b9\ud835\udc65\u00ba=tanh\u00b9\ud835\udc65\u00baandwehappentogetstarted\nat\ud835\udc65=4. As we can see, the gradient of \ud835\udc53is close to nil. More specifically, \ud835\udc530\u00b9\ud835\udc65\u00ba=\n1\u0000tanh2\u00b9\ud835\udc65\u00baand thus\ud835\udc530\u00b94\u00ba=0.0013. Consequently, optimization will get stuck for a\nlong time before we make progress. This turns out to be one of the reasons that training\ndeep learning models was quite tricky prior to the introduction of the ReLU activation\nfunction.\nx=torch .arange( -2.0,5.0,0.01 )\nd2l.plot(x, [torch .tanh(x)], 'x','f(x) ')\nannotate( 'vanishing gradient ', (4,1), ( 2,0.0))\nAs we saw, optimization for deep learning is full of challenges. Fortunately there exists a\nrobust range of algorithms that perform well and that are easy to use even for beginners.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2045, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8f6d3c1-772f-449d-ac29-edc3f2f8aaf9": {"__data__": {"id_": "e8f6d3c1-772f-449d-ac29-edc3f2f8aaf9", "embedding": null, "metadata": {"page_label": "473", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de96f9f4-c76c-49fd-bb9b-9889bf521be9", "node_type": "4", "metadata": {"page_label": "473", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bca2237d968d778fd49363db06ab2714efa37ebfa96d28d364944229f25ee1cc", "class_name": "RelatedNodeInfo"}}, "text": "473 Optimization and Deep Learning\n166Furthermore, it is not really necessary to find thebest solution. Local optima or even ap-\nproximate solutions thereof are still very useful.\n12.1.3Summary\n\u000fMinimizing the training error does notguarantee that we find the best set of parameters\nto minimize the generalization error.\n\u000fThe optimization problems may have many local minima.\n\u000fTheproblemmayhaveevenmoresaddlepoints,asgenerallytheproblemsarenotconvex.\n\u000fVanishing gradients can cause optimization to stall. Often a reparametrization of the\nproblem helps. Good initialization of the parameters can be beneficial, too.\n12.1.4Exercises\n1.Consider a simple MLP with a single hidden layer of, say, \ud835\udc51dimensions in the hid-\nden layer and a single output. Show that for any local minimum there are at least \ud835\udc51!\nequivalent solutions that behave identically.\n2.Assume that we have a symmetric random matrix Mwhere the entries \ud835\udc40\ud835\udc56\ud835\udc57=\ud835\udc40\ud835\udc57\ud835\udc56are\neach drawn from some probability distribution \ud835\udc5d\ud835\udc56\ud835\udc57. Furthermore assume that \ud835\udc5d\ud835\udc56\ud835\udc57\u00b9\ud835\udc65\u00ba=\n\ud835\udc5d\ud835\udc56\ud835\udc57\u00b9\u0000\ud835\udc65\u00ba, i.e., that the distribution is symmetric (see e.g., Wigner ( 1958) for details).\n1.Provethatthedistributionovereigenvaluesisalsosymmetric. Thatis,foranyeigen-\nvector vthe probability that the associated eigenvalue \ud835\udf06satisfies\ud835\udc43\u00b9\ud835\udf06> 0\u00ba=\ud835\udc43\u00b9\ud835\udf06<\n0\u00ba.\n2.Why does the above notimply\ud835\udc43\u00b9\ud835\udf06> 0\u00ba=0.5?\n3.What other challenges involved in deep learning optimization can you think of?\n4.Assume that you want to balance a (real) ball on a (real) saddle.\n1.Why is this hard?\n2.Can you exploit this effect also for optimization algorithms?\nDiscussions166.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1547, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c78a67d3-9479-4494-b5e6-a86abf599687": {"__data__": {"id_": "c78a67d3-9479-4494-b5e6-a86abf599687", "embedding": null, "metadata": {"page_label": "474", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd6e8b1f-a414-49bc-bf58-61f9ef93468b", "node_type": "4", "metadata": {"page_label": "474", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6e7f8fc66eae2a9828acf50b098d6c31fc562250257ff08e54d038f0c86660fd", "class_name": "RelatedNodeInfo"}}, "text": "474 Optimization Algorithms\n12.2Convexity\nConvexity plays a vital role in the design of optimization algorithms. This is largely due\nto the fact that it is much easier to analyze and test algorithms in such a context. In other\nwords, if the algorithm performs poorly even in the convex setting, typically we should not\nhopetoseegreatresultsotherwise. Furthermore,eventhoughtheoptimizationproblemsin\ndeep learning are generally nonconvex, they often exhibit some properties of convex ones\nnear local minima. This can lead to exciting new optimization variants such as ( Izmailov\netal., 2018).\n%matplotlib inline\nimport numpy asnp\nimport torch\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch asd2l\n12.2.1Definitions\nBefore convex analysis, we need to define convex sets andconvex functions . They lead to\nmathematical tools that are commonly applied to machine learning.\nConvexSets\nSets are the basis of convexity. Simply put, a set Xin a vector space is convexif for any\n\ud835\udc4e,\ud835\udc4f2Xthe line segment connecting \ud835\udc4eand\ud835\udc4fis also inX. In mathematical terms this\nmeans that for all \ud835\udf062\u00bb0,1\u00bcwe have\n\ud835\udf06\ud835\udc4e\u00b8\u00b91\u0000\ud835\udf06\u00ba\ud835\udc4f2Xwhenever\ud835\udc4e,\ud835\udc4f2X. (12.2.1)\nThissoundsabitabstract. Consider Fig.12.2.1 . Thefirstsetisnotconvexsincethereexist\nlinesegmentsthatarenotcontainedinit. Theothertwosetssuffernosuchproblem.\ntFig. 12.2.1 The \ufb01rst set is nonconvex and the other two are convex.\nDefinitionsontheirownarenotparticularlyusefulunlessyoucandosomethingwiththem.\nIn this case we can look at intersections as shown in Fig. 12.2.2 . Assume thatXandYare\nconvex sets. ThenX\\Yis also convex. To see this, consider any \ud835\udc4e,\ud835\udc4f2X\\Y . SinceX\nandYare convex, the line segments connecting \ud835\udc4eand\ud835\udc4fare contained in both XandY.\nGiven that, they also need to be contained in X\\Y, thus proving our theorem.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a314db8f-d50c-4326-8022-5d40d858c2c8": {"__data__": {"id_": "a314db8f-d50c-4326-8022-5d40d858c2c8", "embedding": null, "metadata": {"page_label": "475", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "139abc9b-b189-4da5-983b-4102e9712266", "node_type": "4", "metadata": {"page_label": "475", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c6d2517aa272174c58ae056d0fa49996c85d1293f9eb1037f75dcbd51e91d90d", "class_name": "RelatedNodeInfo"}}, "text": "475 Convexity\ntFig. 12.2.2 The intersection between two convex sets is convex.\nWecanstrengthenthisresultwithlittleeffort: givenconvexsets X\ud835\udc56, theirintersection\\\ud835\udc56X\ud835\udc56\nis convex. To see that the converse is not true, consider two disjoint sets X\\Y =;. Now\npick\ud835\udc4e2Xand\ud835\udc4f2Y. Thelinesegmentin Fig.12.2.3 connecting\ud835\udc4eand\ud835\udc4fneedstocontain\nsome part that is neither in Xnor inY, since we assumed that X\\Y =;. Hence the line\nsegment is not inX[Yeither, thus proving that in general unions of convex sets need not\nbe convex.\ntFig. 12.2.3 The union of two convex sets need not be convex.\nTypically the problems in deep learning are defined on convex sets. For instance, R\ud835\udc51, the\nsetof\ud835\udc51-dimensionalvectorsofrealnumbers,isaconvexset(afterall,thelinebetweenany\ntwo points in R\ud835\udc51remains in R\ud835\udc51). In some cases we work with variables of bounded length,\nsuch as balls of radius \ud835\udc5fas defined byfxjx2R\ud835\udc51andkxk\u0014\ud835\udc5fg.\nConvexFunctions\nNow that we have convex sets we can introduce convexfunctions \ud835\udc53. Given a convex set X,\na function\ud835\udc53:X! Risconvexif for all\ud835\udc65,\ud835\udc6502Xand for all\ud835\udf062\u00bb0,1\u00bcwe have\n\ud835\udf06\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\u00b9 1\u0000\ud835\udf06\u00ba\ud835\udc53\u00b9\ud835\udc650\u00ba\u0015\ud835\udc53\u00b9\ud835\udf06\ud835\udc65\u00b8\u00b91\u0000\ud835\udf06\u00ba\ud835\udc650\u00ba. (12.2.2)\nTo illustrate this let\u2019s plot a few functions and check which ones satisfy the requirement.\nBelow we define a few functions, both convex and nonconvex.\nf=lambda x:0.5 *x**2# Convex\ng=lambda x: torch .cos(np .pi*x) # Nonconvex\nh=lambda x: torch .exp( 0.5 *x) # Convex\nx, segment =torch .arange( -2,2,0.01 ), torch .tensor([ -1.5,1])\nd2l.use_svg_display()\n_, axes =d2l.plt.subplots( 1,3, figsize =(9,3))\nfor ax, func inzip(axes, [f, g, h]):\nd2l.plot([x, segment], [func(x), func(segment)], axes =ax)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef4d319d-9aee-4da8-92d8-501068d9e8b1": {"__data__": {"id_": "ef4d319d-9aee-4da8-92d8-501068d9e8b1", "embedding": null, "metadata": {"page_label": "476", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e68ce08a-c3ae-41ab-bc5d-d0d2513130c8", "node_type": "4", "metadata": {"page_label": "476", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0ba8a091311ab0396d012596bc38de1ef3d8f10d618ebbe02ea19ca346079d44", "class_name": "RelatedNodeInfo"}}, "text": "476 Optimization Algorithms\nAs expected, the cosine function is nonconvex , whereas the parabola and the exponential\nfunctionare. Notethattherequirementthat Xisaconvexsetisnecessaryforthecondition\ntomakesense. Otherwisetheoutcomeof \ud835\udc53\u00b9\ud835\udf06\ud835\udc65\u00b8\u00b91\u0000\ud835\udf06\u00ba\ud835\udc650\u00bamightnotbewelldefined.\nJensen\u2019sInequality\nGivenaconvexfunction \ud835\udc53,oneofthemostusefulmathematicaltoolsis Jensen\u2019sinequality .\nIt amounts to a generalization of the definition of convexity:\n\u00d5\n\ud835\udc56\ud835\udefc\ud835\udc56\ud835\udc53\u00b9\ud835\udc65\ud835\udc56\u00ba\u0015\ud835\udc53 \u00d5\n\ud835\udc56\ud835\udefc\ud835\udc56\ud835\udc65\ud835\udc56!\nand\ud835\udc38\ud835\udc4b\u00bb\ud835\udc53\u00b9\ud835\udc4b\u00ba\u00bc\u0015\ud835\udc53\u00b9\ud835\udc38\ud835\udc4b\u00bb\ud835\udc4b\u00bc\u00ba, (12.2.3)\nwhere\ud835\udefc\ud835\udc56are nonnegative real numbers such that\u00cd\n\ud835\udc56\ud835\udefc\ud835\udc56=1and\ud835\udc4bis a random variable. In\nother words, the expectation of a convex function is no less than the convex function of an\nexpectation, where the latter is usually a simpler expression. To prove the first inequality\nwe repeatedly apply the definition of convexity to one term in the sum at a time.\nOne of the common applications of Jensen\u2019s inequality is to bound a more complicated\nexpression by a simpler one. For example, its application can be with regard to the log-\nlikelihood of partially observed random variables. That is, we use\n\ud835\udc38\ud835\udc4c\u0018\ud835\udc43\u00b9\ud835\udc4c\u00ba\u00bb\u0000log\ud835\udc43\u00b9\ud835\udc4bj\ud835\udc4c\u00ba\u00bc\u0015\u0000 log\ud835\udc43\u00b9\ud835\udc4b\u00ba, (12.2.4)\nsince\u00af\n\ud835\udc43\u00b9\ud835\udc4c\u00ba\ud835\udc43\u00b9\ud835\udc4bj\ud835\udc4c\u00ba\ud835\udc51\ud835\udc4c=\ud835\udc43\u00b9\ud835\udc4b\u00ba. This can be used in variational methods. Here \ud835\udc4c\nis typically the unobserved random variable, \ud835\udc43\u00b9\ud835\udc4c\u00bais the best guess of how it might be\ndistributed, and \ud835\udc43\u00b9\ud835\udc4b\u00bais the distribution with \ud835\udc4cintegrated out. For instance, in clustering\n\ud835\udc4cmight be the cluster labels and \ud835\udc43\u00b9\ud835\udc4bj\ud835\udc4c\u00bais the generative model when applying cluster\nlabels.\n12.2.2Properties\nConvex functions have many useful properties. We describe a few commonly-used ones\nbelow.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1564, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13d155d9-2084-4037-a1da-440d699d2760": {"__data__": {"id_": "13d155d9-2084-4037-a1da-440d699d2760", "embedding": null, "metadata": {"page_label": "477", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f4b7344-c284-4406-8be1-4a68d79537ab", "node_type": "4", "metadata": {"page_label": "477", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a455a60d61d6d351e642e8483402e00de1fdb267e7f52208ca14758f6fcc2d5d", "class_name": "RelatedNodeInfo"}}, "text": "477 Convexity\nLocalMinima AreGlobal Minima\nFirst and foremost, the local minima of convex functions are also the global minima. We\ncan prove it by contradiction as follows.\nConsider a convex function \ud835\udc53defined on a convex set X. Suppose that \ud835\udc65\u00032Xis a local\nminimum: thereexistsasmallpositivevalue \ud835\udc5dsothatfor\ud835\udc652Xthatsatisfies 0<j\ud835\udc65\u0000\ud835\udc65\u0003j\u0014\n\ud835\udc5dwe have\ud835\udc53\u00b9\ud835\udc65\u0003\u00ba< \ud835\udc53\u00b9\ud835\udc65\u00ba.\nAssume that the local minimum \ud835\udc65\u0003is not the global minimum of \ud835\udc53: there exists \ud835\udc6502X\nfor which\ud835\udc53\u00b9\ud835\udc650\u00ba< \ud835\udc53\u00b9\ud835\udc65\u0003\u00ba. There also exists \ud835\udf062 \u00bb0,1\u00basuch as\ud835\udf06=1\u0000\ud835\udc5d\nj\ud835\udc65\u0003\u0000\ud835\udc650jso that\n0<j\ud835\udf06\ud835\udc65\u0003\u00b8\u00b91\u0000\ud835\udf06\u00ba\ud835\udc650\u0000\ud835\udc65\u0003j\u0014\ud835\udc5d.\nHowever, according to the definition of convex functions, we have\n\ud835\udc53\u00b9\ud835\udf06\ud835\udc65\u0003\u00b8\u00b91\u0000\ud835\udf06\u00ba\ud835\udc650\u00ba\u0014\ud835\udf06\ud835\udc53\u00b9\ud835\udc65\u0003\u00ba\u00b8\u00b9 1\u0000\ud835\udf06\u00ba\ud835\udc53\u00b9\ud835\udc650\u00ba\n<\ud835\udf06\ud835\udc53\u00b9\ud835\udc65\u0003\u00ba\u00b8\u00b9 1\u0000\ud835\udf06\u00ba\ud835\udc53\u00b9\ud835\udc65\u0003\u00ba\n=\ud835\udc53\u00b9\ud835\udc65\u0003\u00ba,(12.2.5)\nwhich contradicts with our statement that \ud835\udc65\u0003is a local minimum. Therefore, there does\nnot exist\ud835\udc6502 Xfor which\ud835\udc53\u00b9\ud835\udc650\u00ba< \ud835\udc53\u00b9\ud835\udc65\u0003\u00ba. The local minimum \ud835\udc65\u0003is also the global\nminimum.\nFor instance, the convex function \ud835\udc53\u00b9\ud835\udc65\u00ba=\u00b9\ud835\udc65\u00001\u00ba2has a local minimum at \ud835\udc65=1, which is\nalso the global minimum.\nf=lambda x: (x -1)**2\nd2l.set_figsize()\nd2l.plot([x, segment], [f(x), f(segment)], 'x','f(x) ')\nThe fact that the local minima for convex functions are also the global minima is very\nconvenient. It means that if we minimize functions we cannot \u201cget stuck\u201d. Note, though,\nthat this does not mean that there cannot be more than one global minimum or that there\nmightevenexistone. Forinstance,thefunction \ud835\udc53\u00b9\ud835\udc65\u00ba=max\u00b9j\ud835\udc65j\u00001,0\u00baattainsitsminimum\nvalue over the interval \u00bb\u00001,1\u00bc. Conversely, the function \ud835\udc53\u00b9\ud835\udc65\u00ba=exp\u00b9\ud835\udc65\u00badoes not attain a\nminimum value on R: for\ud835\udc65!\u00001it asymptotes to 0, but there is no \ud835\udc65for which\ud835\udc53\u00b9\ud835\udc65\u00ba=\n0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82b78fe3-fdf3-462d-8c0d-8a98719bb378": {"__data__": {"id_": "82b78fe3-fdf3-462d-8c0d-8a98719bb378", "embedding": null, "metadata": {"page_label": "478", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c365f04-87d9-448b-8255-403363fd00ef", "node_type": "4", "metadata": {"page_label": "478", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a83aaf35f56462599b9c9681598061d55526e8e76f471eb9bfa122b7f8a548e9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "115ee5cf-493c-49c8-97ec-e875b43b2502", "node_type": "1", "metadata": {}, "hash": "1ae2268feda813224978a9a6271a631afb780f3a45e8cefdf7274e2f098ec503", "class_name": "RelatedNodeInfo"}}, "text": "478 Optimization Algorithms\nBelowSetsof ConvexFunctions AreConvex\nWe can conveniently define convex sets via below sets of convex functions. Concretely,\ngiven a convex function \ud835\udc53defined on a convex set X, any below set\nS\ud835\udc4fdef=f\ud835\udc65j\ud835\udc652Xand\ud835\udc53\u00b9\ud835\udc65\u00ba\u0014\ud835\udc4fg (12.2.6)\nis convex.\nLet\u2019sprovethisquickly. Recallthatforany \ud835\udc65,\ud835\udc6502S\ud835\udc4fweneedtoshowthat \ud835\udf06\ud835\udc65\u00b8\u00b91\u0000\ud835\udf06\u00ba\ud835\udc6502\nS\ud835\udc4fas long as\ud835\udf062\u00bb0,1\u00bc. Since\ud835\udc53\u00b9\ud835\udc65\u00ba\u0014\ud835\udc4fand\ud835\udc53\u00b9\ud835\udc650\u00ba\u0014\ud835\udc4f, by the definition of convexity we\nhave\n\ud835\udc53\u00b9\ud835\udf06\ud835\udc65\u00b8\u00b91\u0000\ud835\udf06\u00ba\ud835\udc650\u00ba\u0014\ud835\udf06\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\u00b9 1\u0000\ud835\udf06\u00ba\ud835\udc53\u00b9\ud835\udc650\u00ba\u0014\ud835\udc4f. (12.2.7)\nConvexityand Second Derivatives\nWhenever the second derivative of a function \ud835\udc53:R\ud835\udc5b!Rexists it is very easy to check\nwhether\ud835\udc53is convex. All we need to do is check whether the Hessian of \ud835\udc53is positive\nsemidefinite:r2\ud835\udc53\u00170, i.e., denoting the Hessian matrix r2\ud835\udc53byH,x>Hx\u00150for all\nx2R\ud835\udc5b. Forinstance, thefunction \ud835\udc53\u00b9x\u00ba=1\n2kxk2isconvexsincer2\ud835\udc53=1, i.e., itsHessian\nis an identity matrix.\nFormally, atwice-differentiableone-dimensionalfunction \ud835\udc53:R!Risconvexifandonly\nif its second derivative \ud835\udc5300\u00150. For any twice-differentiable multidimensional function\n\ud835\udc53:R\ud835\udc5b!R, it is convex if and only if its Hessian r2\ud835\udc53\u00170.\nFirst,weneedtoprovetheone-dimensionalcase. Toseethatconvexityof \ud835\udc53implies\ud835\udc5300\u00150\nwe use the fact that\n1\n2\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u00b81\n2\ud835\udc53\u00b9\ud835\udc65\u0000\ud835\udf16\u00ba\u0015\ud835\udc53\u0010\ud835\udc65\u00b8\ud835\udf16\n2\u00b8\ud835\udc65\u0000\ud835\udf16\n2\u0011\n=\ud835\udc53\u00b9\ud835\udc65\u00ba. (12.2.8)\nSincethesecondderivativeisgivenbythelimitoverfinitedifferencesitfollowsthat\n\ud835\udc5300\u00b9\ud835\udc65\u00ba=lim\n\ud835\udf16!0\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u00b8\ud835\udc53\u00b9\ud835\udc65\u0000\ud835\udf16\u00ba\u00002\ud835\udc53\u00b9\ud835\udc65\u00ba\n\ud835\udf162\u00150. (12.2.9)\nTo see that\ud835\udc5300\u00150implies that \ud835\udc53is convex we use the fact that \ud835\udc5300\u00150implies that \ud835\udc530\nis a monotonically nondecreasing function. Let \ud835\udc4e < \ud835\udc65 < \ud835\udc4f be three points in R, where\n\ud835\udc65=\u00b91\u0000\ud835\udf06\u00ba\ud835\udc4e\u00b8\ud835\udf06\ud835\udc4fand\ud835\udf062 \u00b90,1\u00ba. According to the mean value theorem, there exist\n\ud835\udefc2\u00bb\ud835\udc4e,\ud835\udc65\u00bcand\ud835\udefd2\u00bb\ud835\udc65,\ud835\udc4f\u00bcsuch that\n\ud835\udc530\u00b9\ud835\udefc\u00ba=\ud835\udc53\u00b9\ud835\udc65\u00ba\u0000\ud835\udc53\u00b9\ud835\udc4e\u00ba\n\ud835\udc65\u0000\ud835\udc4eand\ud835\udc530\u00b9\ud835\udefd\u00ba=\ud835\udc53\u00b9\ud835\udc4f\u00ba\u0000\ud835\udc53\u00b9\ud835\udc65\u00ba\n\ud835\udc4f\u0000\ud835\udc65. (12.2.10)\nBy monotonicity \ud835\udc530\u00b9\ud835\udefd\u00ba\u0015\ud835\udc530\u00b9\ud835\udefc\u00ba, hence\n\ud835\udc65\u0000\ud835\udc4e\n\ud835\udc4f\u0000\ud835\udc4e\ud835\udc53\u00b9\ud835\udc4f\u00ba\u00b8\ud835\udc4f\u0000\ud835\udc65\n\ud835\udc4f\u0000\ud835\udc4e\ud835\udc53\u00b9\ud835\udc4e\u00ba\u0015\ud835\udc53\u00b9\ud835\udc65\u00ba.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1719, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "115ee5cf-493c-49c8-97ec-e875b43b2502": {"__data__": {"id_": "115ee5cf-493c-49c8-97ec-e875b43b2502", "embedding": null, "metadata": {"page_label": "478", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c365f04-87d9-448b-8255-403363fd00ef", "node_type": "4", "metadata": {"page_label": "478", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a83aaf35f56462599b9c9681598061d55526e8e76f471eb9bfa122b7f8a548e9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "82b78fe3-fdf3-462d-8c0d-8a98719bb378", "node_type": "1", "metadata": {"page_label": "478", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "73039c739eb5ab9407e286809db1971ac70cdb8dbf2d3c86aa1bb9b591c739c8", "class_name": "RelatedNodeInfo"}}, "text": "(12.2.10)\nBy monotonicity \ud835\udc530\u00b9\ud835\udefd\u00ba\u0015\ud835\udc530\u00b9\ud835\udefc\u00ba, hence\n\ud835\udc65\u0000\ud835\udc4e\n\ud835\udc4f\u0000\ud835\udc4e\ud835\udc53\u00b9\ud835\udc4f\u00ba\u00b8\ud835\udc4f\u0000\ud835\udc65\n\ud835\udc4f\u0000\ud835\udc4e\ud835\udc53\u00b9\ud835\udc4e\u00ba\u0015\ud835\udc53\u00b9\ud835\udc65\u00ba. (12.2.11)\nSince\ud835\udc65=\u00b91\u0000\ud835\udf06\u00ba\ud835\udc4e\u00b8\ud835\udf06\ud835\udc4f, we have\n\ud835\udf06\ud835\udc53\u00b9\ud835\udc4f\u00ba\u00b8\u00b9 1\u0000\ud835\udf06\u00ba\ud835\udc53\u00b9\ud835\udc4e\u00ba\u0015\ud835\udc53\u00b9\u00b91\u0000\ud835\udf06\u00ba\ud835\udc4e\u00b8\ud835\udf06\ud835\udc4f\u00ba, (12.2.12)", "mimetype": "text/plain", "start_char_idx": 1645, "end_char_idx": 1796, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b6aa26e-9f4e-490a-bb9d-e28f4d9d08ba": {"__data__": {"id_": "2b6aa26e-9f4e-490a-bb9d-e28f4d9d08ba", "embedding": null, "metadata": {"page_label": "479", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dd49bda7-f961-472d-bb42-8b127c3103bf", "node_type": "4", "metadata": {"page_label": "479", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "aaf93aa6b6e24812affe18ffff679a31717f13d55d8de83d84bf9b269295fccd", "class_name": "RelatedNodeInfo"}}, "text": "479 Convexity\nthus proving convexity.\nSecond,weneedalemmabeforeprovingthemultidimensionalcase: \ud835\udc53:R\ud835\udc5b!Risconvex\nif and only if for all x,y2R\ud835\udc5b\n\ud835\udc54\u00b9\ud835\udc67\u00badef=\ud835\udc53\u00b9\ud835\udc67x\u00b8\u00b91\u0000\ud835\udc67\u00bay\u00bawhere\ud835\udc672\u00bb0,1\u00bc (12.2.13)\nis convex.\nToprovethatconvexityof \ud835\udc53impliesthat\ud835\udc54isconvex,wecanshowthatforall \ud835\udc4e,\ud835\udc4f,\ud835\udf062\u00bb0,1\u00bc\n(thus 0\u0014\ud835\udf06\ud835\udc4e\u00b8\u00b91\u0000\ud835\udf06\u00ba\ud835\udc4f\u00141)\n\ud835\udc54\u00b9\ud835\udf06\ud835\udc4e\u00b8\u00b91\u0000\ud835\udf06\u00ba\ud835\udc4f\u00ba\n=\ud835\udc53\u00b9\u00b9\ud835\udf06\ud835\udc4e\u00b8\u00b91\u0000\ud835\udf06\u00ba\ud835\udc4f\u00bax\u00b8\u00b91\u0000\ud835\udf06\ud835\udc4e\u0000\u00b91\u0000\ud835\udf06\u00ba\ud835\udc4f\u00bay\u00ba\n=\ud835\udc53\u00b9\ud835\udf06\u00b9\ud835\udc4ex\u00b8\u00b91\u0000\ud835\udc4e\u00bay\u00ba\u00b8\u00b91\u0000\ud835\udf06\u00ba\u00b9\ud835\udc4fx\u00b8\u00b91\u0000\ud835\udc4f\u00bay\u00ba\u00ba\n\u0014\ud835\udf06\ud835\udc53\u00b9\ud835\udc4ex\u00b8\u00b91\u0000\ud835\udc4e\u00bay\u00ba\u00b8\u00b91\u0000\ud835\udf06\u00ba\ud835\udc53\u00b9\ud835\udc4fx\u00b8\u00b91\u0000\ud835\udc4f\u00bay\u00ba\n=\ud835\udf06\ud835\udc54\u00b9\ud835\udc4e\u00ba\u00b8\u00b9 1\u0000\ud835\udf06\u00ba\ud835\udc54\u00b9\ud835\udc4f\u00ba.(12.2.14)\nTo prove the converse, we can show that for all \ud835\udf062\u00bb0,1\u00bc\n\ud835\udc53\u00b9\ud835\udf06x\u00b8\u00b91\u0000\ud835\udf06\u00bay\u00ba\n=\ud835\udc54\u00b9\ud835\udf06\u00011\u00b8\u00b91\u0000\ud835\udf06\u00ba\u00010\u00ba\n\u0014\ud835\udf06\ud835\udc54\u00b91\u00ba\u00b8\u00b9 1\u0000\ud835\udf06\u00ba\ud835\udc54\u00b90\u00ba\n=\ud835\udf06\ud835\udc53\u00b9x\u00ba\u00b8\u00b9 1\u0000\ud835\udf06\u00ba\ud835\udc53\u00b9y\u00ba.(12.2.15)\nFinally,usingthelemmaaboveandtheresultoftheone-dimensionalcase,themultidimen-\nsional case can be proven as follows. A multidimensional function \ud835\udc53:R\ud835\udc5b!Ris convex\nif and only if for all x,y2R\ud835\udc5b\ud835\udc54\u00b9\ud835\udc67\u00badef=\ud835\udc53\u00b9\ud835\udc67x\u00b8\u00b91\u0000\ud835\udc67\u00bay\u00ba, where\ud835\udc672\u00bb0,1\u00bc, is convex. Ac-\ncording to the one-dimensional case, this holds if and only if \ud835\udc5400=\u00b9x\u0000y\u00ba>H\u00b9x\u0000y\u00ba\u00150\n(Hdef=r2\ud835\udc53) for all x,y2R\ud835\udc5b, which is equivalent to H\u00170per the definition of positive\nsemidefinite matrices.\n12.2.3Constraints\nOneofthenicepropertiesofconvexoptimizationisthatitallowsustohandleconstraintsef-\nficiently. Thatis,itallowsustosolve constrainedoptimization problemsoftheform:\nminimize\nx\ud835\udc53\u00b9x\u00ba\nsubject to\ud835\udc50\ud835\udc56\u00b9x\u00ba\u00140for all\ud835\udc562f1,...,\ud835\udc5bg,(12.2.16)\nwhere\ud835\udc53istheobjectiveandthefunctions \ud835\udc50\ud835\udc56areconstraintfunctions. Toseewhatthisdoes\nconsider the case where \ud835\udc501\u00b9x\u00ba=kxk2\u00001. In this case the parameters xare constrained to\nthe unit ball. If a second constraint is \ud835\udc502\u00b9x\u00ba=v>x\u00b8\ud835\udc4f, then this corresponds to all xlying\non a half-space. Satisfying both constraints simultaneously amounts to selecting a slice of\na ball.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1586, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd740b46-4c1e-4f68-910b-ad0f1017e51d": {"__data__": {"id_": "dd740b46-4c1e-4f68-910b-ad0f1017e51d", "embedding": null, "metadata": {"page_label": "480", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1953d877-8a1e-4b1b-b308-ab5674c29ff3", "node_type": "4", "metadata": {"page_label": "480", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f5cbcb83d61bf816110704ab9b3fb91c410ed48390392766ecf5a4b6b8db4022", "class_name": "RelatedNodeInfo"}}, "text": "480 Optimization Algorithms\nLagrangian\nIn general, solving a constrained optimization problem is difficult. One way of addressing\nit stems from physics with a rather simple intuition. Imagine a ball inside a box. The ball\nwill roll to the place that is lowest and the forces of gravity will be balanced out with the\nforcesthatthesidesoftheboxcanimposeontheball. Inshort,thegradientoftheobjective\nfunction(i.e.,gravity)willbeoffsetbythegradientoftheconstraintfunction(theballneed\nto remain inside the box by virtue of the walls \u201cpushing back\u201d). Note that some constraints\nmay not be active: the walls that are not touched by the ball will not be able to exert any\nforce on the ball.\nSkipping over the derivation of the Lagrangian \ud835\udc3f, the above reasoning can be expressed\nvia the following saddle point optimization problem:\n\ud835\udc3f\u00b9x,\ud835\udefc1,...,\ud835\udefc\ud835\udc5b\u00ba=\ud835\udc53\u00b9x\u00ba\u00b8\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udefc\ud835\udc56\ud835\udc50\ud835\udc56\u00b9x\u00bawhere\ud835\udefc\ud835\udc56\u00150. (12.2.17)\nHere the variables \ud835\udefc\ud835\udc56(\ud835\udc56=1,...,\ud835\udc5b) are the so-called Lagrange multipliers that ensure that\nconstraintsareproperlyenforced. Theyarechosenjustlargeenoughtoensurethat \ud835\udc50\ud835\udc56\u00b9x\u00ba\u0014\n0for all\ud835\udc56. For instance, for any xwhere\ud835\udc50\ud835\udc56\u00b9x\u00ba<0naturally, we\u2019d end up picking \ud835\udefc\ud835\udc56=0.\nMoreover, thisisasaddlepointoptimizationproblemwhereonewantsto maximize\ud835\udc3fwith\nrespect to all \ud835\udefc\ud835\udc56and simultaneously minimize it with respect to x. There is a rich body of\nliterature explaining how to arrive at the function \ud835\udc3f\u00b9x,\ud835\udefc1,...,\ud835\udefc\ud835\udc5b\u00ba. For our purposes it is\nsufficient to know that the saddle point of \ud835\udc3fis where the original constrained optimization\nproblem is solved optimally.\nPenalties\nOne way of satisfying constrained optimization problems at least approximately is to adapt\ntheLagrangian \ud835\udc3f. Ratherthansatisfying \ud835\udc50\ud835\udc56\u00b9x\u00ba\u00140wesimplyadd \ud835\udefc\ud835\udc56\ud835\udc50\ud835\udc56\u00b9x\u00batotheobjective\nfunction\ud835\udc53\u00b9\ud835\udc65\u00ba. This ensures that the constraints will not be violated too badly.\nIn fact, we have been using this trick all along. Consider weight decay in Section 3.7 . In it\nweadd\ud835\udf06\n2kwk2totheobjectivefunctiontoensurethat wdoesnotgrowtoolarge. Fromthe\nconstrained optimization point of view we can see that this will ensure that kwk2\u0000\ud835\udc5f2\u00140\nfor some radius \ud835\udc5f. Adjusting the value of \ud835\udf06allows us to vary the size of w.\nIn general, adding penalties is a good way of ensuring approximate constraint satisfaction.\nIn practice this turns out to be much more robust than exact satisfaction. Furthermore, for\nnonconvex problems many of the properties that make the exact approach so appealing in\nthe convex case (e.g., optimality) no longer hold.\nProjections\nAn alternative strategy for satisfying constraints is projections. Again, we encountered\nthem before, e.g., when dealing with gradient clipping in Section 9.5 . There we ensured", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2614, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1cef72e-6c1e-462b-9b55-2cf473ffeca9": {"__data__": {"id_": "b1cef72e-6c1e-462b-9b55-2cf473ffeca9", "embedding": null, "metadata": {"page_label": "481", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f2579c0-fe51-4401-8f2b-58cc64ff6c23", "node_type": "4", "metadata": {"page_label": "481", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3d7810bebfaa51db685d3c93a0611a78667756e86f0b7968b7e4e06008cccffc", "class_name": "RelatedNodeInfo"}}, "text": "481 Convexity\nthat a gradient has length bounded by \ud835\udf03via\ng g\u0001min\u00b91,\ud835\udf03\u009dkgk\u00ba. (12.2.18)\nThisturnsout tobe a projection ofgontotheball ofradius \ud835\udf03. More generally, aprojection\non a convex setXis defined as\nProjX\u00b9x\u00ba=argmin\nx02Xkx\u0000x0k, (12.2.19)\nwhich is the closest point in Xtox.\ntFig. 12.2.4 Convex Projections.\nThe mathematical definition of projections may sound a bit abstract. Fig. 12.2.4 explains it\nsomewhatmoreclearly. Initwehavetwoconvexsets,acircleandadiamond. Pointsinside\nboth sets (yellow) remain unchanged during projections. Points outside both sets (black)\nare projected to the points inside the sets (red) that are closet to the original points (black).\nWhile for\u21132balls this leaves the direction unchanged, this need not be the case in general,\nas can be seen in the case of the diamond.\nOne of the uses for convex projections is to compute sparse weight vectors. In this case we\nproject weight vectors onto an \u21131ball, which is a generalized version of the diamond case\ninFig. 12.2.4 .\n12.2.4Summary\nIn the context of deep learning the main purpose of convex functions is to motivate opti-\nmization algorithms and help us understand them in detail. In the following we will see\nhow gradient descent and stochastic gradient descent can be derived accordingly.\n\u000fIntersections of convex sets are convex. Unions are not.\n\u000fTheexpectationofaconvexfunctionisnolessthantheconvexfunctionofanexpectation\n(Jensen\u2019s inequality).\n\u000fA twice-differentiable function is convex if and only if its Hessian (a matrix of second\nderivatives) is positive semidefinite.\n\u000fConvex constraints can be added via the Lagrangian. In practice we may simply add\nthem with a penalty to the objective function.\n\u000fProjections map to points in the convex set closest to the original points.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1765, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d24d3b4-ddf7-405d-bab6-d6c54c17adf6": {"__data__": {"id_": "4d24d3b4-ddf7-405d-bab6-d6c54c17adf6", "embedding": null, "metadata": {"page_label": "482", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfab5cc6-7a03-4d26-968a-6d8c016655c7", "node_type": "4", "metadata": {"page_label": "482", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8b70af1852e852cd78d442d251dd5bc7c9f508579c19fdf4a1c22dc98800c798", "class_name": "RelatedNodeInfo"}}, "text": "482 Optimization Algorithms\n16712.2.5Exercises\n1.Assume that we want to verify convexity of a set by drawing all lines between points\nwithin the set and checking whether the lines are contained.\n1.Prove that it is sufficient to check only the points on the boundary.\n2.Prove that it is sufficient to check only the vertices of the set.\n2.Denote byB\ud835\udc5d\u00bb\ud835\udc5f\u00bcdef=fxjx2R\ud835\udc51andkxk\ud835\udc5d\u0014\ud835\udc5fgthe ball of radius \ud835\udc5fusing the\ud835\udc5d-norm.\nProve thatB\ud835\udc5d\u00bb\ud835\udc5f\u00bcis convex for all \ud835\udc5d\u00151.\n3.Givenconvexfunctions \ud835\udc53and\ud835\udc54,showthat max\u00b9\ud835\udc53,\ud835\udc54\u00baisconvex,too. Provethat min\u00b9\ud835\udc53,\ud835\udc54\u00ba\nis not convex.\n4.Prove that the normalization of the softmax function is convex. More specifically prove\nthe convexity of \ud835\udc53\u00b9\ud835\udc65\u00ba=log\u00cd\n\ud835\udc56exp\u00b9\ud835\udc65\ud835\udc56\u00ba.\n5.Prove that linear subspaces, i.e., X=fxjWx=bg, are convex sets.\n6.Provethatinthecaseoflinearsubspaceswith b=0theprojectionProjXcanbewritten\nasMxfor some matrix M.\n7.Show that for twice-differentiable convex functions \ud835\udc53we can write \ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba=\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\n\ud835\udf16\ud835\udc530\u00b9\ud835\udc65\u00ba\u00b81\n2\ud835\udf162\ud835\udc5300\u00b9\ud835\udc65\u00b8\ud835\udf09\u00bafor some\ud835\udf092\u00bb0,\ud835\udf16\u00bc.\n8.Given a convex set Xand two vectors xandy, prove that projections never increase\ndistances, i.e.,kx\u0000yk\u0015kProjX\u00b9x\u00ba\u0000ProjX\u00b9y\u00bak.\nDiscussions167.\n12.3GradientDescent\nIn this section we are going to introduce the basic concepts underlying gradient descent .\nAlthough it is rarely used directly in deep learning, an understanding of gradient descent is\nkeytounderstandingstochasticgradientdescentalgorithms. Forinstance,theoptimization\nproblem might diverge due to an overly large learning rate. This phenomenon can already\nbe seen in gradient descent. Likewise, preconditioning is a common technique in gradient\ndescent and carries over to more advanced algorithms. Let\u2019s start with a simple special\ncase.\n12.3.1One-DimensionalGradient Descent\nGradient descent in one dimension is an excellent example to explain why the gradient\ndescent algorithm may reduce the value of the objective function. Consider some con-\ntinuously differentiable real-valued function \ud835\udc53:R!R. Using a Taylor expansion we", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1942, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f55a9d5b-7a4b-42b8-9ee6-72568d5db22b": {"__data__": {"id_": "f55a9d5b-7a4b-42b8-9ee6-72568d5db22b", "embedding": null, "metadata": {"page_label": "483", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9af52b2-d04f-4895-83e1-21ac849d1688", "node_type": "4", "metadata": {"page_label": "483", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "89bd511d57f327baa1aa87fd2a7a95e680a81bf2265fa8375056009c77ac326a", "class_name": "RelatedNodeInfo"}}, "text": "483 Gradient Descent\nobtain\n\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba=\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc530\u00b9\ud835\udc65\u00ba\u00b8O\u00b9\ud835\udf162\u00ba. (12.3.1)\nThat is, in first-order approximation \ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00bais given by the function value \ud835\udc53\u00b9\ud835\udc65\u00baand the\nfirst derivative \ud835\udc530\u00b9\ud835\udc65\u00baat\ud835\udc65. It is not unreasonable to assume that for small \ud835\udf16moving in the\ndirection of the negative gradient will decrease \ud835\udc53. To keep things simple we pick a fixed\nstep size\ud835\udf02> 0and choose\ud835\udf16=\u0000\ud835\udf02\ud835\udc530\u00b9\ud835\udc65\u00ba. Plugging this into the Taylor expansion above we\nget\n\ud835\udc53\u00b9\ud835\udc65\u0000\ud835\udf02\ud835\udc530\u00b9\ud835\udc65\u00ba\u00ba=\ud835\udc53\u00b9\ud835\udc65\u00ba\u0000\ud835\udf02\ud835\udc5302\u00b9\ud835\udc65\u00ba\u00b8O\u00b9\ud835\udf022\ud835\udc5302\u00b9\ud835\udc65\u00ba\u00ba. (12.3.2)\nIfthederivative \ud835\udc530\u00b9\ud835\udc65\u00ba\u22600doesnotvanishwemakeprogresssince \ud835\udf02\ud835\udc5302\u00b9\ud835\udc65\u00ba>0. Moreover,\nwe can always choose \ud835\udf02small enough for the higher-order terms to become irrelevant.\nHence we arrive at\n\ud835\udc53\u00b9\ud835\udc65\u0000\ud835\udf02\ud835\udc530\u00b9\ud835\udc65\u00ba\u00ba\u2a85\ud835\udc53\u00b9\ud835\udc65\u00ba. (12.3.3)\nThis means that, if we use\n\ud835\udc65 \ud835\udc65\u0000\ud835\udf02\ud835\udc530\u00b9\ud835\udc65\u00ba (12.3.4)\ntoiterate\ud835\udc65,thevalueoffunction \ud835\udc53\u00b9\ud835\udc65\u00bamightdecline. Therefore,ingradientdescentwefirst\nchoose an initial value \ud835\udc65and a constant \ud835\udf02 > 0and then use them to continuously iterate \ud835\udc65\nuntilthestopconditionisreached,forexample,whenthemagnitudeofthegradient j\ud835\udc530\u00b9\ud835\udc65\u00baj\nis small enough or the number of iterations has reached a certain value.\nFor simplicity we choose the objective function \ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc652to illustrate how to implement\ngradient descent. Although we know that \ud835\udc65=0is the solution to minimize \ud835\udc53\u00b9\ud835\udc65\u00ba, we still\nuse this simple function to observe how \ud835\udc65changes.\n%matplotlib inline\nimport numpy asnp\nimport torch\nfrom d2l import torch asd2l\ndef f(x): # Objective function\nreturn x**2\ndef f_grad (x): # Gradient (derivative) of the objective function\nreturn 2*x\nNext, we use \ud835\udc65=10as the initial value and assume \ud835\udf02=0.2. Using gradient descent to\niterate\ud835\udc65for 10 times we can see that, eventually, the value of \ud835\udc65approaches the optimal\nsolution.\ndef gd(eta, f_grad):\nx=10.0\nresults =[x]\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "518b26b4-b22d-4799-8bea-8eb28a132857": {"__data__": {"id_": "518b26b4-b22d-4799-8bea-8eb28a132857", "embedding": null, "metadata": {"page_label": "484", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "426cde84-425e-4b61-99e9-023042bbdd19", "node_type": "4", "metadata": {"page_label": "484", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dde33cac57f1d43b3ae34edab677559547a0921ae62847d5e9da40d30ee4f5aa", "class_name": "RelatedNodeInfo"}}, "text": "484 Optimization Algorithms\n(continued from previous page)\nfor iinrange (10):\nx-=eta *f_grad(x)\nresults .append( float (x))\nprint (f'epoch 10, x: {x:f}')\nreturn results\nresults =gd(0.2, f_grad)\nepoch 10, x: 0.060466\nThe progress of optimizing over \ud835\udc65can be plotted as follows.\ndef show_trace (results, f):\nn=max(abs(min(results)), abs(max(results)))\nf_line =torch .arange( -n, n, 0.01 )\nd2l.set_figsize()\nd2l.plot([f_line, results], [[f(x) for xinf_line], [\nf(x) for xinresults]], 'x','f(x) ', fmts =['-','-o'])\nshow_trace(results, f)\nLearning Rate\nThelearningrate \ud835\udf02canbesetbythealgorithmdesigner. Ifweusealearningratethatistoo\nsmall, it will cause \ud835\udc65to update very slowly, requiring more iterations to get a better solu-\ntion. To show what happens in such a case, consider the progress in the same optimization\nproblem for \ud835\udf02=0.05. As we can see, even after 10 steps we are still very far from the\noptimal solution.\nshow_trace(gd( 0.05 , f_grad), f)\nepoch 10, x: 3.486784\nConversely, if we use an excessively high learning rate, j\ud835\udf02\ud835\udc530\u00b9\ud835\udc65\u00bajmight be too large for\nthe first-order Taylor expansion formula. That is, the term O\u00b9\ud835\udf022\ud835\udc5302\u00b9\ud835\udc65\u00ba\u00bain(12.3.2 )might", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8aa970f8-fcec-440d-af97-16b81bdf624b": {"__data__": {"id_": "8aa970f8-fcec-440d-af97-16b81bdf624b", "embedding": null, "metadata": {"page_label": "485", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "516e1e8f-6204-437f-991a-62c2cb08575d", "node_type": "4", "metadata": {"page_label": "485", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4da1cdb4f2805ef2082399ced3cbf6fe04d2dec95090485847b80459ba3a3f1e", "class_name": "RelatedNodeInfo"}}, "text": "485 Gradient Descent\nbecome significant. In this case, we cannot guarantee that the iteration of \ud835\udc65will be able to\nlowerthevalueof \ud835\udc53\u00b9\ud835\udc65\u00ba. Forexample,whenwesetthelearningrateto \ud835\udf02=1.1,\ud835\udc65overshoots\nthe optimal solution \ud835\udc65=0and gradually diverges.\nshow_trace(gd( 1.1, f_grad), f)\nepoch 10, x: 61.917364\nLocalMinima\nTo illustrate what happens for nonconvex functions consider the case of \ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc65\u0001cos\u00b9\ud835\udc50\ud835\udc65\u00ba\nfor some constant \ud835\udc50. This function has infinitely many local minima. Depending on our\nchoice of the learning rate and depending on how well conditioned the problem is, we may\nend up with one of many solutions. The example below illustrates how an (unrealistically)\nhigh learning rate will lead to a poor local minimum.\nc=torch .tensor( 0.15 *np.pi)\ndef f(x): # Objective function\nreturn x*torch .cos(c *x)\ndef f_grad (x): # Gradient of the objective function\nreturn torch .cos(c *x)-c*x*torch .sin(c *x)\nshow_trace(gd( 2, f_grad), f)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0312e532-d49a-4d7d-b8a6-f48b80cfada4": {"__data__": {"id_": "0312e532-d49a-4d7d-b8a6-f48b80cfada4", "embedding": null, "metadata": {"page_label": "486", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67e5da83-7725-4af6-a4c1-049aed2f8151", "node_type": "4", "metadata": {"page_label": "486", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2fb4ac5f118b627f27f74cd1afb24ebf8e1e34d8923197f44a5d0ac906a8f0f2", "class_name": "RelatedNodeInfo"}}, "text": "486 Optimization Algorithms\nepoch 10, x: -1.528166\n12.3.2MultivariateGradient Descent\nNowthatwehaveabetterintuitionoftheunivariatecase, let\u2019sconsiderthesituationwhere\nx=\u00bb\ud835\udc651,\ud835\udc652,...,\ud835\udc65\ud835\udc51\u00bc>. That is, the objective function \ud835\udc53:R\ud835\udc51!Rmaps vectors into\nscalars. Correspondingly its gradient is multivariate, too. It is a vector consisting of \ud835\udc51\npartial derivatives:\nr\ud835\udc53\u00b9x\u00ba=\u0014\ud835\udf15\ud835\udc53\u00b9x\u00ba\n\ud835\udf15\ud835\udc651,\ud835\udf15\ud835\udc53\u00b9x\u00ba\n\ud835\udf15\ud835\udc652,...,\ud835\udf15\ud835\udc53\u00b9x\u00ba\n\ud835\udf15\ud835\udc65\ud835\udc51\u0015>\n. (12.3.5)\nEach partial derivative element \ud835\udf15\ud835\udc53\u00b9x\u00ba\u009d\ud835\udf15\ud835\udc65\ud835\udc56in the gradient indicates the rate of change of\n\ud835\udc53atxwith respect to the input \ud835\udc65\ud835\udc56. As before in the univariate case we can use the cor-\nresponding Taylor approximation for multivariate functions to get some idea of what we\nshould do. In particular, we have that\n\ud835\udc53\u00b9x\u00b8\ud835\udf50\u00ba=\ud835\udc53\u00b9x\u00ba\u00b8\ud835\udf50>r\ud835\udc53\u00b9x\u00ba\u00b8O\u00b9k \ud835\udf50k2\u00ba. (12.3.6)\nInotherwords,uptosecond-ordertermsin \ud835\udf50thedirectionofsteepestdescentisgivenbythe\nnegative gradient\u0000r\ud835\udc53\u00b9x\u00ba. Choosing a suitable learning rate \ud835\udf02> 0yields the prototypical\ngradient descent algorithm:\nx x\u0000\ud835\udf02r\ud835\udc53\u00b9x\u00ba. (12.3.7)\nTo see how the algorithm behaves in practice let\u2019s construct an objective function \ud835\udc53\u00b9x\u00ba=\n\ud835\udc652\n1\u00b82\ud835\udc652\n2with a two-dimensional vector x=\u00bb\ud835\udc651,\ud835\udc652\u00bc>as input and a scalar as output. The\ngradient is given by r\ud835\udc53\u00b9x\u00ba=\u00bb2\ud835\udc651,4\ud835\udc652\u00bc>. We will observe the trajectory of xby gradient\ndescent from the initial position \u00bb\u00005,\u00002\u00bc.\nTobeginwith,weneedtwomorehelperfunctions. Thefirstusesanupdatefunctionandap-\npliesit20timestotheinitialvalue. Thesecondhelpervisualizesthetrajectoryof x.\ndef train_2d (trainer, steps =20, f_grad =None ): #@save\n\"\"\"Optimize a 2D objective function with a customized trainer.\"\"\"\n# `s1` and `s2` are internal state variables that will be used in Momentum,\n\u21a9!adagrad, RMSProp\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f0de495-1357-4845-ad7c-72f522f1c315": {"__data__": {"id_": "0f0de495-1357-4845-ad7c-72f522f1c315", "embedding": null, "metadata": {"page_label": "487", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e3cf8860-12a1-4ebe-8c91-5087a2184271", "node_type": "4", "metadata": {"page_label": "487", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "753a23c0c2dec2effbfddfd181f770c2fbc593ee9a120a0cc730e96465329fa8", "class_name": "RelatedNodeInfo"}}, "text": "487 Gradient Descent\n(continued from previous page)\nx1, x2, s1, s2 =-5,-2,0,0\nresults =[(x1, x2)]\nfor iinrange (steps):\niff_grad:\nx1, x2, s1, s2 =trainer(x1, x2, s1, s2, f_grad)\nelse :\nx1, x2, s1, s2 =trainer(x1, x2, s1, s2)\nresults .append((x1, x2))\nprint (f'epoch {i+1}, x1: {float (x1) :f}, x2: {float (x2) :f}')\nreturn results\ndef show_trace_2d (f, results): #@save\n\"\"\"Show the trace of 2D variables during optimization.\"\"\"\nd2l.set_figsize()\nd2l.plt.plot( *zip(*results), '-o', color ='#ff7f0e ')\nx1, x2 =torch .meshgrid(torch .arange( -5.5,1.0,0.1),\ntorch .arange( -3.0,1.0,0.1), indexing ='ij')\nd2l.plt.contour(x1, x2, f(x1, x2), colors ='#1f77b4 ')\nd2l.plt.xlabel( 'x1')\nd2l.plt.ylabel( 'x2')\nNext, we observe the trajectory of the optimization variable xfor learning rate \ud835\udf02=0.1.\nWe can see that after 20 steps the value of xapproaches its minimum at \u00bb0,0\u00bc. Progress is\nfairly well-behaved albeit rather slow.\ndef f_2d (x1, x2): # Objective function\nreturn x1**2+2*x2**2\ndef f_2d_grad (x1, x2): # Gradient of the objective function\nreturn (2*x1, 4*x2)\ndef gd_2d (x1, x2, s1, s2, f_grad):\ng1, g2 =f_grad(x1, x2)\nreturn (x1 -eta *g1, x2 -eta *g2, 0,0)\neta =0.1\nshow_trace_2d(f_2d, train_2d(gd_2d, f_grad =f_2d_grad))\nepoch 20, x1: -0.057646 , x2: -0.000073", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "668666ed-c419-423f-a62d-520a23760612": {"__data__": {"id_": "668666ed-c419-423f-a62d-520a23760612", "embedding": null, "metadata": {"page_label": "488", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7fd15b6-c60a-438c-a6c9-e72f78887474", "node_type": "4", "metadata": {"page_label": "488", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "74dd800366f9cb554cf9536adbea8dee69280557b06a732863dcbb0a88af5bdc", "class_name": "RelatedNodeInfo"}}, "text": "488 Optimization Algorithms\n12.3.3AdaptiveMethods\nAs we could see in Section 12.3.1 , getting the learning rate \ud835\udf02\u201cjust right\u201d is tricky. If we\npick it too small, we make little progress. If we pick it too large, the solution oscillates and\nin the worst case it might even diverge. What if we could determine \ud835\udf02automatically or get\nridofhavingtoselectalearningrateatall? Second-ordermethodsthatlooknotonlyatthe\nvalue and gradient of the objective function but also at its curvature can help in this case.\nWhile these methods cannot be applied to deep learning directly due to the computational\ncost, they provide useful intuition into how to design advanced optimization algorithms\nthat mimic many of the desirable properties of the algorithms outlined below.\nNewton\u2019sMethod\nReviewingtheTaylorexpansionofsomefunction \ud835\udc53:R\ud835\udc51!Rthereisnoneedtostopafter\nthe first term. In fact, we can write it as\n\ud835\udc53\u00b9x\u00b8\ud835\udf50\u00ba=\ud835\udc53\u00b9x\u00ba\u00b8\ud835\udf50>r\ud835\udc53\u00b9x\u00ba\u00b81\n2\ud835\udf50>r2\ud835\udc53\u00b9x\u00ba\ud835\udf50\u00b8O\u00b9k \ud835\udf50k3\u00ba. (12.3.8)\nTo avoid cumbersome notation we define Hdef=r2\ud835\udc53\u00b9x\u00bato be the Hessian of \ud835\udc53, which is\na\ud835\udc51\u0002\ud835\udc51matrix. For small \ud835\udc51and simple problems His easy to compute. For deep neural\nnetworks, ontheotherhand, Hmaybeprohibitivelylarge,duetothecostofstoring O\u00b9\ud835\udc512\u00ba\nentries. Furthermore it may be too expensive to compute via backpropagation. For now\nlet\u2019s ignore such considerations and look at what algorithm we would get.\nAfter all, the minimum of \ud835\udc53satisfiesr\ud835\udc53=0. Following calculus rules in Section 2.4.3 , by\ntaking derivatives of (12.3.8 )with regard to \ud835\udf50and ignoring higher-order terms we arrive\nat\nr\ud835\udc53\u00b9x\u00ba\u00b8H\ud835\udf50=0and hence \ud835\udf50=\u0000H\u00001r\ud835\udc53\u00b9x\u00ba. (12.3.9)\nThat is, we need to invert the Hessian Has part of the optimization problem.\nAs a simple example, for \ud835\udc53\u00b9\ud835\udc65\u00ba=1\n2\ud835\udc652we haver\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc65andH=1. Hence for any \ud835\udc65\nwe obtain\ud835\udf16=\u0000\ud835\udc65. In other words, a singlestep is sufficient to converge perfectly without\nthe need for any adjustment! Alas, we got a bit lucky here: the Taylor expansion was exact\nsince\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba=1\n2\ud835\udc652\u00b8\ud835\udf16\ud835\udc65\u00b81\n2\ud835\udf162.\nLet\u2019s see what happens in other problems. Given a convex hyperbolic cosine function\n\ud835\udc53\u00b9\ud835\udc65\u00ba=cosh\u00b9\ud835\udc50\ud835\udc65\u00bafor some constant \ud835\udc50, we can see that the global minimum at \ud835\udc65=0is\nreached after a few iterations.\nc=torch .tensor( 0.5)\ndef f(x): # Objective function\nreturn torch .cosh(c *x)\ndef f_grad (x): # Gradient of the objective function\nreturn c*torch .sinh(c *x)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3123d0a-d1c5-4c46-80d3-222ccc5d5ffd": {"__data__": {"id_": "f3123d0a-d1c5-4c46-80d3-222ccc5d5ffd", "embedding": null, "metadata": {"page_label": "489", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f41b4ef6-160b-43a3-832f-453466286a7b", "node_type": "4", "metadata": {"page_label": "489", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3724fe21391eebdf000c7f77ef12dc23217b42c15ce70b89827dc53aac532788", "class_name": "RelatedNodeInfo"}}, "text": "489 Gradient Descent\n(continued from previous page)\ndef f_hess (x): # Hessian of the objective function\nreturn c**2*torch .cosh(c *x)\ndef newton (eta =1):\nx=10.0\nresults =[x]\nfor iinrange (10):\nx-=eta *f_grad(x) /f_hess(x)\nresults .append( float (x))\nprint ('epoch 10, x: ', x)\nreturn results\nshow_trace(newton(), f)\nepoch 10, x: tensor( 0.)\nNow let\u2019s consider a nonconvex function, such as \ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc65cos\u00b9\ud835\udc50\ud835\udc65\u00bafor some constant \ud835\udc50.\nAfterall,notethatinNewton\u2019smethodweendupdividingbytheHessian. Thismeansthat\nif the second derivative is negative we may walk into the direction of increasing the value\nof\ud835\udc53. That is a fatal flaw of the algorithm. Let\u2019s see what happens in practice.\nc=torch .tensor( 0.15 *np.pi)\ndef f(x): # Objective function\nreturn x*torch .cos(c *x)\ndef f_grad (x): # Gradient of the objective function\nreturn torch .cos(c *x)-c*x*torch .sin(c *x)\ndef f_hess (x): # Hessian of the objective function\nreturn -2*c*torch .sin(c *x)-x*c**2*torch .cos(c *x)\nshow_trace(newton(), f)\nepoch 10, x: tensor( 26.8341 )\nThis went spectacularly wrong. How can we fix it? One way would be to \u201cfix\u201d the Hessian\nby taking its absolute value instead. Another strategy is to bring back the learning rate.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a4311929-6a9c-42e2-bf7b-801dec341307": {"__data__": {"id_": "a4311929-6a9c-42e2-bf7b-801dec341307", "embedding": null, "metadata": {"page_label": "490", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5657d12b-0a6a-47c5-b788-f0e2307bdc50", "node_type": "4", "metadata": {"page_label": "490", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "93923462b27dcde9ae2cf410efddd2f210506d13c8d87e4669fa8327f6d99ab7", "class_name": "RelatedNodeInfo"}}, "text": "490 Optimization Algorithms\nThis seems to defeat the purpose, but not quite. Having second-order information allows\nus to be cautious whenever the curvature is large and to take longer steps whenever the\nobjective function is flatter. Let\u2019s see how this works with a slightly smaller learning rate,\nsay\ud835\udf02=0.5. As we can see, we have quite an efficient algorithm.\nshow_trace(newton( 0.5), f)\nepoch 10, x: tensor( 7.2699 )\nConvergenceAnalysis\nWeonlyanalyzetheconvergencerateofNewton\u2019smethodforsomeconvexandthreetimes\ndifferentiable objective function \ud835\udc53, where the second derivative is nonzero, i.e., \ud835\udc5300>0.\nThe multivariate proof is a straightforward extension of the one-dimensional argument be-\nlow and omitted since it does not help us much in terms of intuition.\nDenoteby\ud835\udc65\u00b9\ud835\udc58\u00bathevalueof\ud835\udc65atthe\ud835\udc58thiterationandlet \ud835\udc52\u00b9\ud835\udc58\u00badef=\ud835\udc65\u00b9\ud835\udc58\u00ba\u0000\ud835\udc65\u0003bethedistancefrom\noptimality at the \ud835\udc58thiteration. By Taylor expansion we have that the condition \ud835\udc530\u00b9\ud835\udc65\u0003\u00ba=0\ncan be written as\n0=\ud835\udc530\u00b9\ud835\udc65\u00b9\ud835\udc58\u00ba\u0000\ud835\udc52\u00b9\ud835\udc58\u00ba\u00ba=\ud835\udc530\u00b9\ud835\udc65\u00b9\ud835\udc58\u00ba\u00ba\u0000\ud835\udc52\u00b9\ud835\udc58\u00ba\ud835\udc5300\u00b9\ud835\udc65\u00b9\ud835\udc58\u00ba\u00ba\u00b81\n2\u00b9\ud835\udc52\u00b9\ud835\udc58\u00ba\u00ba2\ud835\udc53000\u00b9\ud835\udf09\u00b9\ud835\udc58\u00ba\u00ba, (12.3.10)\nwhichholdsforsome \ud835\udf09\u00b9\ud835\udc58\u00ba2\u00bb\ud835\udc65\u00b9\ud835\udc58\u00ba\u0000\ud835\udc52\u00b9\ud835\udc58\u00ba,\ud835\udc65\u00b9\ud835\udc58\u00ba\u00bc. Dividingtheaboveexpansionby \ud835\udc5300\u00b9\ud835\udc65\u00b9\ud835\udc58\u00ba\u00ba", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1099, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4cf0a829-c6e1-48e8-af19-51c5bbb5fb82": {"__data__": {"id_": "4cf0a829-c6e1-48e8-af19-51c5bbb5fb82", "embedding": null, "metadata": {"page_label": "491", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0047b98-d84e-4550-ac43-6a50499d5610", "node_type": "4", "metadata": {"page_label": "491", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6f815c09f0afd3cd827ecbf990d0ffdde84d4e9b57d733db82a579272c7f7e6f", "class_name": "RelatedNodeInfo"}}, "text": "491 Gradient Descent\nyields\n\ud835\udc52\u00b9\ud835\udc58\u00ba\u0000\ud835\udc530\u00b9\ud835\udc65\u00b9\ud835\udc58\u00ba\u00ba\n\ud835\udc5300\u00b9\ud835\udc65\u00b9\ud835\udc58\u00ba\u00ba=1\n2\u00b9\ud835\udc52\u00b9\ud835\udc58\u00ba\u00ba2\ud835\udc53000\u00b9\ud835\udf09\u00b9\ud835\udc58\u00ba\u00ba\n\ud835\udc5300\u00b9\ud835\udc65\u00b9\ud835\udc58\u00ba\u00ba. (12.3.11)\nRecall that we have the update \ud835\udc65\u00b9\ud835\udc58\u00b81\u00ba=\ud835\udc65\u00b9\ud835\udc58\u00ba\u0000\ud835\udc530\u00b9\ud835\udc65\u00b9\ud835\udc58\u00ba\u00ba\u009d\ud835\udc5300\u00b9\ud835\udc65\u00b9\ud835\udc58\u00ba\u00ba. Plugging in this update\nequation and taking the absolute value of both sides, we have\n\f\f\f\ud835\udc52\u00b9\ud835\udc58\u00b81\u00ba\f\f\f=1\n2\u00b9\ud835\udc52\u00b9\ud835\udc58\u00ba\u00ba2\f\f\ud835\udc53000\u00b9\ud835\udf09\u00b9\ud835\udc58\u00ba\u00ba\f\f\n\ud835\udc5300\u00b9\ud835\udc65\u00b9\ud835\udc58\u00ba\u00ba. (12.3.12)\nConsequently, whenever we are in a region of bounded\f\f\ud835\udc53000\u00b9\ud835\udf09\u00b9\ud835\udc58\u00ba\u00ba\f\f\u009d\u00b92\ud835\udc5300\u00b9\ud835\udc65\u00b9\ud835\udc58\u00ba\u00ba\u00ba\u0014\ud835\udc50, we\nhave a quadratically decreasing error\n\f\f\f\ud835\udc52\u00b9\ud835\udc58\u00b81\u00ba\f\f\f\u0014\ud835\udc50\u00b9\ud835\udc52\u00b9\ud835\udc58\u00ba\u00ba2. (12.3.13)\nAsanaside,optimizationresearcherscallthis linearconvergence,whereasaconditionsuch\nas\f\f\ud835\udc52\u00b9\ud835\udc58\u00b81\u00ba\f\f\u0014\ud835\udefc\f\f\ud835\udc52\u00b9\ud835\udc58\u00ba\f\fwouldbecalleda constant rateofconvergence. Notethatthisanalysis\ncomes with a number of caveats. First, we do not really have muchof a guarantee when we\nwill reach the region of rapid convergence. Instead, we only know that once we reach it,\nconvergence will be very quick. Second, this analysis requires that \ud835\udc53is well-behaved up to\nhigher-order derivatives. It comes down to ensuring that \ud835\udc53does not have any \u201csurprising\u201d\nproperties in terms of how it might change its values.\nPreconditioning\nQuite unsurprisingly computing and storing the full Hessian is very expensive. It is thus\ndesirable to find alternatives. One way to improve matters is preconditioning . It avoids\ncomputing the Hessian in its entirety but only computes the diagonal entries. This leads to\nupdate algorithms of the form\nx x\u0000\ud835\udf02diag\u00b9H\u00ba\u00001r\ud835\udc53\u00b9x\u00ba. (12.3.14)\nWhile this is not quite as good as the full Newton\u2019s method, it is still much better than not\nusing it. To see why this might be a good idea consider a situation where one variable\ndenotes height in millimeters and the other one denotes height in kilometers. Assuming\nthatforboththenaturalscaleisinmeters,wehaveaterriblemismatchinparametrizations.\nFortunately, using preconditioning removes this. Effectively preconditioning with gradient\ndescentamountstoselectingadifferentlearningrateforeachvariable(coordinateofvector\nx). Aswewillseelater,preconditioningdrivessomeoftheinnovationinstochasticgradient\ndescent optimization algorithms.\nGradientDescent with Line Search\nOne of the key problems in gradient descent is that we might overshoot the goal or make\ninsufficient progress. A simple fix for the problem is to use line search in conjunction with\ngradient descent. That is, we use the direction given by r\ud835\udc53\u00b9x\u00baand then perform binary\nsearch as to which learning rate \ud835\udf02minimizes\ud835\udc53\u00b9x\u0000\ud835\udf02r\ud835\udc53\u00b9x\u00ba\u00ba.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de58378a-4a53-4bce-a1e4-c082d51b4774": {"__data__": {"id_": "de58378a-4a53-4bce-a1e4-c082d51b4774", "embedding": null, "metadata": {"page_label": "492", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9ba1f8c-c362-4d03-9377-4a31f2b61b7c", "node_type": "4", "metadata": {"page_label": "492", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6b3f01ae12a042e5c127e17358dc82bdab393f01430ca79fe54a206e0fbe1a91", "class_name": "RelatedNodeInfo"}}, "text": "492 Optimization Algorithms\n168This algorithm converges rapidly (for an analysis and proof see e.g., Boyd and Vanden-\nberghe ( 2004)). However, for the purpose of deep learning this is not quite so feasible,\nsince each step of the line search would require us to evaluate the objective function on the\nentire dataset. This is way too costly to accomplish.\n12.3.4Summary\n\u000fLearningratesmatter. Toolargeandwediverge,toosmallandwedonotmakeprogress.\n\u000fGradient descent can get stuck in local minima.\n\u000fIn high dimensions adjusting the learning rate is complicated.\n\u000fPreconditioning can help with scale adjustment.\n\u000fNewton\u2019s method is a lot faster once it has started working properly in convex problems.\n\u000fBeware of using Newton\u2019s method without any adjustments for nonconvex problems.\n12.3.5Exercises\n1.Experiment with different learning rates and objective functions for gradient descent.\n2.Implement line search to minimize a convex function in the interval \u00bb\ud835\udc4e,\ud835\udc4f\u00bc.\n1.Do you need derivatives for binary search, i.e., to decide whether to pick \u00bb\ud835\udc4e,\u00b9\ud835\udc4e\u00b8\n\ud835\udc4f\u00ba\u009d2\u00bcor\u00bb\u00b9\ud835\udc4e\u00b8\ud835\udc4f\u00ba\u009d2,\ud835\udc4f\u00bc.\n2.How rapid is the rate of convergence for the algorithm?\n3.Implement the algorithm and apply it to minimizing log\u00b9exp\u00b9\ud835\udc65\u00ba\u00b8exp\u00b9\u00002\ud835\udc65\u00003\u00ba\u00ba.\n3.Design an objective function defined on R2where gradient descent is exceedingly slow.\nHint: scale different coordinates differently.\n4.Implement the lightweight version of Newton\u2019s method using preconditioning:\n1.Use diagonal Hessian as preconditioner.\n2.Use the absolute values of that rather than the actual (possibly signed) values.\n3.Apply this to the problem above.\n5.Apply the algorithm above to a number of objective functions (convex or not). What\nhappens if you rotate coordinates by 45degrees?\nDiscussions168.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1720, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47f6aa22-3cef-4e04-a3b5-24fb3e679a19": {"__data__": {"id_": "47f6aa22-3cef-4e04-a3b5-24fb3e679a19", "embedding": null, "metadata": {"page_label": "493", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9c76e60-c040-4486-9465-08123f8aa914", "node_type": "4", "metadata": {"page_label": "493", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e4dbce6891907bb349697578e59baa65e7a1bfd25e22e4b7b2b307295a5d3c9e", "class_name": "RelatedNodeInfo"}}, "text": "493 Stochastic Gradient Descent\n12.4StochasticGradient Descent\nInearlierchapterswekeptusingstochasticgradientdescentinourtrainingprocedure,how-\never, withoutexplainingwhyitworks. Toshedsomelightonit, wejustdescribedthebasic\nprinciplesofgradientdescentin Section12.3 . Inthissection,wegoontodiscuss stochastic\ngradientdescent in greater detail.\n%matplotlib inline\nimport math\nimport torch\nfrom d2l import torch asd2l\n12.4.1StochasticGradient Updates\nIn deep learning, the objective function is usually the average of the loss functions for each\nexample in the training dataset. Given a training dataset of \ud835\udc5bexamples, we assume that\n\ud835\udc53\ud835\udc56\u00b9x\u00bais the loss function with respect to the training example of index \ud835\udc56, where xis the\nparameter vector. Then we arrive at the objective function\n\ud835\udc53\u00b9x\u00ba=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc53\ud835\udc56\u00b9x\u00ba. (12.4.1)\nThe gradient of the objective function at xis computed as\nr\ud835\udc53\u00b9x\u00ba=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1r\ud835\udc53\ud835\udc56\u00b9x\u00ba. (12.4.2)\nIf gradient descent is used, the computational cost for each independent variable iteration\nisO\u00b9\ud835\udc5b\u00ba, which grows linearly with \ud835\udc5b. Therefore, when the training dataset is larger, the\ncost of gradient descent for each iteration will be higher.\nStochastic gradient descent (SGD) reduces computational cost at each iteration. At each\niteration of stochastic gradient descent, we uniformly sample an index \ud835\udc562f1,...,\ud835\udc5bgfor\ndata examples at random, and compute the gradient r\ud835\udc53\ud835\udc56\u00b9x\u00bato update x:\nx x\u0000\ud835\udf02r\ud835\udc53\ud835\udc56\u00b9x\u00ba, (12.4.3)\nwhere\ud835\udf02isthelearningrate. Wecanseethatthecomputationalcostforeachiterationdrops\nfromO\u00b9\ud835\udc5b\u00baof the gradient descent to the constant O\u00b91\u00ba. Moreover, we want to empha-\nsize that the stochastic gradient r\ud835\udc53\ud835\udc56\u00b9x\u00bais an unbiased estimate of the full gradient r\ud835\udc53\u00b9x\u00ba\nbecause\nE\ud835\udc56r\ud835\udc53\ud835\udc56\u00b9x\u00ba=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1r\ud835\udc53\ud835\udc56\u00b9x\u00ba=r\ud835\udc53\u00b9x\u00ba. (12.4.4)\nThismeansthat,onaverage,thestochasticgradientisagoodestimateofthegradient.\nNow, we will compare it with gradient descent by adding random noise with a mean of 0\nand a variance of 1 to the gradient to simulate a stochastic gradient descent.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1945, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b804ccd-355c-42b7-b206-8e1c04d6b3f6": {"__data__": {"id_": "1b804ccd-355c-42b7-b206-8e1c04d6b3f6", "embedding": null, "metadata": {"page_label": "494", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec7ede54-4847-4bae-9dc5-590886236728", "node_type": "4", "metadata": {"page_label": "494", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "efa764b9656a4cd126ceb2f54a1ca4957812fb0ee71fdfd24eafc45ca1dde389", "class_name": "RelatedNodeInfo"}}, "text": "494 Optimization Algorithms\ndef f(x1, x2): # Objective function\nreturn x1**2+2*x2**2\ndef f_grad (x1, x2): # Gradient of the objective function\nreturn 2*x1, 4*x2\ndef sgd(x1, x2, s1, s2, f_grad):\ng1, g2 =f_grad(x1, x2)\n# Simulate noisy gradient\ng1+=torch .normal( 0.0,1, (1,)).item()\ng2+=torch .normal( 0.0,1, (1,)).item()\neta_t =eta *lr()\nreturn (x1 -eta_t *g1, x2 -eta_t *g2, 0,0)\ndef constant_lr ():\nreturn 1\neta =0.1\nlr=constant_lr # Constant learning rate\nd2l.show_trace_2d(f, d2l .train_2d(sgd, steps =50, f_grad =f_grad))\nepoch 50, x1: 0.225517 , x2: -0.076646\nAs we can see, the trajectory of the variables in the stochastic gradient descent is much\nmore noisy than the one we observed in gradient descent in Section 12.3 . This is due to\nthe stochastic nature of the gradient. That is, even when we arrive near the minimum,\nwe are still subject to the uncertainty injected by the instantaneous gradient via \ud835\udf02r\ud835\udc53\ud835\udc56\u00b9x\u00ba.\nEven after 50 steps the quality is still not so good. Even worse, it will not improve after\nadditional steps (we encourage you to experiment with a larger number of steps to confirm\nthis). This leaves us with the only alternative: change the learning rate \ud835\udf02. However, if we\npick this too small, we will not make any meaningful progress initially. On the other hand,\nif we pick it too large, we will not get a good solution, as seen above. The only way to\nresolve these conflicting goals is to reduce the learning rate dynamically as optimization\nprogresses.\nThis is also the reason for adding a learning rate function lrinto the sgdstep function. In", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebfd298e-fe6c-499f-9ccd-3faec0475720": {"__data__": {"id_": "ebfd298e-fe6c-499f-9ccd-3faec0475720", "embedding": null, "metadata": {"page_label": "495", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0625b5ef-df5b-4459-a8d7-913314f6d6b8", "node_type": "4", "metadata": {"page_label": "495", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ad46841c76b337bce949e422748efe8444a70b0e893dbef0f8627159a0e1313d", "class_name": "RelatedNodeInfo"}}, "text": "495 Stochastic Gradient Descent\nthe example above any functionality for learning rate scheduling lies dormant as we set the\nassociated lrfunction to be constant.\n12.4.2DynamicLearning Rate\nReplacing\ud835\udf02with a time-dependent learning rate \ud835\udf02\u00b9\ud835\udc61\u00baadds to the complexity of controlling\nconvergence of an optimization algorithm. In particular, we need to figure out how rapidly\n\ud835\udf02should decay. If it is too quick, we will stop optimizing prematurely. If we decrease\nit too slowly, we waste too much time on optimization. The following are a few basic\nstrategies that are used in adjusting \ud835\udf02over time (we will discuss more advanced strategies\nlater):\n\ud835\udf02\u00b9\ud835\udc61\u00ba=\ud835\udf02\ud835\udc56if\ud835\udc61\ud835\udc56\u0014\ud835\udc61\u0014\ud835\udc61\ud835\udc56\u00b81piecewise constant\n\ud835\udf02\u00b9\ud835\udc61\u00ba=\ud835\udf020\u0001\ud835\udc52\u0000\ud835\udf06\ud835\udc61exponential decay\n\ud835\udf02\u00b9\ud835\udc61\u00ba=\ud835\udf020\u0001\u00b9\ud835\udefd\ud835\udc61\u00b81\u00ba\u0000\ud835\udefcpolynomial decay(12.4.5)\nInthefirst piecewiseconstant scenariowedecreasethelearningrate,e.g.,wheneverprogress\nin optimization stalls. This is a common strategy for training deep networks. Alternatively\nwe could decrease it much more aggressively by an exponential decay . Unfortunately this\noften leads to premature stopping before the algorithm has converged. A popular choice is\npolynomial decay with\ud835\udefc=0.5. In the case of convex optimization there are a number of\nproofs that show that this rate is well behaved.\nLet\u2019s see what the exponential decay looks like in practice.\ndef exponential_lr ():\n# Global variable that is defined outside this function and updated inside\nglobal t\nt+=1\nreturn math .exp( -0.1 *t)\nt=1\nlr=exponential_lr\nd2l.show_trace_2d(f, d2l .train_2d(sgd, steps =1000 , f_grad =f_grad))\nepoch 1000 , x1: -0.758829 , x2: -0.115584\nAs expected, the variance in the parameters is significantly reduced. However, this comes", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a99e3950-1ac9-4010-8275-4e3bd1640bd3": {"__data__": {"id_": "a99e3950-1ac9-4010-8275-4e3bd1640bd3", "embedding": null, "metadata": {"page_label": "496", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e32cebb8-df3b-49fa-82a9-3c4700c241fc", "node_type": "4", "metadata": {"page_label": "496", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "912ad0575a6d86d98792dd2cebc18d674023d434d5728b68f68371b657c86879", "class_name": "RelatedNodeInfo"}}, "text": "496 Optimization Algorithms\n169at the expense of failing to converge to the optimal solution x=\u00b90,0\u00ba. Even after 1000\niterationstepsarewearestillveryfarawayfromtheoptimalsolution. Indeed,thealgorithm\nfails to convergeat all. On the other hand, if weuse a polynomialdecaywhere the learning\nrate decays with the inverse square root of the number of steps, convergence gets better\nafter only 50 steps.\ndef polynomial_lr ():\n# Global variable that is defined outside this function and updated inside\nglobal t\nt+=1\nreturn (1+0.1 *t)**(-0.5)\nt=1\nlr=polynomial_lr\nd2l.show_trace_2d(f, d2l .train_2d(sgd, steps =50, f_grad =f_grad))\nepoch 50, x1: 0.144834 , x2: 0.041688\nThereexistmanymorechoicesforhowtosetthelearningrate. Forinstance, wecouldstart\nwith a small rate, then rapidly ramp up and then decrease it again, albeit more slowly. We\ncould even alternate between smaller and larger learning rates. There exists a large variety\nof such schedules. For now let\u2019s focus on learning rate schedules for which a comprehen-\nsive theoretical analysis is possible, i.e., on learning rates in a convex setting. For general\nnonconvexproblemsitis verydifficulttoobtain meaningfulconvergenceguarantees, since\ningeneralminimizing nonlinearnonconvexproblemsisNP hard. Forasurveyseee.g., the\nexcellent lecture notes169of Tibshirani 2015.\n12.4.3ConvergenceAnalysisforConvexObjectives\nThe following convergence analysis of stochastic gradient descent for convex objective\nfunctions is optional and primarily serves to convey more intuition about the problem. We\nlimit ourselves to one of the simplest proofs ( Nesterov and Vial, 2000 ). Significantly more\nadvanced proof techniques exist, e.g., whenever the objective function is particularly well\nbehaved.\nSuppose that the objective function \ud835\udc53\u00b9\ud835\udf43,x\u00bais convex in xfor all \ud835\udf43. More concretely, we", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a295116b-a653-4f4d-b907-55618a397d5a": {"__data__": {"id_": "a295116b-a653-4f4d-b907-55618a397d5a", "embedding": null, "metadata": {"page_label": "497", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a268334-8417-4230-95d0-3bd574353cca", "node_type": "4", "metadata": {"page_label": "497", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8f343657a2d62a463923d55614e6042064b6cd3cac94c54b43a3491e501f9df2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76670990-537d-4735-b4a5-fcde1418c43d", "node_type": "1", "metadata": {}, "hash": "cf3c46a057a28823641673570d7ac5ef80504ba88b85ca900ee8798564a16869", "class_name": "RelatedNodeInfo"}}, "text": "497 Stochastic Gradient Descent\nconsider the stochastic gradient descent update:\nx\ud835\udc61\u00b81=x\ud835\udc61\u0000\ud835\udf02\ud835\udc61\ud835\udf15x\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\u00ba, (12.4.6)\nwhere\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\u00baistheobjectivefunctionwithrespecttothetrainingexample \ud835\udf43\ud835\udc61drawnfrom\nsome distribution at step \ud835\udc61andxis the model parameter. Denote by\n\ud835\udc45\u00b9x\u00ba=\ud835\udc38\ud835\udf43\u00bb\ud835\udc53\u00b9\ud835\udf43,x\u00ba\u00bc (12.4.7)\nthe expected risk and by \ud835\udc45\u0003its minimum with regard to x. Last let x\u0003be the minimizer\n(we assume that it exists within the domain where xis defined). In this case we can track\nthe distance between the current parameter x\ud835\udc61at time\ud835\udc61and the risk minimizer x\u0003and see\nwhether it improves over time:\nkx\ud835\udc61\u00b81\u0000x\u0003k2\n=kx\ud835\udc61\u0000\ud835\udf02\ud835\udc61\ud835\udf15x\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\u00ba\u0000x\u0003k2\n=kx\ud835\udc61\u0000x\u0003k2\u00b8\ud835\udf022\n\ud835\udc61k\ud835\udf15x\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\u00bak2\u00002\ud835\udf02\ud835\udc61\nx\ud835\udc61\u0000x\u0003,\ud835\udf15x\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\u00ba\u000b\n.(12.4.8)\nWeassumethatthe \u21132normofstochasticgradient \ud835\udf15x\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\u00baisboundedbysomeconstant\n\ud835\udc3f, hence we have that\n\ud835\udf022\n\ud835\udc61k\ud835\udf15x\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\u00bak2\u0014\ud835\udf022\n\ud835\udc61\ud835\udc3f2. (12.4.9)\nWe are mostly interested in how the distance between x\ud835\udc61andx\u0003changesin expectation .\nIn fact, for any specific sequence of steps the distance might well increase, depending on\nwhichever \ud835\udf43\ud835\udc61weencounter. Henceweneedtoboundthedotproduct. Sinceforanyconvex\nfunction\ud835\udc53it holds that \ud835\udc53\u00b9y\u00ba\u0015\ud835\udc53\u00b9x\u00ba\u00b8h\ud835\udc530\u00b9x\u00ba,y\u0000xifor all xandy, by convexity we\nhave\n\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\u0003\u00ba\u0015\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\ud835\udc61\u00ba\u00b8\nx\u0003\u0000x\ud835\udc61,\ud835\udf15x\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\ud835\udc61\u00ba\u000b\n. (12.4.10)\nPlugging both inequalities (12.4.9 )and(12.4.10 )into(12.4.8 )we obtain a bound on the\ndistance between parameters at time \ud835\udc61\u00b81as follows:\nkx\ud835\udc61\u0000x\u0003k2\u0000kx\ud835\udc61\u00b81\u0000x\u0003k2\u00152\ud835\udf02\ud835\udc61\u00b9\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\ud835\udc61\u00ba\u0000\ud835\udc53\u00b9\ud835\udf43\ud835\udc61,x\u0003\u00ba\u00ba\u0000\ud835\udf022\n\ud835\udc61\ud835\udc3f2. (12.4.11)\nThis means that we make progress as long as the difference between current loss and the\noptimallossoutweighs \ud835\udf02\ud835\udc61\ud835\udc3f2\u009d2. Sincethisdifferenceisboundtoconvergetozeroitfollows\nthat the learning rate \ud835\udf02\ud835\udc61also needs to vanish.\nNext we take expectations over (12.4.11 ). This yields\n\ud835\udc38\u0002\nkx\ud835\udc61\u0000x\u0003k2\u0003\n\u0000\ud835\udc38\u0002\nkx\ud835\udc61\u00b81\u0000x\u0003k2\u0003\n\u00152\ud835\udf02\ud835\udc61\u00bb\ud835\udc38\u00bb\ud835\udc45\u00b9x\ud835\udc61\u00ba\u00bc\u0000\ud835\udc45\u0003\u00bc\u0000\ud835\udf022\n\ud835\udc61\ud835\udc3f2. (12.4.12)\nThe last step involves summing over the inequalities for \ud835\udc612 f1,...,\ud835\udc47g. Since the sum\ntelescopes and by dropping the lower term we obtain\nkx1\u0000x\u0003k2\u00152 \ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udf02\ud835\udc61!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76670990-537d-4735-b4a5-fcde1418c43d": {"__data__": {"id_": "76670990-537d-4735-b4a5-fcde1418c43d", "embedding": null, "metadata": {"page_label": "497", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a268334-8417-4230-95d0-3bd574353cca", "node_type": "4", "metadata": {"page_label": "497", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8f343657a2d62a463923d55614e6042064b6cd3cac94c54b43a3491e501f9df2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a295116b-a653-4f4d-b907-55618a397d5a", "node_type": "1", "metadata": {"page_label": "497", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a17f8beb735aa78ebb0023127a56cf4345ee8b3a6d2ace11e2bd5632203969a9", "class_name": "RelatedNodeInfo"}}, "text": "Next we take expectations over (12.4.11 ). This yields\n\ud835\udc38\u0002\nkx\ud835\udc61\u0000x\u0003k2\u0003\n\u0000\ud835\udc38\u0002\nkx\ud835\udc61\u00b81\u0000x\u0003k2\u0003\n\u00152\ud835\udf02\ud835\udc61\u00bb\ud835\udc38\u00bb\ud835\udc45\u00b9x\ud835\udc61\u00ba\u00bc\u0000\ud835\udc45\u0003\u00bc\u0000\ud835\udf022\n\ud835\udc61\ud835\udc3f2. (12.4.12)\nThe last step involves summing over the inequalities for \ud835\udc612 f1,...,\ud835\udc47g. Since the sum\ntelescopes and by dropping the lower term we obtain\nkx1\u0000x\u0003k2\u00152 \ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udf02\ud835\udc61!\n\u00bb\ud835\udc38\u00bb\ud835\udc45\u00b9x\ud835\udc61\u00ba\u00bc\u0000\ud835\udc45\u0003\u00bc\u0000\ud835\udc3f2\ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udf022\n\ud835\udc61. (12.4.13)", "mimetype": "text/plain", "start_char_idx": 1589, "end_char_idx": 1904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42c74a1c-18b9-4522-a0ae-a311c9b2ac60": {"__data__": {"id_": "42c74a1c-18b9-4522-a0ae-a311c9b2ac60", "embedding": null, "metadata": {"page_label": "498", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "448358b8-c6d4-4253-9ec9-79e3f79da6df", "node_type": "4", "metadata": {"page_label": "498", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f716552491eb6743e97994b96c850093a22d8c33416db370d9804a034fa653eb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eb9ebbf7-681d-4ed5-9bae-ceb88f1d46fe", "node_type": "1", "metadata": {}, "hash": "5aab48edca8f339c774bbf8b67149f9d623e0998a38c170bd7257d92102fc5df", "class_name": "RelatedNodeInfo"}}, "text": "498 Optimization Algorithms\nNote that we exploited that x1is given and thus the expectation can be dropped. Last\ndefine\n\u00afxdef=\u00cd\ud835\udc47\n\ud835\udc61=1\ud835\udf02\ud835\udc61x\ud835\udc61\u00cd\ud835\udc47\n\ud835\udc61=1\ud835\udf02\ud835\udc61. (12.4.14)\nSince\n\ud835\udc38 \u00cd\ud835\udc47\n\ud835\udc61=1\ud835\udf02\ud835\udc61\ud835\udc45\u00b9x\ud835\udc61\u00ba\n\u00cd\ud835\udc47\n\ud835\udc61=1\ud835\udf02\ud835\udc61!\n=\u00cd\ud835\udc47\n\ud835\udc61=1\ud835\udf02\ud835\udc61\ud835\udc38\u00bb\ud835\udc45\u00b9x\ud835\udc61\u00ba\u00bc\n\u00cd\ud835\udc47\n\ud835\udc61=1\ud835\udf02\ud835\udc61=\ud835\udc38\u00bb\ud835\udc45\u00b9x\ud835\udc61\u00ba\u00bc, (12.4.15)\nby Jensen\u2019s inequality (setting \ud835\udc56=\ud835\udc61,\ud835\udefc\ud835\udc56=\ud835\udf02\ud835\udc61\u009d\u00cd\ud835\udc47\n\ud835\udc61=1\ud835\udf02\ud835\udc61in(12.2.3 )) and convexity of \ud835\udc45it\nfollows that\ud835\udc38\u00bb\ud835\udc45\u00b9x\ud835\udc61\u00ba\u00bc\u0015\ud835\udc38\u00bb\ud835\udc45\u00b9\u00afx\u00ba\u00bc, thus\n\ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udf02\ud835\udc61\ud835\udc38\u00bb\ud835\udc45\u00b9x\ud835\udc61\u00ba\u00bc\u0015\ud835\udc47\u00d5\n\ud835\udc61=1\ud835\udf02\ud835\udc61\ud835\udc38\u00bb\ud835\udc45\u00b9\u00afx\u00ba\u00bc. (12.4.16)\nPlugging this into the inequality (12.4.13 )yields the bound\n\u00bb\ud835\udc38\u00bb\u00afx\u00bc\u00bc\u0000\ud835\udc45\u0003\u0014\ud835\udc5f2\u00b8\ud835\udc3f2\u00cd\ud835\udc47\n\ud835\udc61=1\ud835\udf022\n\ud835\udc61\n2\u00cd\ud835\udc47\n\ud835\udc61=1\ud835\udf02\ud835\udc61, (12.4.17)\nwhere\ud835\udc5f2def=kx1\u0000x\u0003k2is a bound on the distance between the initial choice of parameters\nand the final outcome. In short, the speed of convergence depends on how the norm of\nstochastic gradient is bounded ( \ud835\udc3f) and how far away from optimality the initial parameter\nvalue is (\ud835\udc5f). Note that the bound is in terms of \u00afxrather than x\ud835\udc47. This is the case since \u00afxis\na smoothed version of the optimization path. Whenever \ud835\udc5f,\ud835\udc3f, and\ud835\udc47are known we can pick\nthe learning rate \ud835\udf02=\ud835\udc5f\u009d\u00b9\ud835\udc3fp\n\ud835\udc47\u00ba. This yields as upper bound \ud835\udc5f\ud835\udc3f\u009dp\n\ud835\udc47. That is, we converge\nwith rateO\u00b91\u009dp\n\ud835\udc47\u00bato the optimal solution.\n12.4.4StochasticGradients and Finite Samples\nSo far we have played a bit fast and loose when it comes to talking about stochastic gra-\ndient descent. We posited that we draw instances \ud835\udc65\ud835\udc56, typically with labels \ud835\udc66\ud835\udc56from some\ndistribution \ud835\udc5d\u00b9\ud835\udc65,\ud835\udc66\u00baand that we use this to update the model parameters in some man-\nner. In particular, for a finite sample size we simply argued that the discrete distribution\n\ud835\udc5d\u00b9\ud835\udc65,\ud835\udc66\u00ba=1\n\ud835\udc5b\u00cd\ud835\udc5b\n\ud835\udc56=1\ud835\udeff\ud835\udc65\ud835\udc56\u00b9\ud835\udc65\u00ba\ud835\udeff\ud835\udc66\ud835\udc56\u00b9\ud835\udc66\u00baforsomefunctions \ud835\udeff\ud835\udc65\ud835\udc56and\ud835\udeff\ud835\udc66\ud835\udc56allowsustoperformstochas-\ntic gradient descent over it.\nHowever, this is not really what we did. In the toy examples in the current section we\nsimplyaddednoisetoanotherwisenon-stochasticgradient,i.e.,wepretendedtohavepairs\n\u00b9\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56\u00ba. It turns out that this is justified here (see the exercises for a detailed discussion).\nMore troubling is that in all previous discussions we clearly did not do this. Instead we\niteratedoverallinstances exactlyonce . Toseewhythisispreferableconsidertheconverse,\nnamelythatwearesampling \ud835\udc5bobservationsfromthediscretedistribution withreplacement .\nThe probability of choosing an element \ud835\udc56at random is 1\u009d\ud835\udc5b.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb9ebbf7-681d-4ed5-9bae-ceb88f1d46fe": {"__data__": {"id_": "eb9ebbf7-681d-4ed5-9bae-ceb88f1d46fe", "embedding": null, "metadata": {"page_label": "498", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "448358b8-c6d4-4253-9ec9-79e3f79da6df", "node_type": "4", "metadata": {"page_label": "498", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f716552491eb6743e97994b96c850093a22d8c33416db370d9804a034fa653eb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "42c74a1c-18b9-4522-a0ae-a311c9b2ac60", "node_type": "1", "metadata": {"page_label": "498", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "272765b52e4a4724caae7ad8c3c83ee363169da7b31a97776514332c664ce2c9", "class_name": "RelatedNodeInfo"}}, "text": "However, this is not really what we did. In the toy examples in the current section we\nsimplyaddednoisetoanotherwisenon-stochasticgradient,i.e.,wepretendedtohavepairs\n\u00b9\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc56\u00ba. It turns out that this is justified here (see the exercises for a detailed discussion).\nMore troubling is that in all previous discussions we clearly did not do this. Instead we\niteratedoverallinstances exactlyonce . Toseewhythisispreferableconsidertheconverse,\nnamelythatwearesampling \ud835\udc5bobservationsfromthediscretedistribution withreplacement .\nThe probability of choosing an element \ud835\udc56at random is 1\u009d\ud835\udc5b. Thus to choose it atleastonce\nis\n\ud835\udc43\u00b9choose\ud835\udc56\u00ba=1\u0000\ud835\udc43\u00b9omit\ud835\udc56\u00ba=1\u0000\u00b91\u00001\u009d\ud835\udc5b\u00ba\ud835\udc5b\u00191\u0000\ud835\udc52\u00001\u00190.63. (12.4.18)", "mimetype": "text/plain", "start_char_idx": 1589, "end_char_idx": 2255, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "725a1768-01d5-49ad-a6f0-08747594c5ea": {"__data__": {"id_": "725a1768-01d5-49ad-a6f0-08747594c5ea", "embedding": null, "metadata": {"page_label": "499", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db93a9b8-712c-4c28-947d-25cd3b704785", "node_type": "4", "metadata": {"page_label": "499", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b3f927119627ead562fa3b9fffbd7bd7a30f7de0fd98b56a074c29e66f96e1a7", "class_name": "RelatedNodeInfo"}}, "text": "499 Stochastic Gradient Descent\n170Asimilarreasoningshowsthattheprobabilityofpickingsomesample(i.e., trainingexam-\nple)exactly once is given by\n\u0012\ud835\udc5b\n1\u00131\n\ud835\udc5b\u0012\n1\u00001\n\ud835\udc5b\u0013\ud835\udc5b\u00001\n=\ud835\udc5b\n\ud835\udc5b\u00001\u0012\n1\u00001\n\ud835\udc5b\u0013\ud835\udc5b\n\u0019\ud835\udc52\u00001\u00190.37. (12.4.19)\nSampling with replacement leads to an increased variance and decreased data efficiency\nrelative to sampling without replacement . Hence, in practice we perform the latter (and\nthis is the default choice throughout this book). Last note that repeated passes through the\ntraining dataset traverse it in a different random order.\n12.4.5Summary\n\u000fFor convex problems we can prove that for a wide choice of learning rates stochastic\ngradient descent will converge to the optimal solution.\n\u000fFor deep learning this is generally not the case. However, the analysis of convex prob-\nlems gives us useful insight into how to approach optimization, namely to reduce the\nlearning rate progressively, albeit not too quickly.\n\u000fProblems occur when the learning rate is too small or too large. In practice a suitable\nlearning rate is often found only after multiple experiments.\n\u000fWhen there are more examples in the training dataset, it costs more to compute each\niterationforgradientdescent,sostochasticgradientdescentispreferredinthesecases.\n\u000fOptimalityguaranteesforstochasticgradientdescentareingeneralnotavailableinnon-\nconvex cases since the number of local minima that require checking might well be\nexponential.\n12.4.6Exercises\n1.Experiment with different learning rate schedules for stochastic gradient descent and\nwith different numbers of iterations. In particular, plot the distance from the optimal\nsolution\u00b90,0\u00baas a function of the number of iterations.\n2.Prove that for the function \ud835\udc53\u00b9\ud835\udc651,\ud835\udc652\u00ba=\ud835\udc652\n1\u00b82\ud835\udc652\n2adding normal noise to the gradient is\nequivalent to minimizing a loss function \ud835\udc53\u00b9x,w\u00ba=\u00b9\ud835\udc651\u0000\ud835\udc641\u00ba2\u00b82\u00b9\ud835\udc652\u0000\ud835\udc642\u00ba2where x\nis drawn from a normal distribution.\n3.Compareconvergenceofstochasticgradientdescentwhenyousamplefrom f\u00b9\ud835\udc651,\ud835\udc661\u00ba,...,\u00b9\ud835\udc65\ud835\udc5b,\ud835\udc66\ud835\udc5b\u00bag\nwith replacement and when you sample without replacement.\n4.Howwouldyouchangethestochasticgradientdescentsolverifsomegradient(orrather\nsome coordinate associated with it) was consistently larger than all the other gradients?\n5.Assume that \ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc652\u00b91\u00b8sin\ud835\udc65\u00ba. How many local minima does \ud835\udc53have? Can you\nchange\ud835\udc53in such a way that to minimize it one needs to evaluate all the local minima?\nDiscussions170.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2334, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0327a908-0e23-4773-bbcc-da0dbde9dba6": {"__data__": {"id_": "0327a908-0e23-4773-bbcc-da0dbde9dba6", "embedding": null, "metadata": {"page_label": "500", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2244ed92-81c4-4379-a67e-377749487e92", "node_type": "4", "metadata": {"page_label": "500", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4bfb08a437ee8368d25b40dd1c567900c1130b8700d0fbdde00d5890a03e188c", "class_name": "RelatedNodeInfo"}}, "text": "500 Optimization Algorithms\n17112.5MinibatchStochasticGradient Descent\nSo far we encountered two extremes in the approach to gradient-based learning: Section\n12.3usesthefulldatasettocomputegradientsandtoupdateparameters,onepassatatime.\nConversely Section 12.4 processes one training exampleat a time to make progress. Either\nofthemhasitsowndrawbacks. Gradientdescentisnotparticularly datae\ufb00icient whenever\ndataisverysimilar. Stochasticgradientdescentisnotparticularly computationallye\ufb00icient\nsince CPUs and GPUs cannot exploit the full power of vectorization. This suggests that\nthere might be something in between, and in fact, that is what we have been using so far in\nthe examples we discussed.\n12.5.1Vectorizationand Caches\nAt the heart of the decision to use minibatches is computational efficiency. This is most\neasily understood when considering parallelization to multiple GPUs and multiple servers.\nIn this case we need to send at least one image to each GPU. With 8 GPUs per server and\n16 servers we already arrive at a minibatch size no smaller than 128.\nThings are a bit more subtle when it comes to single GPUs or even CPUs. These devices\nhave multiple types of memory, often multiple types of computational units and different\nbandwidth constraints between them. For instance, a CPU has a small number of registers\nand then the L1, L2, and in some cases even L3 cache (which is shared among different\nprocessor cores). These caches are of increasing size and latency (and at the same time\nthey are of decreasing bandwidth). Suffice to say, the processor is capable of performing\nmany more operations than what the main memory interface is able to provide.\nFirst, a 2GHz CPU with 16 cores and AVX-512 vectorization can process up to 2\u0001109\u0001\n16\u000132=1012bytes per second. The capability of GPUs easily exceeds this number by a\nfactor of 100. On the other hand, a midrange server processor might not have much more\nthan 100 GB/s bandwidth, i.e., less than one tenth of what would be required to keep the\nprocessor fed. To make matters worse, not all memory access is created equal: memory\ninterfaces are typically 64 bit wide or wider (e.g., on GPUs up to 384 bit), hence reading a\nsingle byte incurs the cost of a much wider access.\nSecond, there is significant overhead for the first access whereas sequential access is rela-\ntivelycheap(thisisoftencalledaburstread). Therearemanymorethingstokeepinmind,\nsuch as caching when we have multiple sockets, chiplets, and other structures. See this\nWikipedia article171for a more in-depth discussion.\nThe way to alleviate these constraints is to use a hierarchy of CPU caches that are actu-\nally fast enough to supply the processor with data. This is thedriving force behind batch-\ning in deep learning. To keep matters simple, consider matrix-matrix multiplication, say\nA=BC. We have a number of options for calculating A. For instance, we could try the\nfollowing:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2918, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2864153e-8de1-45d1-b1e3-ed79efa1677a": {"__data__": {"id_": "2864153e-8de1-45d1-b1e3-ed79efa1677a", "embedding": null, "metadata": {"page_label": "501", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7aa6b92-9474-47de-89ee-f40f48c31bb0", "node_type": "4", "metadata": {"page_label": "501", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "49de3197af80453f68df09384a47d813f279ca6be5218896f93bb22ea1d016b6", "class_name": "RelatedNodeInfo"}}, "text": "501 Minibatch Stochastic Gradient Descent\n1.We could compute A\ud835\udc56\ud835\udc57=B\ud835\udc56,:C:,\ud835\udc57, i.e., we could compute it elementwise by means of\ndot products.\n2.We could compute A:,\ud835\udc57=BC :,\ud835\udc57, i.e., we could compute it one column at a time.\nLikewise we could compute Aone row A\ud835\udc56,:at a time.\n3.We could simply compute A=BC.\n4.We could break BandCinto smaller block matrices and compute Aone block at a\ntime.\nIf we follow the first option, we will need to copy one row and one column vector into the\nCPUeachtimewewanttocomputeanelement A\ud835\udc56\ud835\udc57. Evenworse,duetothefactthatmatrix\nelements are aligned sequentially we are thus required to access many disjoint locations\nfor one of the two vectors as we read them from memory. The second option is much\nmore favorable. In it, we are able to keep the column vector C:,\ud835\udc57in the CPU cache while\nwe keep on traversing through B. This halves the memory bandwidth requirement with\ncorrespondingly faster access. Of course, option 3 is most desirable. Unfortunately, most\nmatricesmightnotentirelyfitintocache(thisiswhatwearediscussingafterall). However,\noption4offersapracticallyusefulalternative: wecanmoveblocksofthematrixintocache\nand multiply them locally. Optimized libraries take care of this for us. Let\u2019s have a look at\nhow efficient these operations are in practice.\nBeyondcomputationalefficiency,theoverheadintroducedbyPythonandbythedeeplearn-\ning framework itself is considerable. Recall that each time we execute a command the\nPython interpreter sends a command to the MXNet engine which needs to insert it into\nthe computational graph and deal with it during scheduling. Such overhead can be quite\ndetrimental. In short, it is highly advisable to use vectorization (and matrices) whenever\npossible.\n%matplotlib inline\nimport time\nimport numpy asnp\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\nA=torch .zeros( 256,256)\nB=torch .randn( 256,256)\nC=torch .randn( 256,256)\nSince we will benchmark the running time frequently in the rest of the book, let\u2019s define a\ntimer.\nclass Timer :#@save\n\"\"\"Record multiple running times.\"\"\"\ndef __init__ (self ):\nself .times =[]\nself .start()\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2140, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea50337c-dfe9-4320-bb0e-63a3b7e41c49": {"__data__": {"id_": "ea50337c-dfe9-4320-bb0e-63a3b7e41c49", "embedding": null, "metadata": {"page_label": "502", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50cf0158-ef10-401b-ba59-5729777cdfef", "node_type": "4", "metadata": {"page_label": "502", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1bfdfe032d32b3185cf076ed7f54819e7888d2dde46e86296c06bfb2346d14f0", "class_name": "RelatedNodeInfo"}}, "text": "502 Optimization Algorithms\n(continued from previous page)\ndef start (self ):\n\"\"\"Start the timer.\"\"\"\nself .tik =time .time()\ndef stop (self ):\n\"\"\"Stop the timer and record the time in a list.\"\"\"\nself .times .append(time .time() -self .tik)\nreturn self .times[ -1]\ndef avg(self ):\n\"\"\"Return the average time.\"\"\"\nreturn sum(self .times) /len(self .times)\ndef sum(self ):\n\"\"\"Return the sum of time.\"\"\"\nreturn sum(self .times)\ndef cumsum (self ):\n\"\"\"Return the accumulated time.\"\"\"\nreturn np.array( self .times) .cumsum() .tolist()\ntimer =Timer()\nElement-wiseassignmentsimplyiteratesoverallrowsandcolumnsof BandCrespectively\nto assign the value to A.\n# Compute A = BC one element at a time\ntimer .start()\nfor iinrange (256):\nfor jinrange (256):\nA[i, j] =torch .dot(B[i, :], C[:, j])\ntimer .stop()\n1.7845737934112549\nA faster strategy is to perform column-wise assignment.\n# Compute A = BC one column at a time\ntimer .start()\nfor jinrange (256):\nA[:, j] =torch .mv(B, C[:, j])\ntimer .stop()\n0.06541275978088379\nLast, the most effective manner is to perform the entire operation in one block. Note that\nmultiplyinganytwomatrices B2R\ud835\udc5a\u0002\ud835\udc5bandC2R\ud835\udc5b\u0002\ud835\udc5dtakesapproximately 2\ud835\udc5a\ud835\udc5b\ud835\udc5dfloating\npointoperations,whenscalarmultiplicationandadditionarecountedasseparateoperations", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1251, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6f91792-7378-4dee-af67-2e5e9ef0809f": {"__data__": {"id_": "d6f91792-7378-4dee-af67-2e5e9ef0809f", "embedding": null, "metadata": {"page_label": "503", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "186458d7-dbad-4a6e-a6a6-e6ac1949dd0a", "node_type": "4", "metadata": {"page_label": "503", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6da59c999a273ff50c1f45eae752a5999a8c2c53a90ddcb0428e9b9b9463a6c4", "class_name": "RelatedNodeInfo"}}, "text": "503 Minibatch Stochastic Gradient Descent\n(fused in practice). Thus, multiplying two 256\u0002256matrices takes 0.03billion floating\npoint operations. Let\u2019s see what the respective speed of the operations is.\n# Compute A = BC in one go\ntimer .start()\nA=torch .mm(B, C)\ntimer .stop()\ngigaflops =[0.03 /ifor iintimer .times]\nprint (f'performance in Gigaflops: element {gigaflops[ 0]:.3f},'\nf'column {gigaflops[ 1]:.3f}, full {gigaflops[ 2]:.3f}')\nperformance inGigaflops: element 0.017 , column 0.459 , full 51.633\n12.5.2Minibatches\nIn the past we took it for granted that we would read minibatches of data rather than single\nobservations to update parameters. We now give a brief justification for it. Processing sin-\ngle observations requires us to perform many single matrix-vector (or even vector-vector)\nmultiplications, whichisquiteexpensiveandwhichincursasignificantoverheadonbehalf\noftheunderlyingdeeplearningframework. Thisappliesbothtoevaluatinganetworkwhen\nappliedtodata(oftenreferredtoasinference)andwhencomputinggradientstoupdatepa-\nrameters. That is, this applies whenever we perform w w\u0000\ud835\udf02\ud835\udc61g\ud835\udc61where\ng\ud835\udc61=\ud835\udf15w\ud835\udc53\u00b9x\ud835\udc61,w\u00ba (12.5.1)\nWecanincreasethe computational efficiencyofthisoperationbyapplyingittoaminibatch\nof observations at a time. That is, we replace the gradient g\ud835\udc61over a single observation by\none over a small batch\ng\ud835\udc61=\ud835\udf15w1\njB\ud835\udc61j\u00d5\n\ud835\udc562B\ud835\udc61\ud835\udc53\u00b9x\ud835\udc56,w\u00ba (12.5.2)\nLet\u2019sseewhatthisdoestothestatisticalpropertiesof g\ud835\udc61: sinceboth x\ud835\udc61andalsoallelements\noftheminibatchB\ud835\udc61aredrawnuniformlyatrandomfromthetrainingset,theexpectationof\nthe gradient remains unchanged. The variance, on the other hand, is reduced significantly.\nSince the minibatch gradient is composed of \ud835\udc4fdef=jB\ud835\udc61jindependent gradients which are\nbeingaveraged,itsstandarddeviationisreducedbyafactorof \ud835\udc4f\u00001\n2. This,byitself,isagood\nthing,sinceitmeansthattheupdatesaremorereliablyalignedwiththefullgradient.\nNaively this would indicate that choosing a large minibatch B\ud835\udc61would be universally desir-\nable. Alas,aftersomepoint,theadditionalreductioninstandarddeviationisminimalwhen\ncompared to the linear increase in computational cost. In practice we pick a minibatch that\nislargeenoughtooffergoodcomputationalefficiencywhilestillfittingintothememoryof\na GPU. To illustrate the savings let\u2019s have a look at some code. In it we perform the same\nmatrix-matrix multiplication, but this time broken up into \u201cminibatches\u201d of 64 columns at\na time.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1671e28-32c0-4dd4-b214-6ec152d851bb": {"__data__": {"id_": "e1671e28-32c0-4dd4-b214-6ec152d851bb", "embedding": null, "metadata": {"page_label": "504", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d9476070-4add-4a0c-9c6d-80b1e3f6ade4", "node_type": "4", "metadata": {"page_label": "504", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "504663995cdef60be89ea4135591d4997a5065dd24c5bc5829df1da81164d94e", "class_name": "RelatedNodeInfo"}}, "text": "504 Optimization Algorithms\n172timer .start()\nfor jinrange (0,256,64):\nA[:, j:j +64]=torch .mm(B, C[:, j:j +64])\ntimer .stop()\nprint (f'performance in Gigaflops: block {0.03 /timer .times[ 3]:.3f}')\nperformance inGigaflops: block 37.640\nAs we can see, the computation on the minibatch is essentially as efficient as on the full\nmatrix. A word of caution is in order. In Section 8.5 we used a type of regularization that\nwas heavily dependent on the amount of variance in a minibatch. As we increase the latter,\nthe variance decreases and with it the benefit of the noise-injection due to batch normal-\nization. See e.g., Ioffe ( 2017) for details on how to rescale and compute the appropriate\nterms.\n12.5.3Readingthe Dataset\nLet\u2019s have a look at how minibatches are efficiently generated from data. In the following\nwe use a dataset developed by NASA to test the wing noise from different aircraft172\nto compare these optimization algorithms. For convenience we only use the first 1,500\nexamples. Thedataiswhitenedforpreprocessing,i.e.,weremovethemeanandrescalethe\nvariance to 1per coordinate.\n#@save\nd2l.DATA_HUB[ 'airfoil ']=(d2l .DATA_URL +'airfoil_self_noise.dat ',\n'76e5be1548fd8222e5074cf0faae75edff8cf93f ')\n#@save\ndef get_data_ch11 (batch_size =10, n=1500 ):\ndata =np.genfromtxt(d2l .download( 'airfoil '),\ndtype =np.float32, delimiter ='\\t')\ndata =torch .from_numpy((data -data .mean(axis =0))/data .std(axis =0))\ndata_iter =d2l.load_array((data[:n, : -1], data[:n, -1]),\nbatch_size, is_train =True )\nreturn data_iter, data .shape[ 1]-1\n12.5.4Implementation fromScratch\nRecall the minibatch stochastic gradient descent implementation from Section 3.4 . In the\nfollowing we provide a slightly more general implementation. For convenience it has the\nsame call signature as the other optimization algorithms introduced later in this chapter.\nSpecifically, we add the status input states and place the hyperparameter in dictionary\nhyperparams . Inaddition, wewillaveragethelossofeachminibatchexampleinthetrain-\ning function, so the gradient in the optimization algorithm does not need to be divided by\nthe batch size.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2122, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91648a46-44bb-4961-ac38-4c2a0af334ef": {"__data__": {"id_": "91648a46-44bb-4961-ac38-4c2a0af334ef", "embedding": null, "metadata": {"page_label": "505", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "826de6dd-2bc5-4667-9e86-98f47b9db291", "node_type": "4", "metadata": {"page_label": "505", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "07e7007fdfc94f592d3c207f634d3bff3b4e4c21d015065ac9f6cb8bad6b9e65", "class_name": "RelatedNodeInfo"}}, "text": "505 Minibatch Stochastic Gradient Descent\ndef sgd(params, states, hyperparams):\nfor pinparams:\np.data .sub_(hyperparams[ 'lr']*p.grad)\np.grad .data .zero_()\nNext,weimplementagenerictrainingfunctiontofacilitatetheuseoftheotheroptimization\nalgorithms introduced later in this chapter. It initializes a linear regression model and can\nbe used to train the model with minibatch stochastic gradient descent and other algorithms\nintroduced subsequently.\n#@save\ndef train_ch11 (trainer_fn, states, hyperparams, data_iter,\nfeature_dim, num_epochs =2):\n# Initialization\nw=torch .normal(mean =0.0, std =0.01 , size =(feature_dim, 1),\nrequires_grad =True )\nb=torch .zeros(( 1), requires_grad =True )\nnet, loss =lambda X: d2l .linreg(X, w, b), d2l .squared_loss\n# Train\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[0, num_epochs], ylim =[0.22 ,0.35 ])\nn, timer =0, d2l .Timer()\nfor _inrange (num_epochs):\nfor X, y indata_iter:\nl=loss(net(X), y) .mean()\nl.backward()\ntrainer_fn([w, b], states, hyperparams)\nn+=X.shape[ 0]\nifn%200 ==0:\ntimer .stop()\nanimator .add(n /X.shape[ 0]/len(data_iter),\n(d2l .evaluate_loss(net, data_iter, loss),))\ntimer .start()\nprint (f'loss: {animator .Y[0][-1]:.3f},{timer .sum() /num_epochs :.3f}sec/\n\u21a9!epoch ')\nreturn timer .cumsum(), animator .Y[0]\nLet\u2019s see how optimization proceeds for batch gradient descent. This can be achieved by\nsetting the minibatch size to 1500 (i.e., to the total number of examples). As a result the\nmodel parameters are updated only once per epoch. There is little progress. In fact, after 6\nsteps progress stalls.\ndef train_sgd (lr, batch_size, num_epochs =2):\ndata_iter, feature_dim =get_data_ch11(batch_size)\nreturn train_ch11(\nsgd, None , {'lr': lr}, data_iter, feature_dim, num_epochs)\ngd_res =train_sgd( 1,1500 ,10)\nloss: 0.247 ,0.020 sec/epoch", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb0379aa-6e9e-47aa-8aeb-9b39488f12e7": {"__data__": {"id_": "bb0379aa-6e9e-47aa-8aeb-9b39488f12e7", "embedding": null, "metadata": {"page_label": "506", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0fc3d9ed-ea21-4eb6-a4ce-13eeb347714f", "node_type": "4", "metadata": {"page_label": "506", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bede2f24331abe3cf2b6e6303b9014f3dfbec5312413a9ba4ca3cae509333de4", "class_name": "RelatedNodeInfo"}}, "text": "506 Optimization Algorithms\nWhen the batch size equals 1, we use stochastic gradient descent for optimization. For\nsimplicityofimplementationwepickedaconstant(albeitsmall)learningrate. Instochastic\ngradient descent, the model parameters are updated whenever an example is processed. In\nourcasethisamountsto1500updatesperepoch. Aswecansee, thedeclineinthevalueof\ntheobjectivefunctionslowsdownafteroneepoch. Althoughboththeproceduresprocessed\n1500 examples within one epoch, stochastic gradient descent consumes more time than\ngradient descent in our experiment. This is because stochastic gradient descent updated\nthe parameters more frequently and since it is less efficient to process single observations\none at a time.\nsgd_res =train_sgd( 0.005 ,1)\nloss: 0.245 ,0.685 sec/epoch\nFinally, when the batch size equals 100, we use minibatch stochastic gradient descent for\noptimization. The time required per epoch is shorter than the time needed for stochastic\ngradient descent and the time for batch gradient descent.\nmini1_res =train_sgd( .4,100)\nloss: 0.246 ,0.025 sec/epoch\nReducing the batch size to 10, the time for each epoch increases because the workload for\neach batch is less efficient to execute.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d57b330d-7b84-4fed-ab88-a39b05199ac3": {"__data__": {"id_": "d57b330d-7b84-4fed-ab88-a39b05199ac3", "embedding": null, "metadata": {"page_label": "507", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad54a376-98d7-4605-8568-c5baa0b63502", "node_type": "4", "metadata": {"page_label": "507", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a5aedb9aadcda012bb2e3b0a5cf9a91581c9c593dd33aaf7f3d23f284320e8bc", "class_name": "RelatedNodeInfo"}}, "text": "507 Minibatch Stochastic Gradient Descent\nmini2_res =train_sgd( .05,10)\nloss: 0.246 ,0.090 sec/epoch\nNow we can compare the time vs. loss for the previous four experiments. As can be seen,\nalthough stochastic gradient descent converges faster than GD in terms of number of ex-\namplesprocessed,itusesmoretimetoreachthesamelossthanGDbecausecomputingthe\ngradient example by example is not as efficient. Minibatch stochastic gradient descent is\nable to trade-off convergence speed and computation efficiency. A minibatch size of 10 is\nmore efficient than stochastic gradient descent; a minibatch size of 100 even outperforms\nGD in terms of runtime.\nd2l.set_figsize([ 6,3])\nd2l.plot( *list (map(list ,zip(gd_res, sgd_res, mini1_res, mini2_res))),\n'time (sec) ','loss ', xlim =[1e-2 ,10],\nlegend =['gd','sgd','batch size=100 ','batch size=10 '])\nd2l.plt.gca() .set_xscale( 'log')\n12.5.5ConciseImplementation\nInGluon,wecanusethe Trainer classtocalloptimizationalgorithms. Thisisusedtoim-\nplementagenerictrainingfunction. Wewillusethisthroughoutthecurrentchapter.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1055, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b494b37a-c2e4-4aed-a483-daaebbff3339": {"__data__": {"id_": "b494b37a-c2e4-4aed-a483-daaebbff3339", "embedding": null, "metadata": {"page_label": "508", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6a52ceb-fbdf-4c17-87ec-424a832afab4", "node_type": "4", "metadata": {"page_label": "508", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "03cf50a8e44c348faf1c46296e041a65043beb642cb2d0b24d104dab44918ca0", "class_name": "RelatedNodeInfo"}}, "text": "508 Optimization Algorithms\n#@save\ndef train_concise_ch11 (trainer_fn, hyperparams, data_iter, num_epochs =4):\n# Initialization\nnet =nn.Sequential(nn .Linear( 5,1))\ndef init_weights (module):\niftype (module) ==nn.Linear:\ntorch .nn.init .normal_(module .weight, std =0.01 )\nnet.apply(init_weights)\noptimizer =trainer_fn(net .parameters(), **hyperparams)\nloss =nn.MSELoss(reduction ='none ')\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[0, num_epochs], ylim =[0.22 ,0.35 ])\nn, timer =0, d2l .Timer()\nfor _inrange (num_epochs):\nfor X, y indata_iter:\noptimizer .zero_grad()\nout =net(X)\ny=y.reshape(out .shape)\nl=loss(out, y)\nl.mean() .backward()\noptimizer .step()\nn+=X.shape[ 0]\nifn%200 ==0:\ntimer .stop()\n# `MSELoss` computes squared error without the 1/2 factor\nanimator .add(n /X.shape[ 0]/len(data_iter),\n(d2l .evaluate_loss(net, data_iter, loss) /2,))\ntimer .start()\nprint (f'loss: {animator .Y[0][-1]:.3f},{timer .sum() /num_epochs :.3f}sec/\n\u21a9!epoch ')\nUsing Gluon to repeat the last experiment shows identical behavior.\ndata_iter, _ =get_data_ch11( 10)\ntrainer =torch .optim .SGD\ntrain_concise_ch11(trainer, { 'lr':0.01 }, data_iter)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80b0c327-d722-465e-b108-3ce73792be36": {"__data__": {"id_": "80b0c327-d722-465e-b108-3ce73792be36", "embedding": null, "metadata": {"page_label": "509", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a4dba34-81d2-43f2-8785-c1faf3602d18", "node_type": "4", "metadata": {"page_label": "509", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "001e2fa1dd4f6ac9daa9d0d1ed97b18c2911438c4006d9983ab09c1f841098cf", "class_name": "RelatedNodeInfo"}}, "text": "509 Minibatch Stochastic Gradient Descent\nloss: 0.243 ,0.096 sec/epoch\n12.5.6Summary\n\u000fVectorization makes code more efficient due to reduced overhead arising from the deep\nlearning framework and due to better memory locality and caching on CPUs and\nGPUs.\n\u000fThereisatrade-offbetweenstatisticalefficiencyarisingfromstochasticgradientdescent\nand computational efficiency arising from processing large batches of data at a time.\n\u000fMinibatch stochastic gradient descent offers the best of both worlds: computational and\nstatistical efficiency.\n\u000fInminibatchstochasticgradientdescentweprocessbatchesofdataobtainedbyarandom\npermutation of the training data (i.e., each observation is processed only once per\nepoch, albeit in random order).\n\u000fIt is advisable to decay the learning rates during training.\n\u000fIngeneral,minibatchstochasticgradientdescentisfasterthanstochasticgradientdescent\nand gradient descent for convergence to a smaller risk, when measured in terms of\nclock time.\n12.5.7Exercises\n1.Modify the batch size and learning rate and observe the rate of decline for the value of\nthe objective function and the time consumed in each epoch.\n2.Read the MXNet documentation and use the Trainer class set_learning_rate func-\ntion to reduce the learning rate of the minibatch stochastic gradient descent to 1/10 of\nits previous value after each epoch.\n3.Compareminibatchstochasticgradientdescentwithavariantthatactually sampleswith\nreplacement from the training set. What happens?\n4.An evil genie replicates your dataset without telling you (i.e., each observation occurs\ntwice and your dataset grows to twice its original size, but nobody told you). How does", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ed19751-2b57-4664-b81f-d5a182cb44c5": {"__data__": {"id_": "6ed19751-2b57-4664-b81f-d5a182cb44c5", "embedding": null, "metadata": {"page_label": "510", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "48bb094f-fbf1-4362-85cd-3dc306d3905e", "node_type": "4", "metadata": {"page_label": "510", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "326c8794d65923350c0809d9fc993b34d600eea6db56984931f9accfbd83c37d", "class_name": "RelatedNodeInfo"}}, "text": "510 Optimization Algorithms\n173the behavior of stochastic gradient descent, minibatch stochastic gradient descent and\nthat of gradient descent change?\nDiscussions173.\n12.6Momentum\nInSection 12.4 we reviewed what happens when performing stochastic gradient descent,\ni.e., when performing optimization where only a noisy variant of the gradient is available.\nInparticular,wenoticedthatfornoisygradientsweneedtobeextracautiouswhenitcomes\ntochoosingthe learning rate inthe faceof noise. If wedecrease it too rapidly, convergence\nstalls. Ifwearetoolenient,wefailtoconvergetoagoodenoughsolutionsincenoisekeeps\non driving us away from optimality.\n12.6.1Basics\nIn this section, we will explore more effective optimization algorithms, especially for cer-\ntain types of optimization problems that are common in practice.\nLeaky Averages\nThe previous section saw us discussing minibatch SGD as a means for accelerating com-\nputation. It also had the nice side-effect that averaging gradients reduced the amount of\nvariance. The minibatch stochastic gradient descent can be calculated by:\ng\ud835\udc61,\ud835\udc61\u00001=\ud835\udf15w1\njB\ud835\udc61j\u00d5\n\ud835\udc562B\ud835\udc61\ud835\udc53\u00b9x\ud835\udc56,w\ud835\udc61\u00001\u00ba=1\njB\ud835\udc61j\u00d5\n\ud835\udc562B\ud835\udc61h\ud835\udc56,\ud835\udc61\u00001. (12.6.1)\nTo keep the notation simple, here we used h\ud835\udc56,\ud835\udc61\u00001=\ud835\udf15w\ud835\udc53\u00b9x\ud835\udc56,w\ud835\udc61\u00001\u00baas the stochastic gra-\ndient descent for sample \ud835\udc56using the weights updated at time \ud835\udc61\u00001. It would be nice if we\ncould benefit from the effect of variance reduction even beyond averaging gradients on a\nminibatch. One option to accomplish this task is to replace the gradient computation by a\n\u201cleaky average\u201d:\nv\ud835\udc61=\ud835\udefdv\ud835\udc61\u00001\u00b8g\ud835\udc61,\ud835\udc61\u00001 (12.6.2)\nforsome\ud835\udefd2\u00b90,1\u00ba. Thiseffectivelyreplacestheinstantaneousgradientbyonethatisbeen\naveraged over multiple pastgradients. vis calledvelocity. It accumulates past gradients\nsimilar to how a heavy ball rolling down the objective function landscape integrates over\npastforces. Toseewhatishappeninginmoredetaillet\u2019sexpand v\ud835\udc61recursivelyinto\nv\ud835\udc61=\ud835\udefd2v\ud835\udc61\u00002\u00b8\ud835\udefdg\ud835\udc61\u00001,\ud835\udc61\u00002\u00b8g\ud835\udc61,\ud835\udc61\u00001=...,=\ud835\udc61\u00001\u00d5\n\ud835\udf0f=0\ud835\udefd\ud835\udf0fg\ud835\udc61\u0000\ud835\udf0f,\ud835\udc61\u0000\ud835\udf0f\u00001. (12.6.3)\nLarge\ud835\udefdamounts to a long-range average, whereas small \ud835\udefdamounts to only a slight correc-\ntion relative to a gradient method. The new gradient replacement no longer points into the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "774b5dd4-1626-4e5f-b00c-1ff810728790": {"__data__": {"id_": "774b5dd4-1626-4e5f-b00c-1ff810728790", "embedding": null, "metadata": {"page_label": "511", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37e60bff-86b3-4035-9306-6324a11ab1d4", "node_type": "4", "metadata": {"page_label": "511", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f2916e3f979826af8682ff68c9597535569942810f01d30576ab20eac04df362", "class_name": "RelatedNodeInfo"}}, "text": "511 Momentum\n174direction of steepest descent on a particular instance any longer but rather in the direction\nof a weighted average of past gradients. This allows us to realize most of the benefits of\naveraging over a batch without the cost of actually computing the gradients on it. We will\nrevisit this averaging procedure in more detail later.\nTheabovereasoningformedthebasisforwhatisnowknownas accelerated gradientmeth-\nods, such as gradients with momentum. They enjoy the additional benefit of being much\nmore effective in cases where the optimization problem is ill-conditioned (i.e., where there\nare some directions where progress is much slower than in others, resembling a narrow\ncanyon). Furthermore, they allow us to average over subsequent gradients to obtain more\nstable directions of descent. Indeed, the aspect of acceleration even for noise-free convex\nproblemsisoneofthekeyreasonswhymomentumworksandwhyitworkssowell.\nAsonewouldexpect,duetoitsefficacymomentumisawell-studiedsubjectinoptimization\nfor deep learning and beyond. See e.g., the beautiful expository article174by Goh ( 2017)\nfor an in-depth analysis and interactive animation. It was proposed by Polyak ( 1964). Nes-\nterov (2018) has a detailed theoretical discussion in the context of convex optimization.\nMomentum in deep learning has been known to be beneficial for a long time. See e.g., the\ndiscussion by Sutskever etal.(2013) for details.\nAn Ill-conditioned Problem\nTo get a better understanding of the geometric properties of the momentum method we\nrevisit gradient descent, albeit with a significantly less pleasant objective function. Recall\nthatinSection12.3 weused\ud835\udc53\u00b9x\u00ba=\ud835\udc652\n1\u00b82\ud835\udc652\n2,i.e.,amoderatelydistortedellipsoidobjective.\nWe distort this function further by stretching it out in the \ud835\udc651direction via\n\ud835\udc53\u00b9x\u00ba=0.1\ud835\udc652\n1\u00b82\ud835\udc652\n2. (12.6.4)\nAsbefore\ud835\udc53hasitsminimumat \u00b90,0\u00ba. Thisfunctionis veryflatinthedirectionof \ud835\udc651. Let\u2019s\nsee what happens when we perform gradient descent as before on this new function. We\npick a learning rate of 0.4.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\neta =0.4\ndef f_2d (x1, x2):\nreturn 0.1 *x1**2+2*x2**2\ndef gd_2d (x1, x2, s1, s2):\nreturn (x1 -eta *0.2 *x1, x2 -eta *4*x2, 0,0)\nd2l.show_trace_2d(f_2d, d2l .train_2d(gd_2d))\nepoch 20, x1: -0.943467 , x2: -0.000073\nBy construction, the gradient in the \ud835\udc652direction is muchhigher and changes much more", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b0c0a51-8db8-4906-9e79-cdf48ca53e1b": {"__data__": {"id_": "8b0c0a51-8db8-4906-9e79-cdf48ca53e1b", "embedding": null, "metadata": {"page_label": "512", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "508e586f-4ed6-4ad7-bca8-6914181a85ee", "node_type": "4", "metadata": {"page_label": "512", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b422aa094be38edee280d1bdf59b43a2c6dffcc77d765cb071faad857b9ed346", "class_name": "RelatedNodeInfo"}}, "text": "512 Optimization Algorithms\nrapidly than in the horizontal \ud835\udc651direction. Thus we are stuck between two undesirable\nchoices: if we pick a small learning rate we ensure that the solution does not diverge in\nthe\ud835\udc652direction but we are saddled with slow convergence in the \ud835\udc651direction. Conversely,\nwith a large learning rate we progress rapidly in the \ud835\udc651direction but diverge in \ud835\udc652. The\nexample below illustrates what happens even after a slight increase in learning rate from\n0.4to0.6. Convergenceinthe \ud835\udc651directionimprovesbuttheoverallsolutionqualityismuch\nworse.\neta =0.6\nd2l.show_trace_2d(f_2d, d2l .train_2d(gd_2d))\nepoch 20, x1: -0.387814 , x2: -1673.365109\nThe Momentum Method\nThe momentum method allows us to solve the gradient descent problem described above.\nLooking at the optimization trace above we might intuit that averaging gradients over the\npast would work well. After all, in the \ud835\udc651direction this will aggregate well-aligned gradi-\nents, thus increasing the distance we cover with every step. Conversely, in the \ud835\udc652direction\nwhere gradients oscillate, an aggregate gradient will reduce step size due to oscillations\nthat cancel each other out. Using v\ud835\udc61instead of the gradient g\ud835\udc61yields the following update\nequations:\nv\ud835\udc61 \ud835\udefdv\ud835\udc61\u00001\u00b8g\ud835\udc61,\ud835\udc61\u00001,\nx\ud835\udc61 x\ud835\udc61\u00001\u0000\ud835\udf02\ud835\udc61v\ud835\udc61.(12.6.5)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45cd4fa4-d72c-45f2-9790-7b8ba372fb94": {"__data__": {"id_": "45cd4fa4-d72c-45f2-9790-7b8ba372fb94", "embedding": null, "metadata": {"page_label": "513", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4aaa9779-ea69-4ec3-b054-8befabc9664d", "node_type": "4", "metadata": {"page_label": "513", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ec5d57134db330e2b003f7cfd61a5ad2dcc394f636282033c432ab106ad14667", "class_name": "RelatedNodeInfo"}}, "text": "513 Momentum\nNote that for \ud835\udefd=0we recover regular gradient descent. Before delving deeper into the\nmathematical properties let\u2019s have a quick look at how the algorithm behaves in prac-\ntice.\ndef momentum_2d (x1, x2, v1, v2):\nv1=beta *v1+0.2 *x1\nv2=beta *v2+4*x2\nreturn x1-eta *v1, x2 -eta *v2, v1, v2\neta, beta =0.6,0.5\nd2l.show_trace_2d(f_2d, d2l .train_2d(momentum_2d))\nepoch 20, x1: 0.007188 , x2: 0.002553\nAs we can see, even with the same learning rate that we used before, momentum still con-\nverges well. Let\u2019s see what happens when we decrease the momentum parameter. Halving\nit to\ud835\udefd=0.25leads to a trajectory that barely converges at all. Nonetheless, it is a lot better\nthan without momentum (when the solution diverges).\neta, beta =0.6,0.25\nd2l.show_trace_2d(f_2d, d2l .train_2d(momentum_2d))\nepoch 20, x1: -0.126340 , x2: -0.186632\nNote that we can combine momentum with stochastic gradient descent and in particular,\nminibatch stochastic gradient descent. The only change is that in that case we replace the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad281a15-c177-4155-a6ea-442c1ea0eaf3": {"__data__": {"id_": "ad281a15-c177-4155-a6ea-442c1ea0eaf3", "embedding": null, "metadata": {"page_label": "514", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8ccf932-3def-4315-983f-c35f7a8be0b1", "node_type": "4", "metadata": {"page_label": "514", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "44aed356b57ec2710ef9350cca4e8a3c8d8c3e0626b0e4247020696cc17f2827", "class_name": "RelatedNodeInfo"}}, "text": "514 Optimization Algorithms\ngradients g\ud835\udc61,\ud835\udc61\u00001withg\ud835\udc61. Last, for convenience we initialize v0=0at time\ud835\udc61=0. Let\u2019s\nlook at what leaky averaging actually does to the updates.\nEffectiveSampleWeight\nRecall that v\ud835\udc61=\u00cd\ud835\udc61\u00001\n\ud835\udf0f=0\ud835\udefd\ud835\udf0fg\ud835\udc61\u0000\ud835\udf0f,\ud835\udc61\u0000\ud835\udf0f\u00001. In the limit the terms add up to\u00cd1\n\ud835\udf0f=0\ud835\udefd\ud835\udf0f=1\n1\u0000\ud835\udefd. In\nother words, rather than taking a step of size \ud835\udf02in gradient descent or stochastic gradient\ndescent we take a step of size\ud835\udf02\n1\u0000\ud835\udefdwhile at the same time, dealing with a potentially much\nbetterbehaveddescentdirection. Thesearetwobenefitsinone. Toillustratehowweighting\nbehaves for different choices of \ud835\udefdconsider the diagram below.\nd2l.set_figsize()\nbetas =[0.95 ,0.9,0.6,0]\nfor beta inbetas:\nx=torch .arange( 40).detach() .numpy()\nd2l.plt.plot(x, beta **x, label =f'beta = {beta :.2f}')\nd2l.plt.xlabel( 'time ')\nd2l.plt.legend();\n12.6.2Practical Experiments\nLet\u2019s see how momentum works in practice, i.e., when used within the context of a proper\noptimizer. For this we need a somewhat more scalable implementation.\nImplementation fromScratch\nCompared with (minibatch) stochastic gradient descent the momentum method needs to\nmaintain a set of auxiliary variables, i.e., velocity. It has the same shape as the gradients\n(and variables of the optimization problem). In the implementation below we call these\nvariables states.\ndef init_momentum_states (feature_dim):\nv_w =torch .zeros((feature_dim, 1))\nv_b =torch .zeros( 1)\nreturn (v_w, v_b)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1414, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de27b6f4-2199-4a1a-90f3-3bf53c2a7ac8": {"__data__": {"id_": "de27b6f4-2199-4a1a-90f3-3bf53c2a7ac8", "embedding": null, "metadata": {"page_label": "515", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b17d234b-3de3-4989-9f6c-21e935cbaffd", "node_type": "4", "metadata": {"page_label": "515", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f3ad92c2f5cdebfdbdde8444d661061bced5ac166155cdf17c3ad11cb11e69bc", "class_name": "RelatedNodeInfo"}}, "text": "515 Momentum\ndef sgd_momentum (params, states, hyperparams):\nfor p, v inzip(params, states):\nwith torch .no_grad():\nv[:] =hyperparams[ 'momentum ']*v+p.grad\np[:] -=hyperparams[ 'lr']*v\np.grad .data .zero_()\nLet\u2019s see how this works in practice.\ndef train_momentum (lr, momentum, num_epochs =2):\nd2l.train_ch11(sgd_momentum, init_momentum_states(feature_dim),\n{'lr': lr, 'momentum ': momentum}, data_iter,\nfeature_dim, num_epochs)\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\ntrain_momentum( 0.02 ,0.5)\nloss: 0.245 ,0.153 sec/epoch\nWhen we increase the momentum hyperparameter momentum to 0.9, it amounts to a signif-\nicantly larger effective sample size of1\n1\u00000.9=10. We reduce the learning rate slightly to\n0.01to keep matters under control.\ntrain_momentum( 0.01 ,0.9)\nloss: 0.248 ,0.109 sec/epoch\nReducing the learning rate further addresses any issue of non-smooth optimization prob-\nlems. Setting it to 0.005yields good convergence properties.\ntrain_momentum( 0.005 ,0.9)\nloss: 0.243 ,0.107 sec/epoch", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1017, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a65fdfc-53bd-40fc-a833-88e8728e956c": {"__data__": {"id_": "3a65fdfc-53bd-40fc-a833-88e8728e956c", "embedding": null, "metadata": {"page_label": "516", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5c92d7a3-073d-4feb-b557-efdafb37d046", "node_type": "4", "metadata": {"page_label": "516", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "77bc97a0986fe7f1d7c2230e957bfd892e43941bc151dd0dbfba510d4b4640f1", "class_name": "RelatedNodeInfo"}}, "text": "516 Optimization Algorithms\nConciseImplementation\nThere is very little to do in Gluon since the standard sgdsolver already had momentum\nbuilt in. Setting matching parameters yields a very similar trajectory.\ntrainer =torch .optim .SGD\nd2l.train_concise_ch11(trainer, { 'lr':0.005 ,'momentum ':0.9}, data_iter)\nloss: 0.250 ,0.108 sec/epoch\n12.6.3TheoreticalAnalysis\nSofarthe2Dexampleof \ud835\udc53\u00b9\ud835\udc65\u00ba=0.1\ud835\udc652\n1\u00b82\ud835\udc652\n2seemedrathercontrived. Wewillnowseethat\nthis is actually quite representative of the types of problem one might encounter, at least in\nthe case of minimizing convex quadratic objective functions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 598, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f9d2610-5406-4f42-8ee3-1897f794e964": {"__data__": {"id_": "4f9d2610-5406-4f42-8ee3-1897f794e964", "embedding": null, "metadata": {"page_label": "517", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe38761d-cdcd-4257-999c-78d7448f087c", "node_type": "4", "metadata": {"page_label": "517", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "99cb26c395be2c3cfa989d12aad77d83b48122162cbe092c66dc2b0c0e212c69", "class_name": "RelatedNodeInfo"}}, "text": "517 Momentum\nQuadratic ConvexFunctions\nConsider the function\n\u210e\u00b9x\u00ba=1\n2x>Qx\u00b8x>c\u00b8\ud835\udc4f. (12.6.6)\nThis is a general quadratic function. For positive definite matrices Q\u001f0, i.e., for matrices\nwith positive eigenvalues this has a minimizer at x\u0003=\u0000Q\u00001cwith minimum value \ud835\udc4f\u0000\n1\n2c>Q\u00001c. Hence we can rewrite \u210eas\n\u210e\u00b9x\u00ba=1\n2\u00b9x\u0000Q\u00001c\u00ba>Q\u00b9x\u0000Q\u00001c\u00ba\u00b8\ud835\udc4f\u00001\n2c>Q\u00001c. (12.6.7)\nThe gradient is given by \ud835\udf15x\u210e\u00b9x\u00ba=Q\u00b9x\u0000Q\u00001c\u00ba. That is, it is given by the distance\nbetween xand the minimizer, multiplied by Q. Consequently also the velocity is a linear\ncombination of terms Q\u00b9x\ud835\udc61\u0000Q\u00001c\u00ba.\nSinceQis positive definite it can be decomposed into its eigensystem via Q=O>\ud835\udeb2Ofor\nan orthogonal (rotation) matrix Oand a diagonal matrix \ud835\udeb2of positive eigenvalues. This\nallows us to perform a change of variables from xtozdef=O\u00b9x\u0000Q\u00001c\u00bato obtain a much\nsimplified expression:\n\u210e\u00b9z\u00ba=1\n2z>\ud835\udeb2z\u00b8\ud835\udc4f0. (12.6.8)\nHere\ud835\udc4f0=\ud835\udc4f\u00001\n2c>Q\u00001c. Since Ois only an orthogonal matrix this does not perturb the\ngradients in a meaningful way. Expressed in terms of zgradient descent becomes\nz\ud835\udc61=z\ud835\udc61\u00001\u0000\ud835\udeb2z\ud835\udc61\u00001=\u00b9I\u0000\ud835\udeb2\u00baz\ud835\udc61\u00001. (12.6.9)\nTheimportantfactinthisexpressionisthatgradientdescent doesnotmix betweendifferent\neigenspaces. That is, when expressed in terms of the eigensystem of Qthe optimization\nproblem proceeds in a coordinate-wise manner. This also holds for\nv\ud835\udc61=\ud835\udefdv\ud835\udc61\u00001\u00b8\ud835\udeb2z\ud835\udc61\u00001\nz\ud835\udc61=z\ud835\udc61\u00001\u0000\ud835\udf02\u00b9\ud835\udefdv\ud835\udc61\u00001\u00b8\ud835\udeb2z\ud835\udc61\u00001\u00ba\n=\u00b9I\u0000\ud835\udf02\ud835\udeb2\u00baz\ud835\udc61\u00001\u0000\ud835\udf02\ud835\udefdv\ud835\udc61\u00001.(12.6.10)\nIn doing this we just proved the following theorem: gradient descent with and without\nmomentum for a convex quadratic function decomposes into coordinate-wise optimization\nin the direction of the eigenvectors of the quadratic matrix.\nScalar Functions\nGiven the above result let\u2019s see what happens when we minimize the function \ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udf06\n2\ud835\udc652.\nFor gradient descent we have\n\ud835\udc65\ud835\udc61\u00b81=\ud835\udc65\ud835\udc61\u0000\ud835\udf02\ud835\udf06\ud835\udc65\ud835\udc61=\u00b91\u0000\ud835\udf02\ud835\udf06\u00ba\ud835\udc65\ud835\udc61. (12.6.11)\nWheneverj1\u0000\ud835\udf02\ud835\udf06j<1thisoptimizationconvergesatanexponentialratesinceafter \ud835\udc61steps\nwe have\ud835\udc65\ud835\udc61=\u00b91\u0000\ud835\udf02\ud835\udf06\u00ba\ud835\udc61\ud835\udc650. This shows how the rate of convergence improves initially as", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1905, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c46b4b6b-f01e-4899-9f39-b46f74a538d3": {"__data__": {"id_": "c46b4b6b-f01e-4899-9f39-b46f74a538d3", "embedding": null, "metadata": {"page_label": "518", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac932970-4b56-4655-8609-fcad93ff4662", "node_type": "4", "metadata": {"page_label": "518", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "113ef7c48334773cf20b8cb0e1ce7ca030c6c82c7811a87f04e7fac010421c74", "class_name": "RelatedNodeInfo"}}, "text": "518 Optimization Algorithms\n175we increase the learning rate \ud835\udf02until\ud835\udf02\ud835\udf06=1. Beyond that things diverge and for \ud835\udf02\ud835\udf06> 2the\noptimization problem diverges.\nlambdas =[0.1,1,10,19]\neta =0.1\nd2l.set_figsize(( 6,4))\nfor lam inlambdas:\nt=torch .arange( 20).detach() .numpy()\nd2l.plt.plot(t, ( 1-eta *lam) **t, label =f'lambda = {lam:.2f}')\nd2l.plt.xlabel( 'time ')\nd2l.plt.legend();\nTo analyze convergence in the case of momentum we begin by rewriting the update equa-\ntions in terms of two scalars: one for \ud835\udc65and one for velocity \ud835\udc63. This yields:\n\u0014\ud835\udc63\ud835\udc61\u00b81\n\ud835\udc65\ud835\udc61\u00b81\u0015\n=\u0014\ud835\udefd\ud835\udf06\n\u0000\ud835\udf02\ud835\udefd\u00b91\u0000\ud835\udf02\ud835\udf06\u00ba\u0015 \u0014\ud835\udc63\ud835\udc61\n\ud835\udc65\ud835\udc61\u0015\n=R\u00b9\ud835\udefd,\ud835\udf02,\ud835\udf06\u00ba\u0014\ud835\udc63\ud835\udc61\n\ud835\udc65\ud835\udc61\u0015\n. (12.6.12)\nWe used Rto denote the 2\u00022governing convergence behavior. After \ud835\udc61steps the initial\nchoice\u00bb\ud835\udc630,\ud835\udc650\u00bcbecomes R\u00b9\ud835\udefd,\ud835\udf02,\ud835\udf06\u00ba\ud835\udc61\u00bb\ud835\udc630,\ud835\udc650\u00bc. Hence, it is up to the eigenvalues of Rto\ndetermine the speed of convergence. See the Distill post175of Goh ( 2017) for a great\nanimation and Flammarion and Bach ( 2015) for a detailed analysis. One can show that\n0< \ud835\udf02\ud835\udf06 < 2\u00b82\ud835\udefdvelocity converges. This is a larger range of feasible parameters when\ncompared to 0< \ud835\udf02\ud835\udf06 < 2for gradient descent. It also suggests that in general large values\nof\ud835\udefdare desirable. Further details require a fair amount of technical detail and we suggest\nthat the interested reader consult the original publications.\n12.6.4Summary\n\u000fMomentumreplacesgradientswithaleakyaverageoverpastgradients. Thisaccelerates\nconvergence significantly.\n\u000fItisdesirableforbothnoise-freegradientdescentand(noisy)stochasticgradientdescent.\n\u000fMomentum prevents stalling of the optimization process that is much more likely to\noccur for stochastic gradient descent.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df557906-7bc9-4c37-b375-5e7923984e24": {"__data__": {"id_": "df557906-7bc9-4c37-b375-5e7923984e24", "embedding": null, "metadata": {"page_label": "519", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b864ceb3-a0dd-4467-8acf-6f7b3e6d0f81", "node_type": "4", "metadata": {"page_label": "519", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e3c7d491b5ab0ff77f570f49cd56bf9af99988e97e125b64405054712efbfda1", "class_name": "RelatedNodeInfo"}}, "text": "519 Adagrad\n176\u000fThe effective number of gradients is given by1\n1\u0000\ud835\udefddue to exponentiated downweighting\nof past data.\n\u000fIn the case of convex quadratic problems this can be analyzed explicitly in detail.\n\u000fImplementation is quite straightforward but it requires us to store an additional state\nvector (velocity v).\n12.6.5Exercises\n1.Use other combinations of momentum hyperparameters and learning rates and observe\nand analyze the different experimental results.\n2.Tryoutgradientdescentandmomentumforaquadraticproblemwhereyouhavemulti-\npleeigenvalues, i.e., \ud835\udc53\u00b9\ud835\udc65\u00ba=1\n2\u00cd\n\ud835\udc56\ud835\udf06\ud835\udc56\ud835\udc652\n\ud835\udc56, e.g.,\ud835\udf06\ud835\udc56=2\u0000\ud835\udc56. Plot howthevaluesof \ud835\udc65decrease\nfor the initialization \ud835\udc65\ud835\udc56=1.\n3.Derive minimum value and minimizer for \u210e\u00b9x\u00ba=1\n2x>Qx\u00b8x>c\u00b8\ud835\udc4f.\n4.What changes when we perform stochastic gradient descent with momentum? What\nhappens when we use minibatch stochastic gradient descent with momentum? Experi-\nment with the parameters?\nDiscussions176.\n12.7Adagrad\nLet\u2019s begin by considering learning problems with features that occur infrequently.\n12.7.1SparseFeatures and Learning Rates\nImagine that we are training a language model. To get good accuracy we typically want\nto decrease the learning rate as we keep on training, usually at a rate of O\u00b9\ud835\udc61\u00001\n2\u00baor slower.\nNowconsideramodeltrainingonsparsefeatures,i.e.,featuresthatoccuronlyinfrequently.\nThis is common for natural language, e.g., it is a lot less likely that we will see the word\npreconditioning thanlearning . However,itisalsocommoninotherareassuchascomputa-\ntional advertising and personalized collaborative filtering. After all, there are many things\nthat are of interest only for a small number of people.\nParameters associated with infrequent features only receive meaningful updates whenever\nthese features occur. Given a decreasing learning rate we might end up in a situation\nwhere the parameters for common features converge rather quickly to their optimal values,\nwhereas for infrequent features we are still short of observing them sufficiently frequently\nbefore their optimal values can be determined. In other words, the learning rate either\ndecreases too slowly for frequent features or too quickly for infrequent ones.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f9bc75d-9915-4f23-b4a9-01065081176b": {"__data__": {"id_": "8f9bc75d-9915-4f23-b4a9-01065081176b", "embedding": null, "metadata": {"page_label": "520", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da6bdb74-491c-4e3d-9c6c-94a00ec7ca51", "node_type": "4", "metadata": {"page_label": "520", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "da0bf040d609e47b0accf07cb0fd311f47ebf9ca4f52eb01083395eb8639ec85", "class_name": "RelatedNodeInfo"}}, "text": "520 Optimization Algorithms\nA possible hack to redress this issue would be to count the number of times we see a par-\nticular feature and to use this as a clock for adjusting learning rates. That is, rather than\nchoosing a learning rate of the form \ud835\udf02=\ud835\udf020p\ud835\udc61\u00b8\ud835\udc50we could use \ud835\udf02\ud835\udc56=\ud835\udf020p\n\ud835\udc60\u00b9\ud835\udc56,\ud835\udc61\u00ba\u00b8\ud835\udc50. Here\ud835\udc60\u00b9\ud835\udc56,\ud835\udc61\u00ba\ncounts the number of nonzeros for feature \ud835\udc56that we have observed up to time \ud835\udc61. This is ac-\ntually quite easy to implement at no meaningful overhead. However, it fails whenever we\ndo not quite have sparsity but rather just data where the gradients are often very small and\nonly rarely large. After all, it is unclear where one would draw the line between something\nthat qualifies as an observed feature or not.\nAdagrad by Duchi et al.(2011) addresses this by replacing the rather crude counter \ud835\udc60\u00b9\ud835\udc56,\ud835\udc61\u00ba\nby an aggregate of the squares of previously observed gradients. In particular, it uses\n\ud835\udc60\u00b9\ud835\udc56,\ud835\udc61\u00b81\u00ba=\ud835\udc60\u00b9\ud835\udc56,\ud835\udc61\u00ba\u00b8\u00b9\ud835\udf15\ud835\udc56\ud835\udc53\u00b9x\u00ba\u00ba2asameanstoadjustthelearningrate. Thishastwobenefits:\nfirst, we no longer need to decide just when a gradient is large enough. Second, it scales\nautomatically with the magnitude of the gradients. Coordinates that routinely correspond\nto large gradients are scaled down significantly, whereas others with small gradients re-\nceive a much more gentle treatment. In practice this leads to a very effective optimization\nprocedure for computational advertising and related problems. But this hides some of the\nadditional benefits inherent in Adagrad that are best understood in the context of precondi-\ntioning.\n12.7.2Preconditioning\nConvex optimization problems are good for analyzing the characteristics of algorithms.\nAfter all, for most nonconvex problems it is difficult to derive meaningful theoretical guar-\nantees, but intuition andinsightoften carry over. Let\u2019s look at the problem of minimizing\n\ud835\udc53\u00b9x\u00ba=1\n2x>Qx\u00b8c>x\u00b8\ud835\udc4f.\nAswesawin Section12.6 ,itispossibletorewritethisproblemintermsofitseigendecom-\nposition Q=U>\ud835\udeb2Uto arrive at a much simplified problem where each coordinate can be\nsolved individually:\n\ud835\udc53\u00b9x\u00ba=\u00af\ud835\udc53\u00b9\u00afx\u00ba=1\n2\u00afx>\ud835\udeb2\u00afx\u00b8\u00afc>\u00afx\u00b8\ud835\udc4f. (12.7.1)\nHere we used \u00afx=Uxand consequently \u00afc=Uc. The modified problem has as its min-\nimizer \u00afx=\u0000\ud835\udeb2\u00001\u00afcand minimum value \u00001\n2\u00afc>\ud835\udeb2\u00001\u00afc\u00b8\ud835\udc4f. This is much easier to compute\nsince\ud835\udeb2is a diagonal matrix containing the eigenvalues of Q.\nIf we perturb cslightly we would hope to find only slight changes in the minimizer of \ud835\udc53.\nUnfortunately this is not the case. While slight changes in clead to equally slight changes\nin\u00afc, this is not the case for the minimizer of \ud835\udc53(and of \u00af\ud835\udc53respectively). Whenever the\neigenvalues \ud835\udeb2\ud835\udc56are large we will see only small changes in \u00af\ud835\udc65\ud835\udc56and in the minimum of \u00af\ud835\udc53.\nConversely, for small \ud835\udeb2\ud835\udc56changes in \u00af\ud835\udc65\ud835\udc56can be dramatic. The ratio between the largest and\nthe smallest eigenvalue is called the condition number of an optimization problem.\n\ud835\udf05=\ud835\udeb21\n\ud835\udeb2\ud835\udc51. (12.7.2)\nIftheconditionnumber \ud835\udf05islarge,itisdifficulttosolvetheoptimizationproblemaccurately.\nWe need to ensure that we are careful in getting a large dynamic range of values right. Our", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93fa176e-9cf9-4354-b0bf-198b8059a8a4": {"__data__": {"id_": "93fa176e-9cf9-4354-b0bf-198b8059a8a4", "embedding": null, "metadata": {"page_label": "521", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee113b5d-88b8-4936-8585-e0ca52f133f5", "node_type": "4", "metadata": {"page_label": "521", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "888f3338055afc18e1913901e1a7219972880b2118a75d79042d96317d952b2e", "class_name": "RelatedNodeInfo"}}, "text": "521 Adagrad\nanalysis leads to an obvious, albeit somewhat naive question: couldn\u2019t we simply \u201cfix\u201d the\nproblem by distorting the space such that all eigenvalues are 1. In theory this is quite easy:\nwe only need the eigenvalues and eigenvectors of Qto rescale the problem from xto one\ninzdef=\ud835\udeb21\n2Ux. Inthenewcoordinatesystem x>Qxcouldbesimplifiedto kzk2. Alas,this\nis a rather impractical suggestion. Computing eigenvalues and eigenvectors is in general\nmuchmore expensive than solving the actual problem.\nWhile computing eigenvalues exactly might be expensive, guessing them and computing\nthem even somewhat approximately may already be a lot better than not doing anything at\nall. In particular, we could use the diagonal entries of Qand rescale it accordingly. This is\nmuchcheaper than computing eigenvalues.\n\u02dcQ=diag\u00001\n2\u00b9Q\u00baQdiag\u00001\n2\u00b9Q\u00ba. (12.7.3)\nIn this case we have \u02dcQ\ud835\udc56\ud835\udc57=Q\ud835\udc56\ud835\udc57\u009dp\nQ\ud835\udc56\ud835\udc56Q\ud835\udc57\ud835\udc57and specifically \u02dcQ\ud835\udc56\ud835\udc56=1for all\ud835\udc56. In most cases\nthis simplifies the condition number considerably. For instance, the cases we discussed\npreviously, this would entirely eliminate the problem at hand since the problem is axis\naligned.\nUnfortunately we face yet another problem: in deep learning we typically do not even have\naccess to the second derivative of the objective function: for x2R\ud835\udc51the second derivative\neven on a minibatch may require O\u00b9\ud835\udc512\u00baspace and work to compute, thus making it practi-\ncally infeasible. The ingenious idea of Adagrad is to use a proxy for that elusive diagonal\nof the Hessian that is both relatively cheap to compute and effective\u2014the magnitude of the\ngradient itself.\nIn order to see why this works, let\u2019s look at \u00af\ud835\udc53\u00b9\u00afx\u00ba. We have that\n\ud835\udf15\u00afx\u00af\ud835\udc53\u00b9\u00afx\u00ba=\ud835\udeb2\u00afx\u00b8\u00afc=\ud835\udeb2\u00b9\u00afx\u0000\u00afx0\u00ba, (12.7.4)\nwhere \u00afx0is the minimizer of \u00af\ud835\udc53. Hence the magnitude of the gradient depends both on \ud835\udeb2\nand the distance from optimality. If \u00afx\u0000\u00afx0did not change, this would be all that is needed.\nAfter all, in this case the magnitude of the gradient \ud835\udf15\u00afx\u00af\ud835\udc53\u00b9\u00afx\u00basuffices. Since AdaGrad is a\nstochastic gradient descent algorithm, we will see gradients with nonzero variance even at\noptimality. As a result we can safely use the variance of the gradients as a cheap proxy for\nthe scale of the Hessian. A thorough analysis is beyond the scope of this section (it would\nbe several pages). We refer the reader to ( Duchietal., 2011) for details.\n12.7.3The Algorithm\nLet\u2019s formalize the discussion from above. We use the variable s\ud835\udc61to accumulate past gra-\ndient variance as follows.\ng\ud835\udc61=\ud835\udf15w\ud835\udc59\u00b9\ud835\udc66\ud835\udc61, \ud835\udc53\u00b9x\ud835\udc61,w\u00ba\u00ba,\ns\ud835\udc61=s\ud835\udc61\u00001\u00b8g2\n\ud835\udc61,\nw\ud835\udc61=w\ud835\udc61\u00001\u0000\ud835\udf02ps\ud835\udc61\u00b8\ud835\udf16\u0001g\ud835\udc61.(12.7.5)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "02f150a0-55e3-4a5e-b884-4b89565d3051": {"__data__": {"id_": "02f150a0-55e3-4a5e-b884-4b89565d3051", "embedding": null, "metadata": {"page_label": "522", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d5b8c89-77d8-46de-b6e8-4d36809e55bb", "node_type": "4", "metadata": {"page_label": "522", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ad2e3e19cc84a488dd327fd20b41b92eec4b1916addb796cc6bd233fc4e5ccca", "class_name": "RelatedNodeInfo"}}, "text": "522 Optimization Algorithms\nHeretheoperationareappliedcoordinatewise. Thatis, v2hasentries\ud835\udc632\n\ud835\udc56. Likewise1p\ud835\udc63has\nentries1p\ud835\udc63\ud835\udc56andu\u0001vhas entries\ud835\udc62\ud835\udc56\ud835\udc63\ud835\udc56. As before\ud835\udf02is the learning rate and \ud835\udf16is an additive\nconstant that ensures that we do not divide by 0. Last, we initialize s0=0.\nJust like in the case of momentum we need to keep track of an auxiliary variable, in this\ncase to allow for an individual learning rate per coordinate. This does not increase the cost\nofAdagradsignificantlyrelativetoSGD,simplysincethemaincostistypicallytocompute\n\ud835\udc59\u00b9\ud835\udc66\ud835\udc61, \ud835\udc53\u00b9x\ud835\udc61,w\u00ba\u00baand its derivative.\nNotethataccumulatingsquaredgradientsin s\ud835\udc61meansthat s\ud835\udc61growsessentiallyatlinearrate\n(somewhat slower than linearly in practice, since the gradients initially diminish). This\nleads to anO\u00b9\ud835\udc61\u00001\n2\u00balearning rate, albeit adjusted on a per coordinate basis. For convex\nproblems this is perfectly adequate. In deep learning, though, we might want to decrease\nthe learning rate rather more slowly. This led to a number of Adagrad variants that we will\ndiscuss in the subsequent chapters. For now let\u2019s see how it behaves in a quadratic convex\nproblem. We use the same problem as before:\n\ud835\udc53\u00b9x\u00ba=0.1\ud835\udc652\n1\u00b82\ud835\udc652\n2. (12.7.6)\nWe are going to implement Adagrad using the same learning rate previously, i.e., \ud835\udf02=0.4.\nAs we can see, the iterative trajectory of the independent variable is smoother. However,\nduetothecumulativeeffectof \ud835\udc94\ud835\udc61,thelearningratecontinuouslydecays,sotheindependent\nvariable does not move as much during later stages of iteration.\n%matplotlib inline\nimport math\nimport torch\nfrom d2l import torch asd2l\ndef adagrad_2d (x1, x2, s1, s2):\neps =1e-6\ng1, g2 =0.2 *x1, 4*x2\ns1+=g1**2\ns2+=g2**2\nx1-=eta /math .sqrt(s1 +eps) *g1\nx2-=eta /math .sqrt(s2 +eps) *g2\nreturn x1, x2, s1, s2\ndef f_2d (x1, x2):\nreturn 0.1 *x1**2+2*x2**2\neta =0.4\nd2l.show_trace_2d(f_2d, d2l .train_2d(adagrad_2d))\nepoch 20, x1: -2.382563 , x2: -0.158591\nAs we increase the learning rate to 2we see much better behavior. This already indicates\nthatthedecreaseinlearningratemightberatheraggressive,eveninthenoise-freecaseand\nwe need to ensure that parameters converge appropriately.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2114, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1a70cc4-70b8-4032-bf99-e98e3b512c4e": {"__data__": {"id_": "b1a70cc4-70b8-4032-bf99-e98e3b512c4e", "embedding": null, "metadata": {"page_label": "523", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b794af8a-d3da-4f99-a825-497c8d7c8895", "node_type": "4", "metadata": {"page_label": "523", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ff5a358effb75b139dcfa47bb7051eec884ee4a00eba1ed979a54adbfa6af565", "class_name": "RelatedNodeInfo"}}, "text": "523 Adagrad\neta =2\nd2l.show_trace_2d(f_2d, d2l .train_2d(adagrad_2d))\nepoch 20, x1: -0.002295 , x2: -0.000000\n12.7.4Implementation fromScratch\nJust like the momentum method, Adagrad needs to maintain a state variable of the same\nshape as the parameters.\ndef init_adagrad_states (feature_dim):\ns_w =torch .zeros((feature_dim, 1))\ns_b =torch .zeros( 1)\nreturn (s_w, s_b)\ndef adagrad (params, states, hyperparams):\neps =1e-6\nfor p, s inzip(params, states):\nwith torch .no_grad():\ns[:] +=torch .square(p .grad)\np[:] -=hyperparams[ 'lr']*p.grad /torch .sqrt(s +eps)\np.grad .data .zero_()\nCompared to the experiment in Section 12.5 we use a larger learning rate to train the\nmodel.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eeb74e30-5716-42cf-9a33-a74d67854db1": {"__data__": {"id_": "eeb74e30-5716-42cf-9a33-a74d67854db1", "embedding": null, "metadata": {"page_label": "524", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eddb19a5-9349-4aca-afeb-d4184a87cf76", "node_type": "4", "metadata": {"page_label": "524", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2c5ac3aff039d062c67ef8481abc4480aeb71caddb7f0cdedfc399fd02cb1c56", "class_name": "RelatedNodeInfo"}}, "text": "524 Optimization Algorithms\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\nd2l.train_ch11(adagrad, init_adagrad_states(feature_dim),\n{'lr':0.1}, data_iter, feature_dim);\nloss: 0.243 ,0.162 sec/epoch\n12.7.5ConciseImplementation\nUsing the Trainer instance of the algorithm adagrad , we can invoke the Adagrad algo-\nrithm in Gluon.\ntrainer =torch .optim .Adagrad\nd2l.train_concise_ch11(trainer, { 'lr':0.1}, data_iter)\nloss: 0.242 ,0.129 sec/epoch\n12.7.6Summary\n\u000fAdagrad decreases the learning rate dynamically on a per-coordinate basis.\n\u000fIt uses the magnitude of the gradient as a means of adjusting how quickly progress is\nachieved - coordinates with large gradients are compensated with a smaller learning\nrate.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab275705-6750-4f6f-9d66-3d7effd2c212": {"__data__": {"id_": "ab275705-6750-4f6f-9d66-3d7effd2c212", "embedding": null, "metadata": {"page_label": "525", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7d62a26-3881-4daa-a410-139ba68ebbe5", "node_type": "4", "metadata": {"page_label": "525", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1efb1e8020fa80365271c9357c226fb73a0d2cfbbcbbae7192cd5b7977720e24", "class_name": "RelatedNodeInfo"}}, "text": "525 RMSProp\n177\n178\u000fComputing the exact second derivative is typically infeasible in deep learning problems\ndue to memory and computational constraints. The gradient can be a useful proxy.\n\u000fIf the optimization problem has a rather uneven structure Adagrad can help mitigate the\ndistortion.\n\u000fAdagrad is particularly effective for sparse features where the learning rate needs to de-\ncrease more slowly for infrequently occurring terms.\n\u000fOndeeplearningproblemsAdagradcansometimesbetooaggressiveinreducinglearn-\ningrates. Wewilldiscussstrategiesformitigatingthisinthecontextof Section12.10 .\n12.7.7Exercises\n1.Prove that for an orthogonal matrix Uand a vector cthe following holds: kc\u0000\ufb03k2=\nkUc\u0000U\ufb03k2. Whydoesthismeanthatthemagnitudeofperturbationsdoesnotchange\nafter an orthogonal change of variables?\n2.Try out Adagrad for \ud835\udc53\u00b9x\u00ba=0.1\ud835\udc652\n1\u00b82\ud835\udc652\n2and also for the objective function was rotated\nby 45 degrees, i.e., \ud835\udc53\u00b9x\u00ba=0.1\u00b9\ud835\udc651\u00b8\ud835\udc652\u00ba2\u00b82\u00b9\ud835\udc651\u0000\ud835\udc652\u00ba2. Does it behave differently?\n3.ProveGerschgorin\u2019s circle theorem177which states that eigenvalues \ud835\udf06\ud835\udc56of a matrix M\nsatisfyj\ud835\udf06\ud835\udc56\u0000M\ud835\udc57\ud835\udc57j\u0014\u00cd\n\ud835\udc58\u2260\ud835\udc57jM\ud835\udc57\ud835\udc58jfor at least one choice of \ud835\udc57.\n4.What does Gerschgorin\u2019s theorem tell us about the eigenvalues of the diagonally pre-\nconditioned matrix diag\u00001\n2\u00b9M\u00baMdiag\u00001\n2\u00b9M\u00ba?\n5.TryoutAdagradforaproperdeepnetwork,suchas Section7.6 whenappliedtoFashion-\nMNIST.\n6.How would you need to modify Adagrad to achieve a less aggressive decay in learning\nrate?\nDiscussions178.\n12.8RMSProp\nOne of the key issues in Section 12.7 is that the learning rate decreases at a predefined\nschedule of effectively O\u00b9\ud835\udc61\u00001\n2\u00ba. While this is generally appropriate for convex problems,\nit might not be ideal for nonconvex ones, such as those encountered in deep learning. Yet,\nthe coordinate-wise adaptivity of Adagrad is highly desirable as a preconditioner.\nTieleman and Hinton ( 2012) proposed the RMSProp algorithm as a simple fix to decouple\nrate scheduling from coordinate-adaptive learning rates. The issue is that Adagrad accu-\nmulates the squares of the gradient g\ud835\udc61into a state vector s\ud835\udc61=s\ud835\udc61\u00001\u00b8g2\n\ud835\udc61. As a result s\ud835\udc61\nkeeps on growing without bound due to the lack of normalization, essentially linearly as\nthe algorithm converges.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "270d9956-bcca-46f1-b1bf-11a44a42559b": {"__data__": {"id_": "270d9956-bcca-46f1-b1bf-11a44a42559b", "embedding": null, "metadata": {"page_label": "526", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fc6b2b00-0c49-4eb6-8516-3ee5ef7950bd", "node_type": "4", "metadata": {"page_label": "526", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "058f7dca99dcc20b399a4f76f92d1546871900563d7d674bc7e105364ecc4443", "class_name": "RelatedNodeInfo"}}, "text": "526 Optimization Algorithms\nOne way of fixing this problem would be to use s\ud835\udc61\u009d\ud835\udc61. For reasonable distributions of g\ud835\udc61\nthis will converge. Unfortunately it might take a very long time until the limit behavior\nstarts to matter since the procedure remembers the full trajectory of values. An alternative\nis to use a leaky average in the same way we used in the momentum method, i.e., s\ud835\udc61 \n\ud835\udefes\ud835\udc61\u00001\u00b8\u00b91\u0000\ud835\udefe\u00bag2\n\ud835\udc61for some parameter \ud835\udefe > 0. Keeping all other parts unchanged yields\nRMSProp.\n12.8.1The Algorithm\nLet\u2019s write out the equations in detail.\ns\ud835\udc61 \ud835\udefes\ud835\udc61\u00001\u00b8\u00b91\u0000\ud835\udefe\u00bag2\n\ud835\udc61,\nx\ud835\udc61 x\ud835\udc61\u00001\u0000\ud835\udf02ps\ud835\udc61\u00b8\ud835\udf16\fg\ud835\udc61.(12.8.1)\nThe constant \ud835\udf16 >0is typically set to 10\u00006to ensure that we do not suffer from division by\nzerooroverlylargestepsizes. Giventhisexpansionwearenowfreetocontrolthelearning\nrate\ud835\udf02independently of the scaling that is applied on a per-coordinate basis. In terms of\nleaky averages we can apply the same reasoning as previously applied in the case of the\nmomentum method. Expanding the definition of s\ud835\udc61yields\ns\ud835\udc61=\u00b91\u0000\ud835\udefe\u00bag2\n\ud835\udc61\u00b8\ud835\udefes\ud835\udc61\u00001\n=\u00b91\u0000\ud835\udefe\u00ba\u0010\ng2\n\ud835\udc61\u00b8\ud835\udefeg2\n\ud835\udc61\u00001\u00b8\ud835\udefe2g\ud835\udc61\u00002\u00b8...,\u0011\n.(12.8.2)\nAs before in Section 12.6 we use 1\u00b8\ud835\udefe\u00b8\ud835\udefe2\u00b8...,=1\n1\u0000\ud835\udefe. Hence the sum of weights is\nnormalized to 1with a half-life time of an observation of \ud835\udefe\u00001. Let\u2019s visualize the weights\nfor the past 40 time steps for various choices of \ud835\udefe.\nimport math\nimport torch\nfrom d2l import torch asd2l\nd2l.set_figsize()\ngammas =[0.95 ,0.9,0.8,0.7]\nfor gamma ingammas:\nx=torch .arange( 40).detach() .numpy()\nd2l.plt.plot(x, ( 1-gamma) *gamma **x, label =f'gamma = {gamma :.2f}')\nd2l.plt.xlabel( 'time ');\n12.8.2Implementation fromScratch\nAs before we use the quadratic function \ud835\udc53\u00b9x\u00ba=0.1\ud835\udc652\n1\u00b82\ud835\udc652\n2to observe the trajectory of\nRMSProp. Recall that in Section 12.7 , when we used Adagrad with a learning rate of\n0.4, the variables moved only very slowly in the later stages of the algorithm since the\nlearning rate decreased too quickly. Since \ud835\udf02is controlled separately this does not happen\nwith RMSProp.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e481b69-5509-4d1f-a550-6fd325450016": {"__data__": {"id_": "9e481b69-5509-4d1f-a550-6fd325450016", "embedding": null, "metadata": {"page_label": "527", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1c69d01-cee3-43fe-bac1-f6a736c9653b", "node_type": "4", "metadata": {"page_label": "527", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ebd7ae0923deed352ec580b7a22af366b1b737c2f775273f76829dc0ccea51e2", "class_name": "RelatedNodeInfo"}}, "text": "527 RMSProp\ndef rmsprop_2d (x1, x2, s1, s2):\ng1, g2, eps =0.2 *x1, 4*x2, 1e-6\ns1=gamma *s1+(1-gamma) *g1**2\ns2=gamma *s2+(1-gamma) *g2**2\nx1-=eta /math .sqrt(s1 +eps) *g1\nx2-=eta /math .sqrt(s2 +eps) *g2\nreturn x1, x2, s1, s2\ndef f_2d (x1, x2):\nreturn 0.1 *x1**2+2*x2**2\neta, gamma =0.4,0.9\nd2l.show_trace_2d(f_2d, d2l .train_2d(rmsprop_2d))\nepoch 20, x1: -0.010599 , x2: 0.000000\nNext, we implement RMSProp to be used in a deep network. This is equally straightfor-\nward.\ndef init_rmsprop_states (feature_dim):\ns_w =torch .zeros((feature_dim, 1))\ns_b =torch .zeros( 1)\nreturn (s_w, s_b)\ndef rmsprop (params, states, hyperparams):\ngamma, eps =hyperparams[ 'gamma '],1e-6\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17e87fec-57f9-4b0c-9905-9d6211dcbb04": {"__data__": {"id_": "17e87fec-57f9-4b0c-9905-9d6211dcbb04", "embedding": null, "metadata": {"page_label": "528", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b724465-a69c-4c19-b8c7-d2b9c33329ea", "node_type": "4", "metadata": {"page_label": "528", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0cdf165f12f8de6502a5ee62eaf31adc31e06428f32895bd9078713f87c8758e", "class_name": "RelatedNodeInfo"}}, "text": "528 Optimization Algorithms\n(continued from previous page)\nfor p, s inzip(params, states):\nwith torch .no_grad():\ns[:] =gamma *s+(1-gamma) *torch .square(p .grad)\np[:] -=hyperparams[ 'lr']*p.grad /torch .sqrt(s +eps)\np.grad .data .zero_()\nWesettheinitiallearningrateto0.01andtheweightingterm \ud835\udefeto0.9. Thatis, saggregates\non average over the past 1\u009d\u00b91\u0000\ud835\udefe\u00ba=10observations of the square gradient.\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\nd2l.train_ch11(rmsprop, init_rmsprop_states(feature_dim),\n{'lr':0.01 ,'gamma ':0.9}, data_iter, feature_dim);\nloss: 0.245 ,0.245 sec/epoch\n12.8.3ConciseImplementation\nSince RMSProp is a rather popular algorithm it is also available in the Trainer instance.\nAll we need to do is instantiate it using an algorithm named rmsprop , assigning\ud835\udefeto the\nparameter gamma1.\ntrainer =torch .optim .RMSprop\nd2l.train_concise_ch11(trainer, { 'lr':0.01 ,'alpha ':0.9},\ndata_iter)\nloss: 0.246 ,0.129 sec/epoch\n12.8.4Summary\n\u000fRMSProp is very similar to Adagrad insofar as both use the square of the gradient to\nscale coefficients.\n\u000fRMSProp shares with momentum the leaky averaging. However, RMSProp uses the\ntechnique to adjust the coefficient-wise preconditioner.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af846b12-9501-485a-94c3-4c84f6ed9bd5": {"__data__": {"id_": "af846b12-9501-485a-94c3-4c84f6ed9bd5", "embedding": null, "metadata": {"page_label": "529", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a1c726c9-27a8-46ab-8672-52c69a2b5b76", "node_type": "4", "metadata": {"page_label": "529", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1d9384eff52c70cc089d65df30ac1eef9dadd10ddd0634cd47f5c3579524cf12", "class_name": "RelatedNodeInfo"}}, "text": "529 Adadelta\n179\u000fThe learning rate needs to be scheduled by the experimenter in practice.\n\u000fThe coefficient \ud835\udefedetermines how long the history is when adjusting the per-coordinate\nscale.\n12.8.5Exercises\n1.What happens experimentally if we set \ud835\udefe=1? Why?\n2.Rotate the optimization problem to minimize \ud835\udc53\u00b9x\u00ba=0.1\u00b9\ud835\udc651\u00b8\ud835\udc652\u00ba2\u00b82\u00b9\ud835\udc651\u0000\ud835\udc652\u00ba2. What\nhappens to the convergence?\n3.TryoutwhathappenstoRMSProponarealmachinelearningproblem,suchastraining\non Fashion-MNIST. Experiment with different choices for adjusting the learning rate.\n4.Would you want to adjust \ud835\udefeas optimization progresses? How sensitive is RMSProp to\nthis?\nDiscussions179.\n12.9Adadelta\nAdadelta is yet another variant of AdaGrad ( Section 12.7 ). The main difference lies in\nthe fact that it decreases the amount by which the learning rate is adaptive to coordinates.\nMoreover,traditionallyitreferredtoasnothavingalearningratesinceitusestheamountof\nchangeitselfascalibrationforfuturechange. ThealgorithmwasproposedinZeiler( 2012).\nIt is fairly straightforward, given the discussion of previous algorithms so far.\n12.9.1The Algorithm\nIn a nutshell, Adadelta uses two state variables, s\ud835\udc61to store a leaky average of the second\nmomentofthegradientand \u0394x\ud835\udc61tostorealeakyaverageofthesecondmomentofthechange\nof parameters in the model itself. Note that we use the original notation and naming of the\nauthors for compatibility with other publications and implementations (there is no other", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e5bc680-f409-4ef7-bfa5-6249d7565ca8": {"__data__": {"id_": "5e5bc680-f409-4ef7-bfa5-6249d7565ca8", "embedding": null, "metadata": {"page_label": "530", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7274af2d-02a5-4f48-a3a7-0d7b7050b6a3", "node_type": "4", "metadata": {"page_label": "530", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c6ce819fd903ac5b0980c8567570a160f7cbc04ec904b61afa413d5670900b2d", "class_name": "RelatedNodeInfo"}}, "text": "530 Optimization Algorithms\nreal reason why one should use different Greek variables to indicate a parameter serving\nthe same purpose in momentum, Adagrad, RMSProp, and Adadelta).\nHere are the technical details of Adadelta. Given the parameter du jour is \ud835\udf0c, we obtain the\nfollowing leaky updates similarly to Section 12.8 :\ns\ud835\udc61=\ud835\udf0cs\ud835\udc61\u00001\u00b8\u00b91\u0000\ud835\udf0c\u00bag2\n\ud835\udc61. (12.9.1)\nThe difference to Section 12.8 is that we perform updates with the rescaled gradient g0\n\ud835\udc61,\ni.e.,\nx\ud835\udc61=x\ud835\udc61\u00001\u0000g0\n\ud835\udc61. (12.9.2)\nSo what is the rescaled gradient g0\n\ud835\udc61? We can calculate it as follows:\ng0\n\ud835\udc61=p\u0394x\ud835\udc61\u00001\u00b8\ud835\udf16ps\ud835\udc61\u00b8\ud835\udf16\fg\ud835\udc61, (12.9.3)\nwhere \u0394x\ud835\udc61\u00001is the leaky average of the squared rescaled gradients g0\n\ud835\udc61. We initialize \u0394x0\nto be 0and update it at each step with g0\n\ud835\udc61, i.e.,\n\u0394x\ud835\udc61=\ud835\udf0c\u0394x\ud835\udc61\u00001\u00b8\u00b91\u0000\ud835\udf0c\u00bag0\n\ud835\udc612, (12.9.4)\nand\ud835\udf16(a small value such as 10\u00005) is added to maintain numerical stability.\n12.9.2Implementation\nAdadelta needs to maintain two state variables for each variable, s\ud835\udc61and\u0394x\ud835\udc61. This yields\nthe following implementation.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\ndef init_adadelta_states (feature_dim):\ns_w, s_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)\ndelta_w, delta_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)\nreturn ((s_w, delta_w), (s_b, delta_b))\ndef adadelta (params, states, hyperparams):\nrho, eps =hyperparams[ 'rho'],1e-5\nfor p, (s, delta) inzip(params, states):\nwith torch .no_grad():\n# In-place updates via [:]\ns[:] =rho *s+(1-rho) *torch .square(p .grad)\ng=(torch .sqrt(delta +eps) /torch .sqrt(s +eps)) *p.grad\np[:] -=g\ndelta[:] =rho *delta +(1-rho) *g*g\np.grad .data .zero_()\nChoosing\ud835\udf0c=0.9amounts to a half-life time of 10 for each parameter update. This tends\nto work quite well. We get the following behavior.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0ce8097-c695-41ba-ad6d-900d72d7733a": {"__data__": {"id_": "a0ce8097-c695-41ba-ad6d-900d72d7733a", "embedding": null, "metadata": {"page_label": "531", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78abfc42-d53c-4bec-bd06-ed1412506650", "node_type": "4", "metadata": {"page_label": "531", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6242185c22e889a93d1c653533b45c5c7490e01a3889b98d99677476ca403bf4", "class_name": "RelatedNodeInfo"}}, "text": "531 Adadelta\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\nd2l.train_ch11(adadelta, init_adadelta_states(feature_dim),\n{'rho':0.9}, data_iter, feature_dim);\nloss: 0.245 ,0.160 sec/epoch\nFor a concise implementation we simply use the Adadelta algorithm from high-level APIs.\nThis yields the following one-liner for a much more compact invocation.\ntrainer =torch .optim .Adadelta\nd2l.train_concise_ch11(trainer, { 'rho':0.9}, data_iter)\nloss: 0.243 ,0.119 sec/epoch\n12.9.3Summary\n\u000fAdadeltahasnolearningrateparameter. Instead, itusestherateofchangeintheparam-\neters itself to adapt the learning rate.\n\u000fAdadelta requires two state variables to store the second moments of gradient and the\nchange in parameters.\n\u000fAdadelta uses leaky averages to keep a running estimate of the appropriate statistics.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38130fa0-c0df-432f-b804-70ecc89990e8": {"__data__": {"id_": "38130fa0-c0df-432f-b804-70ecc89990e8", "embedding": null, "metadata": {"page_label": "532", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5bc8225c-2bb1-4d0d-a78e-36abfda4fc71", "node_type": "4", "metadata": {"page_label": "532", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0507b9b3168b5d99b2234ed74a096fb6a480596f48ec281d6034c537099d4ee0", "class_name": "RelatedNodeInfo"}}, "text": "532 Optimization Algorithms\n18012.9.4Exercises\n1.Adjust the value of \ud835\udf0c. What happens?\n2.Show how to implement the algorithm without the use of g0\n\ud835\udc61. Why might this be a good\nidea?\n3.Is Adadelta really learning rate free? Could you find optimization problems that break\nAdadelta?\n4.Compare Adadelta to Adagrad and RMS prop to discuss their convergence behavior.\nDiscussions180.\n12.10Adam\nIn the discussions leading up to this section we encountered a number of techniques for\nefficient optimization. Let\u2019s recap them in detail here:\n\u000fWe saw that Section 12.4 is more effective than Gradient Descent when solving opti-\nmization problems, e.g., due to its inherent resilience to redundant data.\n\u000fWe saw that Section 12.5 affords significant additional efficiency arising from vector-\nization, using larger sets of observations in one minibatch. This is the key to efficient\nmulti-machine, multi-GPU and overall parallel processing.\n\u000fSection12.6 addedamechanismforaggregatingahistoryofpastgradientstoaccelerate\nconvergence.\n\u000fSection12.7 usedper-coordinatescalingtoallowforacomputationallyefficientprecon-\nditioner.\n\u000fSection 12.8 decoupled per-coordinate scaling from a learning rate adjustment.\nAdam (Kingma and Ba, 2014 ) combines all these techniques into one efficient learning\nalgorithm. As expected, this is an algorithm that has become rather popular as one of the\nmore robust and effective optimization algorithms to use in deep learning. It is not without\nissues,though. Inparticular,( Reddietal.,2019)showthattherearesituationswhereAdam\ncandivergeduetopoorvariancecontrol. Inafollow-upworkZaheer etal.(2018)proposed\na hotfix to Adam, called Yogi which addresses these issues. More on this later. For now\nlet\u2019s review the Adam algorithm.\n12.10.1TheAlgorithm\nOne of the key components of Adam is that it uses exponential weighted moving averages\n(also known as leaky averaging) to obtain an estimate of both the momentum and also the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f2bb612-e388-46aa-8194-8b6aaa56c818": {"__data__": {"id_": "9f2bb612-e388-46aa-8194-8b6aaa56c818", "embedding": null, "metadata": {"page_label": "533", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e26418c-902e-4c72-aa2d-02d45af7e4c8", "node_type": "4", "metadata": {"page_label": "533", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a66882ceb4f8b7924c5750e26e0a8048755fb330f28a49dba075630a500c1e3a", "class_name": "RelatedNodeInfo"}}, "text": "533 Adam\nsecond moment of the gradient. That is, it uses the state variables\nv\ud835\udc61 \ud835\udefd1v\ud835\udc61\u00001\u00b8\u00b91\u0000\ud835\udefd1\u00bag\ud835\udc61,\ns\ud835\udc61 \ud835\udefd2s\ud835\udc61\u00001\u00b8\u00b91\u0000\ud835\udefd2\u00bag2\n\ud835\udc61.(12.10.1)\nHere\ud835\udefd1and\ud835\udefd2are nonnegative weighting parameters. Common choices for them are\n\ud835\udefd1=0.9and\ud835\udefd2=0.999. That is, the variance estimate moves much more slowly than the\nmomentumterm. Notethatifweinitialize v0=s0=0wehaveasignificantamountofbias\ninitiallytowardssmallervalues. Thiscanbeaddressedbyusingthefactthat\u00cd\ud835\udc61\u00001\n\ud835\udc56=0\ud835\udefd\ud835\udc56=1\u0000\ud835\udefd\ud835\udc61\n1\u0000\ud835\udefd\ntore-normalizeterms. Correspondinglythenormalizedstatevariablesaregivenby\n\u02c6v\ud835\udc61=v\ud835\udc61\n1\u0000\ud835\udefd\ud835\udc61\n1and \u02c6s\ud835\udc61=s\ud835\udc61\n1\u0000\ud835\udefd\ud835\udc61\n2. (12.10.2)\nArmed with the proper estimates we can now write out the update equations. First, we\nrescale the gradient in a manner very much akin to that of RMSProp to obtain\ng0\n\ud835\udc61=\ud835\udf02\u02c6v\ud835\udc61p\u02c6s\ud835\udc61\u00b8\ud835\udf16. (12.10.3)\nUnlike RMSProp our update uses the momentum \u02c6v\ud835\udc61rather than the gradient itself. More-\nover, there is a slight cosmetic difference as the rescaling happens using1p\u02c6s\ud835\udc61\u00b8\ud835\udf16instead of\n1p\u02c6s\ud835\udc61\u00b8\ud835\udf16. Theformerworksarguablyslightlybetterinpractice,hencethedeviationfromRM-\nSProp. Typically we pick \ud835\udf16=10\u00006for a good trade-off between numerical stability and\nfidelity.\nNow we have all the pieces in place to compute updates. This is slightly anticlimactic and\nwe have a simple update of the form\nx\ud835\udc61 x\ud835\udc61\u00001\u0000g0\n\ud835\udc61. (12.10.4)\nReviewing the design of Adam its inspiration is clear. Momentum and scale are clearly\nvisible in the state variables. Their rather peculiar definition forces us to debias terms\n(this could be fixed by a slightly different initialization and update condition). Second, the\ncombination of both terms is pretty straightforward, given RMSProp. Last, the explicit\nlearningrate \ud835\udf02allowsustocontrolthesteplengthtoaddressissuesofconvergence.\n12.10.2Implementation\nImplementing Adam from scratch is not very daunting. For convenience we store the time\nstep counter \ud835\udc61in the hyperparams dictionary. Beyond that all is straightforward.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\ndef init_adam_states (feature_dim):\nv_w, v_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)\ns_w, s_b =torch .zeros((feature_dim, 1)), torch .zeros( 1)\nreturn ((v_w, s_w), (v_b, s_b))\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55321e10-e786-4399-8c77-a8ff6977d2af": {"__data__": {"id_": "55321e10-e786-4399-8c77-a8ff6977d2af", "embedding": null, "metadata": {"page_label": "534", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5679fb97-eb15-4f02-bb8d-7c4628433ecb", "node_type": "4", "metadata": {"page_label": "534", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "65b99933f6878c0a57fc8d73025f00ae01d178a07f605fd5a76509ea35e9ef99", "class_name": "RelatedNodeInfo"}}, "text": "534 Optimization Algorithms\n(continued from previous page)\ndef adam (params, states, hyperparams):\nbeta1, beta2, eps =0.9,0.999 ,1e-6\nfor p, (v, s) inzip(params, states):\nwith torch .no_grad():\nv[:] =beta1 *v+(1-beta1) *p.grad\ns[:] =beta2 *s+(1-beta2) *torch .square(p .grad)\nv_bias_corr =v/(1-beta1 **hyperparams[ 't'])\ns_bias_corr =s/(1-beta2 **hyperparams[ 't'])\np[:] -=hyperparams[ 'lr']*v_bias_corr /(torch .sqrt(s_bias_corr)\n+eps)\np.grad .data .zero_()\nhyperparams[ 't']+=1\nWe are ready to use Adam to train the model. We use a learning rate of \ud835\udf02=0.01.\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\nd2l.train_ch11(adam, init_adam_states(feature_dim),\n{'lr':0.01 ,'t':1}, data_iter, feature_dim);\nloss: 0.243 ,0.193 sec/epoch\nA more concise implementation is straightforward since adamis one of the algorithms pro-\nvided as part of the Gluon trainer optimization library. Hence we only need to pass\nconfiguration parameters for an implementation in Gluon.\ntrainer =torch .optim .Adam\nd2l.train_concise_ch11(trainer, { 'lr':0.01 }, data_iter)\nloss: 0.243 ,0.152 sec/epoch\n12.10.3Yogi\nOneoftheproblemsofAdamisthatitcanfailtoconvergeeveninconvexsettingswhenthe\nsecond moment estimate in s\ud835\udc61blows up. As a fix Zaheer et al.(2018) proposed a refined", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1260, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef0033be-f40c-423e-b394-18e66f8b86cd": {"__data__": {"id_": "ef0033be-f40c-423e-b394-18e66f8b86cd", "embedding": null, "metadata": {"page_label": "535", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de9c11ea-b086-4ac9-9dce-68e826004ffe", "node_type": "4", "metadata": {"page_label": "535", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "07ff0b95f33eba542394a7b13ef46af9ca169951f23417bff650ce9c1a89a54b", "class_name": "RelatedNodeInfo"}}, "text": "535 Adam\nupdate (and initialization) for s\ud835\udc61. To understand what\u2019s going on, let\u2019s rewrite the Adam\nupdate as follows:\ns\ud835\udc61 s\ud835\udc61\u00001\u00b8\u00b91\u0000\ud835\udefd2\u00ba\u0010\ng2\n\ud835\udc61\u0000s\ud835\udc61\u00001\u0011\n. (12.10.5)\nWhenever g2\n\ud835\udc61hashighvarianceorupdatesaresparse, s\ud835\udc61mightforgetpastvaluestooquickly.\nApossiblefixforthisistoreplace g2\n\ud835\udc61\u0000s\ud835\udc61\u00001byg2\n\ud835\udc61\fsgn\u00b9g2\n\ud835\udc61\u0000s\ud835\udc61\u00001\u00ba. Nowthemagnitudeofthe\nupdate no longer depends on the amount of deviation. This yields the Yogi updates\ns\ud835\udc61 s\ud835\udc61\u00001\u00b8\u00b91\u0000\ud835\udefd2\u00bag2\n\ud835\udc61\fsgn\u00b9g2\n\ud835\udc61\u0000s\ud835\udc61\u00001\u00ba. (12.10.6)\nThe authors furthermore advise to initialize the momentum on a larger initial batch rather\nthan just initial pointwise estimate. We omit the details since they are not material to the\ndiscussion and since even without this convergence remains pretty good.\ndef yogi (params, states, hyperparams):\nbeta1, beta2, eps =0.9,0.999 ,1e-3\nfor p, (v, s) inzip(params, states):\nwith torch .no_grad():\nv[:] =beta1 *v+(1-beta1) *p.grad\ns[:] =s+(1-beta2) *torch .sign(\ntorch .square(p .grad) -s)*torch .square(p .grad)\nv_bias_corr =v/(1-beta1 **hyperparams[ 't'])\ns_bias_corr =s/(1-beta2 **hyperparams[ 't'])\np[:] -=hyperparams[ 'lr']*v_bias_corr /(torch .sqrt(s_bias_corr)\n+eps)\np.grad .data .zero_()\nhyperparams[ 't']+=1\ndata_iter, feature_dim =d2l.get_data_ch11(batch_size =10)\nd2l.train_ch11(yogi, init_adam_states(feature_dim),\n{'lr':0.01 ,'t':1}, data_iter, feature_dim);\nloss: 0.243 ,0.165 sec/epoch\n12.10.4Summary\n\u000fAdam combines features of many optimization algorithms into a fairly robust update\nrule.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1449, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35187c62-7086-4005-9d3e-23dc6973f575": {"__data__": {"id_": "35187c62-7086-4005-9d3e-23dc6973f575", "embedding": null, "metadata": {"page_label": "536", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "35482ce9-2813-4334-aeb4-ed2a6cd74a16", "node_type": "4", "metadata": {"page_label": "536", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8c64b77e9d144ccedfd586d40389988d139b2761868a608e3c105a3397a38cf3", "class_name": "RelatedNodeInfo"}}, "text": "536 Optimization Algorithms\n181\u000fCreated on the basis of RMSProp, Adam also uses EWMA on the minibatch stochastic\ngradient.\n\u000fAdam uses bias correction to adjust for a slow startup when estimating momentum and\na second moment.\n\u000fForgradientswithsignificantvariancewemayencounterissueswithconvergence. They\ncan be amended by using larger minibatches or by switching to an improved estimate\nfors\ud835\udc61. Yogi offers such an alternative.\n12.10.5Exercises\n1.Adjust the learning rate and observe and analyze the experimental results.\n2.Can you rewrite momentum and second moment updates such that it does not require\nbias correction?\n3.Why do you need to reduce the learning rate \ud835\udf02as we converge?\n4.Try to construct a case for which Adam diverges and Yogi converges?\nDiscussions181.\n12.11LearningRate Scheduling\nSofarweprimarilyfocusedonoptimization algorithms forhowtoupdatetheweightvectors\nratherthanonthe rateatwhichtheyarebeingupdated. Nonetheless,adjustingthelearning\nrate is often just as important as the actual algorithm. There are a number of aspects to\nconsider:\n\u000fMostobviouslythe magnitude ofthelearningratematters. Ifitistoolarge,optimization\ndiverges, if it is too small, it takes too long to train or we end up with a suboptimal\nresult. We saw previously that the condition number of the problem matters (see e.g.,\nSection 12.6 for details). Intuitively it is the ratio of the amount of change in the least\nsensitive direction vs. the most sensitive one.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "320f6ce3-69bb-4050-b83b-5671238190ae": {"__data__": {"id_": "320f6ce3-69bb-4050-b83b-5671238190ae", "embedding": null, "metadata": {"page_label": "537", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "509661a8-4ef8-495d-bbd9-be671f4a0185", "node_type": "4", "metadata": {"page_label": "537", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1eced8778df561fc3fd53bcef1a194c5ce71acde288d3fde0f7ce0cbfd4d4746", "class_name": "RelatedNodeInfo"}}, "text": "537 Learning Rate Scheduling\n\u000fSecondly,therateofdecayisjustasimportant. Ifthelearningrateremainslargewemay\nsimply end up bouncing around the minimum and thus not reach optimality. Section\n12.5discussedthisinsomedetailandweanalyzedperformanceguaranteesin Section\n12.4. Inshort,wewanttheratetodecay,butprobablymoreslowlythan O\u00b9\ud835\udc61\u00001\n2\u00bawhich\nwould be a good choice for convex problems.\n\u000fAnother aspect that is equally important is initialization . This pertains both to how the\nparameters are set initially (review Section 5.4 for details) and also how they evolve\ninitially. This goes under the moniker of warmup, i.e., how rapidly we start moving\ntowards the solution initially. Large steps in the beginning might not be beneficial, in\nparticular since the initial set of parameters is random. The initial update directions\nmight be quite meaningless, too.\n\u000fLastly, there are a number of optimization variants that perform cyclical learning rate\nadjustment. This is beyond the scope of the current chapter. We recommend the\nreader to review details in Izmailov et al.(2018), e.g., how to obtain better solutions\nby averaging over an entire pathof parameters.\nGiventhe factthat thereis a lot ofdetail needed to managelearningrates, mostdeeplearn-\ning frameworks have tools to deal with this automatically. In the current chapter we will\nreview the effects that different schedules have on accuracy and also show how this can be\nmanaged efficiently via a learning ratescheduler .\n12.11.1ToyProblem\nWe begin with a toy problem that is cheap enough to compute easily, yet sufficiently non-\ntrivial to illustrate some of the key aspects. For that we pick a slightly modernized version\nof LeNet ( reluinstead of sigmoid activation, MaxPooling rather than AveragePooling),\nasappliedtoFashion-MNIST.Moreover, wehybridizethenetworkforperformance. Since\nmostofthecodeisstandardwejustintroducethebasicswithoutfurtherdetaileddiscussion.\nSeeChapter 7 for a refresher as needed.\n%matplotlib inline\nimport math\nimport torch\nfrom torch import nn\nfrom torch .optim import lr_scheduler\nfrom d2l import torch asd2l\ndef net_fn ():\nmodel =nn.Sequential(\nnn.Conv2d( 1,6, kernel_size =5, padding =2), nn .ReLU(),\nnn.MaxPool2d(kernel_size =2, stride =2),\nnn.Conv2d( 6,16, kernel_size =5), nn .ReLU(),\nnn.MaxPool2d(kernel_size =2, stride =2),\nnn.Flatten(),\nnn.Linear( 16*5*5,120), nn .ReLU(),\nnn.Linear( 120,84), nn .ReLU(),\nnn.Linear( 84,10))\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2440, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0a75e71-0781-46ae-84f8-fb55ed2ac311": {"__data__": {"id_": "e0a75e71-0781-46ae-84f8-fb55ed2ac311", "embedding": null, "metadata": {"page_label": "538", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f621cc47-5312-418c-b270-1151e347880d", "node_type": "4", "metadata": {"page_label": "538", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9d8a9f9a0e14bab80981e2cf88d4f8a3700426041434f3b530ad27b1ce7c43f9", "class_name": "RelatedNodeInfo"}}, "text": "538 Optimization Algorithms\n(continued from previous page)\nreturn model\nloss =nn.CrossEntropyLoss()\ndevice =d2l.try_gpu()\nbatch_size =256\ntrain_iter, test_iter =d2l.load_data_fashion_mnist(batch_size =batch_size)\n# The code is almost identical to `d2l.train_ch6` defined in the\n# lenet section of chapter convolutional neural networks\ndef train (net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler =None ):\nnet.to(device)\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[0, num_epochs],\nlegend =['train loss ','train acc ','test acc '])\nfor epoch inrange (num_epochs):\nmetric =d2l.Accumulator( 3)# train_loss, train_acc, num_examples\nfor i, (X, y) inenumerate (train_iter):\nnet.train()\ntrainer .zero_grad()\nX, y =X.to(device), y .to(device)\ny_hat =net(X)\nl=loss(y_hat, y)\nl.backward()\ntrainer .step()\nwith torch .no_grad():\nmetric .add(l *X.shape[ 0], d2l .accuracy(y_hat, y), X .shape[ 0])\ntrain_loss =metric[ 0]/metric[ 2]\ntrain_acc =metric[ 1]/metric[ 2]\nif(i+1)%50==0:\nanimator .add(epoch +i/len(train_iter),\n(train_loss, train_acc, None ))\ntest_acc =d2l.evaluate_accuracy_gpu(net, test_iter)\nanimator .add(epoch +1, (None ,None , test_acc))\nifscheduler:\nifscheduler .__module__ ==lr_scheduler .__name__ :\n# Using PyTorch In-Built scheduler\nscheduler .step()\nelse :\n# Using custom defined scheduler\nfor param_group intrainer .param_groups:\nparam_group[ 'lr']=scheduler(epoch)\nprint (f'train loss {train_loss :.3f}, train acc {train_acc :.3f},'\nf'test acc {test_acc :.3f}')\nLet\u2019s have a look at what happens if we invoke this algorithm with default settings, such as\na learning rate of 0.3and train for 30iterations. Note how the training accuracy keeps on\nincreasing while progress in terms of test accuracy stalls beyond a point. The gap between\nboth curves indicates overfitting.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1805, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1be13b88-df6e-401f-ac76-ec6e1d03e2ad": {"__data__": {"id_": "1be13b88-df6e-401f-ac76-ec6e1d03e2ad", "embedding": null, "metadata": {"page_label": "539", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "63e2ed12-14f8-480e-ae4a-2f100494ac8b", "node_type": "4", "metadata": {"page_label": "539", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dab7dee573bdc2e664832e3db12dc55c557ccd5caec761c8ab67cee01ac8e17b", "class_name": "RelatedNodeInfo"}}, "text": "539 Learning Rate Scheduling\nlr, num_epochs =0.3,30\nnet =net_fn()\ntrainer =torch .optim .SGD(net .parameters(), lr =lr)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device)\ntrain loss 0.145 , train acc 0.944 , test acc 0.877\n12.11.2Schedulers\nOne way of adjusting the learning rate is to set it explicitly at each step. This is conve-\nniently achieved by the set_learning_rate method. We could adjust it downward after\nevery epoch (or even after every minibatch), e.g., in a dynamic manner in response to how\noptimization is progressing.\nlr=0.1\ntrainer .param_groups[ 0][\"lr\"]=lr\nprint (f'learning rate is now {trainer .param_groups[ 0][\"lr\"]:.2f}')\nlearning rate isnow 0.10\nMore generally we want to define a scheduler. When invoked with the number of updates\nit returns the appropriate value of the learning rate. Let\u2019s define a simple one that sets the\nlearning rate to \ud835\udf02=\ud835\udf020\u00b9\ud835\udc61\u00b81\u00ba\u00001\n2.\nclass SquareRootScheduler :\ndef __init__ (self , lr =0.1):\nself .lr=lr\ndef __call__ (self , num_update):\nreturn self .lr*pow(num_update +1.0,-0.5)\nLet\u2019s plot its behavior over a range of values.\nscheduler =SquareRootScheduler(lr =0.1)\nd2l.plot(torch .arange(num_epochs), [scheduler(t) for tinrange (num_epochs)])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ce51d43-d267-4af5-ad44-2bbacca68d3b": {"__data__": {"id_": "5ce51d43-d267-4af5-ad44-2bbacca68d3b", "embedding": null, "metadata": {"page_label": "540", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68700776-4698-41dd-b88f-c3d7b77893be", "node_type": "4", "metadata": {"page_label": "540", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "628d1bd2181b23291731a107b235433c02407ff8232e32582424ce35540ed583", "class_name": "RelatedNodeInfo"}}, "text": "540 Optimization Algorithms\nNow let\u2019s see how this plays out for training on Fashion-MNIST. We simply provide the\nscheduler as an additional argument to the training algorithm.\nnet =net_fn()\ntrainer =torch .optim .SGD(net .parameters(), lr)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)\ntrain loss 0.273 , train acc 0.900 , test acc 0.886\nThis worked quite a bit better than previously. Two things stand out: the curve was rather\nmoresmooththan previously. Secondly, therewaslessoverfitting. Unfortunatelyitis nota\nwell-resolved question as to why certain strategies lead to less overfitting in theory. There\nis some argument that a smaller stepsize will lead to parameters that are closer to zero and\nthussimpler. However,thisdoesnotexplainthephenomenonentirelysincewedonotreally\nstop early but simply reduce the learning rate gently.\n12.11.3Policies\nWhile we cannot possibly cover the entire variety of learning rate schedulers, we attempt\nto give a brief overview of popular policies below. Common choices are polynomial decay\nand piecewise constant schedules. Beyond that, cosine learning rate schedules have been\nfoundtoworkwellempiricallyonsomeproblems. Lastly,onsomeproblemsitisbeneficial\nto warm up the optimizer prior to using large learning rates.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1291, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f77e1580-3202-4069-8396-b4d301c9eb63": {"__data__": {"id_": "f77e1580-3202-4069-8396-b4d301c9eb63", "embedding": null, "metadata": {"page_label": "541", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "04719050-31d9-4f6f-b79f-cdf83f9eb875", "node_type": "4", "metadata": {"page_label": "541", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "90c6f9c6c528601d0189c39aa2ad65bf68679b1ed57335fee5fd8f480a127b6a", "class_name": "RelatedNodeInfo"}}, "text": "541 Learning Rate Scheduling\nFactorScheduler\nOne alternative to a polynomial decay would be a multiplicative one, that is \ud835\udf02\ud835\udc61\u00b81 \ud835\udf02\ud835\udc61\u0001\ud835\udefc\nfor\ud835\udefc2\u00b90,1\u00ba. Topreventthelearningratefromdecayingbeyondareasonablelowerbound\nthe update equation is often modified to \ud835\udf02\ud835\udc61\u00b81 max\u00b9\ud835\udf02min,\ud835\udf02\ud835\udc61\u0001\ud835\udefc\u00ba.\nclass FactorScheduler :\ndef __init__ (self , factor =1, stop_factor_lr =1e-7 , base_lr =0.1):\nself .factor =factor\nself .stop_factor_lr =stop_factor_lr\nself .base_lr =base_lr\ndef __call__ (self , num_update):\nself .base_lr =max(self .stop_factor_lr, self .base_lr *self .factor)\nreturn self .base_lr\nscheduler =FactorScheduler(factor =0.9, stop_factor_lr =1e-2 , base_lr =2.0)\nd2l.plot(torch .arange( 50), [scheduler(t) for tinrange (50)])\nThis can also be accomplished by a built-in scheduler in MXNet via the lr_scheduler.\nFactorScheduler object. Ittakesafewmoreparameters,suchaswarmupperiod,warmup\nmode (linear or constant), the maximum number of desired updates, etc.; Going forward\nwe will use the built-in schedulers as appropriate and only explain their functionality here.\nAs illustrated, it is fairly straightforward to build your own scheduler if needed.\nMulti FactorScheduler\nA common strategy for training deep networks is to keep the learning rate piecewise con-\nstant and to decrease it by a given amount every so often. That is, given a set of times\nwhen to decrease the rate, such as \ud835\udc60=f5,10,20gdecrease\ud835\udf02\ud835\udc61\u00b81 \ud835\udf02\ud835\udc61\u0001\ud835\udefcwhenever\ud835\udc612\ud835\udc60.\nAssuming that the values are halved at each step we can implement this as follows.\nnet =net_fn()\ntrainer =torch .optim .SGD(net .parameters(), lr =0.5)\nscheduler =lr_scheduler .MultiStepLR(trainer, milestones =[15,30], gamma =0.5)\ndef get_lr (trainer, scheduler):\nlr=scheduler .get_last_lr()[ 0]\ntrainer .step()\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3a9d673-29be-4d30-9ce1-122a3ebc38bc": {"__data__": {"id_": "f3a9d673-29be-4d30-9ce1-122a3ebc38bc", "embedding": null, "metadata": {"page_label": "542", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "45604538-8ec0-4b22-8efe-e581291900fd", "node_type": "4", "metadata": {"page_label": "542", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4a11240852c05b75ff0e10551041809ccfc9146fda946f4ec50e548bbc15d8bb", "class_name": "RelatedNodeInfo"}}, "text": "542 Optimization Algorithms\n(continued from previous page)\nscheduler .step()\nreturn lr\nd2l.plot(torch .arange(num_epochs), [get_lr(trainer, scheduler)\nfor tinrange (num_epochs)])\nThe intuition behind this piecewise constant learning rate schedule is that one lets opti-\nmization proceed until a stationary point has been reached in terms of the distribution of\nweight vectors. Then (and only then) do we decrease the rate such as to obtain a higher\nquality proxy to a good local minimum. The example below shows how this can produce\never slightly better solutions.\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)\ntrain loss 0.194 , train acc 0.927 , test acc 0.869\nCosine Scheduler\nA rather perplexing heuristic was proposed by Loshchilov and Hutter ( 2016). It relies on\nthe observation that we might not want to decrease the learning rate too drastically in the\nbeginningandmoreover,thatwemightwantto\u201crefine\u201dthesolutionintheendusingavery\nsmall learning rate. This results in a cosine-like schedule with the following functional", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36308a0f-39e2-4f05-bcbd-418eec319b20": {"__data__": {"id_": "36308a0f-39e2-4f05-bcbd-418eec319b20", "embedding": null, "metadata": {"page_label": "543", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7706c506-7aab-4131-b8fd-7cc38fb6d9b9", "node_type": "4", "metadata": {"page_label": "543", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bc398e83d48269f482513b3272d775e239966e711c5bb1dfceee143215476504", "class_name": "RelatedNodeInfo"}}, "text": "543 Learning Rate Scheduling\nform for learning rates in the range \ud835\udc612\u00bb0,\ud835\udc47\u00bc.\n\ud835\udf02\ud835\udc61=\ud835\udf02\ud835\udc47\u00b8\ud835\udf020\u0000\ud835\udf02\ud835\udc47\n2\u00b91\u00b8cos\u00b9\ud835\udf0b\ud835\udc61\u009d\ud835\udc47\u00ba\u00ba (12.11.1)\nHere\ud835\udf020is the initial learning rate, \ud835\udf02\ud835\udc47is the target rate at time \ud835\udc47. Furthermore, for \ud835\udc61 > \ud835\udc47\nwe simply pin the value to \ud835\udf02\ud835\udc47without increasing it again. In the following example, we set\nthe max update step \ud835\udc47=20.\nclass CosineScheduler :\ndef __init__ (self , max_update, base_lr =0.01 , final_lr =0,\nwarmup_steps =0, warmup_begin_lr =0):\nself .base_lr_orig =base_lr\nself .max_update =max_update\nself .final_lr =final_lr\nself .warmup_steps =warmup_steps\nself .warmup_begin_lr =warmup_begin_lr\nself .max_steps =self .max_update -self .warmup_steps\ndef get_warmup_lr (self , epoch):\nincrease =(self .base_lr_orig -self .warmup_begin_lr) \\\n*float (epoch) /float (self .warmup_steps)\nreturn self .warmup_begin_lr +increase\ndef __call__ (self , epoch):\nifepoch <self .warmup_steps:\nreturn self .get_warmup_lr(epoch)\nifepoch <=self .max_update:\nself .base_lr =self .final_lr +(\nself .base_lr_orig -self .final_lr) *(1+math .cos(\nmath .pi*(epoch -self .warmup_steps) /self .max_steps)) /2\nreturn self .base_lr\nscheduler =CosineScheduler(max_update =20, base_lr =0.3, final_lr =0.01 )\nd2l.plot(torch .arange(num_epochs), [scheduler(t) for tinrange (num_epochs)])\nIn the context of computer vision this schedule canlead to improved results. Note, though,\nthat such improvements are not guaranteed (as can be seen below).\nnet =net_fn()\ntrainer =torch .optim .SGD(net .parameters(), lr =0.3)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d317841-d6b4-4d9a-9d91-aea659919974": {"__data__": {"id_": "6d317841-d6b4-4d9a-9d91-aea659919974", "embedding": null, "metadata": {"page_label": "544", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b770969-928a-4557-91cc-3b75cf385d6c", "node_type": "4", "metadata": {"page_label": "544", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cc9766e702723f1caed402b89b647d8c4127e5775c99af72eea32fc4e6851b70", "class_name": "RelatedNodeInfo"}}, "text": "544 Optimization Algorithms\ntrain loss 0.159 , train acc 0.942 , test acc 0.904\nWarmup\nInsomecasesinitializingtheparametersisnotsufficienttoguaranteeagoodsolution. This\nis particularly a problem for some advanced network designs that may lead to unstable\noptimization problems. We could address this by choosing a sufficiently small learning\nrateto preventdivergencein the beginning. Unfortunatelythismeans that progress isslow.\nConversely, a large learning rate initially leads to divergence.\nArathersimplefixforthisdilemmaistouseawarmupperiodduringwhichthelearningrate\nincreases to its initial maximum and to cool down the rate until the end of the optimization\nprocess. For simplicity one typically uses a linear increase for this purpose. This leads to\na schedule of the form indicated below.\nscheduler =CosineScheduler( 20, warmup_steps =5, base_lr =0.3, final_lr =0.01 )\nd2l.plot(torch .arange(num_epochs), [scheduler(t) for tinrange (num_epochs)])\nNote that the network converges better initially (in particular observe the performance dur-\ning the first 5 epochs).\nnet =net_fn()\ntrainer =torch .optim .SGD(net .parameters(), lr =0.3)\ntrain(net, train_iter, test_iter, num_epochs, loss, trainer, device,\nscheduler)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e37ee13f-6795-4075-b3ab-6daa57838177": {"__data__": {"id_": "e37ee13f-6795-4075-b3ab-6daa57838177", "embedding": null, "metadata": {"page_label": "545", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1615bf7e-867c-4039-9a78-65cbede6fa37", "node_type": "4", "metadata": {"page_label": "545", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c4cc4c9610e822b4a76e0f9f0fbd5c0ce173b66994f33caa066d471e0b048da4", "class_name": "RelatedNodeInfo"}}, "text": "545 Learning Rate Scheduling\n182train loss 0.181 , train acc 0.934 , test acc 0.901\nWarmup can be applied to any scheduler (not just cosine). For a more detailed discussion\nof learning rate schedules and many more experiments see also ( Gotmare et al., 2018). In\nparticular they find that a warmup phase limits the amount of divergence of parameters\nin very deep networks. This makes intuitively sense since we would expect significant\ndivergence due to random initialization in those parts of the network that take the most\ntime to make progress in the beginning.\n12.11.4Summary\n\u000fDecreasing the learning rate during training can lead to improved accuracy and (most\nperplexingly) reduced overfitting of the model.\n\u000fA piecewise decrease of the learning rate whenever progress has plateaued is effective\nin practice. Essentially this ensures that we converge efficiently to a suitable solution\nand only then reduce the inherent variance of the parameters by reducing the learning\nrate.\n\u000fCosine schedulers are popular for some computer vision problems. See e.g., GluonCV\n182for details of such a scheduler.\n\u000fA warmup period before optimization can prevent divergence.\n\u000fOptimizationservesmultiplepurposesindeeplearning. Besidesminimizingthetraining\nobjective, different choices of optimization algorithms and learning rate scheduling\ncan lead to rather different amounts of generalization and overfitting on the test set\n(for the same amount of training error).\n12.11.5Exercises\n1.Experiment with the optimization behavior for a given fixed learning rate. What is the\nbest model you can obtain this way?\n2.Howdoesconvergencechangeifyouchangetheexponentofthedecreaseinthelearning\nrate? Use PolyScheduler for your convenience in the experiments.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1739, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66e85f99-0c67-41b0-be5d-5979d68087e3": {"__data__": {"id_": "66e85f99-0c67-41b0-be5d-5979d68087e3", "embedding": null, "metadata": {"page_label": "546", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5bc341a-b7d8-4df2-b0bf-2b6f04d8114d", "node_type": "4", "metadata": {"page_label": "546", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6352cdf8171f2e18a2d62e91c73fb374eac865dcd633584351a5dbc668bf10dd", "class_name": "RelatedNodeInfo"}}, "text": "546 Optimization Algorithms\n1833.Apply the cosine scheduler to large computer vision problems, e.g., training ImageNet.\nHow does it affect performance relative to other schedulers?\n4.How long should warmup last?\n5.Can you connect optimization and sampling? Start by using results from Welling and\nTeh (2011) on Stochastic Gradient Langevin Dynamics.\nDiscussions183.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9984db5b-b7c4-48ad-9de7-012bb6960189": {"__data__": {"id_": "9984db5b-b7c4-48ad-9de7-012bb6960189", "embedding": null, "metadata": {"page_label": "547", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "690ad7bd-6c9c-4e1d-a8ef-3911b671bf0a", "node_type": "4", "metadata": {"page_label": "547", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c2cd9c90302b4e2299b99ef56eea0244024451eb2f1b7dae05914c991141d52e", "class_name": "RelatedNodeInfo"}}, "text": "13 Computational Performance\nIn deep learning, datasets and models are usually large, which involves heavy computa-\ntion. Therefore, computational performance matters a lot. This chapter will focus on the\nmajor factors that affect computational performance: imperative programming, symbolic\nprogramming, asynchronous computing, automatic parallelism, and multi-GPU computa-\ntion. By studying this chapter, you may further improve computational performance of\nthosemodelsimplementedinthepreviouschapters, forexample, byreducingtrainingtime\nwithout affecting accuracy.\n13.1Compilersand Interpreters\nSo far, this book has focused on imperative programming, which makes use of statements\nsuch as print,+, and ifto change a program\u2019s state. Consider the following example of a\nsimple imperative program.\ndef add(a, b):\nreturn a+b\ndef fancy_func (a, b, c, d):\ne=add(a, b)\nf=add(c, d)\ng=add(e, f)\nreturn g\nprint (fancy_func( 1,2,3,4))\n10\nPython is an interpreted language . When evaluating the above fancy_func function it\nperformstheoperationsmakingupthefunction\u2019sbody insequence . Thatis,itwillevaluate\ne = add(a, b) and store the results as variable e, thereby changing the program\u2019s state.\nThe next two statements f = add(c, d) andg = add(e, f) will be executed similarly,\nperforming additions and storing the results as variables. Fig. 13.1.1 illustrates the flow of\ndata.\n547", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1374, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5d4d179-a075-4fd0-96c2-389d6964f07d": {"__data__": {"id_": "e5d4d179-a075-4fd0-96c2-389d6964f07d", "embedding": null, "metadata": {"page_label": "548", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fac7fc97-7b4d-4e5f-88ba-33534d065572", "node_type": "4", "metadata": {"page_label": "548", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "de63025aba993188dc8d5dc954a32776f7520817f0ac4e333f2a2be3fdb9c09d", "class_name": "RelatedNodeInfo"}}, "text": "548 Computational Performance\ntFig. 13.1.1 Data \ufb02ow in an imperative program.\nAlthough imperative programming is convenient, it may be inefficient. On the one hand,\neven if the addfunction is repeatedly called throughout fancy_func , Python will execute\nthe three function calls individually. If these are executed, say, on a GPU (or even on mul-\ntiple GPUs), the overhead arising from the Python interpreter can become overwhelming.\nMoreover, it will need to save the variable values of eandfuntil all the statements in\nfancy_func have been executed. This is because we do not know whether the variables e\nandfwill be used by other parts of the program after the statements e = add(a, b) and\nf = add(c, d) are executed.\n13.1.1SymbolicProgramming\nConsider the alternative, symbolicprogramming , where computation is usually performed\nonlyoncetheprocesshasbeenfullydefined. Thisstrategyisusedbymultipledeeplearning\nframeworks, including Theano and TensorFlow (the latter has acquired imperative exten-\nsions). It usually involves the following steps:\n1.Define the operations to be executed.\n2.Compile the operations into an executable program.\n3.Provide the required inputs and call the compiled program for execution.\nThis allows for a significant amount of optimization. First, we can skip the Python inter-\npreter in many cases, thus removing a performance bottleneck that can become significant\non multiple fast GPUs paired with a single Python thread on a CPU. Second, a compiler\nmight optimize and rewrite the above code into print((1 + 2) + (3 + 4)) or even\nprint(10) . This is possible since a compiler gets to see the full code before turning it into\nmachine instructions. For instance, it can release memory (or never allocate it) whenever a\nvariableisnolongerneeded. Oritcantransformthecodeentirelyintoanequivalentpiece.\nTo get a better idea, consider the following simulation of imperative programming (it is\nPython after all) below.\ndef add_ ():\nreturn '''\ndef add(a, b):\nreturn a + b\n'''\ndef fancy_func_ ():\nreturn '''\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d1934f06-3852-4b7d-a61c-356bc1cb2483": {"__data__": {"id_": "d1934f06-3852-4b7d-a61c-356bc1cb2483", "embedding": null, "metadata": {"page_label": "549", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c4bf5345-77f6-43e9-b79d-21a790aa1ebd", "node_type": "4", "metadata": {"page_label": "549", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bcc1e9236f05decc456f9c2563533609fe0d01b85c6a82f7c3f8c053068c23fd", "class_name": "RelatedNodeInfo"}}, "text": "549 Compilers and Interpreters\n(continued from previous page)\ndef fancy_func(a, b, c, d):\ne = add(a, b)\nf = add(c, d)\ng = add(e, f)\nreturn g\n'''\ndef evoke_ ():\nreturn add_() +fancy_func_() +'print(fancy_func(1, 2, 3, 4)) '\nprog =evoke_()\nprint (prog)\ny=compile (prog, '','exec ')\nexec(y)\ndef add(a, b):\nreturn a+b\ndef fancy_func (a, b, c, d):\ne=add(a, b)\nf=add(c, d)\ng=add(e, f)\nreturn g\nprint (fancy_func( 1,2,3,4))\n10\nThedifferencesbetweenimperative(interpreted)programmingandsymbolicprogramming\nare as follows:\n\u000fImperative programming is easier. When imperative programming is used in Python,\nthe majority of the code is straightforward and easy to write. It is also easier to de-\nbug imperative programming code. This is because it is easier to obtain and print all\nrelevant intermediate variable values, or use Python\u2019s built-in debugging tools.\n\u000fSymbolic programming is more efficient and easier to port. Symbolic programming\nmakes it easier to optimize the code during compilation, while also having the ability\nto port the program into a format independent of Python. This allows the program to\nbe run in a non-Python environment, thus avoiding any potential performance issues\nrelated to the Python interpreter.\n13.1.2Hybrid Programming\nHistorically most deep learning frameworks choose between an imperative or a symbolic\napproach. For example, Theano, TensorFlow (inspired by the former), Keras, and CNTK\nformulate models symbolically. Conversely, Chainer and PyTorch take an imperative ap-\nproach. AnimperativemodewasaddedtoTensorFlow2.0andKerasinlaterrevisions.\nAsmentionedabove,PyTorchisbasedonimperativeprogrammingandusesdynamiccom-\nputationgraphs. Inanefforttoleveragetheportabilityandefficiencyofsymbolicprogram-\nming, developers considered whether it would be possible to combine the benefits of both", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "783a9e90-e70b-472b-977a-bee0fdd8ef0f": {"__data__": {"id_": "783a9e90-e70b-472b-977a-bee0fdd8ef0f", "embedding": null, "metadata": {"page_label": "550", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce46e09e-73ba-438d-b8bf-95a85710187a", "node_type": "4", "metadata": {"page_label": "550", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "83045cbd2aa971ce5883155bac7ad9f1904fcc2db11aefd56ab2ea72590153ba", "class_name": "RelatedNodeInfo"}}, "text": "550 Computational Performance\nprogramming paradigms. This led to a torchscript that lets users develop and debug us-\ning pure imperative programming, while having the ability to convert most programs into\nsymbolic programs to be run when product-level computing performance and deployment\nare required.\n13.1.3Hybridizing the Sequential Class\nTheeasiestwaytogetafeelforhowhybridizationworksistoconsiderdeepnetworkswith\nmultiple layers. Conventionally the Python interpreter will need to execute the code for all\nlayerstogenerateaninstructionthatcanthenbeforwardedtoaCPUoraGPU.Forasingle\n(fast) computing device this does not cause any major issues. On the other hand, if we use\nanadvanced8-GPUserversuchasanAWSP3dn.24xlargeinstancePythonwillstruggleto\nkeep all GPUs busy. The single-threaded Python interpreter becomes the bottleneck here.\nLet\u2019sseehowwecanaddressthisforsignificantpartsofthecodebyreplacing Sequential\nwith HybridSequential . We begin by defining a simple MLP.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n# Factory for networks\ndef get_net ():\nnet =nn.Sequential(nn .Linear( 512,256),\nnn.ReLU(),\nnn.Linear( 256,128),\nnn.ReLU(),\nnn.Linear( 128,2))\nreturn net\nx=torch .randn(size =(1,512))\nnet =get_net()\nnet(x)\ntensor([[ -0.1602 ,0.0003 ]], grad_fn =<AddmmBackward0 >)\nByconvertingthemodelusing torch.jit.script function,weareabletocompileandop-\ntimizethecomputationintheMLP.Themodel\u2019scomputationresultremainsunchanged.\nnet =torch .jit.script(net)\nnet(x)\ntensor([[ -0.1602 ,0.0003 ]], grad_fn =<AddmmBackward0 >)\nThis seems almost too good to be true: write the same code as before and simply convert\nthe model using torch.jit.script . Once this happens the network is optimized (we will\nbenchmark the performance below).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d0df072-f17d-4e20-95f6-4889ffc8832e": {"__data__": {"id_": "4d0df072-f17d-4e20-95f6-4889ffc8832e", "embedding": null, "metadata": {"page_label": "551", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37c84395-2870-4773-8d1e-8e0bd03df537", "node_type": "4", "metadata": {"page_label": "551", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "63898d3283ce547158f443b1016f1fca3cf41fe7fb83af245f96bbc1c6b88e70", "class_name": "RelatedNodeInfo"}}, "text": "551 Compilers and Interpreters\nAccelerationbyHybridization\nTo demonstrate the performance improvement gained by compilation we compare the time\nneeded to evaluate net(x)before and after hybridization. Let\u2019s define a class to measure\nthis time first. It will come handy throughout the chapter as we set out to measure (and\nimprove) performance.\n#@save\nclass Benchmark :\n\"\"\"For measuring running time.\"\"\"\ndef __init__ (self , description ='Done '):\nself .description =description\ndef __enter__ (self ):\nself .timer =d2l.Timer()\nreturn self\ndef __exit__ (self ,*args):\nprint (f'{self .description }:{self .timer .stop() :.4f}sec')\nNow we can invoke the network twice, once with and once without torchscript.\nnet =get_net()\nwith Benchmark( 'Without torchscript '):\nfor iinrange (1000 ): net(x)\nnet =torch .jit.script(net)\nwith Benchmark( 'With torchscript '):\nfor iinrange (1000 ): net(x)\nWithout torchscript: 2.1447 sec\nWith torchscript: 4.0545 sec\nAs is observed in the above results, after an nn.Sequential instance is scripted using\nthetorch.jit.script function, computing performance is improved through the use of\nsymbolic programming.\nSerialization\nOne of the benefits of compiling the models is that we can serialize (save) the model and\nits parameters to disk. This allows us to store a model in a manner that is independent of\nthe front-end language of choice. This allows us to deploy trained models to other devices\nand easily use other front-end programming languages. At the same time the code is often\nfaster than what can be achieved in imperative programming. Let\u2019s see the savefunction\nin action.\nnet.save('my_mlp')\n!ls -lh my_mlp*", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1645, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e4669fb-533f-4142-93ba-d9cf72c3cda0": {"__data__": {"id_": "9e4669fb-533f-4142-93ba-d9cf72c3cda0", "embedding": null, "metadata": {"page_label": "552", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b706f7a6-6818-471b-8e59-a68955d784a0", "node_type": "4", "metadata": {"page_label": "552", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a371b06d5522e7364e2c84a19b2440101b3a6f6dfd6c6dd28f52a8963147fae7", "class_name": "RelatedNodeInfo"}}, "text": "552 Computational Performance\n184-rw-r--r--1ci ci 651K Aug 1819:32my_mlp\n13.1.4Summary\n\u000fImperativeprogrammingmakesiteasytodesignnewmodelssinceitispossibletowrite\ncode with control flow and the ability to use a large amount of the Python software\necosystem.\n\u000fSymbolic programming requires that we specify the program and compile it before exe-\ncuting it. The benefit is improved performance.\n13.1.5Exercises\n1.Review the models that interest you in the previous chapters. Can you improve their\ncomputational performance by reimplementing them?\nDiscussions184.\n13.2AsynchronousComputation\nToday\u2019s computers are highly parallel systems, consisting of multiple CPU cores (often\nmultiplethreadspercore),multipleprocessingelementsperGPU,andoftenmultipleGPUs\nperdevice. Inshort, wecanprocessmanydifferentthingsatthesametime, oftenondiffer-\nent devices. Unfortunately Python is not a great way of writing parallel and asynchronous\ncode, at least not without some extra help. After all, Python is single-threaded and this is\nunlikely to change in the future. Deep learning frameworks such as MXNet and Tensor-\nFlow adopt an asynchronousprogramming model to improve performance, while PyTorch\nuses Python\u2019s own scheduler leading to a different performance trade-off. For PyTorch, by\ndefault,GPUoperationsareasynchronous. WhenyoucallafunctionthatusestheGPU,the\noperations are enqueued to the particular device, but not necessarily executed until later.\nThis allows us to execute more computations in parallel, including operations on the CPU\nor other GPUs.\nHence, understanding how asynchronous programming works helps us to develop more\nefficient programs, by proactively reducing computational requirements and mutual de-\npendencies. This allows us to reduce memory overhead and increase processor utiliza-\ntion.\nimport os\nimport subprocess\nimport numpy\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1906, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f4d91e1-4e91-4182-aac8-26c46f018ffb": {"__data__": {"id_": "8f4d91e1-4e91-4182-aac8-26c46f018ffb", "embedding": null, "metadata": {"page_label": "553", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "73bd739d-87cc-4c9a-a1db-05fdd5fedaf2", "node_type": "4", "metadata": {"page_label": "553", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "34bcfcfbc62d3792c832d3ccebd3fc4d1cb23fde3e8842606f4dc7f4d2e3ba50", "class_name": "RelatedNodeInfo"}}, "text": "553 Asynchronous Computation\n13.2.1Asynchronyvia Backend\nFor a warmup consider the following toy problem: we want to generate a random matrix\nand multiply it. Let\u2019s do that both in NumPy and in PyTorch tensor to see the difference.\nNote that PyTorch tensoris defined on a GPU.\n# Warmup for GPU computation\ndevice =d2l.try_gpu()\na=torch .randn(size =(1000 ,1000 ), device =device)\nb=torch .mm(a, a)\nwith d2l.Benchmark( 'numpy '):\nfor _inrange (10):\na=numpy .random .normal(size =(1000 ,1000 ))\nb=numpy .dot(a, a)\nwith d2l.Benchmark( 'torch '):\nfor _inrange (10):\na=torch .randn(size =(1000 ,1000 ), device =device)\nb=torch .mm(a, a)\nnumpy: 1.4693 sec\ntorch: 0.0022 sec\nThebenchmarkoutputviaPyTorchisordersofmagnitudefaster. NumPydotproductisex-\necuted on the CPU processor while PyTorchmatrix multiplication is executedon GPU and\nhence the latter is expected to be much faster. But the huge time difference suggests some-\nthing else must be going on. By default, GPU operations are asynchronous in PyTorch.\nForcing PyTorch to finish all computation prior to returning shows what happened previ-\nously: computation is being executed by the backend while the frontend returns control to\nPython.\nwith d2l.Benchmark():\nfor _inrange (10):\na=torch .randn(size =(1000 ,1000 ), device =device)\nb=torch .mm(a, a)\ntorch .cuda .synchronize(device)\nDone: 0.0058 sec\nBroadly speaking, PyTorch has a frontend for direct interaction with the users, e.g., via\nPython, as well as a backend used by the system to perform the computation. As shown\ninFig. 13.2.1 , users can write PyTorch programs in various frontend languages, such as\nPython and C++. Regardless of the frontend programming language used, the execution of\nPyTorch programs occurs primarily in the backend of C++ implementations. Operations\nissued by the frontend language are passed on to the backend for execution. The backend\nmanages its own threads that continuously collect and execute queued tasks. Note that for\nthis to work the backend must be able to keep track of the dependencies between various", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2052, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a9697de-950f-44de-81a2-b7c06bf8d755": {"__data__": {"id_": "1a9697de-950f-44de-81a2-b7c06bf8d755", "embedding": null, "metadata": {"page_label": "554", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edd683e8-36dd-437d-b3d1-210e8e63af37", "node_type": "4", "metadata": {"page_label": "554", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5a1703f6c29197e6465bef6bb8b999a02885affcaa6dc71a069c189fdfa007a1", "class_name": "RelatedNodeInfo"}}, "text": "554 Computational Performance\nsteps in the computational graph. Hence, it is not possible to parallelize operations that\ndepend on each other.\ntFig. 13.2.1 Programming language frontends and deep learning framework backends.\nLet\u2019s look at another toy example to understand the dependency graph a bit better.\nx=torch .ones(( 1,2), device =device)\ny=torch .ones(( 1,2), device =device)\nz=x*y+2\nz\ntensor([[ 3.,3.]], device ='cuda:0 ')\ntFig. 13.2.2 The backend tracks dependencies between various steps in the computational graph.\nThe code snippet above is also illustrated in Fig. 13.2.2 . Whenever the Python frontend\nthread executes one of the first three statements, it simply returns the task to the backend\nqueue. When the last statement\u2019s results need to be printed, the Python frontend thread\nwill wait for the C++ backend thread to finish computing the result of the variable z. One\nbenefit of this design is that the Python frontend thread does not need to perform actual\ncomputations. Thus,thereislittleimpactontheprogram\u2019soverallperformance,regardless\nofPython\u2019sperformance. Fig.13.2.3 illustrateshowfrontendandbackendinteract.\n13.2.2Barriers and Blockers\n13.2.3ImprovingComputation", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1190, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ebd354d-e48e-4be1-acf7-54a730a1a065": {"__data__": {"id_": "8ebd354d-e48e-4be1-acf7-54a730a1a065", "embedding": null, "metadata": {"page_label": "555", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2dba794-2510-4452-b49c-c8653cc43b56", "node_type": "4", "metadata": {"page_label": "555", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6a523074d23b5aa4699090ef41f618a8e007edb4cc26abb3afe98fe91c14d135", "class_name": "RelatedNodeInfo"}}, "text": "555 Automatic Parallelism\ntFig. 13.2.3 Interactions of the frontend and backend.\n18513.2.4Summary\n\u000fDeep learning frameworks may decouple the Python frontend from an execution back-\nend. This allows for fast asynchronous insertion of commands into the backend and\nassociated parallelism.\n\u000fAsynchrony leads to a rather responsive frontend. However, use caution not to overfill\nthetaskqueuesinceitmayleadtoexcessivememoryconsumption. Itisrecommended\nto synchronize for each minibatch to keep frontend and backend approximately syn-\nchronized.\n\u000fChip vendors offer sophisticated performance analysis tools to obtain a much more fine-\ngrained insight into the efficiency of deep learning.\n13.2.5Exercises\n1.On the CPU, benchmark the same matrix multiplication operations in this section. Can\nyou still observe asynchrony via the backend?\nDiscussions185.\n13.3AutomaticParallelism\nDeep learning frameworks (e.g., MXNet and PyTorch) automatically construct computa-\ntional graphs at the backend. Using a computational graph, the system is aware of all the\ndependencies, and can selectively execute multiple non-interdependent tasks in parallel to\nimprove speed. For instance, Fig. 13.2.2 inSection 13.2 initializes two variables indepen-\ndently. Consequently the system can choose to execute them in parallel.\nTypically,asingleoperatorwilluseallthecomputationalresourcesonallCPUsoronasin-\ngleGPU.Forexample,the dotoperatorwilluseallcores(andthreads)onallCPUs,evenif\nthere are multiple CPU processors on a single machine. The same applies to a single GPU.\nHence parallelization is not quite so useful for single-device computers. With multiple de-\nvicesthingsmattermore. Whileparallelizationistypicallymostrelevantbetweenmultiple\nGPUs,addingthelocalCPUwillincreaseperformanceslightly. Forexample,seeHadjis et", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1798, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89db3279-9a74-4327-8723-d4e708eabda9": {"__data__": {"id_": "89db3279-9a74-4327-8723-d4e708eabda9", "embedding": null, "metadata": {"page_label": "556", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8d02010-ef0d-4d79-a340-710361d24a40", "node_type": "4", "metadata": {"page_label": "556", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c22073ce3e1932361cd703d48024055ba4440a17b63715868b2e3395562e83e7", "class_name": "RelatedNodeInfo"}}, "text": "556 Computational Performance\nal.(2016) that focuses on training computer vision models combining a GPU and a CPU.\nWith the convenience of an automatically parallelizing framework we can accomplish the\nsamegoalinafewlinesofPythoncode. Morebroadly,ourdiscussionofautomaticparallel\ncomputation focuses on parallel computation using both CPUs and GPUs, as well as the\nparallelization of computation and communication.\nNote that we need at least two GPUs to run the experiments in this section.\nimport torch\nfrom d2l import torch asd2l\n13.3.1ParallelComputation on GPUs\nLet\u2019s start by defining a reference workload to test: the runfunction below performs 10\nmatrix-matrix multiplications on the device of our choice using data allocated into two\nvariables: x_gpu1andx_gpu2.\ndevices =d2l.try_all_gpus()\ndef run(x):\nreturn [x.mm(x) for _inrange (50)]\nx_gpu1 =torch .rand(size =(4000 ,4000 ), device =devices[ 0])\nx_gpu2 =torch .rand(size =(4000 ,4000 ), device =devices[ 1])\nNow we apply the function to the data. To ensure that caching does not play a role in the\nresults we warm up the devices by performing a single pass on either of them prior to mea-\nsuring. torch.cuda.synchronize() waitsforallkernelsinallstreamsonaCUDAdevice\nto complete. It takes in a deviceargument, the device for which we need to synchronize.\nIt uses the current device, given by current_device() , if the device argument is None\n(default).\nrun(x_gpu1)\nrun(x_gpu2) # Warm-up all devices\ntorch .cuda .synchronize(devices[ 0])\ntorch .cuda .synchronize(devices[ 1])\nwith d2l.Benchmark( 'GPU1 time '):\nrun(x_gpu1)\ntorch .cuda .synchronize(devices[ 0])\nwith d2l.Benchmark( 'GPU2 time '):\nrun(x_gpu2)\ntorch .cuda .synchronize(devices[ 1])\nGPU1 time: 0.4660 sec\nGPU2 time: 0.4510 sec\nIfweremovethe synchronize statementbetweenbothtasksthesystemisfreetoparallelize\ncomputation on both devices automatically.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1871, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c34a3bc8-fb03-4163-9904-b87f75e1a492": {"__data__": {"id_": "c34a3bc8-fb03-4163-9904-b87f75e1a492", "embedding": null, "metadata": {"page_label": "557", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6f6cfdc5-d3c9-47f6-9435-b1066c34f7ce", "node_type": "4", "metadata": {"page_label": "557", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "29cb082ff5e4c2bb4cf01d3fbddd8f1c19dcb4e7821d696596923830a97d84a2", "class_name": "RelatedNodeInfo"}}, "text": "557 Automatic Parallelism\nwith d2l.Benchmark( 'GPU1 & GPU2 '):\nrun(x_gpu1)\nrun(x_gpu2)\ntorch .cuda .synchronize()\nGPU1 &GPU2: 0.4659 sec\nIn the above case the total execution time is less than the sum of its parts, since the deep\nlearningframeworkautomaticallyschedulescomputationonbothGPUdeviceswithoutthe\nneed for sophisticated code on behalf of the user.\n13.3.2ParallelComputation and Communication\nIn many cases we need to move data between different devices, say between the CPU and\nGPU, or between different GPUs. For instance, this occurs when we want to perform dis-\ntributed optimization where we need to aggregate the gradients over multiple accelerator\ncards. Let\u2019s simulate this by computing on the GPU and then copying the results back to\nthe CPU.\ndef copy_to_cpu (x, non_blocking =False ):\nreturn [y.to('cpu', non_blocking =non_blocking) for yinx]\nwith d2l.Benchmark( 'Run on GPU1 '):\ny=run(x_gpu1)\ntorch .cuda .synchronize()\nwith d2l.Benchmark( 'Copy to CPU '):\ny_cpu =copy_to_cpu(y)\ntorch .cuda .synchronize()\nRun on GPU1: 0.4656 sec\nCopy to CPU: 2.3125 sec\nThisissomewhatinefficient. Notethatwecouldalreadystartcopyingpartsof ytotheCPU\nwhile the remainder of the list is still being computed. This situation occurs, e.g., when we\ncompute the (backprop) gradient on a minibatch. The gradients of some of the parameters\nwill be available earlier than that of others. Hence it works to our advantage to start using\nPCI-Express bus bandwidth while the GPU is still running. In PyTorch, several functions\nsuchas to()andcopy_() admitanexplicit non_blocking argument, whichletsthecaller\nbypass synchronization when it is unnecessary. Setting non_blocking=True allows us to\nsimulate this scenario.\nwith d2l.Benchmark( 'Run on GPU1 and copy to CPU '):\ny=run(x_gpu1)\ny_cpu =copy_to_cpu(y, True )\ntorch .cuda .synchronize()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "159bbe55-fb8f-43a7-a4a5-a5e4ec4bc2e3": {"__data__": {"id_": "159bbe55-fb8f-43a7-a4a5-a5e4ec4bc2e3", "embedding": null, "metadata": {"page_label": "558", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "98f68874-6f42-498c-9919-9a2cd3809441", "node_type": "4", "metadata": {"page_label": "558", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fdc0226da651949bc5b2138e0d0b9f623d769244c8d9fe07dde134d45bb5277b", "class_name": "RelatedNodeInfo"}}, "text": "558 Computational Performance\nRun on GPU1 and copy to CPU: 1.6907 sec\nThe total time required for both operations is (as expected) less than the sum of their parts.\nNote that this task is different from parallel computation as it uses a different resource: the\nbusbetweentheCPUandGPUs. Infact,wecouldcomputeonbothdevicesandcommuni-\ncate, all at the same time. As noted above, there is a dependency between computation and\ncommunication: y[i]must be computed before it can be copied to the CPU. Fortunately,\nthesystemcancopy y[i-1]whilecomputing y[i]toreducethetotalrunningtime.\nWe conclude with an illustration of the computational graph and its dependencies for a\nsimpletwo-layerMLPwhentrainingonaCPUandtwoGPUs,asdepictedin Fig.13.3.1 . It\nwouldbequitepainfultoscheduletheparallelprogramresultingfromthismanually. Thisis\nwhereitisadvantageoustohaveagraph-basedcomputingbackendforoptimization.\ntFig. 13.3.1 The computational graph and its dependencies of a two-layer MLP on a CPU and two\nGPUs.\n13.3.3Summary\n\u000fModern systems have a variety of devices, such as multiple GPUs and CPUs. They can\nbe used in parallel, asynchronously.\n\u000fModern systems also have a variety of resources for communication, such as PCI Ex-\npress, storage (typically solid-state drives or via networks), and network bandwidth.\nThey can be used in parallel for peak efficiency.\n\u000fThebackendcanimproveperformancethroughautomaticparallelcomputationandcom-\nmunication.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "734fe262-1096-4705-a8a1-a1bdd0f6366b": {"__data__": {"id_": "734fe262-1096-4705-a8a1-a1bdd0f6366b", "embedding": null, "metadata": {"page_label": "559", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8b14827-cf2c-4046-ac29-ad251910988d", "node_type": "4", "metadata": {"page_label": "559", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "74ce644fda97fe25c370386d769b9fa3721fef5120eda66a9994d551e7870090", "class_name": "RelatedNodeInfo"}}, "text": "559 Hardware\n186\n18713.3.4Exercises\n1.Eight operations were performed in the runfunction defined in this section. There\nare no dependencies between them. Design an experiment to see if the deep learning\nframework will automatically execute them in parallel.\n2.When the workload of an individual operator is sufficiently small, parallelization can\nhelp even on a single CPU or GPU. Design an experiment to verify this.\n3.Design an experiment that uses parallel computation on CPUs, GPUs, and communica-\ntion between both devices.\n4.Use a debugger such as NVIDIA\u2019s Nsight186to verify that your code is efficient.\n5.Designing computation tasks that include more complex data dependencies, and run\nexperiments to see if you can obtain the correct results while improving performance.\nDiscussions187.\n13.4Hardware\nBuilding systems with great performance requires a good understanding of the algorithms\nand models to capture the statistical aspects of the problem. At the same time it is also\nindispensable to have at least a modicum of knowledge of the underlying hardware. The\ncurrentsectionisnosubstituteforapropercourseonhardwareandsystemdesign. Instead,\nit might serve as a starting point for understanding why some algorithms are more efficient\nthan others and how to achieve good throughput. A good design can easily make a differ-\nenceofanorderofmagnitudeand,inturn,thiscanmakethedifferencebetweenbeingable\nto train a network (e.g., in a week) and not at all (in 3 months, thus missing the deadline).\nWe will start by looking at computers. Then we will zoom in to look more carefully at\nCPUs and GPUs. Lastly we zoom out to review how multiple computers are connected in\na server center or in the cloud.\ntFig. 13.4.1 Latency Numbers that every programmer should know.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1769, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06c68378-00f0-40e1-93ba-9ba74a519054": {"__data__": {"id_": "06c68378-00f0-40e1-93ba-9ba74a519054", "embedding": null, "metadata": {"page_label": "560", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ac5b7a1-2d51-4b10-95df-633045b4f0e5", "node_type": "4", "metadata": {"page_label": "560", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "15a170525e61dac95658b5d0ef9f2d691bc9801ff94b163b98a752824889cfc4", "class_name": "RelatedNodeInfo"}}, "text": "560 Computational Performance\n188\n189\n190Impatientreadersmaybeabletogetbywith Fig.13.4.1 . ItistakenfromColinScott\u2019s inter-\nactivepost188thatgivesagoodoverviewoftheprogressoverthepastdecade. Theoriginal\nnumbersareduetoJeffDean\u2019s Stanfordtalkfrom2010189. Thediscussionbelowexplains\nsomeoftherationaleforthesenumbersandhowtheycanguideusindesigningalgorithms.\nThe discussion below is very high level and cursory. It is clearly no substitute for a proper\ncoursebutratherjustmeanttoprovideenoughinformationforastatisticalmodelertomake\nsuitable design decisions. For an in-depth overview of computer architecture we refer the\nreader to ( Hennessy and Patterson, 2011 ) or a recent course on the subject, such as the one\nbyArste Asanovic190.\n13.4.1Computers\nMost deep learning researchers and practitioners have access to a computer with a fair\namount of memory, computation, some form of an accelerator such as a GPU, or multiples\nthereof. A computer consists of the following key components:\n\u000fAprocessor(alsoreferredtoasaCPU)thatisabletoexecutetheprogramswegiveit(in\naddition to running an operating system and many other things), typically consisting\nof 8 or more cores.\n\u000fMemory (RAM) to store and retrieve the results from computation, such as weight vec-\ntors and activations, and training data.\n\u000fAnEthernetnetworkconnection(sometimesmultiple)withspeedsrangingfrom1GB/s\nto 100 GB/s. On high end servers more advanced interconnects can be found.\n\u000fA high speed expansion bus (PCIe) to connect the system to one or more GPUs. Servers\nhave up to 8 accelerators, often connected in an advanced topology, while desktop\nsystems have 1 or 2, depending on the budget of the user and the size of the power\nsupply.\n\u000fDurable storage, such as a magnetic hard disk drive, a solid state drive, in many cases\nconnected using the PCIe bus. It provides efficient transfer of training data to the\nsystem and storage of intermediate checkpoints as needed.\ntFig. 13.4.2 Connectivity of components of a computer.\nAsFig. 13.4.2 indicates, most components (network, GPU, and storage) are connected to\nthe CPU across the PCIe bus. It consists of multiple lanes that are directly attached to the\nCPU. For instance AMD\u2019s Threadripper 3 has 64 PCIe 4.0 lanes, each of which is capable\n16Gbit/sdatatransferinbothdirections. ThememoryisdirectlyattachedtotheCPUwith\na total bandwidth of up to 100 GB/s.\nWhenweruncodeonacomputerweneedtoshuffledatatotheprocessors(CPUsorGPUs),", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f18da28c-51e1-4828-9eb2-29f1f2a0feff": {"__data__": {"id_": "f18da28c-51e1-4828-9eb2-29f1f2a0feff", "embedding": null, "metadata": {"page_label": "561", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c6af9c4-15a0-4449-9e92-5d81a4dad94e", "node_type": "4", "metadata": {"page_label": "561", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4fa7ed4197a60379cbb17e2a2315dce164214bfc3bc32904a21bba448be5be17", "class_name": "RelatedNodeInfo"}}, "text": "561 Hardware\n191\n192\n193\n194performcomputation,andthenmovetheresultsofftheprocessorbacktoRAManddurable\nstorage. Hence, in order to get good performance we need to make sure that this works\nseamlessly without any one of the systems becoming a major bottleneck. For instance, if\nwecannotloadimagesquicklyenoughtheprocessorwillnothaveanyworktodo. Likewise,\nif we cannot move matrices quickly enough to the CPU (or GPU), its processing elements\nwill starve. Finally, if we want to synchronize multiple computers across the network, the\nlatter should not slow down computation. One option is to interleave communication and\ncomputation. Let\u2019s have a look at the various components in more detail.\n13.4.2Memory\nAt its most basic memory is used to store data that needs to be readily accessible. At\npresent CPU RAM is typically of the DDR4191variety, offering 20\u201325 GB/s bandwidth\nper module. Each module has a 64-bit-wide bus. Typically pairs of memory modules are\nused to allow for multiple channels. CPUs have between 2 and 4 memory channels, i.e.,\nthey have between 4 0GB/s and 100 GB/s peak memory bandwidth. Often there are two\nbanks per channel. For instance AMD\u2019s Zen 3 Threadripper has 8 slots.\nWhile these numbers are impressive, indeed, they only tell part of the story. When we\nwant to read a portion from memory we first need to tell the memory module where the\ninformation can be found. That is, we first need to send the address to RAM. Once this\nis accomplished we can choose to read just a single 64 bit record or a long sequence of\nrecords. The latter is called burst read . In a nutshell, sending an address to memory and\nsetting up the transfer takes approximately 100 ns (details depend on the specific timing\ncoefficients of the memory chips used), every subsequent transfer takes only 0.2 ns. In\nshort, the first read is 500 times as expensive as subsequent ones! Note that we could\nperform up to 10,000,000 random reads per second. This suggests that we avoid random\nmemory access as far as possible and use burst reads (and writes) instead.\nMatters are a bit more complex when we take into account that we have multiple banks.\nEach bank can read memory largely independently. This means two things. On the one\nhand, the effective number of random reads is up to 4 times higher, provided that they are\nspread evenly across memory. It also means that it is still a bad idea to perform random\nreadssinceburstreadsare4timesfaster, too. Ontheotherhand, duetomemoryalignment\nto 64 bit boundaries it is a good idea to align any data structures with the same boundaries.\nCompilersdothisprettymuch automatically192whentheappropriateflagsareset. Curious\nreaders are encouraged to review a lecture on DRAMs such as the one by Zeshan Chishti\n193.\nGPUmemoryissubjecttoevenhigherbandwidthrequirementssincetheyhavemanymore\nprocessing elements than CPUs. By and large there are two options to address them. The\nfirst is to make the memory bus significantly wider. For instance, NVIDIA\u2019s RTX 2080\nTi has a 352-bit-wide bus. This allows for much more information to be transferred at\nthe same time. Second, GPUs use specific high-performance memory. Consumer-grade\ndevices, such as NVIDIA\u2019s RTX and Titan series typically use GDDR6194chips with over\n500 GB/s aggregate bandwidth. An alternative is to use HBM (high bandwidth memory)\nmodules. TheyuseaverydifferentinterfaceandconnectdirectlywithGPUsonadedicated", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2417396-627e-4ec8-b0db-0b873c97d11f": {"__data__": {"id_": "e2417396-627e-4ec8-b0db-0b873c97d11f", "embedding": null, "metadata": {"page_label": "562", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd2a8623-68d2-4d97-96cb-8a119015e1ae", "node_type": "4", "metadata": {"page_label": "562", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "949ee6ce814da9833f038ce2a4d16014d32b3353fdc89a04686866ef91de4010", "class_name": "RelatedNodeInfo"}}, "text": "562 Computational Performance\nsiliconwafer. Thismakesthemveryexpensiveandtheiruseistypicallylimitedtohigh-end\nserver chips, such as the NVIDIA Volta V100 series of accelerators. Quite unsurprisingly,\nGPU memory is generally muchsmaller than CPU memory due to the higher cost of the\nformer. For our purposes, by and large their performance characteristics are similar, just a\nlot faster. We can safely ignore the details for the purpose of this book. They only matter\nwhen tuning GPU kernels for high throughput.\n13.4.3Storage\nWe saw that some of the key characteristics of RAM are bandwidth andlatency. The same\nis true for storage devices, just that the differences can be even more extreme.\nHardDisk Drives\nHarddiskdrives (HDDs)havebeeninuseforoverhalfacentury. Inanutshelltheycontain\nanumberofspinningplatterswithheadsthatcanbepositionedtoreadorwriteatanygiven\ntrack. High-end disks hold up to 16 TB on 9 platters. One of the key benefits of HDDs\nis that they are relatively inexpensive. One of their many downsides are their typically\ncatastrophic failure modes and their relatively high read latency.\nTounderstandthelatter,considerthefactthatHDDsspinataround7,200RPM(revolutions\nperminute). Iftheyweremuchfastertheywouldshatterduetothecentrifugalforceexerted\non the platters. This has a major downside when it comes to accessing a specific sector\non the disk: we need to wait until the platter has rotated in position (we can move the\nheads but not accelerate the actual disks). Hence it can take over 8 ms until the requested\ndata is available. A common way this is expressed is to say that HDDs can operate at\napproximately100IOPs(input/outputoperationspersecond). Thisnumberhasessentially\nremained unchanged for the past two decades. Worse still, it is equally difficult to increase\nbandwidth (it is in the order of 100\u2013200 MB/s). After all, each head reads a track of bits,\nhence the bit rate only scales with the square root of the information density. As a result,\nHDDs are quickly becoming relegated to archival storage and low-grade storage for very\nlarge datasets.\nSolidState Drives\nSolid state drives (SSDs) use flash memory to store information persistently. This allows\nformuch faster access to stored records. Modern SSDs can operate at 100,000 to 500,000\nIOPs, i.e., up to 3 orders of magnitude faster than HDDs. Furthermore, their bandwidth\ncan reach 1\u20133GB/s, i.e., one order of magnitude faster than HDDs. These improvements\nsound almost too good to be true. Indeed, they come with the following caveats, due to the\nway SSDs are designed.\n\u000fSSDsstoreinformationinblocks(256KBorlarger). Theycanonlybewrittenasawhole,\nwhich takes significant time. Consequently bit-wise random writes on SSD have very\npoor performance. Likewise, writing data in general takes significant time since the\nblock has to be read, erased and then rewritten with new information. By now SSD", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f4765b9-948a-4531-b929-e23b4713ed4c": {"__data__": {"id_": "7f4765b9-948a-4531-b929-e23b4713ed4c", "embedding": null, "metadata": {"page_label": "563", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5ca777d-e9f3-4fe7-9439-d1199975d918", "node_type": "4", "metadata": {"page_label": "563", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "36527b36ff46df7b75df9f82d0c78d3c5c33de60406e7032ba26a23c00d749a2", "class_name": "RelatedNodeInfo"}}, "text": "563 Hardware\ncontrollers and firmware have developed algorithms to mitigate this. Nonetheless,\nwrites can be much slower, in particular for QLC (quad level cell) SSDs. The key\nfor improved performance is to maintain a queueof operations, to prefer reads and to\nwrite in large blocks if possible.\n\u000fThememorycellsinSSDswearoutrelativelyquickly(oftenalreadyafterafewthousand\nwrites). Wear-levelprotectionalgorithmsareabletospreadthedegradationovermany\ncells. That said, it is not recommended to use SSDs for swapping files or for large\naggregations of log-files.\n\u000fLastly, the massive increase in bandwidth has forced computer designers to attach SSDs\ndirectly to the PCIe bus. The drives capable of handling this, referred to as NVMe\n(Non Volatile Memory enhanced), can use up to 4 PCIe lanes. This amounts to up to\n8GB/s on PCIe 4.0.\nCloud Storage\nCloud storage provides a configurable range of performance. That is, the assignment of\nstorage to virtual machines is dynamic, both in terms of quantity and in terms of speed,\nas chosen by users. We recommend that users increase the provisioned number of IOPs\nwhenever latency is too high, e.g., during training with many small records.\n13.4.4CPUs\nCentral processing units (CPUs) are the centerpiece of any computer. They consist of a\nnumber of key components: processor cores that are able to execute machine code, a bus\nconnectingthem(thespecifictopologydifferssignificantlybetweenprocessormodels,gen-\nerations,andvendors),and cachestoallowforhigherbandwidthandlowerlatencymemory\naccess than what is possible by reads from main memory. Lastly, almost all modern CPUs\ncontainvector processing units to aid with high performance linear algebra and convolu-\ntions, as they are common in media processing and machine learning.\ntFig. 13.4.3 Intel Skylake consumer quad-core CPU.\nFig. 13.4.3 depicts an Intel Skylake consumer-grade quad-core CPU. It has an integrated", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa543bf2-77e4-47e0-88c6-f18ae73ac5c6": {"__data__": {"id_": "fa543bf2-77e4-47e0-88c6-f18ae73ac5c6", "embedding": null, "metadata": {"page_label": "564", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b022788-9596-4a0f-8a8c-a1b63a64704b", "node_type": "4", "metadata": {"page_label": "564", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "750abd1ecc6f7a924dcca6d8ca49ed53043ef317fb004ab3183e52fa7649232b", "class_name": "RelatedNodeInfo"}}, "text": "564 Computational Performance\n195GPU,caches, andaringbusconnectingthefourcores. Peripherals, suchasEthernet, WiFi,\nBluetooth, SSD controller, and USB, are either part of the chipset or directly attached\n(PCIe) to the CPU.\nMicroarchitecture\nEach of the processor cores consists of a rather sophisticated set of components. While\ndetails differ between generations and vendors, the basic functionality is pretty much stan-\ndard. The front-end loads instructions and tries to predict which path will be taken (e.g.,\nfor control flow). Instructions are then decoded from assembly code to microinstructions.\nAssembly code is often not the lowest level code that a processor executes. Instead, com-\nplex instructions may be decoded into a set of more lower level operations. These are then\nprocessed by the actual execution core. Often the latter is capable of performing many op-\nerations simultaneously. For instance, the ARM Cortex A77 core of Fig. 13.4.4 is able to\nperform up to 8 operations simultaneously.\ntFig. 13.4.4 ARM Cortex A77 Microarchitecture.\nThis means that efficient programs might be able to perform more than one instruction per\nclock cycle, provided that they can be carried out independently. Not all units are created\nequal. Some specialize in integer instructions whereas others are optimized for floating\npoint performance. To increase throughput, the processor might also follow multiple code\npathssimultaneouslyinabranchinginstructionandthendiscardtheresultsofthebranches\nnot taken. This is why branch prediction units matter (on the front-end) such that only the\nmost promising paths are pursued.\nVectorization\nDeep learning is extremely compute-hungry. Hence, to make CPUs suitable for machine\nlearning, one needs to perform many operations in one clock cycle. This is achieved via\nvector units. They have different names: on ARM they are called NEON, on x86 they (a\nrecentgeneration)arereferredtoas AVX2195units. Acommonaspectisthattheyareable\nto perform SIMD (single instruction multiple data) operations. Fig. 13.4.5 shows how 8\nshort integers can be added in one clock cycle on ARM.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "235932b9-15ba-4d94-b75c-84e35134c9b0": {"__data__": {"id_": "235932b9-15ba-4d94-b75c-84e35134c9b0", "embedding": null, "metadata": {"page_label": "565", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68b29de0-926b-462c-b958-f686ee61e541", "node_type": "4", "metadata": {"page_label": "565", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3145de1b06e903b5582a11b137945559625a30f880c54dcfd95f567e30ce39ef", "class_name": "RelatedNodeInfo"}}, "text": "565 Hardware\ntFig. 13.4.5 128 bit NEON vectorization.\n196Depending on architecture choices, such registers are up to 512 bits long, allowing for the\ncombination of up to 64 pairs of numbers. For instance, we might be multiplying two\nnumbers and adding them to a third, which is also known as a fused multiply-add. Intel\u2019s\nOpenVino196uses these to achieve respectable throughput for deep learning on server-\ngrade CPUs. Note, though, that this number is entirely dwarfed by what GPUs are capable\nof achieving. For instance, NVIDIA\u2019s RTX 2080 Ti has 4,352 CUDA cores, each of which\nis capable of processing such an operation at any time.\nCache\nConsider the following situation: we have a modest CPU core with 4 cores as depicted in\nFig.13.4.3 above,runningat2GHzfrequency. Moreover,let\u2019sassumethatwehaveanIPC\n(instructionsperclock)countof1andthattheunitshaveAVX2with256-bitwidthenabled.\nLet\u2019s furthermore assume that at least one of the registers used for AVX2 operations needs\nto be retrieved from memory. This means that the CPU consumes 4\u0002256bit=128bytes\nof data per clock cycle. Unless we are able to transfer 2\u0002109\u0002128=256\u0002109bytes\nto the processor per second the processing elements are going to starve. Unfortunately the\nmemory interface of such a chip only supports 20\u201340 GB/s data transfer, i.e., one order of\nmagnitude less. The fix is to avoid loading newdata from memory as far as possible and\nrather to cache it locally on the CPU. This is where caches come in handy. Commonly the\nfollowing names or concepts are used:\n\u000fRegisters are strictly speaking not part of the cache. They help stage instructions. That\nsaid, CPU registers are memory locations that a CPU can access at clock speed with-\nout any delay penalty. CPUs have tens of registers. It is up to the compiler (or pro-\ngrammer) to use registers efficiently. For instance the C programming language has a\nregister keyword.\n\u000fL1 caches are the first line of defense against high memory bandwidth requirements.\nL1 caches are tiny (typical sizes might be 32\u201364 KB) and often split into data and\ninstructions caches. When data is found in the L1 cache, access is very fast. If they\ncannot be found there, the search progresses down the cache hierarchy.\n\u000fL2 caches are the next stop. Depending on architecture design and processor size they\nmight be exclusive. They might be accessible only by a given core or shared among\nmultiple cores. L2 caches are larger (typically 256\u2013512 KB per core) and slower than", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "705529df-d07e-4589-8098-3c91d6107682": {"__data__": {"id_": "705529df-d07e-4589-8098-3c91d6107682", "embedding": null, "metadata": {"page_label": "566", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c82822c6-d9d4-4013-8ebd-a1bb97c792ca", "node_type": "4", "metadata": {"page_label": "566", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d553a98d41c8edbfc6894c8805424f37f5b9b8ef7f9a668dc1fc8efde2676a54", "class_name": "RelatedNodeInfo"}}, "text": "566 Computational Performance\nL1. Furthermore, to access something in L2 we first need to check to realize that the\ndata is not in L1, which adds a small amount of extra latency.\n\u000fL3caches aresharedamongmultiplecoresandcanbequitelarge. AMD\u2019sEpyc3server\nCPUshaveawhopping256MBofcachespreadacrossmultiplechiplets. Moretypical\nnumbers are in the 4\u20138 MB range.\nPredicting which memory elements will be needed next is one of the key optimization pa-\nrametersinchipdesign. Forinstance,itisadvisabletotraversememoryina forward direc-\ntionsincemostcachingalgorithmswilltryto readahead ratherthanbackwards. Likewise,\nkeeping memory access patterns local is a good way of improving performance.\nAdding caches is a double-edge sword. On the one hand they ensure that the processor\ncores do not starve of data. At the same time they increase chip size, using up area that\notherwise could have been spent on increasing processing power. Moreover, cache misses\ncanbeexpensive. Considertheworstcasescenario, falsesharing ,asdepictedin Fig.13.4.6 .\nA memory location is cached on processor 0 when a thread on processor 1 requests the\ndata. To obtain it, processor 0 needs to stop what it is doing, write the information back\nto main memory and then let processor 1 read it from memory. During this operation both\nprocessorswait. Quitepotentiallysuchcoderuns moreslowly onmultipleprocessorswhen\ncompared with an efficient single-processor implementation. This is one more reason for\nwhy there is a practical limit to cache sizes (besides their physical size).\ntFig. 13.4.6 False sharing (image courtesy of Intel).\n13.4.5GPUs and otherAccelerators\nItisnotanexaggerationtoclaimthatdeeplearningwouldnothavebeensuccessfulwithout\nGPUs. Bythesametoken, itisquitereasonabletoarguethatGPUmanufacturers\u2019fortunes\nhave increased significantly due to deep learning. This co-evolution of hardware and al-\ngorithms has led to a situation where for better or worse deep learning is the preferable\nstatistical modeling paradigm. Hence it pays to understand the specific benefits that GPUs\nand related accelerators such as the TPU ( Jouppietal., 2017).\nOf note is a distinction that is often made in practice: accelerators are optimized either for\ntraining or inference. For the latter we only need to compute the forward propagation in\na network. No storage of intermediate data is needed for backpropagation. Moreover, we\nmaynotneedveryprecisecomputation(FP16orINT8typicallysuffice). Ontheotherhand,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "828cba8d-8dc6-441c-8d30-8673b94ec39f": {"__data__": {"id_": "828cba8d-8dc6-441c-8d30-8673b94ec39f", "embedding": null, "metadata": {"page_label": "567", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e7f80ed-82dc-4ca0-b969-b4240c770c45", "node_type": "4", "metadata": {"page_label": "567", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d9c75a7a9416c3671c9c2d455a98b095999f155c9834980ea090f46327fa62ac", "class_name": "RelatedNodeInfo"}}, "text": "567 Hardware\n197during training all intermediate results need storage to compute gradients. Moreover, ac-\ncumulatinggradientsrequireshigherprecisiontoavoidnumericalunderflow(oroverflow).\nThis means that FP16 (or mixed precision with FP32) is the minimum requirement. All\nof this necessitates faster and larger memory (HBM2 vs. GDDR6) and more processing\npower. For instance, NVIDIA\u2019s Turing197T4 GPUs are optimized for inference whereas\nthe V100 GPUs are preferable for training.\nRecall vectorization as illustrated in Fig. 13.4.5 . Adding vector units to a processor core\nallowed us to increase throughput significantly. For example, in the example in Fig. 13.4.5\nwe were able to perform 16 operations simultaneously. First, what if we added operations\nthatoptimizednotjustoperationsbetweenvectorsbutalsobetweenmatrices? Thisstrategy\nled to tensor cores (to be covered shortly). Second, what if we added many more cores?\nIn a nutshell, these two strategies summarize the design decisions in GPUs. Fig. 13.4.7\ngives an overview of a basic processing block. It contains 16 integer and 16 floating point\nunits. In addition to that, two tensor cores accelerate a narrow subset of additional op-\nerations relevant for deep learning. Each streaming multiprocessor consists of four such\nblocks.\ntFig. 13.4.7 NVIDIA Turing processing block (image courtesy of NVIDIA).\nNext, 12 streaming multiprocessors are grouped into graphics processing clusters which\nmake up the high-end TU102 processors. Ample memory channels and an L2 cache com-\nplement the setup. Fig. 13.4.8 has the relevant details. One of the reasons for designing\nsuchadeviceisthatindividualblockscanbeaddedorremovedasneededtoallowformore\ncompactchipsandtodealwithyieldissues(faultymodulesmightnotbeactivated). Fortu-\nnately programming such devices is well hidden from the casual deep learning researcher\nbeneathlayersofCUDAandframeworkcode. Inparticular,morethanoneoftheprograms\nmight well be executed simultaneously on the GPU, provided that there are available re-\nsources. Nonetheless it pays to be aware of the limitations of the devices to avoid picking\nmodels that do not fit into device memory.\nAlastaspectthatisworthmentioninginmoredetailare tensorcores . Theyareanexample\nof a recent trend of adding more optimized circuits that are specifically effective for deep\nlearning. Forinstance,theTPUaddedasystolicarray( Kung,1988 )forfastmatrixmultipli-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2414, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ba10c19-4cd6-484c-9c63-2e336b42f5a2": {"__data__": {"id_": "0ba10c19-4cd6-484c-9c63-2e336b42f5a2", "embedding": null, "metadata": {"page_label": "568", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f4485722-3856-4f6d-ae7f-46363560e2fb", "node_type": "4", "metadata": {"page_label": "568", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3f339a6de51fd1efb0fcc51b00a88ab9e7de79ae3c3634890a3cbddec091dbb5", "class_name": "RelatedNodeInfo"}}, "text": "568 Computational Performance\ntFig. 13.4.8 NVIDIA Turing architecture (image courtesy of NVIDIA)\n198cation. Therethedesignwastosupportaverysmallnumber(oneforthefirstgenerationof\nTPUs) of large operations. Tensor cores are at the other end. They are optimized for small\noperations involving between 4\u00024and 16\u000216matrices, depending on their numerical\nprecision. Fig. 13.4.9 gives an overview of the optimizations.\ntFig. 13.4.9 NVIDIA tensor cores in Turing (image courtesy of NVIDIA).\nObviously when optimizing for computation we end up making certain compromises. One\nof them is that GPUs are not very good at handling interrupts and sparse data. While there\narenotableexceptions,suchas Gunrock198(Wangetal.,2016),theaccesspatternofsparse\nmatrices and vectors do not go well with the high bandwidth burst read operations where", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 825, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2e8f69f-bd2b-4a1e-a835-42e28276d859": {"__data__": {"id_": "d2e8f69f-bd2b-4a1e-a835-42e28276d859", "embedding": null, "metadata": {"page_label": "569", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f605cfc-0e3e-4581-aaee-17390f20445a", "node_type": "4", "metadata": {"page_label": "569", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0882b7ffbe3aadd1cbb6c08c086afe5461c9c761a4e83ab626800f86b898bb91", "class_name": "RelatedNodeInfo"}}, "text": "569 Hardware\n199\n200\n201\n202\n203\n204GPUs excel. Matching both goals is an area of active research. See e.g., DGL199, a library\ntuned for deep learning on graphs.\n13.4.6Networksand Buses\nWhenever a single device is insufficient for optimization we need to transfer data to and\nfrom it to synchronize processing. This is where networks and buses come in handy. We\nhaveanumberofdesignparameters: bandwidth, cost, distance, andflexibility. Ononeend\nwehaveWiFithathasaprettygoodrange,isveryeasytouse(nowires,afterall),cheapbut\nit offers comparatively mediocre bandwidth and latency. No machine learning researcher\nwithin their right mind would use it to build a cluster of servers. In what follows we focus\non interconnects that are suitable for deep learning.\n\u000fPCIeis a dedicated bus for very high bandwidth point-to-point connections (up to 32\nGB/s on PCIe 4.0 in a 16-lane slot) per lane. Latency is in the order of single-digit\nmicroseconds (5 \u03bcs). PCIe links are precious. Processors only have a limited number\nof them: AMD\u2019s EPYC 3 has 128 lanes, Intel\u2019s Xeon has up to 48 lanes per chip;\non desktop-grade CPUs the numbers are 20 (Ryzen 9) and 16 (Core i9) respectively.\nSince GPUs have typically 16 lanes, this limits the number of GPUs that can connect\nto the CPU at full bandwidth. After all, they need to share the links with other high\nbandwidth peripherals such as storageand Ethernet. Justlike with RAM access, large\nbulk transfers are preferable due to reduced packet overhead.\n\u000fEthernet is the most commonly used way of connecting computers. While it is signifi-\ncantlyslowerthanPCIe,itisverycheapandresilienttoinstallandcoversmuchlonger\ndistances. Typical bandwidth for low-grade servers is 1 GBit/s. Higher-end devices\n(e.g.,C5 instances200in the cloud) offer between 10 and 100 GBit/s bandwidth. As\nin all previous cases data transmission has significant overheads. Note that we al-\nmost never use raw Ethernet directly but rather a protocol that is executed on top of\nthe physical interconnect (such as UDP or TCP/IP). This adds further overhead. Like\nPCIe, Ethernet is designed to connect two devices, e.g., a computer and a switch.\n\u000fSwitches allow us to connect multiple devices in a manner where any pair of them can\ncarry out a (typically full bandwidth) point-to-point connection simultaneously. For\ninstance, Ethernet switches might connect 40 servers at high cross-sectional band-\nwidth. Notethatswitchesarenotuniquetotraditionalcomputernetworks. EvenPCIe\nlanes can be switched201. This occurs, e.g., to connect a large number of GPUs to a\nhost processor, as is the case for the P2 instances202.\n\u000fNVLink is an alternative to PCIe when it comes to very high bandwidth interconnects.\nIt offers up to 300 Gbit/s data transfer rate per link. Server GPUs (Volta V100) have\nsixlinkswhereasconsumer-gradeGPUs(RTX2080Ti)haveonlyonelink, operating\nat a reduced 100 Gbit/s rate. We recommend to use NCCL203to achieve high data\ntransfer between GPUs.\n13.4.7MoreLatency Numbers", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03b27506-59c7-414b-9a62-3ac8ce856953": {"__data__": {"id_": "03b27506-59c7-414b-9a62-3ac8ce856953", "embedding": null, "metadata": {"page_label": "570", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27b568b7-1fc0-46f2-bf96-8650bda62a52", "node_type": "4", "metadata": {"page_label": "570", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "54347e55841bf74339c03d9bf16e03d9986718a8bae17d40d4346335c2659b8e", "class_name": "RelatedNodeInfo"}}, "text": "570 Computational Performance\n205The summary in Table 13.4.1 andTable 13.4.2 are from Eliot Eshelman204who maintains\nan updated version of the numbers as a GitHub gist205.\nTable 13.4.1: Common Latency Numbers.\nAction Time Notes\nL1 cache reference/hit 1.5 ns 4 cycles\nFloating-point add/mult/FMA 1.5 ns 4 cycles\nL2 cache reference/hit 5 ns 12 ~ 17 cycles\nBranch mispredict 6 ns 15 ~ 20 cycles\nL3 cache hit (unshared cache) 16 ns 42 cycles\nL3 cache hit (shared in another core) 25 ns 65 cycles\nMutex lock/unlock 25 ns\nL3 cache hit (modified in another core) 29 ns 75 cycles\nL3 cache hit (on a remote CPU socket) 40 ns 100 ~ 300 cycles (40 ~ 116 ns)\nQPI hop to a another CPU (per hop) 40 ns\n64MB memory ref. (local CPU) 46 ns TinyMemBench on Broadwell E5-2690v4\n64MB memory ref. (remote CPU) 70 ns TinyMemBench on Broadwell E5-2690v4\n256MB memory ref. (local CPU) 75 ns TinyMemBench on Broadwell E5-2690v4\nIntel Optane random write 94 ns UCSD Non-Volatile Systems Lab\n256MB memory ref. (remote CPU) 120 ns TinyMemBench on Broadwell E5-2690v4\nIntel Optane random read 305 ns UCSD Non-Volatile Systems Lab\nSend 4KB over 100 Gbps HPC fabric 1 \u03bcs MVAPICH2 over Intel Omni-Path\nCompress 1KB with Google Snappy 3 \u03bcs\nSend 4KB over 10 Gbps ethernet 10 \u03bcs\nWrite 4KB randomly to NVMe SSD 30 \u03bcs DC P3608 NVMe SSD (QOS 99% is 500\u03bcs)\nTransfer 1MB to/from NVLink GPU 30 \u03bcs ~33GB/s on NVIDIA 40GB NVLink\nTransfer 1MB to/from PCI-E GPU 80 \u03bcs ~12GB/s on PCIe 3.0 x16 link\nRead 4KB randomly from NVMe SSD 120 \u03bcs DC P3608 NVMe SSD (QOS 99%)\nRead 1MB sequentially from NVMe SSD 208 \u03bcs ~4.8GB/s DC P3608 NVMe SSD\nWrite 4KB randomly to SATA SSD 500 \u03bcs DC S3510 SATA SSD (QOS 99.9%)\nRead 4KB randomly from SATA SSD 500 \u03bcs DC S3510 SATA SSD (QOS 99.9%)\nRound trip within same data center 500 \u03bcs One-way ping is ~250\u03bcs\nRead 1MB sequentially from SATA SSD 2 ms ~550MB/s DC S3510 SATA SSD\nRead 1MB sequentially from disk 5 ms ~200MB/s server HDD\nRandom Disk Access (seek+rotation) 10 ms\nSend packet CA->Netherlands->CA 150 ms\nTable 13.4.2: Latency Numbers for NVIDIA Tesla GPUs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "699f744c-1efb-405e-ab46-ceed66236686": {"__data__": {"id_": "699f744c-1efb-405e-ab46-ceed66236686", "embedding": null, "metadata": {"page_label": "571", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f72d1ce6-1097-4635-9923-744aa6d44144", "node_type": "4", "metadata": {"page_label": "571", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "14d2d0b7db6644e6e204249bcd856e59dc5ce322e398fb24b8c37b1a514fff99", "class_name": "RelatedNodeInfo"}}, "text": "571 Hardware\nAction Time Notes\nGPU Shared Memory access 30 ns 30~90 cycles (bank conflicts add la-\ntency)\nGPU Global Memory access 200\nns200~800 cycles\nLaunch CUDA kernel on GPU 10 \u03bcs Host CPU instructs GPU to start kernel\nTransfer 1MB to/from NVLink\nGPU30 \u03bcs ~33GB/s on NVIDIA 40GB NVLink\nTransfer 1MB to/from PCI-E GPU 80 \u03bcs ~12GB/s on PCI-Express x16 link\n13.4.8Summary\n\u000fDevices have overheads for operations. Hence it is important to aim for a small number\nof large transfers rather than many small ones. This applies to RAM, SSDs, networks\nand GPUs.\n\u000fVectorization is key for performance. Make sure you are aware of the specific abilities\nof your accelerator. E.g., some Intel Xeon CPUs are particularly good for INT8 op-\nerations, NVIDIA Volta GPUs excel at FP16 matrix-matrix operations and NVIDIA\nTuring shines at FP16, INT8, and INT4 operations.\n\u000fNumerical overflow due to small data types can be a problem during training (and to a\nlesser extent during inference).\n\u000fAliasing can significantly degrade performance. For instance, memory alignment on 64\nbit CPUs should be done with respect to 64 bit boundaries. On GPUs it is a good idea\nto keep convolution sizes aligned, e.g., to tensor cores.\n\u000fMatch your algorithms to the hardware (e.g., memory footprint, and bandwidth). Great\nspeedup(ordersofmagnitude)canbeachievedwhenfittingtheparametersintocaches.\n\u000fWerecommendthatyousketchouttheperformanceofanovelalgorithmonpaperbefore\nverifying the experimental results. Discrepancies of an order-of-magnitude or more\nare reasons for concern.\n\u000fUse profilers to debug performance bottlenecks.\n\u000fTraining and inference hardware have different sweet spots in terms of price and perfor-\nmance.\n13.4.9Exercises\n1.WriteCcodetotestwhetherthereisanydifferenceinspeedbetweenaccessingmemory\naligned or misaligned relative to the external memory interface. Hint: be careful of\ncaching effects.\n2.Test the difference in speed between accessing memory in sequence or with a given\nstride.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1976, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd4c3018-ee48-4766-872f-39649b85cb85": {"__data__": {"id_": "bd4c3018-ee48-4766-872f-39649b85cb85", "embedding": null, "metadata": {"page_label": "572", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "794f82f1-88db-47ab-b47c-42079e9292cd", "node_type": "4", "metadata": {"page_label": "572", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c4f638baaa430f754a23444aa186af40dc617509c8dcad246207f7ba27f81a32", "class_name": "RelatedNodeInfo"}}, "text": "572 Computational Performance\n2063.How could you measure the cache sizes on a CPU?\n4.Howwouldyoulayoutdataacrossmultiplememorychannelsformaximumbandwidth?\nHow would you lay it out if you had many small threads?\n5.An enterprise-class HDD is spinning at 10,000 rpm. What is the absolutely minimum\ntime an HDD needs to spend worst case before it can read data (you can assume that\nheads move almost instantaneously)? Why are 2.5\u201d HDDs becoming popular for com-\nmercial servers (relative to 3.5\u201d and 5.25\u201d drives)?\n6.AssumethatanHDDmanufacturerincreasesthestoragedensityfrom1Tbitpersquare\ninch to 5 Tbit per square inch. How much information can you store on a ring on a 2.5\u201d\nHDD? Is there a difference between the inner and outer tracks?\n7.Going from 8 bit to 16 bit data types increases the amount of silicon approximately by\nfour times. Why? Why might NVIDIA have added INT4 operations to their Turing\nGPUs?\n8.How much faster is it to read forward through memory vs. reading backwards? Does\nthis number differ between different computers and CPU vendors? Why? Write C code\nand experiment with it.\n9.Can you measure the cache size of your disk? What is it for a typical HDD? Do SSDs\nneed a cache?\n10.Measure the packet overhead when sending messages across the Ethernet. Look up the\ndifference between UDP and TCP/IP connections.\n11.Direct memory access allows devices other than the CPU to write (and read) directly to\n(from) memory. Why is this a good idea?\n12.Look at the performance numbers for the Turing T4 GPU. Why does the performance\n\u201conly\u201d double as you go from FP16 to INT8 and INT4?\n13.What is the shortest time it should take for a packet on a round trip between San Fran-\ncisco and Amsterdam? Hint: you can assume that the distance is 10,000 km.\nDiscussions206.\n13.5Trainingon Multiple GPUs\nSo far we discussed how to train models efficiently on CPUs and GPUs. We even showed\nhow deep learning frameworks allow one to parallelize computation and communication\nautomatically between them in Section 13.3 . We also showed in Section 6.7 how to list\nall the available GPUs on a computer using the nvidia-smi command. What we did not\ndiscuss is how to actually parallelize deep learning training. Instead, we implied in pass-\ning that one would somehow split the data across multiple devices and make it work. The", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2321, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "276f3dd7-abb2-4a06-9f63-f2a29f1afc7d": {"__data__": {"id_": "276f3dd7-abb2-4a06-9f63-f2a29f1afc7d", "embedding": null, "metadata": {"page_label": "573", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81d8bfe7-6bc6-4eec-a784-5f6d073b01ae", "node_type": "4", "metadata": {"page_label": "573", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "86ef680b056f89ceb98d1093c974a486793d04df28d2938023728c5e6deb8c0b", "class_name": "RelatedNodeInfo"}}, "text": "573 Training on Multiple GPUs\npresentsectionfillsinthedetailsandshowshowtotrainanetworkinparallelwhenstarting\nfrom scratch. Details on how to take advantage of functionality in high-level APIs is rele-\ngated to Section 13.6 . We assume that you are familiar with minibatch stochastic gradient\ndescent algorithms such as the ones described in Section 12.5 .\n13.5.1Splittingthe Problem\nLet\u2019sstartwithasimplecomputervisionproblemandaslightlyarchaicnetwork, e.g., with\nmultiple layers of convolutions, pooling, and possibly a few fully connected layers in the\nend. That is, let\u2019s start with a network that looks quite similar to LeNet ( LeCunet al.,\n1998) or AlexNet ( Krizhevsky et al., 2012). Given multiple GPUs (2 if it is a desktop\nserver, 4 on an AWS g4dn.12xlarge instance, 8 on a p3.16xlarge, or 16 on a p2.16xlarge),\nwe want to partition training in a manner as to achieve good speedup while simultaneously\nbenefittingfromsimpleandreproducibledesignchoices. MultipleGPUs,afterall,increase\nbothmemory andcomputation ability. In a nutshell, we have the following choices, given\na minibatch of training data that we want to classify.\nFirst, we could partition the network across multiple GPUs. That is, each GPU takes as\ninput the data flowing into a particular layer, processes data across a number of subsequent\nlayers and then sends the data to the next GPU. This allows us to process data with larger\nnetworkswhencomparedwithwhatasingleGPUcouldhandle. Besides,memoryfootprint\nper GPU can be well controlled (it is a fraction of the total network footprint).\nHowever,theinterfacebetweenlayers(andthusGPUs)requirestightsynchronization. This\ncan be tricky, in particular if the computational workloads are not properly matched be-\ntween layers. The problem is exacerbated for large numbers of GPUs. The interface be-\ntween layers also requires large amounts of data transfer, such as activations and gradients.\nThis may overwhelm the bandwidth of the GPU buses. Moreover, compute-intensive, yet\nsequentialoperationsarenontrivialtopartition. Seee.g.,Mirhoseini etal.(2017)forabest\neffort in this regard. It remains a difficult problem and it is unclear whether it is possible\nto achieve good (linear) scaling on nontrivial problems. We do not recommend it unless\nthere is excellent framework or operating system support for chaining together multiple\nGPUs.\nSecond,wecouldsplittheworklayerwise. Forinstance,ratherthancomputing64channels\nonasingleGPUwecouldsplituptheproblemacross4GPUs,eachofwhichgeneratesdata\nfor 16 channels. Likewise, for a fully connected layer we could split the number of output\nunits.Fig. 13.5.1 (taken from Krizhevsky et al.(2012)) illustrates this design, where this\nstrategy was used to deal with GPUs that had a very small memory footprint (2 GB at the\ntime). This allows for good scaling in terms of computation, provided that the number of\nchannels(orunits)isnottoosmall. Besides,multipleGPUscanprocessincreasinglylarger\nnetworks since the available memory scales linearly.\nHowever, we need a verylarge number of synchronization or barrier operations since each\nlayer depends on the results from all the other layers. Moreover, the amount of data that\nneedstobetransferredispotentiallyevenlargerthanwhendistributinglayersacrossGPUs.\nThus,wedonotrecommendthisapproachduetoitsbandwidthcostandcomplexity.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca920b6a-bd01-4e88-9aae-bb57565bbbec": {"__data__": {"id_": "ca920b6a-bd01-4e88-9aae-bb57565bbbec", "embedding": null, "metadata": {"page_label": "574", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a45869e4-360d-451d-8b34-6ead2327eb4f", "node_type": "4", "metadata": {"page_label": "574", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e471ccc1d998cac56ccbe7c63e50b7224190436c38a843badd228729bb55f25d", "class_name": "RelatedNodeInfo"}}, "text": "574 Computational Performance\ntFig. 13.5.1 Model parallelism in the original AlexNet design due to limited GPU memory.\nLast, we could partition data across multiple GPUs. This way all GPUs perform the same\ntype of work, albeit on different observations. Gradients are aggregated across GPUs after\neach minibatch of training data. This is the simplest approach and it can be applied in any\nsituation. Weonlyneedtosynchronizeaftereachminibatch. Thatsaid,itishighlydesirable\nto start exchanging gradients parameters already while others are still being computed.\nMoreover, larger numbers of GPUs lead to larger minibatch sizes, thus increasing training\nefficiency. However, adding more GPUs does not allow us to train larger models.\ntFig. 13.5.2 Parallelization on multiple GPUs. From left to right: original problem, network\npartitioning, layerwise partitioning, data parallelism.\nA comparison of different ways of parallelization on multiple GPUs is depicted in Fig.\n13.5.2. By and large, data parallelism is the most convenient way to proceed, provided\nthat we have access to GPUs with sufficiently large memory. See also ( Liet al., 2014) for\na detailed description of partitioning for distributed training. GPU memory used to be a\nproblem in the early days of deep learning. By now this issue has been resolved for all but\nthe most unusual cases. We focus on data parallelism in what follows.\n13.5.2Data Parallelism\nAssumethatthereare \ud835\udc58GPUsonamachine. Giventhemodeltobetrained,eachGPUwill\nmaintainacompletesetofmodelparametersindependentlythoughparametervaluesacross", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7098d7be-8914-4667-9c03-04622aa104f2": {"__data__": {"id_": "7098d7be-8914-4667-9c03-04622aa104f2", "embedding": null, "metadata": {"page_label": "575", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e1838c6-de90-41ec-bce8-046b2fade43d", "node_type": "4", "metadata": {"page_label": "575", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5e71e7828f9d6ca2542a7441778d6a4540563488896860169b051bc46ba61970", "class_name": "RelatedNodeInfo"}}, "text": "575 Training on Multiple GPUs\nthe GPUs are identical and synchronized. As an example, Fig. 13.5.3 illustrates training\nwith data parallelism when \ud835\udc58=2.\ntFig. 13.5.3 Calculation of minibatch stochastic gradient descent using data parallelism on two GPUs.\nIn general, the training proceeds as follows:\n\u000fInanyiterationoftraining, givenarandomminibatch,wesplittheexamplesinthebatch\ninto\ud835\udc58portions and distribute them evenly across the GPUs.\n\u000fEach GPU calculates loss and gradient of the model parameters based on the minibatch\nsubset it was assigned.\n\u000fThelocalgradientsofeachofthe \ud835\udc58GPUsareaggregatedtoobtainthecurrentminibatch\nstochastic gradient.\n\u000fThe aggregate gradient is re-distributed to each GPU.\n\u000fEach GPU uses this minibatch stochastic gradient to update the complete set of model\nparameters that it maintains.\nNote that in practice we increase the minibatch size \ud835\udc58-fold when training on \ud835\udc58GPUs such\nthat each GPU has the same amount of work to do as if we were training on a single GPU\nonly. On a 16-GPU server this can increase the minibatch size considerably and we may\nhavetoincreasethelearningrateaccordingly. Alsonotethatbatchnormalizationin Section\n8.5needs to be adjusted, e.g., by keeping a separate batch normalization coefficient per\nGPU. In what follows we will use a toy network to illustrate multi-GPU training.\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\n13.5.3A ToyNetwork\nWe use LeNet as introduced in Section 7.6 (with slight modifications). We define it from\nscratch to illustrate parameter exchange and synchronization in detail.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb389878-18f1-4267-8852-fc3e013cf2c6": {"__data__": {"id_": "cb389878-18f1-4267-8852-fc3e013cf2c6", "embedding": null, "metadata": {"page_label": "576", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "02bdeffd-9cf4-4f2d-9bbb-641e7ba856f6", "node_type": "4", "metadata": {"page_label": "576", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "21db92751d01feed0da2e300acb2eb2afa793e4bfb5ab7de2252166600e66ef7", "class_name": "RelatedNodeInfo"}}, "text": "576 Computational Performance\n# Initialize model parameters\nscale =0.01\nW1=torch .randn(size =(20,1,3,3))*scale\nb1=torch .zeros( 20)\nW2=torch .randn(size =(50,20,5,5))*scale\nb2=torch .zeros( 50)\nW3=torch .randn(size =(800,128))*scale\nb3=torch .zeros( 128)\nW4=torch .randn(size =(128,10))*scale\nb4=torch .zeros( 10)\nparams =[W1, b1, W2, b2, W3, b3, W4, b4]\n# Define the model\ndef lenet (X, params):\nh1_conv =F.conv2d( input =X, weight =params[ 0], bias =params[ 1])\nh1_activation =F.relu(h1_conv)\nh1=F.avg_pool2d( input =h1_activation, kernel_size =(2,2), stride =(2,2))\nh2_conv =F.conv2d( input =h1, weight =params[ 2], bias =params[ 3])\nh2_activation =F.relu(h2_conv)\nh2=F.avg_pool2d( input =h2_activation, kernel_size =(2,2), stride =(2,2))\nh2=h2.reshape(h2 .shape[ 0],-1)\nh3_linear =torch .mm(h2, params[ 4])+params[ 5]\nh3=F.relu(h3_linear)\ny_hat =torch .mm(h3, params[ 6])+params[ 7]\nreturn y_hat\n# Cross-entropy loss function\nloss =nn.CrossEntropyLoss(reduction ='none ')\n13.5.4Data Synchronization\nFor efficient multi-GPU training we need two basic operations. First we need to have\nthe ability to distribute a list of parameters to multiple devices and to attach gradients\n(get_params ). Without parameters it is impossible to evaluate the network on a GPU.\nSecond, we need the ability to sum parameters across multiple devices, i.e., we need an\nallreduce function.\ndef get_params (params, device):\nnew_params =[p.to(device) for pinparams]\nfor pinnew_params:\np.requires_grad_()\nreturn new_params\nLet\u2019s try it out by copying the model parameters to one GPU.\nnew_params =get_params(params, d2l .try_gpu( 0))\nprint ('b1 weight: ', new_params[ 1])\nprint ('b1 grad: ', new_params[ 1].grad)\nb1 weight: tensor([ 0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,0.,\u2423\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1783, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fc8f2db-8d87-4812-9933-348645e74b88": {"__data__": {"id_": "2fc8f2db-8d87-4812-9933-348645e74b88", "embedding": null, "metadata": {"page_label": "577", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29efb7e4-d9ea-4eb8-bcfb-285aa7fa0115", "node_type": "4", "metadata": {"page_label": "577", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0fb6f69c086790aa36be1af025dc5348e72622c903589f872a0e0f9d03cc847f", "class_name": "RelatedNodeInfo"}}, "text": "577 Training on Multiple GPUs\n(continued from previous page)\n\u21a9!0.,0.,0.,0.,0.],\ndevice ='cuda:0 ', requires_grad =True )\nb1 grad: None\nSince we did not perform any computation yet, the gradient with regard to the bias param-\neter is still zero. Now let\u2019s assume that we have a vector distributed across multiple GPUs.\nThe following allreduce function adds up all vectors and broadcasts the result back to all\nGPUs. Note that for this to work we need to copy the data to the device accumulating the\nresults.\ndef allreduce (data):\nfor iinrange (1,len(data)):\ndata[ 0][:] +=data[i] .to(data[ 0].device)\nfor iinrange (1,len(data)):\ndata[i][:] =data[ 0].to(data[i] .device)\nLet\u2019s test this by creating vectors with different values on different devices and aggregate\nthem.\ndata =[torch .ones(( 1,2), device =d2l.try_gpu(i)) *(i+1)for iinrange (2)]\nprint ('before allreduce: \\n', data[ 0],'\\n', data[ 1])\nallreduce(data)\nprint ('after allreduce: \\n', data[ 0],'\\n', data[ 1])\nbefore allreduce:\ntensor([[ 1.,1.]], device ='cuda:0 ')\ntensor([[ 2.,2.]], device ='cuda:1 ')\nafter allreduce:\ntensor([[ 3.,3.]], device ='cuda:0 ')\ntensor([[ 3.,3.]], device ='cuda:1 ')\n13.5.5DistributingData\nWe need a simple utility function to distribute a minibatch evenly across multiple GPUs.\nFor instance, on two GPUs we would like to have half of the data to be copied to either of\nthe GPUs. Since it is more convenient and more concise, we use the built-in function from\nthe deep learning framework to try it out on a 4\u00025matrix.\ndata =torch .arange( 20).reshape( 4,5)\ndevices =[torch .device( 'cuda:0 '), torch .device( 'cuda:1 ')]\nsplit =nn.parallel .scatter(data, devices)\nprint ('input : ', data)\nprint ('load into ', devices)\nprint ('output: ', split)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c832131-72c3-49ea-aad5-8305fac61540": {"__data__": {"id_": "6c832131-72c3-49ea-aad5-8305fac61540", "embedding": null, "metadata": {"page_label": "578", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16eff6d4-b728-40cb-9d17-5492a4d3db27", "node_type": "4", "metadata": {"page_label": "578", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "75135b56feb6094b76ed53bd88d59a158ff58431a686f6b88467de96b2fe00cd", "class_name": "RelatedNodeInfo"}}, "text": "578 Computational Performance\ninput : tensor([[ 0,1,2,3,4],\n[5,6,7,8,9],\n[10,11,12,13,14],\n[15,16,17,18,19]])\nload into [device( type ='cuda ', index =0), device( type ='cuda ', index =1)]\noutput: (tensor([[ 0,1,2,3,4],\n[5,6,7,8,9]], device ='cuda:0 '), tensor([[ 10,11,12,13,14],\n[15,16,17,18,19]], device ='cuda:1 '))\nFor later reuse we define a split_batch function that splits both data and labels.\n#@save\ndef split_batch (X, y, devices):\n\"\"\"Split `X` and `y` into multiple devices.\"\"\"\nassert X.shape[ 0]==y.shape[ 0]\nreturn (nn.parallel .scatter(X, devices),\nnn.parallel .scatter(y, devices))\n13.5.6Training\nNow we can implement multi-GPU training on a single minibatch. Its implementation is\nprimarily based on the data parallelism approach described in this section. We will use the\nauxiliaryfunctionswejustdiscussed, allreduce andsplit_and_load ,tosynchronizethe\ndata among multiple GPUs. Note that we do not need to write any specific code to achieve\nparallelism. Since the computational graph does not haveany dependencies across devices\nwithin a minibatch, it is executed in parallel automatically .\ndef train_batch (X, y, device_params, devices, lr):\nX_shards, y_shards =split_batch(X, y, devices)\n# Loss is calculated separately on each GPU\nls=[loss(lenet(X_shard, device_W), y_shard) .sum()\nfor X_shard, y_shard, device_W inzip(\nX_shards, y_shards, device_params)]\nfor linls: # Backpropagation is performed separately on each GPU\nl.backward()\n# Sum all gradients from each GPU and broadcast them to all GPUs\nwith torch .no_grad():\nfor iinrange (len(device_params[ 0])):\nallreduce([device_params[c][i] .grad for cinrange (len(devices))])\n# The model parameters are updated separately on each GPU\nfor param indevice_params:\nd2l.sgd(param, lr, X .shape[ 0])# Here, we use a full-size batch\nNow, we can define the training function. It is slightly different from the ones used in the\nprevious chapters: we need to allocate the GPUs and copy all the model parameters to all\nthe devices. Obviously each batch is processed using the train_batch function to deal\nwith multiple GPUs. For convenience (and conciseness of code) we compute the accuracy\non a single GPU, though this is ine\ufb00icient since the other GPUs are idle.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58872b50-a0f7-4fb6-ba8d-207e791c3a4f": {"__data__": {"id_": "58872b50-a0f7-4fb6-ba8d-207e791c3a4f", "embedding": null, "metadata": {"page_label": "579", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b197b28-e3f9-45ed-bced-90d2a1d64e87", "node_type": "4", "metadata": {"page_label": "579", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a4ba771e87e38fd216e66fc23a969b2fee02ef516b0cfb580de4010967a52cb3", "class_name": "RelatedNodeInfo"}}, "text": "579 Training on Multiple GPUs\ndef train (num_gpus, batch_size, lr):\ntrain_iter, test_iter =d2l.load_data_fashion_mnist(batch_size)\ndevices =[d2l .try_gpu(i) for iinrange (num_gpus)]\n# Copy model parameters to `num_gpus` GPUs\ndevice_params =[get_params(params, d) for dindevices]\nnum_epochs =10\nanimator =d2l.Animator( 'epoch ','test acc ', xlim =[1, num_epochs])\ntimer =d2l.Timer()\nfor epoch inrange (num_epochs):\ntimer .start()\nfor X, y intrain_iter:\n# Perform multi-GPU training for a single minibatch\ntrain_batch(X, y, device_params, devices, lr)\ntorch .cuda .synchronize()\ntimer .stop()\n# Evaluate the model on GPU 0\nanimator .add(epoch +1, (d2l .evaluate_accuracy_gpu(\nlambda x: lenet(x, device_params[ 0]), test_iter, devices[ 0]),))\nprint (f'test acc: {animator .Y[0][-1]:.2f},{timer .avg() :.1f}sec/epoch '\nf'on{str(devices) }')\nLet\u2019s see how well this works on a single GPU. We first use a batch size of 256 and a\nlearning rate of 0.2.\ntrain(num_gpus =1, batch_size =256, lr =0.2)\ntest acc: 0.83 ,3.0 sec/epoch on [device( type ='cuda ', index =0)]\nBykeepingthebatchsizeandlearningrateunchangedandincreasingthenumberofGPUs\nto 2, we can see that the test accuracy roughly stays the same compared with the previous\nexperiment. Intermsoftheoptimizationalgorithms,theyareidentical. Unfortunatelythere\nis no meaningful speedup to be gained here: the model is simply too small; moreover we\nonly have a small dataset, where our slightly unsophisticated approach to implementing\nmulti-GPU training suffered from significant Python overhead. We will encounter more\ncomplex models and more sophisticated ways of parallelization going forward. Let\u2019s see\nwhat happens nonetheless for Fashion-MNIST.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "474e4628-d6fb-4f34-b0e2-f9a8b1274c44": {"__data__": {"id_": "474e4628-d6fb-4f34-b0e2-f9a8b1274c44", "embedding": null, "metadata": {"page_label": "580", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "568eab24-2c2a-4de1-af32-850ff3baba0b", "node_type": "4", "metadata": {"page_label": "580", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ee54dd1704432de150a84539b0e4d2f0a55731f04b8b7a6f02fb4be3d57bda89", "class_name": "RelatedNodeInfo"}}, "text": "580 Computational Performance\n207train(num_gpus =2, batch_size =256, lr =0.2)\ntest acc: 0.84 ,2.8 sec/epoch on [device( type ='cuda ', index =0), device( type =\n\u21a9!'cuda ', index =1)]\n13.5.7Summary\n\u000fThere are multiple ways to split deep network training over multiple GPUs. We could\nsplitthembetweenlayers,acrosslayers,oracrossdata. Theformertworequiretightly\nchoreographed data transfers. Data parallelism is the simplest strategy.\n\u000fData parallel training is straightforward. However, it increases the effective minibatch\nsize to be efficient.\n\u000fIn data parallelism, data is split across multiple GPUs, where each GPU executes its\nown forward and backward operation and subsequently gradients are aggregated and\nresults are broadcast back to the GPUs.\n\u000fWe may use slightly increased learning rates for larger minibatches.\n13.5.8Exercises\n1.When training on \ud835\udc58GPUs, change the minibatch size from \ud835\udc4fto\ud835\udc58\u0001\ud835\udc4f, i.e., scale it up by\nthe number of GPUs.\n2.Compare accuracy for different learning rates. How does it scale with the number of\nGPUs?\n3.Implementamoreefficient allreduce functionthataggregatesdifferent parameterson\ndifferent GPUs? Why is it more efficient?\n4.Implement multi-GPU test accuracy computation.\nDiscussions207.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1222, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e8c48c2-16a7-4be9-9f46-84898ecfb370": {"__data__": {"id_": "0e8c48c2-16a7-4be9-9f46-84898ecfb370", "embedding": null, "metadata": {"page_label": "581", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6e6c0c9-4fd3-4aa7-9ddc-a997aa5a136a", "node_type": "4", "metadata": {"page_label": "581", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ab62b1986ef05711395b7c80493290496803faba8203b335670f4d4d14e62d74", "class_name": "RelatedNodeInfo"}}, "text": "581 Concise Implementation for Multiple GPUs\n13.6ConciseImplementation forMultiple GPUs\nImplementing parallelism from scratch for every new model is no fun. Moreover, there is\nsignificantbenefitinoptimizingsynchronizationtoolsforhighperformance. Inthefollow-\ning we will show how to do this using high-level APIs of deep learning frameworks. The\nmathematics and the algorithms are the same as in Section 13.5 . Quite unsurprisingly you\nwill need at least two GPUs to run code of this section.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n13.6.1A ToyNetwork\nLet\u2019s use a slightly more meaningful network than LeNet from Section 13.5 that is still\nsufficiently easy and quick to train. We pick a ResNet-18 variant ( Heet al., 2016). Since\ntheinputimagesaretinywemodifyitslightly. Inparticular,thedifferencefrom Section8.6\nisthatweuseasmallerconvolutionkernel,stride,andpaddingatthebeginning. Moreover,\nwe remove the max-pooling layer.\n#@save\ndef resnet18 (num_classes, in_channels =1):\n\"\"\"A slightly modified ResNet-18 model.\"\"\"\ndef resnet_block (in_channels, out_channels, num_residuals,\nfirst_block =False ):\nblk =[]\nfor iinrange (num_residuals):\nifi==0and not first_block:\nblk.append(d2l .Residual(out_channels, use_1x1conv =True ,\nstrides =2))\nelse :\nblk.append(d2l .Residual(out_channels))\nreturn nn.Sequential( *blk)\n# This model uses a smaller convolution kernel, stride, and padding and\n# removes the max-pooling layer\nnet =nn.Sequential(\nnn.Conv2d(in_channels, 64, kernel_size =3, stride =1, padding =1),\nnn.BatchNorm2d( 64),\nnn.ReLU())\nnet.add_module( \"resnet_block1 \", resnet_block( 64,64,2, first_block =True ))\nnet.add_module( \"resnet_block2 \", resnet_block( 64,128,2))\nnet.add_module( \"resnet_block3 \", resnet_block( 128,256,2))\nnet.add_module( \"resnet_block4 \", resnet_block( 256,512,2))\nnet.add_module( \"global_avg_pool \", nn .AdaptiveAvgPool2d(( 1,1)))\nnet.add_module( \"fc\", nn .Sequential(nn .Flatten(),\nnn.Linear( 512, num_classes)))\nreturn net", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "403573a8-ea4a-4b93-8ac4-df6370ddd886": {"__data__": {"id_": "403573a8-ea4a-4b93-8ac4-df6370ddd886", "embedding": null, "metadata": {"page_label": "582", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d3978ba-fbef-479f-8fb9-f735eb335f0f", "node_type": "4", "metadata": {"page_label": "582", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5614611257882709bad5fb00689b8f1e84480eeb2fb923a2182af6d961bf8a9b", "class_name": "RelatedNodeInfo"}}, "text": "582 Computational Performance\n13.6.2NetworkInitialization\nWe will initialize the network inside the training loop. For a refresher on initialization\nmethods see Section 5.4 .\nnet =resnet18( 10)\n# Get a list of GPUs\ndevices =d2l.try_all_gpus()\n# We will initialize the network inside the training loop\n13.6.3Training\nAs before, the training code needs to perform several basic functions for efficient paral-\nlelism:\n\u000fNetwork parameters need to be initialized across all devices.\n\u000fWhile iterating over the dataset minibatches are to be divided across all devices.\n\u000fWe compute the loss and its gradient in parallel across devices.\n\u000fGradients are aggregated and parameters are updated accordingly.\nIn the end we compute the accuracy (again in parallel) to report the final performance of\nthe network. The training routine is quite similar to implementations in previous chapters,\nexcept that we need to split and aggregate data.\ndef train (net, num_gpus, batch_size, lr):\ntrain_iter, test_iter =d2l.load_data_fashion_mnist(batch_size)\ndevices =[d2l .try_gpu(i) for iinrange (num_gpus)]\ndef init_weights (module):\niftype (module) in[nn.Linear, nn .Conv2d]:\nnn.init .normal_(module .weight, std =0.01 )\nnet.apply(init_weights)\n# Set the model on multiple GPUs\nnet =nn.DataParallel(net, device_ids =devices)\ntrainer =torch .optim .SGD(net .parameters(), lr)\nloss =nn.CrossEntropyLoss()\ntimer, num_epochs =d2l.Timer(), 10\nanimator =d2l.Animator( 'epoch ','test acc ', xlim =[1, num_epochs])\nfor epoch inrange (num_epochs):\nnet.train()\ntimer .start()\nfor X, y intrain_iter:\ntrainer .zero_grad()\nX, y =X.to(devices[ 0]), y .to(devices[ 0])\nl=loss(net(X), y)\nl.backward()\ntrainer .step()\ntimer .stop()\nanimator .add(epoch +1, (d2l .evaluate_accuracy_gpu(net, test_iter),))\nprint (f'test acc: {animator .Y[0][-1]:.2f},{timer .avg() :.1f}sec/epoch '\nf'on{str(devices) }')", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "744885fd-4700-410a-bad7-c3f7bc3f018a": {"__data__": {"id_": "744885fd-4700-410a-bad7-c3f7bc3f018a", "embedding": null, "metadata": {"page_label": "583", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7ee356cf-f18d-4658-be1d-3eecd39517a0", "node_type": "4", "metadata": {"page_label": "583", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6d9955092f0afd7429099428ce7fbd9458f17a5f1086ff71c3f7f84acd502365", "class_name": "RelatedNodeInfo"}}, "text": "583 Concise Implementation for Multiple GPUs\nLet\u2019s see how this works in practice. As a warm-up we train the network on a single\nGPU.\ntrain(net, num_gpus =1, batch_size =256, lr =0.1)\ntest acc: 0.91 ,12.2 sec/epoch on [device( type ='cuda ', index =0)]\nNext we use 2 GPUs for training. Compared with LeNet evaluated in Section 13.5 , the\nmodel for ResNet-18 is considerably more complex. This is where parallelization shows\nits advantage. The time for computation is meaningfully larger than the time for synchro-\nnizing parameters. This improves scalability since the overhead for parallelization is less\nrelevant.\ntrain(net, num_gpus =2, batch_size =512, lr =0.2)\ntest acc: 0.73 ,7.5 sec/epoch on [device( type ='cuda ', index =0), device( type =\n\u21a9!'cuda ', index =1)]\n13.6.4Summary\n\u000fData is automatically evaluated on the devices where the data can be found.\n\u000fTakecaretoinitializethenetworksoneachdevicebeforetryingtoaccesstheparameters\non that device. Otherwise you will encounter an error.\n\u000fThe optimization algorithms automatically aggregate over multiple GPUs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1067, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be20fee5-c4ec-4130-96fb-bad5abb096a7": {"__data__": {"id_": "be20fee5-c4ec-4130-96fb-bad5abb096a7", "embedding": null, "metadata": {"page_label": "584", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "62d9fc53-f44c-4af1-9459-75ac958c1f4a", "node_type": "4", "metadata": {"page_label": "584", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a175f885f21f38f21e7d9cd27906213e24a8d96a1567cec54d65c3457dd7011c", "class_name": "RelatedNodeInfo"}}, "text": "584 Computational Performance\n20813.6.5Exercises\n1.This section uses ResNet-18. Try different epochs, batch sizes, and learning rates. Use\nmore GPUs for computation. What happens if you try this with 16 GPUs (e.g., on an\nAWS p2.16xlarge instance)?\n2.Sometimes, different devices provide different computing power. We could use the\nGPUs and the CPU at the same time. How should we divide the work? Is it worth the\neffort? Why? Why not?\nDiscussions208.\n13.7ParameterServers\nAs we move from a single GPU to multiple GPUs and then to multiple servers containing\nmultiple GPUs, possibly all spread out across multiple racks and network switches, our\nalgorithms for distributed and parallel training need to become much more sophisticated.\nDetails matter since different interconnects have very different bandwidth (e.g., NVLink\ncan offer up to 100 GB/s across 6 links in an appropriate setting, PCIe 4.0 (16-lane) offers\n32 GB/s, while even high speed 100GbE Ethernet only amounts to 10 GB/s). At the same\ntime it is unreasonable to expect that a statistical modeler be an expert in networking and\nsystems.\nThecoreideaoftheparameterserverwasintroducedinSmolaandNarayanamurthy( 2010)\nin the context of distributed latent variable models. A description of the push and pull\nsemantics then followed in Ahmed et al.(2012) and a description of the system and an\nopen source library followed in Li et al.(2014). In the following we will motivate the\ncomponents needed for efficiency.\n13.7.1Data-ParallelTraining\nLet\u2019s review the data parallel training approach to distributed training. We will use this\nto the exclusion of all others in this section since it is significantly simpler to implement\nin practice. There are virtually no use cases (besides deep learning on graphs) where any\nother strategy for parallelism is preferred since GPUs have plenty of memory nowadays.\nFig. 13.7.1 describes the variant of data parallelism that we implemented in Section 13.5 .\nThe key aspect in it is that the aggregation of gradients occurs on one single GPU (GPU 0)\nbefore the updated parameters are rebroadcast to all GPUs.\nIn retrospect, the decision to aggregate on GPU 0 seems rather ad-hoc. After all, we might\njust as well aggregate on the CPU. In fact, we could even decide to aggregate some of\nthe parameters on one GPU and some others on another. Provided that the optimization\nalgorithm supports this, there is no real reason for why we could not. For instance, if we\nhave four parameter vectors with associated gradients g1,...,g4we could aggregate the\ngradients on one GPU for each g\ud835\udc56(\ud835\udc56=1,..., 4).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea2e285e-6b32-41bb-96a9-6089b0137326": {"__data__": {"id_": "ea2e285e-6b32-41bb-96a9-6089b0137326", "embedding": null, "metadata": {"page_label": "585", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d8bcc7d-99ce-4225-b038-e4d772436273", "node_type": "4", "metadata": {"page_label": "585", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "62d4c2e0de32b51234f3ec354abb26c82ee1ae421c2bc7cafdd2075c780788e8", "class_name": "RelatedNodeInfo"}}, "text": "585 Parameter Servers\ntFig. 13.7.1 Left: single GPU training. Right: a variant of multi-GPU training: (1) we compute loss\nand gradient, (2) all gradients are aggregated on one GPU, (3) parameter update happens\nand the parameters are re-distributed to all GPUs.\n209Thisreasoningseemsarbitraryandfrivolous. Afterall,themathematicsisthesamethrough-\nout. However,wearedealingwithrealphysicalhardwarewheredifferentbuseshavediffer-\nentbandwidthasdiscussedin Section13.4 . Considerareal4-wayGPUserverasdescribed\ninFig. 13.7.2 . If it is particularly well connected, it might have a 100 GbE network card.\nMore typical numbers are in the 1\u201310 GbE range with an effective bandwidth of 100 MB/s\nto 1 GB/s. Since the CPUs have too few PCIe lanes to connect to all GPUs directly (e.g.,\nconsumer-grade Intel CPUs have24 lanes) we need a multiplexer209. The bandwidth from\nthe CPU on a 16x Gen3 link is 16 GB/s. This is also the speed at which eachof the GPUs\nis connected to the switch. This means that it is more effective to communicate between\nthe devices.\ntFig. 13.7.2 A 4-way GPU server.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1078, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3793f74e-f64e-4348-85a6-26c776d5d76a": {"__data__": {"id_": "3793f74e-f64e-4348-85a6-26c776d5d76a", "embedding": null, "metadata": {"page_label": "586", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f52bea60-2849-4f3c-bd0b-75b27307e96f", "node_type": "4", "metadata": {"page_label": "586", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f0e67ff5835e869520a00937729a44a7f958085e1e6fd19e7aca67fe1ad0c552", "class_name": "RelatedNodeInfo"}}, "text": "586 Computational Performance\n210For the sake of the argument let\u2019s assume that the gradients are of 160 MB. In this case\nit takes 30 ms to send the gradients from all 3 remaining GPUs to the fourth one (each\ntransfer takes 10 ms = 160 MB / 16 GB/s). Adding another 30 ms to transmit the weight\nvectorsbackwearriveatatotalof60ms. IfwesendalldatatotheCPUweincurapenalty\nof 40 ms since eachof the four GPUs needs to send the data to the CPU, yielding a total\nof 80 ms. Lastly assume that we are able to split the gradients into 4 parts of 40 MB each.\nNow we can aggregate each of the parts on a different GPU simultaneously since the PCIe\nswitch offers a full-bandwidth operation between all links. Instead of 30 ms this takes 7.5\nms, yielding a total of 15 ms for a synchronization operation. In short, depending on how\nwe synchronize parameters the same operation can take anywhere from 15 ms to 80 ms.\nFig. 13.7.3 depicts the different strategies for exchanging parameters.\ntFig. 13.7.3 Parameter synchronization strategies.\nNotethatwehaveyetanothertoolatourdisposalwhenitcomestoimprovingperformance:\nin a deep network it takes some time to compute all gradients from the top to the bottom.\nWe can begin synchronizing gradients for some parameter groups even while we are still\nbusycomputingthemforothers. Seee.g., SergeevandDelBalso( 2018)fordetailsonhow\nto do this in Horovod210.\n13.7.2RingSynchronization\nWhen it comes to synchronization on modern deep learning hardware we often encounter\nsignificantlybespokenetworkconnectivity. Forinstance,theAWSp3.16xlargeandNVIDIA\nDGX-2 instances share the connectivity structure of Fig. 13.7.4 . Each GPU connects to a\nhost CPU via a PCIe link which operates at best at 16 GB/s. Additionally each GPU also\nhas 6 NVLink connections, each of which is capable of transferring 300 Gbit/s bidirec-\ntionally. This amounts to around 18 GB/s per link per direction. In short, the aggregate\nNVLink bandwidth is significantly higher than the PCIe bandwidth. The question is how\nto use it most efficiently.\nIt turns out that the optimal synchronization strategy is to decompose the network into two\nringsandtousethemtosynchronizedatadirectly( Wangetal.,2018).Fig.13.7.5 illustrates\nthatthenetworkcanbedecomposedintoonering(1-2-3-4-5-6-7-8-1)withdoubleNVLink\nbandwidthandintoone(1-4-6-3-5-8-2-7-1)withregularbandwidth. Designinganefficient\nsynchronization protocol in this case is nontrivial.\nConsider the following thought experiment: given a ring of \ud835\udc5bcomputing nodes (or GPUs)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2510, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2454e6ca-7c1b-44d0-acf1-3d17236067b2": {"__data__": {"id_": "2454e6ca-7c1b-44d0-acf1-3d17236067b2", "embedding": null, "metadata": {"page_label": "587", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5385cf2-6400-4ad0-930e-d3759c81f28f", "node_type": "4", "metadata": {"page_label": "587", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "193a7c268d7e7868a5404f1502b564e91e0ace95632618d3417f78028659010b", "class_name": "RelatedNodeInfo"}}, "text": "587 Parameter Servers\ntFig. 13.7.4 NVLink connectivity on 8 V100 GPU servers (image courtesy of NVIDIA).\ntFig. 13.7.5 Decomposition of the NVLink network into two rings.\nwe can send gradients from the first to the second node. There it is added to the local\ngradient and sent on to the third node, and so on. After \ud835\udc5b\u00001steps the aggregate gradient\ncanbefoundinthelast-visitednode. Thatis,thetimetoaggregategradientsgrowslinearly\nwith the number of nodes. But if we do this the algorithm is quite inefficient. After all,\nat any time there is only one of the nodes communicating. What if we broke the gradients\ninto\ud835\udc5bchunks and started synchronizing chunk \ud835\udc56starting at node \ud835\udc56? Since each chunk is of\nsize 1\u009d\ud835\udc5bthe total time is now \u00b9\ud835\udc5b\u00001\u00ba\u009d\ud835\udc5b\u00191. In other words, the time spent to aggregate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 780, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d2454f9-3542-4d87-817c-6c1cb452dd59": {"__data__": {"id_": "5d2454f9-3542-4d87-817c-6c1cb452dd59", "embedding": null, "metadata": {"page_label": "588", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a1038074-afb3-4d44-86d9-0bd27efe70e2", "node_type": "4", "metadata": {"page_label": "588", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ca05582ef66d7b4dbab83d2563063d944210eda9cc5592cec1fdbb54d8923de9", "class_name": "RelatedNodeInfo"}}, "text": "588 Computational Performance\ngradients does not grow as we increase the size of the ring. This is quite an astonishing\nresult.Fig. 13.7.6 illustrates the sequence of steps on \ud835\udc5b=4nodes.\ntFig. 13.7.6 Ring synchronization across 4 nodes. Each node starts transmitting parts of gradients to\nits left neighbor until the assembled gradient can be found in its right neighbor.\nIf we use the same example of synchronizing 160 MB across 8 V100 GPUs we arrive\nat approximately 2\u0001160MB\u009d\u00b93\u000118GB/s\u00ba \u0019 6ms. This is better than using the PCIe\nbus, even though we are now using 8 GPUs. Note that in practice these numbers are a\nbit worse, since deep learning frameworks often fail to assemble communication into large\nburst transfers.\nNote that there is a common misconception that ring synchronization is fundamentally\ndifferent from other synchronization algorithms. The only difference is that the synchro-\nnization path is somewhat more elaborate when compared with a simple tree.\n13.7.3Multi-MachineTraining\nDistributed training on multiple machines adds a further challenge: we need to communi-\ncatewithserversthatareonlyconnectedacrossacomparativelylowerbandwidthfabricthat\ncan be over an order of magnitude slower in some cases. Synchronization across devices is\ntricky. After all, different machines running training code will have subtly different speed.\nHenceweneedto synchronize themifwewanttousesynchronousdistributedoptimization.\nFig. 13.7.7 illustrates how distributed parallel training occurs.\n1.A (different) batch of data is read on each machine, split across multiple GPUs and\ntransferred to GPU memory. There predictions and gradients are computed on each\nGPU batch separately.\n2.The gradients from all local GPUs are aggregated on one GPU (or parts of it are aggre-\ngated over different GPUs).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baf25430-f8b0-40bc-9fe0-02623a72acf1": {"__data__": {"id_": "baf25430-f8b0-40bc-9fe0-02623a72acf1", "embedding": null, "metadata": {"page_label": "589", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6542d896-6b73-473b-a9ec-7253daf085bf", "node_type": "4", "metadata": {"page_label": "589", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4f59d11b1c4be6086b72e04e9a78aabd7310fcadd21c018c0d3755b6440b871e", "class_name": "RelatedNodeInfo"}}, "text": "589 Parameter Servers\n3.The gradients are sent to the CPUs.\n4.The CPUs send the gradients to a central parameter server which aggregates all the\ngradients.\n5.The aggregate gradients are then used to update the parameters and the updated param-\neters are broadcast back to the individual CPUs.\n6.The information is sent to one (or multiple) GPUs.\n7.The updated parameters are spread across all GPUs.\ntFig. 13.7.7 Multi-machine multi-GPU distributed parallel training.\nEach of these operations seems rather straightforward. And, indeed, they can be carried\nout efficiently withina single machine. Once we look at multiple machines, though, we\ncan see that the central parameter server becomes the bottleneck. After all, the bandwidth\nper server is limited, hence for \ud835\udc5aworkers the time it takes to send all gradients to the\nserver isO\u00b9\ud835\udc5a\u00ba. We can break through this barrier by increasing the number of servers\nto\ud835\udc5b. At this point each server only needs to store O\u00b91\u009d\ud835\udc5b\u00baof the parameters, hence the\ntotal time for updates and optimization becomes O\u00b9\ud835\udc5a\u009d\ud835\udc5b\u00ba. Matching both numbers yields\nconstantscalingregardlessofhowmanyworkerswearedealingwith. Inpracticeweusethe\nsamemachines both as workers and as servers. Fig. 13.7.8 illustrates the design (see also\n(Liet al., 2014) for details). In particular, ensuring that multiple machines work without\nunreasonable delays is nontrivial.\n13.7.4Key\u2013ValueStores\nImplementingthestepsrequiredfordistributedmulti-GPUtraininginpracticeisnontrivial.\nThis is why it pays to use a common abstraction, namely that of a key\u2013value store with\nredefined update semantics.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1590, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66642baa-b01e-409f-8652-a123639d3c04": {"__data__": {"id_": "66642baa-b01e-409f-8652-a123639d3c04", "embedding": null, "metadata": {"page_label": "590", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b85c8ed2-ac2f-493d-8710-e2bf7fe1c5db", "node_type": "4", "metadata": {"page_label": "590", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "044ecdd8d8adc20f21afe28530319b2eb0e1a4db6e099998a94db81bffacf860", "class_name": "RelatedNodeInfo"}}, "text": "590 Computational Performance\ntFig. 13.7.8 Top: a single parameter server is a bottleneck since its bandwidth is \ufb01nite. Bottom:\nmultiple parameter servers store parts of the parameters with aggregate bandwidth.\nAcrossmanyworkersandmanyGPUsthecomputationforgradient \ud835\udc56canbedefinedas\ng\ud835\udc56=\u00d5\n\ud835\udc582workers\u00d5\n\ud835\udc572GPUsg\ud835\udc56\ud835\udc57\ud835\udc58,(13.7.1)\nwhereg\ud835\udc56\ud835\udc57\ud835\udc58ispartofgradient \ud835\udc56splitonGPU \ud835\udc57ofworker\ud835\udc58. Thekeyaspectinthisoperation\nis that it is a commutativereduction , that is, it turns many vectors into one and the order in\nwhich the operation is applied does not matter. This is great for our purposes since we do\nnot(needto)havefinegrainedcontroloverwhenwhichgradientisreceived. Besides, note\nthat this operation is independent among different \ud835\udc56.\nThis allows us to define the following two operations: push, which accumulates gradients,\nandpull, which retrieves aggregate gradients. Since we have many different sets of gra-\ndients (after all, we have many layers), we need to index the gradients with a key \ud835\udc56. This\nsimilarity to key\u2013value stores, such as the one introduced in Dynamo ( DeCandia et al.,\n2007) is not by coincidence. They, too, satisfy many similar characteristics, in particular\nwhen it comes to distributing the parameters across multiple servers.\nThe push and pull operations for key-value stores are described as follows:\n\u000fpush(key, value) sends a particular gradient (the value) from a worker to a common\nstorage. There the value is aggregated, e.g., by summing it up.\n\u000fpull(key,value) retrievesanaggregatevaluefromcommonstorage,e.g.,aftercombining\nthe gradients from all workers.\nByhidingallthecomplexityaboutsynchronizationbehindasimplepushandpulloperation", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1648, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c58d7660-4fab-44bb-add9-27f8177afe25": {"__data__": {"id_": "c58d7660-4fab-44bb-add9-27f8177afe25", "embedding": null, "metadata": {"page_label": "591", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "afc6bf40-77f4-485f-b417-a6df5031ca7b", "node_type": "4", "metadata": {"page_label": "591", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6b9f424faa689f472e3a43f7298a77f44632323da4be7bf815b0e4e9e5dc5190", "class_name": "RelatedNodeInfo"}}, "text": "591 Parameter Servers\n211we can decouple the concerns of statistical modelers who want to be able to express opti-\nmization in simple terms and the system engineers who need to deal with the complexity\ninherent in distributed synchronization.\n13.7.5Summary\n\u000fSynchronization needs to be highly adaptive to specific network infrastructure and con-\nnectivity within a server. This can make a significant difference to the time it takes to\nsynchronize.\n\u000fRing-synchronization can be optimal for p3 and DGX-2 servers. For others possibly not\nso much.\n\u000fA hierarchical synchronization strategy works well when adding multiple parameter\nservers for increased bandwidth.\n13.7.6Exercises\n1.Can you increase the ring synchronization even further? Hint: you can send messages\nin both directions.\n2.Is it possible to allow asynchronous communication (while computation is still ongo-\ning)? How does it affect performance?\n3.Whatifwelostaserverduringalong-runningcomputation? Howcanwedesigna fault\ntolerance mechanism to avoid restarting the computation fully?\nDiscussions211.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd0240fe-f10e-4f03-b1a2-ea94732890bc": {"__data__": {"id_": "bd0240fe-f10e-4f03-b1a2-ea94732890bc", "embedding": null, "metadata": {"page_label": "592", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a32da5a-e4b0-43f8-97e0-7c0a2cf4a8e6", "node_type": "4", "metadata": {"page_label": "592", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f96e5aae772373fbcb5ea07e76ec29369f3d93d47639b0b1ca67862d4fb09482", "class_name": "RelatedNodeInfo"}}, "text": "14 Computer Vision\nWhether it is medical diagnosis, self-driving vehicles, camera monitoring, or smart filters,\nmany applications in the field of computer vision are closely related to our current and fu-\nture lives. In recent years, deep learning has been the transformative power for advancing\nthe performance of computer vision systems. It can be said that the most advanced com-\nputer vision applications are almost inseparable from deep learning. In view of this, this\nchapterwillfocusonthefieldofcomputervision,andinvestigatemethodsandapplications\nthat have recently been influential in academia and industry.\nInChapter 7 andChapter 8 , we studied various convolutional neural networks that are\ncommonly used in computer vision, and applied them to simple image classification tasks.\nAt the beginning of this chapter, we will describe two methods that may improve model\ngeneralization, namely imageaugmentation andfine-tuning , andapplythemtoimageclas-\nsification. Since deep neural networks can effectively represent images in multiple lev-\nels, such layerwise representations have been successfully used in various computer vision\ntaskssuchas objectdetection ,semanticsegmentation ,andstyletransfer . Followingthekey\nidea of leveraging layerwise representations in computer vision, we will begin with major\ncomponents and techniques for object detection. Next, we will show how to use fully con-\nvolutionalnetworks forsemanticsegmentationofimages. Thenwewillexplainhowtouse\nstyle transfer techniques to generate images like the cover of this book. In the end, we con-\nclude this chapter by applying the materials of this chapter and several previous chapters\non two popular computer vision benchmark datasets.\n14.1ImageAugmentation\nInSection 8.1 , we mentioned that large datasets are a prerequisite for the success of deep\nneuralnetworksinvariousapplications. Imageaugmentation generatessimilarbutdistinct\ntraining examples after a series of random changes to the training images, thereby expand-\ning the size of the training set. Alternatively, image augmentation can be motivated by the\nfactthatrandomtweaksoftrainingexamplesallowmodelstorelylessoncertainattributes,\nthereby improving their generalization ability. For example, we can crop an image in dif-\nferent ways to make the object of interest appear in different positions, thereby reducing\nthe dependence of a model on the position of the object. We can also adjust factors such as\nbrightness and color to reduce a model\u2019s sensitivity to color. It is probably true that image\n592", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2548, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a4126d3-22e1-4249-afb2-225a9fa318fe": {"__data__": {"id_": "2a4126d3-22e1-4249-afb2-225a9fa318fe", "embedding": null, "metadata": {"page_label": "593", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3285bfd-8ddb-4c86-af0e-d32bd0d6c98d", "node_type": "4", "metadata": {"page_label": "593", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b8fb7b10921392fb10e01e3f2eefccc371344ebe4aeb20a0e63b97de579d0fec", "class_name": "RelatedNodeInfo"}}, "text": "593 Image Augmentation\naugmentation was indispensable for the success of AlexNet at that time. In this section we\nwill discuss this widely used technique in computer vision.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\n14.1.1CommonImageAugmentation Methods\nIn our investigation of common image augmentation methods, we will use the following\n400\u0002500image an example.\nd2l.set_figsize()\nimg =d2l.Image .open( '../img/cat1.jpg ')\nd2l.plt.imshow(img);\nMost image augmentation methods have a certain degree of randomness. To make it easier\nfor us to observe the effect of image augmentation, next we define an auxiliary function\napply. Thisfunctionrunstheimageaugmentationmethod augmultipletimesontheinput\nimage imgand shows all the results.\ndef apply (img, aug, num_rows =2, num_cols =4, scale =1.5):\nY=[aug(img) for _inrange (num_rows *num_cols)]\nd2l.show_images(Y, num_rows, num_cols, scale =scale)\nFlipping and Cropping\nFlippingtheimageleftandrightusuallydoesnotchangethecategoryoftheobject. Thisis\none of the earliest and most widely used methods of image augmentation. Next, we use the\ntransforms module to create the RandomHorizontalFlip instance, which flips an image\nleft and right with a 50% chance.\napply(img, torchvision .transforms .RandomHorizontalFlip())\nFlipping up and down is not as common as flipping left and right. But at least for this", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1407, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b2ba7a9e-8818-4c58-9117-dff8b1eb6848": {"__data__": {"id_": "b2ba7a9e-8818-4c58-9117-dff8b1eb6848", "embedding": null, "metadata": {"page_label": "594", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5201ede0-37a0-4b28-9a30-6a619a089421", "node_type": "4", "metadata": {"page_label": "594", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3f1d8ccb52701b81b6cb29f5fb16164baaddfef8458e4f068cbe291b9797d03d", "class_name": "RelatedNodeInfo"}}, "text": "594 Computer Vision\nexample image, flipping up and down does not hinder recognition. Next, we create a Ran-\ndomVerticalFlip instance to flip an image up and down with a 50% chance.\napply(img, torchvision .transforms .RandomVerticalFlip())\nInthe exampleimageweused, the cat isin the middle of the image, but thismaynot be the\ncaseingeneral. In Section7.5 ,weexplainedthatthepoolinglayercanreducethesensitivity\nof a convolutional layer to the target position. In addition, we can also randomly crop the\nimage to make objects appear in different positions in the image at different scales, which\ncan also reduce the sensitivity of a model to the target position.\nIn the code below, we randomly crop an area with an area of 10%\u0018100%of the original\nareaeachtime, andtheratioofwidthtoheightofthisareaisrandomlyselectedfrom 0.5\u0018\n2. Then, the width and height of the region are both scaled to 200 pixels. Unless otherwise\nspecified, the random number between \ud835\udc4eand\ud835\udc4fin this section refers to a continuous value\nobtained by random and uniform sampling from the interval \u00bb\ud835\udc4e,\ud835\udc4f\u00bc.\nshape_aug =torchvision .transforms .RandomResizedCrop(\n(200,200), scale =(0.1,1), ratio =(0.5,2))\napply(img, shape_aug)\nChangingColors\nAnotheraugmentationmethodischangingcolors. Wecanchangefouraspectsoftheimage\ncolor: brightness,contrast,saturation,andhue. Intheexamplebelow,werandomlychange\nthe brightness of the image to a value between 50% ( 1\u00000.5) and 150% ( 1\u00b80.5) of the\noriginal image.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1458, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "141f90ab-c2ff-4353-91f2-bb15abedc1dd": {"__data__": {"id_": "141f90ab-c2ff-4353-91f2-bb15abedc1dd", "embedding": null, "metadata": {"page_label": "595", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "02f2515f-45a9-49ea-8ccc-9abfa8f60b54", "node_type": "4", "metadata": {"page_label": "595", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e7d371c2e2273077f80021deab97e923b35edd1292907a16745b1a9935b96d49", "class_name": "RelatedNodeInfo"}}, "text": "595 Image Augmentation\napply(img, torchvision .transforms .ColorJitter(\nbrightness =0.5, contrast =0, saturation =0, hue =0))\nSimilarly, we can randomly change the hue of the image.\napply(img, torchvision .transforms .ColorJitter(\nbrightness =0, contrast =0, saturation =0, hue =0.5))\nWe can also create a RandomColorJitter instance and set how to randomly change the\nbrightness ,contrast ,saturation , and hueof the image at the same time.\ncolor_aug =torchvision .transforms .ColorJitter(\nbrightness =0.5, contrast =0.5, saturation =0.5, hue =0.5)\napply(img, color_aug)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5a6670a-d8e2-4b86-91ba-a238000dc3ef": {"__data__": {"id_": "f5a6670a-d8e2-4b86-91ba-a238000dc3ef", "embedding": null, "metadata": {"page_label": "596", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cd571c8b-3d36-42b6-977d-a6763e091468", "node_type": "4", "metadata": {"page_label": "596", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "77b44a08182da788593e6efd28b45058488a1d4d213168a83e823bf8108be159", "class_name": "RelatedNodeInfo"}}, "text": "596 Computer Vision\nCombiningMultiple ImageAugmentation Methods\nIn practice, we will combine multiple image augmentation methods. For example, we can\ncombine the different image augmentation methods defined above and apply them to each\nimage via a Compose instance.\naugs =torchvision .transforms .Compose([\ntorchvision .transforms .RandomHorizontalFlip(), color_aug, shape_aug])\napply(img, augs)\n14.1.2Trainingwith Image Augmentation\nLet\u2019s train a model with image augmentation. Here we use the CIFAR-10 dataset instead\nof the Fashion-MNIST dataset that we used before. This is because the position and size\nof the objects in the Fashion-MNIST dataset have been normalized, while the color and\nsize of the objects in the CIFAR-10 dataset have more significant differences. The first 32\ntraining images in the CIFAR-10 dataset are shown below.\nall_images =torchvision .datasets .CIFAR10(train =True , root =\"../data \",\ndownload =True )\nd2l.show_images([all_images[i][ 0]for iinrange (32)], 4,8, scale =0.8);\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/\n\u21a9!cifar-10-python.tar.gz\n100%|\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff| 170498071/170498071 [00:04<00:00, 37716809.52it/s]\nExtracting ../data/cifar-10-python.tar.gz to ../data\nIn order to obtain definitive results during prediction, we usually only apply image aug-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1322, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc3aa15d-caff-45d6-af58-701f219c5b97": {"__data__": {"id_": "fc3aa15d-caff-45d6-af58-701f219c5b97", "embedding": null, "metadata": {"page_label": "597", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d92b267-b467-4321-8bf1-7a35a3dd2511", "node_type": "4", "metadata": {"page_label": "597", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6e39d6945506e6ffd9d401d814684765321f90aea51bbce2de29832b2371a3c2", "class_name": "RelatedNodeInfo"}}, "text": "597 Image Augmentation\nmentation to training examples, and do not use image augmentation with random opera-\ntions during prediction. Here we only use the simplest random left-right flipping method.\nIn addition, we use a ToTensor instance to convert a minibatch of images into the format\nrequired by the deep learning framework, i.e., 32-bit floating point numbers between 0 and\n1 with the shape of (batch size, number of channels, height, width).\ntrain_augs =torchvision .transforms .Compose([\ntorchvision .transforms .RandomHorizontalFlip(),\ntorchvision .transforms .ToTensor()])\ntest_augs =torchvision .transforms .Compose([\ntorchvision .transforms .ToTensor()])\nNext, we define an auxiliary function to facilitate reading the image and applying image\naugmentation. The transform argument provided by PyTorch\u2019s dataset applies augmen-\ntation to transform the images. For a detailed introduction to DataLoader , please refer to\nSection 4.2 .\ndef load_cifar10 (is_train, augs, batch_size):\ndataset =torchvision .datasets .CIFAR10(root =\"../data \", train =is_train,\ntransform =augs, download =True )\ndataloader =torch .utils .data .DataLoader(dataset, batch_size =batch_size,\nshuffle =is_train, num_workers =d2l.get_dataloader_workers())\nreturn dataloader\nMulti-GPUTraining\nWe train the ResNet-18 model from Section 8.6 on the CIFAR-10 dataset. Recall the in-\ntroduction to multi-GPU training in Section 13.6 . In the following, we define a function to\ntrain and evaluate the model using multiple GPUs.\n#@save\ndef train_batch_ch13 (net, X, y, loss, trainer, devices):\n\"\"\"Train for a minibatch with multiple GPUs (defined in Chapter 13).\"\"\"\nifisinstance (X, list ):\n# Required for BERT fine-tuning (to be covered later)\nX=[x.to(devices[ 0])for xinX]\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08cc121d-7520-4e8d-a75f-0f157dcbd0bc": {"__data__": {"id_": "08cc121d-7520-4e8d-a75f-0f157dcbd0bc", "embedding": null, "metadata": {"page_label": "598", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4adbe36d-d504-4a00-bfc9-7edef0d7580b", "node_type": "4", "metadata": {"page_label": "598", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "246a05f7dc6f4103f827badcaf33a4259ac25cf0d75ef1c745478272d2f47a95", "class_name": "RelatedNodeInfo"}}, "text": "598 Computer Vision\n(continued from previous page)\nelse :\nX=X.to(devices[ 0])\ny=y.to(devices[ 0])\nnet.train()\ntrainer .zero_grad()\npred =net(X)\nl=loss(pred, y)\nl.sum() .backward()\ntrainer .step()\ntrain_loss_sum =l.sum()\ntrain_acc_sum =d2l.accuracy(pred, y)\nreturn train_loss_sum, train_acc_sum\n#@save\ndef train_ch13 (net, train_iter, test_iter, loss, trainer, num_epochs,\ndevices =d2l.try_all_gpus()):\n\"\"\"Train a model with multiple GPUs (defined in Chapter 13).\"\"\"\ntimer, num_batches =d2l.Timer(), len(train_iter)\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs], ylim =[0,1],\nlegend =['train loss ','train acc ','test acc '])\nnet =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])\nfor epoch inrange (num_epochs):\n# Sum of training loss, sum of training accuracy, no. of examples,\n# no. of predictions\nmetric =d2l.Accumulator( 4)\nfor i, (features, labels) inenumerate (train_iter):\ntimer .start()\nl, acc =train_batch_ch13(\nnet, features, labels, loss, trainer, devices)\nmetric .add(l, acc, labels .shape[ 0], labels .numel())\ntimer .stop()\nif(i+1)%(num_batches //5)==0ori==num_batches -1:\nanimator .add(epoch +(i+1)/num_batches,\n(metric[ 0]/metric[ 2], metric[ 1]/metric[ 3],\nNone ))\ntest_acc =d2l.evaluate_accuracy_gpu(net, test_iter)\nanimator .add(epoch +1, (None ,None , test_acc))\nprint (f'loss {metric[ 0]/metric[ 2]:.3f}, train acc '\nf'{metric[ 1]/metric[ 3]:.3f}, test acc {test_acc :.3f}')\nprint (f'{metric[ 2]*num_epochs /timer .sum() :.1f}examples/sec on '\nf'{str(devices) }')\nNow we can define the train_with_data_aug function to train the model with image\naugmentation. This function gets all available GPUs, uses Adam as the optimization algo-\nrithm, appliesimageaugmentationtothetrainingdataset, andfinallycallsthe train_ch13\nfunction just defined to train and evaluate the model.\nbatch_size, devices, net =256, d2l .try_all_gpus(), d2l .resnet18( 10,3)\nnet.apply(d2l .init_cnn)\ndef train_with_data_aug (train_augs, test_augs, net, lr =0.001 ):\ntrain_iter =load_cifar10( True , train_augs, batch_size)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2064, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f58b013-9f2b-48a9-a38c-4a0d10e9a9cf": {"__data__": {"id_": "7f58b013-9f2b-48a9-a38c-4a0d10e9a9cf", "embedding": null, "metadata": {"page_label": "599", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7079f221-2dd2-40e8-b574-f0ba821db952", "node_type": "4", "metadata": {"page_label": "599", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "75c8546a215b67cec7e17009eb671af69ea4e890b0eabc14d8a4d0438d76c8c4", "class_name": "RelatedNodeInfo"}}, "text": "599 Image Augmentation\n(continued from previous page)\ntest_iter =load_cifar10( False , test_augs, batch_size)\nloss =nn.CrossEntropyLoss(reduction =\"none \")\ntrainer =torch .optim .Adam(net .parameters(), lr =lr)\nnet( next (iter (train_iter))[ 0])\ntrain_ch13(net, train_iter, test_iter, loss, trainer, 10, devices)\nLet\u2019strainthemodelusingimageaugmentationbasedonrandomleft-rightflipping.\ntrain_with_data_aug(train_augs, test_augs, net)\nloss 0.215 , train acc 0.925 , test acc 0.810\n4728.8 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]\n14.1.3Summary\n\u000fImageaugmentationgeneratesrandomimagesbasedonexistingtrainingdatatoimprove\nthe generalization ability of models.\n\u000fIn order to obtain definitive results during prediction, we usually only apply image aug-\nmentation to training examples, and do not use image augmentation with random op-\nerations during prediction.\n\u000fDeep learning frameworks provide many different image augmentation methods, which\ncan be applied simultaneously.\n14.1.4Exercises\n1.Trainthemodelwithoutusingimageaugmentation: train_with_data_aug(test_augs,\ntest_augs) . Compare training and testing accuracy when using and not using image\naugmentation. Can this comparative experiment support the argument that image aug-\nmentation can mitigate overfitting? Why?\n2.CombinemultipledifferentimageaugmentationmethodsinmodeltrainingontheCIFAR-\n10 dataset. Does it improve test accuracy?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d1503bd-979b-450a-b79b-cfd70c19c97c": {"__data__": {"id_": "5d1503bd-979b-450a-b79b-cfd70c19c97c", "embedding": null, "metadata": {"page_label": "600", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91d3978d-23ed-4116-921c-a8045e65fd37", "node_type": "4", "metadata": {"page_label": "600", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ba612094df0713f0a6951b1554e42282448a3fa8d28c9113da2ad0e6f8af6e26", "class_name": "RelatedNodeInfo"}}, "text": "600 Computer Vision\n2123.Refer to the online documentation of the deep learning framework. What other image\naugmentation methods does it also provide?\nDiscussions212.\n14.2Fine-Tuning\nIn earlier chapters, we discussed how to train models on the Fashion-MNIST training\ndataset with only 60000 images. We also described ImageNet, the most widely used large-\nscale image dataset in academia, which has more than 10 million images and 1000 objects.\nHowever, the size of the dataset that we usually encounter is between those of the two\ndatasets.\nSuppose that we want to recognize different types of chairs from images, and then recom-\nmend purchase links to users. One possible method is to first identify 100 common chairs,\ntake 1000 images of different angles for each chair, and then train a classification model\non the collected image dataset. Although this chair dataset may be larger than the Fashion-\nMNIST dataset, the number of examples is still less than one-tenth of that in ImageNet.\nThis may lead to overfitting of complicated models that are suitable for ImageNet on this\nchair dataset. Besides, due to the limited amount of training examples, the accuracy of the\ntrained model may not meet practical requirements.\nIn order to address the above problems, an obvious solution is to collect more data. How-\never, collecting and labeling data can take a lot of time and money. For example, in order\nto collect the ImageNet dataset, researchers have spent millions of dollars from research\nfunding. Although the current data collection cost has been significantly reduced, this cost\nstill cannot be ignored.\nAnother solution is to apply transfer learning to transfer the knowledge learned from the\nsourcedataset to thetargetdataset . For example, although most of the images in the Ima-\ngeNet dataset have nothing to do with chairs, the model trained on this dataset may extract\nmore general image features, which can help identify edges, textures, shapes, and object\ncomposition. These similar features may also be effective for recognizing chairs.\n14.2.1Steps\nIn this section, we will introduce a common technique in transfer learning: fine-tuning . As\nshown in Fig. 14.2.1 , fine-tuning consists of the following four steps:\n1.Pretrain a neural network model, i.e., the source model , on a source dataset (e.g., the\nImageNet dataset).\n2.Create a new neural network model, i.e., the target model . This copies all model de-\nsigns and their parameters on the source model except the output layer. We assume that\nthese model parameters contain the knowledge learned from the source dataset and this", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b09ce588-62e9-41da-bf9a-a3e29c664f45": {"__data__": {"id_": "b09ce588-62e9-41da-bf9a-a3e29c664f45", "embedding": null, "metadata": {"page_label": "601", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "727a5a3b-4138-41ae-911b-743af3f007bd", "node_type": "4", "metadata": {"page_label": "601", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "948a4a86dec0262b358bd6254da5a98fe80ba7edcecea97290bca156092acda6", "class_name": "RelatedNodeInfo"}}, "text": "601 Fine-Tuning\nknowledge will also be applicable to the target dataset. We also assume that the output\nlayer of the source model is closely related to the labels of the source dataset; thus it is\nnot used in the target model.\n3.Add an output layer to the target model, whose number of outputs is the number of\ncategories in the target dataset. Then randomly initialize the model parameters of this\nlayer.\n4.Train the target model on the target dataset, such as a chair dataset. The output layer\nwill be trained from scratch, while the parameters of all the other layers are fine-tuned\nbased on the parameters of the source model.\ntFig. 14.2.1 Fine tuning.\nWhen target datasets are much smaller than source datasets, fine-tuning helps to improve\nmodels\u2019 generalization ability.\n14.2.2HotDog Recognition\nLet\u2019s demonstrate fine-tuning via a concrete case: hot dog recognition. We will fine-tune\na ResNet model on a small dataset, which was pretrained on the ImageNet dataset. This\nsmall dataset consists of thousands of images with and without hot dogs. We will use the\nfine-tuned model to recognize hot dogs from images.\n%matplotlib inline\nimport os\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\nReadingthe Dataset\nThe hot dog dataset we use was taken from online images. This dataset consists of 1400\npositive-class images containing hot dogs, and as many negative-class images containing\nother foods. 1000 images of both classes are used for training and the rest are for test-\ning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1518, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3562958f-4766-4dea-b564-00fcc221ddcc": {"__data__": {"id_": "3562958f-4766-4dea-b564-00fcc221ddcc", "embedding": null, "metadata": {"page_label": "602", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0dda1ba1-7dc8-4a51-bdd0-373731d9da18", "node_type": "4", "metadata": {"page_label": "602", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "66a514b02bc91c2deec1196ac34b24b3020dd3a6140f7a31835dfbe4d74f1301", "class_name": "RelatedNodeInfo"}}, "text": "602 Computer Vision\nAfterunzippingthedownloadeddataset,weobtaintwofolders hotdog/train andhotdog/\ntest. Both folders have hotdog andnot-hotdog subfolders, either of which contains\nimages of the corresponding class.\n#@save\nd2l.DATA_HUB[ 'hotdog ']=(d2l .DATA_URL +'hotdog.zip ',\n'fba480ffa8aa7e0febbb511d181409f899b9baa5 ')\ndata_dir =d2l.download_extract( 'hotdog ')\nDownloading ../data /hotdog .zip from http ://d2l-data .s3-accelerate .amazonaws .\n\u21a9!com/hotdog .zip...\nWe create two instances to read all the image files in the training and testing datasets, re-\nspectively.\ntrain_imgs =torchvision .datasets .ImageFolder(os .path .join(data_dir, 'train '))\ntest_imgs =torchvision .datasets .ImageFolder(os .path .join(data_dir, 'test '))\nThe first 8 positive examples and the last 8 negative images are shown below. As you can\nsee, the images vary in size and aspect ratio.\nhotdogs =[train_imgs[i][ 0]for iinrange (8)]\nnot_hotdogs =[train_imgs[ -i-1][0]for iinrange (8)]\nd2l.show_images(hotdogs +not_hotdogs, 2,8, scale =1.4);\nDuring training, we first crop a random area of random size and random aspect ratio from\ntheimage,andthenscalethisareatoa 224\u0002224inputimage. Duringtesting,wescaleboth\nthe height and width of an image to 256 pixels, and then crop a central 224\u0002224area as\ninput. In addition, for the three RGB (red, green, and blue) color channels we standardize\ntheirvalueschannelbychannel. Concretely,themeanvalueofachannelissubtractedfrom\neach value of that channel and then the result is divided by the standard deviation of that\nchannel.\n# Specify the means and standard deviations of the three RGB channels to\n# standardize each channel\nnormalize =torchvision .transforms .Normalize(\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df019df8-06de-40b2-ad3b-5d55e799fe4f": {"__data__": {"id_": "df019df8-06de-40b2-ad3b-5d55e799fe4f", "embedding": null, "metadata": {"page_label": "603", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46e6e8bb-f018-4f19-be2d-74344601ef18", "node_type": "4", "metadata": {"page_label": "603", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0eb0404456ff56d1011984aeecb0027b83be647cf61596b6a911e610624e60eb", "class_name": "RelatedNodeInfo"}}, "text": "603 Fine-Tuning\n(continued from previous page)\n[0.485 ,0.456 ,0.406 ], [ 0.229 ,0.224 ,0.225 ])\ntrain_augs =torchvision .transforms .Compose([\ntorchvision .transforms .RandomResizedCrop( 224),\ntorchvision .transforms .RandomHorizontalFlip(),\ntorchvision .transforms .ToTensor(),\nnormalize])\ntest_augs =torchvision .transforms .Compose([\ntorchvision .transforms .Resize([ 256,256]),\ntorchvision .transforms .CenterCrop( 224),\ntorchvision .transforms .ToTensor(),\nnormalize])\nDefiningand Initializing the Model\nWe use ResNet-18, which was pretrained on the ImageNet dataset, as the source model.\nHere, we specify pretrained=True to automatically download the pretrained model pa-\nrameters. If this model is used for the first time, Internet connection is required for down-\nload.\npretrained_net =torchvision .models .resnet18(pretrained =True )\nThe pretrained source model instance contains a number of feature layers and an output\nlayer fc. The main purpose of this division is to facilitate the fine-tuning of model param-\neters of all layers but the output layer. The member variable fcof source model is given\nbelow.\npretrained_net .fc\nLinear(in_features =512, out_features =1000 , bias =True )\nAs a fully connected layer, it transforms ResNet\u2019s final global average pooling outputs into\n1000 class outputs of the ImageNet dataset. We then construct a new neural network as\nthe target model. It is defined in the same way as the pretrained source model except that\nits number of outputs in the final layer is set to the number of classes in the target dataset\n(rather than 1000).\nIn the code below, the model parameters before the output layer of the target model in-\nstance finetune_net are initialized to model parameters of the corresponding layers from\nthe source model. Since these model parameters were obtained via pretraining on Ima-\ngeNet, they are effective. Therefore, we can only use a small learning rate to fine-tune such\npretrained parameters. In contrast, model parameters in the output layer are randomly ini-\ntialized and generally require a larger learning rate to be learned from scratch. Letting the\nbase learning rate be \ud835\udf02, a learning rate of 10\ud835\udf02will be used to iterate the model parameters\nin the output layer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e869199-d91f-4abd-9841-7cb53f3acfe0": {"__data__": {"id_": "4e869199-d91f-4abd-9841-7cb53f3acfe0", "embedding": null, "metadata": {"page_label": "604", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dec8a953-54e9-4870-8a75-ca890ceae60c", "node_type": "4", "metadata": {"page_label": "604", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ce01ac741ba9b990c59b6397a49e9f42ad6d8fa893580940eb496789c6b3b1eb", "class_name": "RelatedNodeInfo"}}, "text": "604 Computer Vision\nfinetune_net =torchvision .models .resnet18(pretrained =True )\nfinetune_net .fc=nn.Linear(finetune_net .fc.in_features, 2)\nnn.init .xavier_uniform_(finetune_net .fc.weight);\nFine-Tuningthe Model\nFirst, we define a training function train_fine_tuning that uses fine-tuning so it can be\ncalled multiple times.\n# If `param_group=True`, the model parameters in the output layer will be\n# updated using a learning rate ten times greater\ndef train_fine_tuning (net, learning_rate, batch_size =128, num_epochs =5,\nparam_group =True ):\ntrain_iter =torch .utils .data .DataLoader(torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'train '), transform =train_augs),\nbatch_size =batch_size, shuffle =True )\ntest_iter =torch .utils .data .DataLoader(torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'test '), transform =test_augs),\nbatch_size =batch_size)\ndevices =d2l.try_all_gpus()\nloss =nn.CrossEntropyLoss(reduction =\"none \")\nifparam_group:\nparams_1x =[param for name, param innet.named_parameters()\nifname not in[\"fc.weight \",\"fc.bias \"]]\ntrainer =torch .optim .SGD([{ 'params ': params_1x},\n{'params ': net .fc.parameters(),\n'lr': learning_rate *10}],\nlr=learning_rate, weight_decay =0.001 )\nelse :\ntrainer =torch .optim .SGD(net .parameters(), lr =learning_rate,\nweight_decay =0.001 )\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\ndevices)\nWe set the base learning rate to a small value in order to fine-tune the model parameters\nobtained via pretraining. Based on the previous settings, we will train the output layer\nparameters of the target model from scratch using a learning rate ten times greater.\ntrain_fine_tuning(finetune_net, 5e-5 )\nloss 0.242 , train acc 0.909 , test acc 0.940\n1062.4 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]\nFor comparison, we define an identical model, but initialize all of its model parameters to\nrandomvalues. Sincetheentiremodelneedstobetrainedfromscratch,wecanusealarger\nlearning rate.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f03b89b-7596-4b66-a30d-abdded90a372": {"__data__": {"id_": "7f03b89b-7596-4b66-a30d-abdded90a372", "embedding": null, "metadata": {"page_label": "605", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae1446e9-16aa-4ed9-bcd8-3662ba9e36a8", "node_type": "4", "metadata": {"page_label": "605", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8d63e5927a09b165f4c19d3504d02a6a7921779b63e0c94c719105e456f94103", "class_name": "RelatedNodeInfo"}}, "text": "605 Fine-Tuning\nscratch_net =torchvision .models .resnet18()\nscratch_net .fc=nn.Linear(scratch_net .fc.in_features, 2)\ntrain_fine_tuning(scratch_net, 5e-4 , param_group =False )\nloss 0.352 , train acc 0.846 , test acc 0.850\n1525.4 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]\nAs we can see, the fine-tuned model tends to perform better for the same epoch because its\ninitial parameter values are more effective.\n14.2.3Summary\n\u000fTransferlearningtransfersknowledgelearnedfromthesourcedatasettothetargetdataset.\nFine-tuning is a common technique for transfer learning.\n\u000fThe target model copies all model designs with their parameters from the source model\nexcept the output layer, and fine-tunes these parameters based on the target dataset. In\ncontrast, the output layer of the target model needs to be trained from scratch.\n\u000fGenerally, fine-tuning parameters uses a smaller learning rate, while training the output\nlayer from scratch can use a larger learning rate.\n14.2.4Exercises", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "314e95de-31ee-473b-8b94-539b4a500bd4": {"__data__": {"id_": "314e95de-31ee-473b-8b94-539b4a500bd4", "embedding": null, "metadata": {"page_label": "606", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ff1bc0f-9812-4b4f-9e0d-d1504c786d94", "node_type": "4", "metadata": {"page_label": "606", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9eaa2f72bebd1b8411416fba8a589694489451daa0aa4c26a99e5591cf57a1e7", "class_name": "RelatedNodeInfo"}}, "text": "606 Computer Vision\n2131.Keep increasing the learning rate of finetune_net . How does the accuracy of the\nmodel change?\n2.Furtheradjusthyperparametersof finetune_net andscratch_net inthecomparative\nexperiment. Do they still differ in accuracy?\n3.Settheparametersbeforetheoutputlayerof finetune_net tothoseofthesourcemodel\nand donotupdate them during training. How does the accuracy of the model change?\nYou can use the following code.\nfor param infinetune_net .parameters():\nparam .requires_grad =False\n4.In fact, there is a \u201chotdog\u201d class in the ImageNet dataset. Its corresponding weight\nparameter in the output layer can be obtained via the following code. How can we\nleverage this weight parameter?\nweight =pretrained_net .fc.weight\nhotdog_w =torch .split(weight .data, 1, dim =0)[934]\nhotdog_w .shape\ntorch .Size([ 1,512])\nDiscussions213.\n14.3Object Detectionand Bounding Boxes\nIn earlier sections (e.g., Section 8.1 \u2013Section 8.4 ), we introduced various models for image\nclassification. In image classification tasks, we assume that there is only onemajor object\nin the image and we only focus on how to recognize its category. However, there are often\nmultiple objects in the image of interest. We not only want to know their categories, but\nalso their specific positions in the image. In computer vision, we refer to such tasks as\nobjectdetection (orobjectrecognition ).\nObjectdetectionhasbeenwidelyappliedinmanyfields. Forexample,self-drivingneedsto\nplantravelingroutesbydetectingthepositionsofvehicles,pedestrians,roads,andobstacles\nin the captured video images. Besides, robots may use this technique to detect and localize\nobjectsofinterestthroughoutitsnavigationofanenvironment. Moreover,securitysystems\nmay need to detect abnormal objects, such as intruders or bombs.\nIn the next few sections, we will introduce several deep learning methods for object detec-\ntion. We will begin with an introduction to positions (orlocations ) of objects.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1954, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0796f4c-7754-4851-a0a9-da343529f29b": {"__data__": {"id_": "a0796f4c-7754-4851-a0a9-da343529f29b", "embedding": null, "metadata": {"page_label": "607", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23ca34a3-6f46-4ffb-8aab-b38b43d770cc", "node_type": "4", "metadata": {"page_label": "607", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3db66786bb80f33d6e6f4917e8603547e42810793c1285138e7c882c50f684c7", "class_name": "RelatedNodeInfo"}}, "text": "607 Object Detection and Bounding Boxes\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\nWe will load the sample image to be used in this section. We can see that there is a dog\non the left side of the image and a cat on the right. They are the two major objects in this\nimage.\nd2l.set_figsize()\nimg =d2l.plt.imread( '../img/catdog.jpg ')\nd2l.plt.imshow(img);\n14.3.1Bounding Boxes\nIn object detection, we usually use a bounding box to describe the spatial location of an\nobject. The bounding box is rectangular, which is determined by the \ud835\udc65and\ud835\udc66coordinates\nof the upper-left corner of the rectangle and the such coordinates of the lower-right corner.\nAnother commonly used bounding box representation is the \u00b9\ud835\udc65,\ud835\udc66\u00ba-axis coordinates of the\nbounding box center, and the width and height of the box.\nHerewedefinefunctionstoconvertbetweenthesetworepresentations: box_corner_to_center\nconverts from the two-corner representation to the center-width-height presentation, and\nbox_center_to_corner viceversa. Theinputargument boxesshouldbeatwo-dimensional\ntensor of shape ( \ud835\udc5b, 4), where\ud835\udc5bis the number of bounding boxes.\n#@save\ndef box_corner_to_center (boxes):\n\"\"\"Convert from (upper-left, lower-right) to (center, width, height).\"\"\"\nx1, y1, x2, y2 =boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\ncx=(x1 +x2) /2\ncy=(y1 +y2) /2\nw=x2-x1\nh=y2-y1\nboxes =torch .stack((cx, cy, w, h), axis =-1)\nreturn boxes\n#@save\ndef box_center_to_corner (boxes):\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97403974-4a13-4e2f-bd52-9abdc013d60f": {"__data__": {"id_": "97403974-4a13-4e2f-bd52-9abdc013d60f", "embedding": null, "metadata": {"page_label": "608", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "44292833-a1c8-4799-b97e-bed602ff59c5", "node_type": "4", "metadata": {"page_label": "608", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5acce0ffa6d87998dbc3dbba2aafb3892373dc1304822638de7640fba1e2d43b", "class_name": "RelatedNodeInfo"}}, "text": "608 Computer Vision\n(continued from previous page)\n\"\"\"Convert from (center, width, height) to (upper-left, lower-right).\"\"\"\ncx, cy, w, h =boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\nx1=cx-0.5 *w\ny1=cy-0.5 *h\nx2=cx+0.5 *w\ny2=cy+0.5 *h\nboxes =torch .stack((x1, y1, x2, y2), axis =-1)\nreturn boxes\nWe will define the bounding boxes of the dog and the cat in the image based on the co-\nordinate information. The origin of the coordinates in the image is the upper-left corner\nof the image, and to the right and down are the positive directions of the \ud835\udc65and\ud835\udc66axes,\nrespectively.\n# Here `bbox` is the abbreviation for bounding box\ndog_bbox, cat_bbox =[60.0 ,45.0 ,378.0 ,516.0 ], [ 400.0 ,112.0 ,655.0 ,493.0 ]\nWe can verify the correctness of the two bounding box conversion functions by converting\ntwice.\nboxes =torch .tensor((dog_bbox, cat_bbox))\nbox_center_to_corner(box_corner_to_center(boxes)) ==boxes\ntensor([[ True ,True ,True ,True ],\n[True ,True ,True ,True ]])\nLet\u2019s draw the bounding boxes in the image to check if they are accurate. Before drawing,\nwe will define a helper function bbox_to_rect . It represents the bounding box in the\nbounding box format of the matplotlib package.\n#@save\ndef bbox_to_rect (bbox, color):\n\"\"\"Convert bounding box to matplotlib format.\"\"\"\n# Convert the bounding box (upper-left x, upper-left y, lower-right x,\n# lower-right y) format to the matplotlib format: ((upper-left x,\n# upper-left y), width, height)\nreturn d2l.plt.Rectangle(\nxy=(bbox[ 0], bbox[ 1]), width =bbox[ 2]-bbox[ 0], height =bbox[ 3]-bbox[ 1],\nfill =False , edgecolor =color, linewidth =2)\nAfter adding the bounding boxes on the image, we can see that the main outline of the two\nobjects are basically inside the two boxes.\nfig =d2l.plt.imshow(img)\nfig.axes .add_patch(bbox_to_rect(dog_bbox, 'blue '))\nfig.axes .add_patch(bbox_to_rect(cat_bbox, 'red'));", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24204954-f247-4abf-b85a-2a17fc553d4b": {"__data__": {"id_": "24204954-f247-4abf-b85a-2a17fc553d4b", "embedding": null, "metadata": {"page_label": "609", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d1878d8d-8751-48aa-9f6d-4b42cf115000", "node_type": "4", "metadata": {"page_label": "609", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "faf85df3e636d1ca8acdfb76d502b4162c61017337e2f3e62ed59c91d370c6f6", "class_name": "RelatedNodeInfo"}}, "text": "609 Anchor Boxes\n21414.3.2Summary\n\u000fObjectdetectionnotonlyrecognizesalltheobjectsofinterestintheimage,butalsotheir\npositions. The position is generally represented by a rectangular bounding box.\n\u000fWe can convert between two commonly used bounding box representations.\n14.3.3Exercises\n1.Find another image and try to label a bounding box that contains the object. Compare\nlabeling bounding boxes and categories: which usually takes longer?\n2.Whyistheinnermostdimensionoftheinputargument boxesofbox_corner_to_center\nandbox_center_to_corner always 4?\nDiscussions214.\n14.4AnchorBoxes\nObject detection algorithms usually sample a large number of regions in the input image,\ndetermine whether these regions contain objects of interest, and adjust the boundaries of\ntheregionssoastopredictthe ground-truthboundingboxes oftheobjectsmoreaccurately.\nDifferent models may adopt different region sampling schemes. Here we introduce one of\nsuch methods: it generates multiple bounding boxes with varying scales and aspect ratios\ncentered on each pixel. These bounding boxes are called anchor boxes . We will design an\nobject detection model based on anchor boxes in Section 14.7 .\nFirst, let\u2019s modify the printing accuracy just for more concise outputs.\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\ntorch .set_printoptions( 2)# Simplify printing accuracy", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0afe720a-1989-418c-8728-c59f2604d4df": {"__data__": {"id_": "0afe720a-1989-418c-8728-c59f2604d4df", "embedding": null, "metadata": {"page_label": "610", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2bb13cc-55a1-482b-a220-e4038c6857cf", "node_type": "4", "metadata": {"page_label": "610", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "31c9af26c6dc2c9a335773b31563076d6335456ba5b4b17fb6544104be1b4898", "class_name": "RelatedNodeInfo"}}, "text": "610 Computer Vision\n14.4.1Generating Multiple AnchorBoxes\nSuppose that the input image has a height of \u210eand width of \ud835\udc64. We generate anchor boxes\nwithdifferentshapescenteredoneachpixeloftheimage. Letthe scalebe\ud835\udc602\u00b90,1\u00bcandthe\naspectratio (ratioofwidthtoheight)is \ud835\udc5f >0. Thenthewidthandheightoftheanchorbox\nare\ud835\udc64\ud835\udc60p\ud835\udc5fand\u210e\ud835\udc60\u009dp\ud835\udc5f, respectively. Note that when the center position is given, an anchor\nbox with known width and height is determined.\nTogeneratemultipleanchorboxeswithdifferentshapes,let\u2019ssetaseriesofscales \ud835\udc601,...,\ud835\udc60\ud835\udc5b\nand a series of aspect ratios \ud835\udc5f1,...,\ud835\udc5f\ud835\udc5a. When using all the combinations of these scales\nand aspect ratios with each pixel as the center, the input image will have a total of \ud835\udc64\u210e\ud835\udc5b\ud835\udc5a\nanchorboxes. Althoughtheseanchorboxesmaycoveralltheground-truthboundingboxes,\nthe computational complexity is easily too high. In practice, we can only consider those\ncombinations containing \ud835\udc601or\ud835\udc5f1:\n\u00b9\ud835\udc601,\ud835\udc5f1\u00ba,\u00b9\ud835\udc601,\ud835\udc5f2\u00ba,...,\u00b9\ud835\udc601,\ud835\udc5f\ud835\udc5a\u00ba,\u00b9\ud835\udc602,\ud835\udc5f1\u00ba,\u00b9\ud835\udc603,\ud835\udc5f1\u00ba,...,\u00b9\ud835\udc60\ud835\udc5b,\ud835\udc5f1\u00ba. (14.4.1)\nThat is to say, the number of anchor boxes centered on the same pixel is \ud835\udc5b\u00b8\ud835\udc5a\u00001. For the\nentire input image, we will generate a total of \ud835\udc64\u210e\u00b9\ud835\udc5b\u00b8\ud835\udc5a\u00001\u00baanchor boxes.\nThe above method of generating anchor boxes is implemented in the following multi-\nbox_prior function. Wespecifythe inputimage, alistofscales, and alistofaspect ratios,\nthen this function will return all the anchor boxes.\n#@save\ndef multibox_prior (data, sizes, ratios):\n\"\"\"Generate anchor boxes with different shapes centered on each pixel.\"\"\"\nin_height, in_width =data .shape[ -2:]\ndevice, num_sizes, num_ratios =data .device, len(sizes), len(ratios)\nboxes_per_pixel =(num_sizes +num_ratios -1)\nsize_tensor =torch .tensor(sizes, device =device)\nratio_tensor =torch .tensor(ratios, device =device)\n# Offsets are required to move the anchor to the center of a pixel. Since\n# a pixel has height=1 and width=1, we choose to offset our centers by 0.5\noffset_h, offset_w =0.5,0.5\nsteps_h =1.0 /in_height # Scaled steps in y axis\nsteps_w =1.0 /in_width # Scaled steps in x axis\n# Generate all center points for the anchor boxes\ncenter_h =(torch .arange(in_height, device =device) +offset_h) *steps_h\ncenter_w =(torch .arange(in_width, device =device) +offset_w) *steps_w\nshift_y, shift_x =torch .meshgrid(center_h, center_w, indexing ='ij')\nshift_y, shift_x =shift_y .reshape( -1), shift_x .reshape( -1)\n# Generate `boxes_per_pixel` number of heights and widths that are later\n# used to create anchor box corner coordinates (xmin, xmax, ymin, ymax)\nw=torch .cat((size_tensor *torch .sqrt(ratio_tensor[ 0]),\nsizes[ 0]*torch .sqrt(ratio_tensor[ 1:])))\\\n*in_height /in_width # Handle rectangular inputs\nh=torch .cat((size_tensor /torch .sqrt(ratio_tensor[ 0]),\nsizes[ 0]/torch .sqrt(ratio_tensor[ 1:])))\n# Divide by 2 to get half height and half width\nanchor_manipulations =torch .stack(( -w,-h, w, h)) .T.repeat(\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11e4fd97-41c4-44a1-a78a-3a6324b6e080": {"__data__": {"id_": "11e4fd97-41c4-44a1-a78a-3a6324b6e080", "embedding": null, "metadata": {"page_label": "611", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a4aea1f-f7ab-428a-8644-31751fda865e", "node_type": "4", "metadata": {"page_label": "611", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d58c7aef7bb7906482aed3b4475710e6115c70185ca48be69f91fcd6c84dac44", "class_name": "RelatedNodeInfo"}}, "text": "611 Anchor Boxes\n(continued from previous page)\nin_height *in_width, 1)/2\n# Each center point will have `boxes_per_pixel` number of anchor boxes, so\n# generate a grid of all anchor box centers with `boxes_per_pixel` repeats\nout_grid =torch .stack([shift_x, shift_y, shift_x, shift_y],\ndim=1).repeat_interleave(boxes_per_pixel, dim =0)\noutput =out_grid +anchor_manipulations\nreturn output .unsqueeze( 0)\nWe can see that the shape of the returned anchor box variable Yis (batch size, number of\nanchor boxes, 4).\nimg =d2l.plt.imread( '../img/catdog.jpg ')\nh, w =img.shape[: 2]\nprint (h, w)\nX=torch .rand(size =(1,3, h, w)) # Construct input data\nY=multibox_prior(X, sizes =[0.75 ,0.5,0.25 ], ratios =[1,2,0.5])\nY.shape\n561 728\ntorch .Size([ 1,2042040 ,4])\nAfter changing the shape of the anchor box variable Yto (image height, image width, num-\nber of anchor boxes centered on the same pixel, 4), we can obtain all the anchor boxes\ncentered on a specified pixel position. In the following, we access the first anchor box cen-\nteredon(250,250). Ithasfourelements: the \u00b9\ud835\udc65,\ud835\udc66\u00ba-axiscoordinatesattheupper-leftcorner\nand the\u00b9\ud835\udc65,\ud835\udc66\u00ba-axis coordinates at the lower-right corner of the anchor box. The coordinate\nvalues of both axes are divided by the width and height of the image, respectively.\nboxes =Y.reshape(h, w, 5,4)\nboxes[ 250,250,0, :]\ntensor([ 0.06 ,0.07 ,0.63 ,0.82 ])\nIn order to show all the anchor boxes centered on one pixel in the image, we define the\nfollowing show_bboxes function to draw multiple bounding boxes on the image.\n#@save\ndef show_bboxes (axes, bboxes, labels =None , colors =None ):\n\"\"\"Show bounding boxes.\"\"\"\ndef make_list (obj, default_values =None ):\nifobj isNone :\nobj =default_values\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8359c892-d0fa-4c7a-82a6-8c7a1542897b": {"__data__": {"id_": "8359c892-d0fa-4c7a-82a6-8c7a1542897b", "embedding": null, "metadata": {"page_label": "612", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "262c00e1-9ce0-455b-a193-6dbe6b572095", "node_type": "4", "metadata": {"page_label": "612", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c95ed356326711567e38db8b61ea1c101fe4ce2f10a3772f9b46c4a40703d62c", "class_name": "RelatedNodeInfo"}}, "text": "612 Computer Vision\n(continued from previous page)\nelif not isinstance (obj, ( list ,tuple )):\nobj =[obj]\nreturn obj\nlabels =make_list(labels)\ncolors =make_list(colors, [ 'b','g','r','m','c'])\nfor i, bbox inenumerate (bboxes):\ncolor =colors[i %len(colors)]\nrect =d2l.bbox_to_rect(bbox .detach() .numpy(), color)\naxes .add_patch(rect)\niflabels and len(labels) >i:\ntext_color ='k'ifcolor =='w'else 'w'\naxes .text(rect .xy[0], rect .xy[1], labels[i],\nva='center ', ha ='center ', fontsize =9, color =text_color,\nbbox =dict (facecolor =color, lw =0))\nAs we just saw, the coordinate values of the \ud835\udc65and\ud835\udc66axes in the variable boxeshave been\ndivided by the width and height of the image, respectively. When drawing anchor boxes,\nwe need to restore their original coordinate values; thus, we define variable bbox_scale\nbelow. Now, we can draw all the anchor boxes centered on (250, 250) in the image. As you\ncan see, the blue anchor box with a scale of 0.75 and an aspect ratio of 1 well surrounds\nthe dog in the image.\nd2l.set_figsize()\nbbox_scale =torch .tensor((w, h, w, h))\nfig =d2l.plt.imshow(img)\nshow_bboxes(fig .axes, boxes[ 250,250, :, :] *bbox_scale,\n['s=0.75, r=1 ','s=0.5, r=1 ','s=0.25, r=1 ','s=0.75, r=2 ',\n's=0.75, r=0.5 '])\n14.4.2IntersectionoverUnion(IoU)\nWejustmentionedthatananchorbox\u201cwell\u201dsurroundsthedogintheimage. Iftheground-\ntruth bounding box of the object is known, how can \u201cwell\u201d here be quantified? Intuitively,\nwe can measure the similarity between the anchor box and the ground-truth bounding box.\nWe know that the Jaccard index can measure the similarity between two sets. Given sets\nAandB, their Jaccard index is the size of their intersection divided by the size of their", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9ccd6805-88ab-4279-b96c-86f020514ae4": {"__data__": {"id_": "9ccd6805-88ab-4279-b96c-86f020514ae4", "embedding": null, "metadata": {"page_label": "613", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "359bd711-5c5d-4d8d-8daa-b337116a5316", "node_type": "4", "metadata": {"page_label": "613", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "76029cab683774c95148716caaa69d35171a0f003b2d03b31a378e3f5ac6e3d9", "class_name": "RelatedNodeInfo"}}, "text": "613 Anchor Boxes\nunion:\n\ud835\udc3d\u00b9A,B\u00ba=jA\\Bj\njA[Bj. (14.4.2)\nIn fact, we can consider the pixel area of any bounding box as a set of pixels. In this way,\nwecanmeasurethesimilarityofthetwoboundingboxesbytheJaccardindexoftheirpixel\nsets. For two bounding boxes, we usually refer their Jaccard index as intersection over\nunion(IoU), which is the ratio of their intersection area to their union area, as shown in\nFig. 14.4.1 . The range of an IoU is between 0 and 1: 0 means that two bounding boxes do\nnot overlap at all, while 1 indicates that the two bounding boxes are equal.\ntFig. 14.4.1 IoU is the ratio of the intersection area to the union area of two bounding boxes.\nFortheremainderofthissection,wewilluseIoUtomeasurethesimilaritybetweenanchor\nboxes and ground-truth bounding boxes, and between different anchor boxes. Given two\nlists of anchor or bounding boxes, the following box_iou computes their pairwise IoU\nacross these two lists.\n#@save\ndef box_iou (boxes1, boxes2):\n\"\"\"Compute pairwise IoU across two lists of anchor or bounding boxes.\"\"\"\nbox_area =lambda boxes: ((boxes[:, 2]-boxes[:, 0])*\n(boxes[:, 3]-boxes[:, 1]))\n# Shape of `boxes1`, `boxes2`, `areas1`, `areas2`: (no. of boxes1, 4),\n# (no. of boxes2, 4), (no. of boxes1,), (no. of boxes2,)\nareas1 =box_area(boxes1)\nareas2 =box_area(boxes2)\n# Shape of `inter_upperlefts`, `inter_lowerrights`, `inters`: (no. of\n# boxes1, no. of boxes2, 2)\ninter_upperlefts =torch .max(boxes1[:, None , :2], boxes2[:, : 2])\ninter_lowerrights =torch .min(boxes1[:, None ,2:], boxes2[:, 2:])\ninters =(inter_lowerrights -inter_upperlefts) .clamp( min=0)\n# Shape of `inter_areas` and `union_areas`: (no. of boxes1, no. of boxes2)\ninter_areas =inters[:, :, 0]*inters[:, :, 1]\nunion_areas =areas1[:, None ]+areas2 -inter_areas\nreturn inter_areas /union_areas\n14.4.3Labeling AnchorBoxesin TrainingData\nIn a training dataset, we consider each anchor box as a training example. In order to train\nan object detection model, we need classandoffsetlabels for each anchor box, where the\nformer is the class of the object relevant to the anchor box and the latter is the offset of the\nground-truth bounding box relative to the anchor box. During the prediction, for each im-\nage we generate multiple anchor boxes, predict classes and offsets for all the anchor boxes,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b2927f5-45ce-4a31-ae8f-2f9c3587fd93": {"__data__": {"id_": "2b2927f5-45ce-4a31-ae8f-2f9c3587fd93", "embedding": null, "metadata": {"page_label": "614", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d41e905d-f1c5-45a5-8149-92963e0f5e0b", "node_type": "4", "metadata": {"page_label": "614", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "728826b4ea3e301c092dd0241b7955ce07b950a8fc5827161e6d14f65cba34a5", "class_name": "RelatedNodeInfo"}}, "text": "614 Computer Vision\nadjust their positions according to the predicted offsets to obtain the predicted bounding\nboxes, and finally only output those predicted bounding boxes that satisfy certain crite-\nria.\nAsweknow,anobjectdetectiontrainingsetcomeswithlabelsforlocationsof ground-truth\nboundingboxes andclassesoftheirsurroundedobjects. Tolabelanygenerated anchorbox ,\nwe refer to the labeled location and class of its assigned ground-truth bounding box that is\nclosest to the anchor box. In the following, we describe an algorithm for assigning closest\nground-truth bounding boxes to anchor boxes.\nAssigningGround-TruthBounding Boxesto AnchorBoxes\nGiven an image, suppose that the anchor boxes are \ud835\udc341,\ud835\udc342,...,\ud835\udc34\ud835\udc5b\ud835\udc4eand the ground-truth\nbounding boxes are \ud835\udc351,\ud835\udc352,...,\ud835\udc35\ud835\udc5b\ud835\udc4f, where\ud835\udc5b\ud835\udc4e\u0015\ud835\udc5b\ud835\udc4f. Let\u2019s define a matrix X2R\ud835\udc5b\ud835\udc4e\u0002\ud835\udc5b\ud835\udc4f,\nwhose element \ud835\udc65\ud835\udc56\ud835\udc57in the\ud835\udc56throw and\ud835\udc57thcolumn is the IoU of the anchor box \ud835\udc34\ud835\udc56and the\nground-truth bounding box \ud835\udc35\ud835\udc57. The algorithm consists of the following steps:\n1.Find the largest element in matrix Xand denote its row and column indices as \ud835\udc561and\n\ud835\udc571, respectively. Then the ground-truth bounding box \ud835\udc35\ud835\udc571is assigned to the anchor box\n\ud835\udc34\ud835\udc561. This is quite intuitive because \ud835\udc34\ud835\udc561and\ud835\udc35\ud835\udc571are the closest among all the pairs of\nanchor boxes and ground-truth bounding boxes. After the first assignment, discard all\nthe elements in the \ud835\udc561throw and the \ud835\udc571thcolumn in matrix X.\n2.Find the largest of the remaining elements in matrix Xand denote its row and column\nindices as\ud835\udc562and\ud835\udc572, respectively. We assign ground-truth bounding box \ud835\udc35\ud835\udc572to anchor\nbox\ud835\udc34\ud835\udc562and discard all the elements in the \ud835\udc562throw and the \ud835\udc572thcolumn in matrix X.\n3.At this point, elements in two rows and two columns in matrix Xhave been discarded.\nWe proceed until all elements in \ud835\udc5b\ud835\udc4fcolumns in matrix Xare discarded. At this time,\nwe have assigned a ground-truth bounding box to each of \ud835\udc5b\ud835\udc4fanchor boxes.\n4.Only traverse through the remaining \ud835\udc5b\ud835\udc4e\u0000\ud835\udc5b\ud835\udc4fanchor boxes. For example, given any\nanchor box\ud835\udc34\ud835\udc56, find the ground-truth bounding box \ud835\udc35\ud835\udc57with the largest IoU with \ud835\udc34\ud835\udc56\nthroughout the \ud835\udc56throw of matrix X, and assign \ud835\udc35\ud835\udc57to\ud835\udc34\ud835\udc56only if this IoU is greater than\na predefined threshold.\nLet\u2019sillustratetheabovealgorithmusingaconcreteexample. Asshownin Fig.14.4.2 (left),\nassumingthatthemaximumvalueinmatrix Xis\ud835\udc6523, weassigntheground-truthbounding\nbox\ud835\udc353totheanchorbox \ud835\udc342. Then,wediscardalltheelementsinrow2andcolumn3ofthe\nmatrix, findthelargest \ud835\udc6571intheremainingelements(shadedarea), andassigntheground-\ntruthboundingbox \ud835\udc351totheanchorbox \ud835\udc347. Next,asshownin Fig.14.4.2 (middle),discard\nall the elements in row 7 and column 1 of the matrix, find the largest \ud835\udc6554in the remaining\nelements (shaded area), and assign the ground-truth bounding box \ud835\udc354to the anchor box\n\ud835\udc345. Finally, as shown in Fig. 14.4.2 (right), discard all the elements in row 5 and column 4\nof the matrix, find the largest \ud835\udc6592in the remaining elements (shaded area), and assign the\nground-truth bounding box \ud835\udc352to the anchor box \ud835\udc349. After that, we only need to traverse\nthrough the remaining anchor boxes \ud835\udc341,\ud835\udc343,\ud835\udc344,\ud835\udc346,\ud835\udc348and determine whether to assign\nthem ground-truth bounding boxes according to the threshold.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aee3e991-6615-4a6d-9f51-6d3e09bd144d": {"__data__": {"id_": "aee3e991-6615-4a6d-9f51-6d3e09bd144d", "embedding": null, "metadata": {"page_label": "615", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edf96e66-a450-4cfe-8470-b59f90525f58", "node_type": "4", "metadata": {"page_label": "615", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "79110be5aa410fda2ba248412101ed692e7fa046051235d929423f21efe11c15", "class_name": "RelatedNodeInfo"}}, "text": "615 Anchor Boxes\ntFig. 14.4.2 Assigning ground-truth bounding boxes to anchor boxes.\nThisalgorithmisimplementedinthefollowing assign_anchor_to_bbox function.\n#@save\ndef assign_anchor_to_bbox (ground_truth, anchors, device, iou_threshold =0.5):\n\"\"\"Assign closest ground-truth bounding boxes to anchor boxes.\"\"\"\nnum_anchors, num_gt_boxes =anchors .shape[ 0], ground_truth .shape[ 0]\n# Element x_ij in the i-th row and j-th column is the IoU of the anchor\n# box i and the ground-truth bounding box j\njaccard =box_iou(anchors, ground_truth)\n# Initialize the tensor to hold the assigned ground-truth bounding box for\n# each anchor\nanchors_bbox_map =torch .full((num_anchors,), -1, dtype =torch .long,\ndevice =device)\n# Assign ground-truth bounding boxes according to the threshold\nmax_ious, indices =torch .max(jaccard, dim =1)\nanc_i =torch .nonzero(max_ious >=iou_threshold) .reshape( -1)\nbox_j =indices[max_ious >=iou_threshold]\nanchors_bbox_map[anc_i] =box_j\ncol_discard =torch .full((num_anchors,), -1)\nrow_discard =torch .full((num_gt_boxes,), -1)\nfor _inrange (num_gt_boxes):\nmax_idx =torch .argmax(jaccard) # Find the largest IoU\nbox_idx =(max_idx %num_gt_boxes) .long()\nanc_idx =(max_idx /num_gt_boxes) .long()\nanchors_bbox_map[anc_idx] =box_idx\njaccard[:, box_idx] =col_discard\njaccard[anc_idx, :] =row_discard\nreturn anchors_bbox_map\nLabeling Classes and Offsets\nNow we can label the class and offset for each anchor box. Suppose that an anchor box\n\ud835\udc34is assigned a ground-truth bounding box \ud835\udc35. On the one hand, the class of the anchor\nbox\ud835\udc34will be labeled as that of \ud835\udc35. On the other hand, the offset of the anchor box \ud835\udc34will\nbe labeled according to the relative position between the central coordinates of \ud835\udc35and\ud835\udc34\ntogether with the relative size between these two boxes. Given varying positions and sizes", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1804, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fee0525-b7d6-4464-a8f6-7a6c36a87773": {"__data__": {"id_": "9fee0525-b7d6-4464-a8f6-7a6c36a87773", "embedding": null, "metadata": {"page_label": "616", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b4a7024-577d-48f9-b074-3a56bfe3f453", "node_type": "4", "metadata": {"page_label": "616", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "90aedc90c8ed17a9961508b42820a62aa9d6c84094b5f98d020d3bce096445c9", "class_name": "RelatedNodeInfo"}}, "text": "616 Computer Vision\nof different boxes in the dataset, we can apply transformations to those relative positions\nand sizes that may lead to more uniformly distributed offsets that are easier to fit. Here we\ndescribe a common transformation. Given the central coordinates of \ud835\udc34and\ud835\udc35as\u00b9\ud835\udc65\ud835\udc4e,\ud835\udc66\ud835\udc4e\u00ba\nand\u00b9\ud835\udc65\ud835\udc4f,\ud835\udc66\ud835\udc4f\u00ba, their widths as \ud835\udc64\ud835\udc4eand\ud835\udc64\ud835\udc4f, and their heights as \u210e\ud835\udc4eand\u210e\ud835\udc4f, respectively. We\nmay label the offset of \ud835\udc34as\n \ud835\udc65\ud835\udc4f\u0000\ud835\udc65\ud835\udc4e\n\ud835\udc64\ud835\udc4e\u0000\ud835\udf07\ud835\udc65\n\ud835\udf0e\ud835\udc65,\ud835\udc66\ud835\udc4f\u0000\ud835\udc66\ud835\udc4e\n\u210e\ud835\udc4e\u0000\ud835\udf07\ud835\udc66\n\ud835\udf0e\ud835\udc66,log\ud835\udc64\ud835\udc4f\n\ud835\udc64\ud835\udc4e\u0000\ud835\udf07\ud835\udc64\n\ud835\udf0e\ud835\udc64,log\u210e\ud835\udc4f\n\u210e\ud835\udc4e\u0000\ud835\udf07\u210e\n\ud835\udf0e\u210e!\n, (14.4.3)\nwhere default values of the constants are \ud835\udf07\ud835\udc65=\ud835\udf07\ud835\udc66=\ud835\udf07\ud835\udc64=\ud835\udf07\u210e=0,\ud835\udf0e\ud835\udc65=\ud835\udf0e\ud835\udc66=0.1,\nand\ud835\udf0e\ud835\udc64=\ud835\udf0e\u210e=0.2. This transformation is implemented below in the offset_boxes\nfunction.\n#@save\ndef offset_boxes (anchors, assigned_bb, eps =1e-6 ):\n\"\"\"Transform for anchor box offsets.\"\"\"\nc_anc =d2l.box_corner_to_center(anchors)\nc_assigned_bb =d2l.box_corner_to_center(assigned_bb)\noffset_xy =10*(c_assigned_bb[:, : 2]-c_anc[:, : 2])/c_anc[:, 2:]\noffset_wh =5*torch .log(eps +c_assigned_bb[:, 2:]/c_anc[:, 2:])\noffset =torch .cat([offset_xy, offset_wh], axis =1)\nreturn offset\nIf an anchor box is not assigned a ground-truth bounding box, we just label the class of\nthe anchor box as \u201cbackground\u201d. Anchor boxes whose classes are background are often\nreferred to as negative anchor boxes, and the rest are called positive anchor boxes. We\nimplementthefollowing multibox_target functiontolabelclassesandoffsetsforanchor\nboxes (the anchors argument) using ground-truth bounding boxes (the labelsargument).\nThis function sets the background class to zero and increments the integer index of a new\nclass by one.\n#@save\ndef multibox_target (anchors, labels):\n\"\"\"Label anchor boxes using ground-truth bounding boxes.\"\"\"\nbatch_size, anchors =labels .shape[ 0], anchors .squeeze( 0)\nbatch_offset, batch_mask, batch_class_labels =[], [], []\ndevice, num_anchors =anchors .device, anchors .shape[ 0]\nfor iinrange (batch_size):\nlabel =labels[i, :, :]\nanchors_bbox_map =assign_anchor_to_bbox(\nlabel[:, 1:], anchors, device)\nbbox_mask =((anchors_bbox_map >=0).float() .unsqueeze( -1)).repeat(\n1,4)\n# Initialize class labels and assigned bounding box coordinates with\n# zeros\nclass_labels =torch .zeros(num_anchors, dtype =torch .long,\ndevice =device)\nassigned_bb =torch .zeros((num_anchors, 4), dtype =torch .float32,\ndevice =device)\n# Label classes of anchor boxes using their assigned ground-truth\n# bounding boxes. If an anchor box is not assigned any, we label its\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36853a7d-d51e-4425-96b2-04f3424f080a": {"__data__": {"id_": "36853a7d-d51e-4425-96b2-04f3424f080a", "embedding": null, "metadata": {"page_label": "617", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fb4b9232-e0fd-4ca7-bf8c-3b5550024f88", "node_type": "4", "metadata": {"page_label": "617", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d663ad90706fecd983f50af73feac5d9704b80c8d6b752e34bdb73979d476a55", "class_name": "RelatedNodeInfo"}}, "text": "617 Anchor Boxes\n(continued from previous page)\n# class as background (the value remains zero)\nindices_true =torch .nonzero(anchors_bbox_map >=0)\nbb_idx =anchors_bbox_map[indices_true]\nclass_labels[indices_true] =label[bb_idx, 0].long() +1\nassigned_bb[indices_true] =label[bb_idx, 1:]\n# Offset transformation\noffset =offset_boxes(anchors, assigned_bb) *bbox_mask\nbatch_offset .append(offset .reshape( -1))\nbatch_mask .append(bbox_mask .reshape( -1))\nbatch_class_labels .append(class_labels)\nbbox_offset =torch .stack(batch_offset)\nbbox_mask =torch .stack(batch_mask)\nclass_labels =torch .stack(batch_class_labels)\nreturn (bbox_offset, bbox_mask, class_labels)\nAnExample\nLet\u2019sillustrateanchorboxlabelingviaaconcreteexample. Wedefineground-truthbound-\ning boxes for the dog and cat in the loaded image, where the first element is the class (0\nfor dog and 1 for cat) and the remaining four elements are the \u00b9\ud835\udc65,\ud835\udc66\u00ba-axis coordinates at\nthe upper-left corner and the lower-right corner (range is between 0 and 1). We also con-\nstruct five anchor boxes to be labeled using the coordinates of the upper-left corner and the\nlower-right corner: \ud835\udc340,...,\ud835\udc34 4(the index starts from 0). Then we plot these ground-truth\nbounding boxes and anchor boxes in the image.\nground_truth =torch .tensor([[ 0,0.1,0.08 ,0.52 ,0.92 ],\n[1,0.55 ,0.2,0.9,0.88 ]])\nanchors =torch .tensor([[ 0,0.1,0.2,0.3], [ 0.15 ,0.2,0.4,0.4],\n[0.63 ,0.05 ,0.88 ,0.98 ], [ 0.66 ,0.45 ,0.8,0.8],\n[0.57 ,0.3,0.92 ,0.9]])\nfig =d2l.plt.imshow(img)\nshow_bboxes(fig .axes, ground_truth[:, 1:]*bbox_scale, [ 'dog','cat'],'k')\nshow_bboxes(fig .axes, anchors *bbox_scale, [ '0','1','2','3','4']);\nUsing the multibox_target function defined above, we can label classes and offsets of\nthese anchor boxes based on the ground-truth bounding boxes for the dog and cat. In this\nexample, indices of the background, dog, and cat classes are 0, 1, and 2, respectively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "857de485-41cc-4094-9320-20939a5cbad7": {"__data__": {"id_": "857de485-41cc-4094-9320-20939a5cbad7", "embedding": null, "metadata": {"page_label": "618", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07556098-21e0-4d69-9ee8-34a69a059a42", "node_type": "4", "metadata": {"page_label": "618", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a270fef6cd748d053b2e83acd42a24272bd6a8fa21d275feb1f49fe981915e0d", "class_name": "RelatedNodeInfo"}}, "text": "618 Computer Vision\nBelow we add an dimension for examples of anchor boxes and ground-truth bounding\nboxes.\nlabels =multibox_target(anchors .unsqueeze(dim =0),\nground_truth .unsqueeze(dim =0))\nThere are three items in the returned result, all of which are in the tensor format. The third\nitem contains the labeled classes of the input anchor boxes.\nLet\u2019sanalyzethereturnedclasslabelsbelowbasedonanchorboxandground-truthbound-\ning box positions in the image. First, among all the pairs of anchor boxes and ground-truth\nbounding boxes, the IoU of the anchor box \ud835\udc344and the ground-truth bounding box of the\ncat is the largest. Thus, the class of \ud835\udc344is labeled as the cat. Taking out pairs containing\n\ud835\udc344or the ground-truth bounding box of the cat, among the rest the pair of the anchor box\n\ud835\udc341and the ground-truth bounding box of the dog has the largest IoU. So the class of \ud835\udc341is\nlabeledasthedog. Next, weneedtotraversethroughtheremainingthreeunlabeledanchor\nboxes:\ud835\udc340,\ud835\udc342, and\ud835\udc343. For\ud835\udc340, the class of the ground-truth bounding box with the largest\nIoU is the dog, but the IoU is below the predefined threshold (0.5), so the class is labeled\nas background; for \ud835\udc342, the class of the ground-truth bounding box with the largest IoU is\nthe cat and the IoU exceedsthe threshold, so the class is labeled as the cat; for \ud835\udc343, the class\nof the ground-truth bounding box with the largest IoU is the cat, but the value is below the\nthreshold, so the class is labeled as background.\nlabels[ 2]\ntensor([[ 0,1,2,0,2]])\nThesecondreturneditemisamaskvariableoftheshape(batchsize,fourtimesthenumber\nof anchor boxes). Every four elements in the mask variable correspond to the four offset\nvaluesofeachanchorbox. Sincewedonotcareaboutbackgrounddetection,offsetsofthis\nnegative class should not affect the objective function. Through elementwise multiplica-\ntions, zeros in the mask variable will filter out negative class offsets before calculating the\nobjective function.\nlabels[ 1]\ntensor([[ 0.,0.,0.,0.,1.,1.,1.,1.,1.,1.,1.,1.,0.,0.,0.,0.,1.,1.\n\u21a9!,\n1.,1.]])\nThe first returned item contains the four offset values labeled for each anchor box. Note\nthat the offsets of negative-class anchor boxes are labeled as zeros.\nlabels[ 0]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2201, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "804b1b48-6006-4201-9caa-678c69b031d4": {"__data__": {"id_": "804b1b48-6006-4201-9caa-678c69b031d4", "embedding": null, "metadata": {"page_label": "619", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1aa3c3c3-b099-4721-9ede-af0e77295c21", "node_type": "4", "metadata": {"page_label": "619", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "24f91d75fb58824154ee9b6835599dafb58a60bf74dcf6662030f47caa984310", "class_name": "RelatedNodeInfo"}}, "text": "619 Anchor Boxes\ntensor([[ -0.00e+00 ,-0.00e+00 ,-0.00e+00 ,-0.00e+00 ,1.40e+00 ,1.00e+01 ,\n2.59e+00 ,7.18e+00 ,-1.20e+00 ,2.69e-01 ,1.68e+00 ,-1.57e+00 ,\n-0.00e+00 ,-0.00e+00 ,-0.00e+00 ,-0.00e+00 ,-5.71e-01 ,-1.00e+00 ,\n4.17e-06 ,6.26e-01 ]])\n14.4.4Predicting Bounding Boxeswith Non-MaximumSuppression\nDuringprediction,wegeneratemultipleanchorboxesfortheimageandpredictclassesand\noffsets for each of them. A predictedboundingbox is thus obtained according to an anchor\nboxwithitspredictedoffset. Belowweimplementthe offset_inverse functionthattakes\nin anchors and offset predictions as inputs and applies inverse offset transformations to\nreturn the predicted bounding box coordinates.\n#@save\ndef offset_inverse (anchors, offset_preds):\n\"\"\"Predict bounding boxes based on anchor boxes with predicted offsets.\"\"\"\nanc =d2l.box_corner_to_center(anchors)\npred_bbox_xy =(offset_preds[:, : 2]*anc[:, 2:]/10)+anc[:, : 2]\npred_bbox_wh =torch .exp(offset_preds[:, 2:]/5)*anc[:, 2:]\npred_bbox =torch .cat((pred_bbox_xy, pred_bbox_wh), axis =1)\npredicted_bbox =d2l.box_center_to_corner(pred_bbox)\nreturn predicted_bbox\nWhentherearemanyanchorboxes,manysimilar(withsignificantoverlap)predictedbound-\ningboxescanbepotentiallyoutputforsurroundingthesameobject. Tosimplifytheoutput,\nwe can merge similar predicted bounding boxes that belong to the same object by using\nnon-maximumsuppression (NMS).\nHereishownon-maximumsuppressionworks. Forapredictedboundingbox \ud835\udc35, theobject\ndetectionmodelcalculatesthepredictedlikelihoodforeachclass. Denotingby \ud835\udc5dthelargest\npredictedlikelihood,theclasscorrespondingtothisprobabilityisthepredictedclassfor \ud835\udc35.\nSpecifically,wereferto \ud835\udc5dastheconfidence (score)ofthepredictedboundingbox \ud835\udc35. Onthe\nsame image, all the predicted non-background bounding boxes are sorted by confidence in\ndescendingordertogeneratealist \ud835\udc3f. Thenwemanipulatethesortedlist \ud835\udc3finthefollowing\nsteps:\n1.Select the predicted bounding box \ud835\udc351with the highest confidence from \ud835\udc3fas a basis and\nremoveallnon-basispredictedboundingboxeswhoseIoUwith \ud835\udc351exceedsapredefined\nthreshold\ud835\udf16from\ud835\udc3f. At this point, \ud835\udc3fkeeps the predicted bounding box with the highest\nconfidence but drops others that are too similar to it. In a nutshell, those with non-\nmaximum confidence scores are suppressed .\n2.Select the predicted bounding box \ud835\udc352with the second highest confidence from \ud835\udc3fas\nanother basis and remove all non-basis predicted bounding boxes whose IoU with \ud835\udc352\nexceeds\ud835\udf16from\ud835\udc3f.\n3.Repeat the above process until all the predicted bounding boxes in \ud835\udc3fhave been used as", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87275339-0bb9-48b1-ac2a-97cfb52a21bf": {"__data__": {"id_": "87275339-0bb9-48b1-ac2a-97cfb52a21bf", "embedding": null, "metadata": {"page_label": "620", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "933df950-0564-4583-98e1-16f9361c55f3", "node_type": "4", "metadata": {"page_label": "620", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "19d431574614c70c66d4fbba1221674316e921ca2dbab5b8d4c175c6e3526578", "class_name": "RelatedNodeInfo"}}, "text": "620 Computer Vision\na basis. At this time, the IoU of any pair of predicted bounding boxes in \ud835\udc3fis below the\nthreshold\ud835\udf16; thus, no pair is too similar with each other.\n4.Output all the predicted bounding boxes in the list \ud835\udc3f.\nThe following nmsfunction sorts confidence scores in descending order and returns their\nindices.\n#@save\ndef nms(boxes, scores, iou_threshold):\n\"\"\"Sort confidence scores of predicted bounding boxes.\"\"\"\nB=torch .argsort(scores, dim =-1, descending =True )\nkeep =[] # Indices of predicted bounding boxes that will be kept\nwhile B.numel() >0:\ni=B[0]\nkeep .append(i)\nifB.numel() ==1:break\niou =box_iou(boxes[i, :] .reshape( -1,4),\nboxes[B[ 1:], :] .reshape( -1,4)).reshape( -1)\ninds =torch .nonzero(iou <=iou_threshold) .reshape( -1)\nB=B[inds +1]\nreturn torch .tensor(keep, device =boxes .device)\nWedefinethefollowing multibox_detection toapplynon-maximumsuppressiontopre-\ndicting bounding boxes. Do not worry if you find the implementation a bit complicated:\nwe will show how it works with a concrete example right after the implementation.\n#@save\ndef multibox_detection (cls_probs, offset_preds, anchors, nms_threshold =0.5,\npos_threshold =0.009999999 ):\n\"\"\"Predict bounding boxes using non-maximum suppression.\"\"\"\ndevice, batch_size =cls_probs .device, cls_probs .shape[ 0]\nanchors =anchors .squeeze( 0)\nnum_classes, num_anchors =cls_probs .shape[ 1], cls_probs .shape[ 2]\nout =[]\nfor iinrange (batch_size):\ncls_prob, offset_pred =cls_probs[i], offset_preds[i] .reshape( -1,4)\nconf, class_id =torch .max(cls_prob[ 1:], 0)\npredicted_bb =offset_inverse(anchors, offset_pred)\nkeep =nms(predicted_bb, conf, nms_threshold)\n# Find all non-`keep` indices and set the class to background\nall_idx =torch .arange(num_anchors, dtype =torch .long, device =device)\ncombined =torch .cat((keep, all_idx))\nuniques, counts =combined .unique(return_counts =True )\nnon_keep =uniques[counts ==1]\nall_id_sorted =torch .cat((keep, non_keep))\nclass_id[non_keep] =-1\nclass_id =class_id[all_id_sorted]\nconf, predicted_bb =conf[all_id_sorted], predicted_bb[all_id_sorted]\n# Here `pos_threshold` is a threshold for positive (non-background)\n# predictions\nbelow_min_idx =(conf <pos_threshold)\nclass_id[below_min_idx] =-1\nconf[below_min_idx] =1-conf[below_min_idx]\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f3c18ba-5c68-45cc-8145-12b68dc2c4a7": {"__data__": {"id_": "9f3c18ba-5c68-45cc-8145-12b68dc2c4a7", "embedding": null, "metadata": {"page_label": "621", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be88b7d8-26c4-4c50-aca0-baae8e319692", "node_type": "4", "metadata": {"page_label": "621", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f5d3bacfd5985fe5d14b129b6ba375b93d8ef83b2a1399052948bbffe52cbc2a", "class_name": "RelatedNodeInfo"}}, "text": "621 Anchor Boxes\n(continued from previous page)\npred_info =torch .cat((class_id .unsqueeze( 1),\nconf .unsqueeze( 1),\npredicted_bb), dim =1)\nout.append(pred_info)\nreturn torch .stack(out)\nNow let\u2019s apply the above implementations to a concrete example with four anchor boxes.\nFor simplicity, we assume that the predicted offsets are all zeros. This means that the\npredicted bounding boxes are anchor boxes. For each class among the background, dog,\nand cat, we also define its predicted likelihood.\nanchors =torch .tensor([[ 0.1,0.08 ,0.52 ,0.92 ], [ 0.08 ,0.2,0.56 ,0.95 ],\n[0.15 ,0.3,0.62 ,0.91 ], [ 0.55 ,0.2,0.9,0.88 ]])\noffset_preds =torch .tensor([ 0]*anchors .numel())\ncls_probs =torch .tensor([[ 0]*4,# Predicted background likelihood\n[0.9,0.8,0.7,0.1], # Predicted dog likelihood\n[0.1,0.2,0.3,0.9]]) # Predicted cat likelihood\nWe can plot these predicted bounding boxes with their confidence on the image.\nfig =d2l.plt.imshow(img)\nshow_bboxes(fig .axes, anchors *bbox_scale,\n['dog=0.9 ','dog=0.8 ','dog=0.7 ','cat=0.9 '])\nNowwecaninvokethe multibox_detection functiontoperformnon-maximumsuppres-\nsion, where the threshold is set to 0.5. Note that we add a dimension for examples in the\ntensor input.\nWe can see that the shape of the returned result is (batch size, number of anchor boxes,\n6). The six elements in the innermost dimension gives the output information for the same\npredicted bounding box. The first element is the predicted class index, which starts from\n0 (0 is dog and 1 is cat). The value -1 indicates background or removal in non-maximum\nsuppression. The second element is the confidence of the predicted bounding box. The\nremaining four elements are the \u00b9\ud835\udc65,\ud835\udc66\u00ba-axis coordinates of the upper-left corner and the\nlower-right corner of the predicted bounding box, respectively (range is between 0 and\n1).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1827, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "090eda9e-38a8-465a-8498-859013acd003": {"__data__": {"id_": "090eda9e-38a8-465a-8498-859013acd003", "embedding": null, "metadata": {"page_label": "622", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6167ee3f-81a2-4b5d-8fa1-486501fd4fc7", "node_type": "4", "metadata": {"page_label": "622", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2fffa77046580ecf36e48efcf8d4d93c75723dc87ce52444a39224c440b7b5e0", "class_name": "RelatedNodeInfo"}}, "text": "622 Computer Vision\noutput =multibox_detection(cls_probs .unsqueeze(dim =0),\noffset_preds .unsqueeze(dim =0),\nanchors .unsqueeze(dim =0),\nnms_threshold =0.5)\noutput\ntensor([[[ 0.00 ,0.90 ,0.10 ,0.08 ,0.52 ,0.92 ],\n[1.00 ,0.90 ,0.55 ,0.20 ,0.90 ,0.88 ],\n[-1.00 ,0.80 ,0.08 ,0.20 ,0.56 ,0.95 ],\n[-1.00 ,0.70 ,0.15 ,0.30 ,0.62 ,0.91 ]]])\nAfterremovingthosepredictedboundingboxesofclass-1,wecanoutputthefinalpredicted\nbounding box kept by non-maximum suppression.\nfig =d2l.plt.imshow(img)\nfor iinoutput[ 0].detach() .numpy():\nifi[0]==-1:\ncontinue\nlabel =('dog= ','cat= ')[int(i[0])] +str(i[1])\nshow_bboxes(fig .axes, [torch .tensor(i[ 2:]) *bbox_scale], label)\nIn practice, we can remove predicted bounding boxes with lower confidence even before\nperforming non-maximum suppression, thereby reducing computation in this algorithm.\nWe may also post-process the output of non-maximum suppression, for example, by only\nkeeping results with higher confidence in the final output.\n14.4.5Summary\n\u000fWe generate anchor boxes with different shapes centered on each pixel of the image.\n\u000fIntersection over union (IoU), also known as Jaccard index, measures the similarity of\ntwo bounding boxes. It is the ratio of their intersection area to their union area.\n\u000fIn a training set, we need two types of labels for each anchor box. One is the class of\nthe object relevant to the anchor box and the other is the offset of the ground-truth\nbounding box relative to the anchor box.\n\u000fDuringprediction,wecanusenon-maximumsuppression(NMS)toremovesimilarpre-\ndicted bounding boxes, thereby simplifying the output.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1586, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a55d121-2e0c-4174-9224-368c74c199c3": {"__data__": {"id_": "1a55d121-2e0c-4174-9224-368c74c199c3", "embedding": null, "metadata": {"page_label": "623", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd59607a-61ff-4d76-be0e-acf057d46d33", "node_type": "4", "metadata": {"page_label": "623", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bc8cf08ddfc0d1bd38e0c36bb4f8b5607ced42cd9860fb2a52780464311f4a44", "class_name": "RelatedNodeInfo"}}, "text": "623 Multiscale Object Detection\n21514.4.6Exercises\n1.Change values of sizesandratiosin the multibox_prior function. What are the\nchanges to the generated anchor boxes?\n2.Construct and visualize two bounding boxes with an IoU of 0.5. How do they overlap\nwith each other?\n3.Modify the variable anchors inSection 14.4.3 andSection 14.4.4 . How do the results\nchange?\n4.Non-maximum suppression is a greedy algorithm that suppresses predicted bounding\nboxes by removing them. Is it possible that some of these removed ones are actually\nuseful? How can this algorithm be modified to suppress softly? You may refer to Soft-\nNMS (Bodlaetal., 2017).\n5.Rather than being hand-crafted, can non-maximum suppression be learned?\nDiscussions215.\n14.5MultiscaleObject Detection\nInSection 14.4 , we generated multiple anchor boxes centered on each pixel of an input\nimage. Essentially these anchor boxes represent samples of different regions of the image.\nHowever, we may end up with too many anchor boxes to compute if they are generated for\neverypixel. Think of a 561\u0002728input image. If five anchor boxes with varying shapes\nare generated for each pixel as their center, over two million anchor boxes ( 561\u0002728\u00025)\nneed to be labeled and predicted on the image.\n14.5.1Multiscale AnchorBoxes\nYoumayrealizethatitisnotdifficulttoreduceanchorboxesonanimage. Forinstance,we\ncanjustuniformlysampleasmallportionofpixelsfromtheinputimagetogenerateanchor\nboxes centered on them. In addition, at different scales we can generate different numbers\nof anchor boxes of different sizes. Intuitively, smaller objects are more likely to appear on\nan image than larger ones. As an example, 1\u00021,1\u00022, and 2\u00022objects can appear on a\n2\u00022imagein4,2,and1possibleways,respectively. Therefore,whenusingsmalleranchor\nboxes to detect smaller objects, we can sample more regions, while for larger objects we\ncan sample fewer regions.\nTo demonstrate how to generate anchor boxes at multiple scales, let\u2019s read an image. Its\nheight and width are 561 and 728 pixels, respectively.\n%matplotlib inline\nimport torch\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2091, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "44f9b8c3-a6b9-445b-95d1-ed5c9c8ef8a4": {"__data__": {"id_": "44f9b8c3-a6b9-445b-95d1-ed5c9c8ef8a4", "embedding": null, "metadata": {"page_label": "624", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "53bfea96-22e7-447a-a2a5-ea7f4db3fb44", "node_type": "4", "metadata": {"page_label": "624", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "43526046997cf2dd79a3657aa23196763ae87d480e2e2a47e500eca29b5f081f", "class_name": "RelatedNodeInfo"}}, "text": "624 Computer Vision\n(continued from previous page)\nfrom d2l import torch asd2l\nimg =d2l.plt.imread( '../img/catdog.jpg ')\nh, w =img.shape[: 2]\nh, w\n(561,728)\nRecall that in Section 7.2 we call a two-dimensional array output of a convolutional layer\na feature map. By defining the feature map shape, we can determine centers of uniformly\nsampled anchor boxes on any image.\nThedisplay_anchors functionisdefinedbelow. Wegenerateanchorboxes( anchors )on\nthe feature map ( fmap) with each unit (pixel) as the anchor box center. Since the \u00b9\ud835\udc65,\ud835\udc66\u00ba-\naxis coordinate values in the anchor boxes ( anchors ) have been divided by the width and\nheight of the feature map ( fmap), these values are between 0 and 1, which indicate the\nrelative positions of anchor boxes in the feature map.\nSince centers of the anchor boxes ( anchors ) are spread over all units on the feature map\n(fmap), these centers must be uniformly distributed on any input image in terms of their\nrelative spatial positions. More concretely, given the width and height of the feature map\nfmap_wandfmap_h, respectively, the following function will uniformly sample pixels in\nfmap_hrows and fmap_wcolumns on any input image. Centered on these uniformly sam-\npled pixels, anchor boxes of scale s(assuming the length of the list sis 1) and different\naspect ratios ( ratios) will be generated.\ndef display_anchors (fmap_w, fmap_h, s):\nd2l.set_figsize()\n# Values on the first two dimensions do not affect the output\nfmap =torch .zeros(( 1,10, fmap_h, fmap_w))\nanchors =d2l.multibox_prior(fmap, sizes =s, ratios =[1,2,0.5])\nbbox_scale =torch .tensor((w, h, w, h))\nd2l.show_bboxes(d2l .plt.imshow(img) .axes,\nanchors[ 0]*bbox_scale)\nFirst, let\u2019s consider detection of small objects. In order to make it easier to distinguish\nwhen displayed, the anchor boxes with different centers here do not overlap: the anchor\nbox scale is set to 0.15 and the height and width of the feature map are set to 4. We can see\nthat the centers of the anchor boxes in 4 rows and 4 columns on the image are uniformly\ndistributed.\ndisplay_anchors(fmap_w =4, fmap_h =4, s=[0.15 ])\nWemoveontoreducetheheightandwidthofthefeaturemapbyhalfanduselargeranchor\nboxes to detect larger objects. When the scale is set to 0.4, some anchor boxes will overlap\nwith each other.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2287, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e94a7e1f-0324-4ca3-9438-3a3522b1bc63": {"__data__": {"id_": "e94a7e1f-0324-4ca3-9438-3a3522b1bc63", "embedding": null, "metadata": {"page_label": "625", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27479ff1-2c3e-4715-b37b-914fc1a1e01e", "node_type": "4", "metadata": {"page_label": "625", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a17b0d567dd5b4a071e9669146474f34f5e8e065a3ce15284fb7b976f9c30f94", "class_name": "RelatedNodeInfo"}}, "text": "625 Multiscale Object Detection\ndisplay_anchors(fmap_w =2, fmap_h =2, s=[0.4])\nFinally, we further reduce the height and width of the feature map by half and increase the\nanchorboxscaleto0.8. Nowthecenteroftheanchorboxisthecenteroftheimage.\ndisplay_anchors(fmap_w =1, fmap_h =1, s=[0.8])\n14.5.2Multiscale Detection\nSince we have generated multiscale anchor boxes, we will use them to detect objects of\nvarious sizes at different scales. In the following we introduce a CNN-based multiscale\nobject detection method that we will implement in Section 14.7 .\nAtsomescale,saythatwehave \ud835\udc50featuremapsofshape \u210e\u0002\ud835\udc64. Usingthemethodin Section\n14.5.1, we generate \u210e\ud835\udc64sets of anchor boxes, where each set has \ud835\udc4eanchor boxes with the\nsame center. For example, at the first scale in the experiments in Section 14.5.1 , given ten", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 810, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9817471d-9a7e-4746-abc4-0151edc25779": {"__data__": {"id_": "9817471d-9a7e-4746-abc4-0151edc25779", "embedding": null, "metadata": {"page_label": "626", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "966f48ab-c8e6-45d3-9941-45aa91d8449f", "node_type": "4", "metadata": {"page_label": "626", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "335083d9e96390971925ed825182d27ee9e264bb7b156dd3050de0abc98517b9", "class_name": "RelatedNodeInfo"}}, "text": "626 Computer Vision\n(numberofchannels) 4\u00024featuremaps, wegenerated16setsofanchorboxes, whereeach\nset contains 3 anchor boxes with the same center. Next, each anchor box is labeled with\nthe class and offset based on ground-truth bounding boxes. At the current scale, the object\ndetection model needs to predict the classes and offsets of \u210e\ud835\udc64sets of anchor boxes on the\ninput image, where different sets have different centers.\nAssume that the \ud835\udc50featuremaps here are the intermediate outputs obtained bythe CNN for-\nward propagation based on the input image. Since there are \u210e\ud835\udc64different spatial positions\non each feature map, the same spatial position can be thought of as having \ud835\udc50units. Ac-\ncording to the definition of receptive field in Section 7.2 , these\ud835\udc50units at the same spatial\nposition of the feature maps have the same receptive field on the input image: they repre-\nsent the input image information in the same receptive field. Therefore, we can transform\nthe\ud835\udc50unitsofthefeaturemapsatthesamespatialpositionintotheclassesandoffsetsofthe\n\ud835\udc4eanchor boxes generated using this spatial position. In essence, we use the information of\nthe input image in a certain receptive field to predict the classes and offsets of the anchor\nboxes that are close to that receptive field on the input image.\nWhen the feature maps at different layers have varying-size receptive fields on the input\nimage, they can be used to detect objects of different sizes. For example, we can design a\nneural network where units of feature maps that are closer to the output layer have wider\nreceptive fields, so they can detect larger objects from the input image.\nInanutshell,wecanleveragelayerwiserepresentationsofimagesatmultiplelevelsbydeep\nneural networks for multiscale object detection. We will show how this works through a\nconcrete example in Section 14.7 .\n14.5.3Summary\n\u000fAt multiple scales, we can generate anchor boxes with different sizes to detect objects\nwith different sizes.\n\u000fBy defining the shape of feature maps, we can determine centers of uniformly sampled\nanchor boxes on any image.\n\u000fWeusetheinformationoftheinputimageinacertainreceptivefieldtopredicttheclasses\nandoffsetsoftheanchorboxesthatareclosetothatreceptivefieldontheinputimage.\n\u000fThrough deep learning, we can leverage its layerwise representations of images at mul-\ntiple levels for multiscale object detection.\n14.5.4Exercises\n1.According to our discussions in Section 8.1 , deep neural networks learn hierarchical\nfeatures with increasing levels of abstraction for images. In multiscale object detection,\ndo featuremaps at different scales correspond to different levels of abstraction? Whyor\nwhy not?\n2.At the first scale ( fmap_w=4, fmap_h=4 ) in the experiments in Section 14.5.1 , generate\nuniformly distributed anchor boxes that may overlap.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5f01008-7f85-42a2-bdcd-84158742819a": {"__data__": {"id_": "e5f01008-7f85-42a2-bdcd-84158742819a", "embedding": null, "metadata": {"page_label": "627", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "264c24cc-2c56-4f38-a1dd-96b261b31ad6", "node_type": "4", "metadata": {"page_label": "627", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e77459679316af92b3ca2ce0d10a134c0675d9df32dd97c8fc6bff2aa1658175", "class_name": "RelatedNodeInfo"}}, "text": "627 The Object Detection Dataset\n2163.Given a feature map variable with shape 1\u0002\ud835\udc50\u0002\u210e\u0002\ud835\udc64, where\ud835\udc50,\u210e, and\ud835\udc64are the\nnumber of channels, height, and width of the feature maps, respectively. How can you\ntransform this variable into the classes and offsets of anchor boxes? What is the shape\nof the output?\nDiscussions216.\n14.6The Object DetectionDataset\nThere is no small dataset such as MNIST and Fashion-MNIST in the field of object detec-\ntion. In order to quickly demonstrate object detection models, we collected and labeled a\nsmall dataset. First, we took photos of free bananas from our office and generated 1000\nbanana images with different rotations and sizes. Then we placed each banana image at a\nrandom position on some background image. In the end, we labeled bounding boxes for\nthose bananas on the images.\n14.6.1Downloadingthe Dataset\nThe banana detection dataset with all the image and csv label files can be downloaded\ndirectly from the Internet.\n%matplotlib inline\nimport os\nimport pandas aspd\nimport torch\nimport torchvision\nfrom d2l import torch asd2l\n#@save\nd2l.DATA_HUB[ 'banana-detection ']=(\nd2l.DATA_URL +'banana-detection.zip ',\n'5de26c8fce5ccdea9f91267273464dc968d20d72 ')\n14.6.2Readingthe Dataset\nWe are going to read the banana detection dataset in the read_data_bananas function\nbelow. The dataset includes a csv file for object class labels and ground-truth bounding\nbox coordinates at the upper-left and lower-right corners.\n#@save\ndef read_data_bananas (is_train =True ):\n\"\"\"Read the banana detection dataset images and labels.\"\"\"\ndata_dir =d2l.download_extract( 'banana-detection ')\ncsv_fname =os.path .join(data_dir, 'bananas_train 'ifis_train\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2815cdf3-a62d-43cc-9777-467928a74411": {"__data__": {"id_": "2815cdf3-a62d-43cc-9777-467928a74411", "embedding": null, "metadata": {"page_label": "628", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "49e51f27-125c-41f4-ac7b-fc8997c40b9d", "node_type": "4", "metadata": {"page_label": "628", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b48cf6552491ea3ef732a66b44da0e6b09725e07fe930fc6f67395ce8eced02a", "class_name": "RelatedNodeInfo"}}, "text": "628 Computer Vision\n(continued from previous page)\nelse 'bananas_val ','label.csv ')\ncsv_data =pd.read_csv(csv_fname)\ncsv_data =csv_data .set_index( 'img_name ')\nimages, targets =[], []\nfor img_name, target incsv_data .iterrows():\nimages .append(torchvision .io.read_image(\nos.path .join(data_dir, 'bananas_train 'ifis_train else\n'bananas_val ','images ',f'{img_name }')))\n# Here `target` contains (class, upper-left x, upper-left y,\n# lower-right x, lower-right y), where all the images have the same\n# banana class (index 0)\ntargets .append( list (target))\nreturn images, torch .tensor(targets) .unsqueeze( 1)/256\nBy using the read_data_bananas function to read images and labels, the following Ba-\nnanasDataset class will allow us to create a customized Dataset instance for loading the\nbanana detection dataset.\n#@save\nclass BananasDataset (torch .utils .data .Dataset):\n\"\"\"A customized dataset to load the banana detection dataset.\"\"\"\ndef __init__ (self , is_train):\nself .features, self .labels =read_data_bananas(is_train)\nprint ('read '+str(len(self .features)) +(f'training examples 'if\nis_train else f'validation examples '))\ndef __getitem__ (self , idx):\nreturn (self .features[idx] .float(), self .labels[idx])\ndef __len__ (self ):\nreturn len(self .features)\nFinally, we define the load_data_bananas function to return two data iterator instances\nfor both the training and test sets. For the test dataset, there is no need to read it in random\norder.\n#@save\ndef load_data_bananas (batch_size):\n\"\"\"Load the banana detection dataset.\"\"\"\ntrain_iter =torch .utils .data .DataLoader(BananasDataset(is_train =True ),\nbatch_size, shuffle =True )\nval_iter =torch .utils .data .DataLoader(BananasDataset(is_train =False ),\nbatch_size)\nreturn train_iter, val_iter\nLet\u2019s read a minibatch and print the shapes of both images and labels in this minibatch.\nThe shape of the image minibatch, (batch size, number of channels, height, width), looks\nfamiliar: it is the same as in our earlier image classification tasks. The shape of the label\nminibatch is (batch size, \ud835\udc5a, 5), where\ud835\udc5ais the largest possible number of bounding boxes\nthat any image has in the dataset.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2160, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6b0351d-56cc-47c5-8e11-b6711ee6a7c4": {"__data__": {"id_": "d6b0351d-56cc-47c5-8e11-b6711ee6a7c4", "embedding": null, "metadata": {"page_label": "629", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a980f5a8-1f58-4c64-b94d-4a51beb0c9cc", "node_type": "4", "metadata": {"page_label": "629", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c6a75358f657082811c526763692c11b38087e6247b87db950b3e8514aa076e6", "class_name": "RelatedNodeInfo"}}, "text": "629 The Object Detection Dataset\nAlthough computation in minibatches is more efficient, it requires that all the image exam-\nples contain the same number of bounding boxes to form a minibatch via concatenation.\nIn general, images may have a varying number of bounding boxes; thus, images with fewer\nthan\ud835\udc5aboundingboxeswillbepaddedwithillegalboundingboxesuntil \ud835\udc5aisreached. Then\nthe label of each bounding box is represented by an array of length 5. The first element in\nthearrayistheclassoftheobjectintheboundingbox, where-1indicatesanillegalbound-\ning box for padding. The remaining four elements of the array are the ( \ud835\udc65,\ud835\udc66)-coordinate\nvalues of the upper-left corner and the lower-right corner of the bounding box (the range\nis between 0 and 1). For the banana dataset, since there is only one bounding box on each\nimage, we have \ud835\udc5a=1.\nbatch_size, edge_size =32,256\ntrain_iter, _ =load_data_bananas(batch_size)\nbatch =next (iter (train_iter))\nbatch[ 0].shape, batch[ 1].shape\nDownloading ../data /banana -detection .zip from http ://d2l-data .s3-accelerate .\n\u21a9!amazonaws .com/banana -detection .zip...\nread 1000 training examples\nread 100 validation examples\n(torch .Size([ 32,3,256,256]), torch .Size([ 32,1,5]))\n14.6.3Demonstration\nLet\u2019s demonstrate ten images with their labeled ground-truth bounding boxes. We can see\nthat the rotations, sizes, and positions of bananas vary across all these images. Of course,\nthisisjustasimpleartificialdataset. Inpractice,real-worlddatasetsareusuallymuchmore\ncomplicated.\nimgs =(batch[ 0][:10].permute( 0,2,3,1))/255\naxes =d2l.show_images(imgs, 2,5, scale =2)\nfor ax, label inzip(axes, batch[ 1][:10]):\nd2l.show_bboxes(ax, [label[ 0][1:5]*edge_size], colors =['w'])\n14.6.4Summary\n\u000fThe banana detection dataset we collected can be used to demonstrate object detection\nmodels.\n\u000fThedataloadingforobjectdetectionissimilartothatforimageclassification. However,\ninobjectdetectionthelabelsalsocontaininformationofground-truthboundingboxes,\nwhich is missing in image classification.\n14.6.5Exercises", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2029, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1128b6a0-2285-499e-b14a-038e5091251b": {"__data__": {"id_": "1128b6a0-2285-499e-b14a-038e5091251b", "embedding": null, "metadata": {"page_label": "630", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca7ee332-c272-494a-8d1a-a4fbb41d91c4", "node_type": "4", "metadata": {"page_label": "630", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9642db9ac8497dc46e132a6d3f120cdca305c91ee8d68d105156f715bf34fb3f", "class_name": "RelatedNodeInfo"}}, "text": "630 Computer Vision\n2171.Demonstrate other images with ground-truth bounding boxes in the banana detection\ndataset. How do they differ with respect to bounding boxes and objects?\n2.Saythatwewanttoapplydataaugmentation, suchasrandomcropping, toobjectdetec-\ntion. How can it be different from that in image classification? Hint: what if a cropped\nimage only contains a small portion of an object?\nDiscussions217.\n14.7SingleShotMultiboxDetection\nInSection 14.3 \u2013Section 14.6 , we introduced bounding boxes, anchor boxes, multiscale\nobject detection, and the dataset for object detection. Now we are ready to use such back-\nground knowledge to design an object detection model: single shot multibox detection\n(SSD) (Liuet al., 2016). This model is simple, fast, and widely used. Although this is\njust one of vast amounts of object detection models, some of the design principles and\nimplementation details in this section are also applicable to other models.\n14.7.1Model\nFig. 14.7.1 provides an overview of the design of single-shot multibox detection. This\nmodelmainlyconsistsofabasenetworkfollowedbyseveralmultiscalefeaturemapblocks.\nThebasenetworkisforextractingfeaturesfromtheinputimage, soitcanuseadeepCNN.\nForexample,theoriginalsingle-shotmultiboxdetectionpaperadoptsaVGGnetworktrun-\ncatedbeforetheclassificationlayer( Liuetal.,2016),whileResNethasalsobeencommonly\nused. Through our design we can make the base network output larger feature maps so as\ntogeneratemoreanchorboxesfordetectingsmallerobjects. Subsequently,eachmultiscale\nfeature map block reduces (e.g., by half) the height and width of the feature maps from the\nprevious block, and enables each unit of the feature maps to increase its receptive field on\nthe input image.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e24853a3-614c-4452-9116-3302db507075": {"__data__": {"id_": "e24853a3-614c-4452-9116-3302db507075", "embedding": null, "metadata": {"page_label": "631", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55a84ef4-aeba-4c8b-a610-78cf28f6a425", "node_type": "4", "metadata": {"page_label": "631", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2b682ff981b86771dbec261e8c1e7fbb66d12e301995c8e475c70d447b443c7c", "class_name": "RelatedNodeInfo"}}, "text": "631 Single Shot Multibox Detection\nRecallthedesignofmultiscaleobjectdetectionthroughlayerwiserepresentationsofimages\nby deep neural networks in Section 14.5 . Since multiscale feature maps closer to the top of\nFig. 14.7.1 are smaller but have larger receptive fields, they are suitable for detecting fewer\nbut larger objects.\nIn a nutshell, via its base network and several multiscale feature map blocks, single-shot\nmultibox detection generates a varying number of anchor boxes with different sizes, and\ndetects varying-size objects by predicting classes and offsets of these anchor boxes (thus\nthe bounding boxes); thus, this is a multiscale object detection model.\ntFig. 14.7.1 As a multiscale object detection model, single-shot multibox detection mainly consists of\na base network followed by several multiscale feature map blocks.\nIn the following, we will describe the implementation details of different blocks in Fig.\n14.7.1. To begin with, we discuss how to implement the class and bounding box predic-\ntion.\nClassPrediction Layer\nLetthenumberofobjectclassesbe \ud835\udc5e. Thenanchorboxeshave \ud835\udc5e\u00b81classes,whereclass0is\nbackground. Atsomescale, supposethattheheightandwidthoffeaturemapsare \u210eand\ud835\udc64,\nrespectively. When \ud835\udc4eanchorboxesaregeneratedwitheachspatialpositionofthesefeature\nmaps as their center, a total of \u210e\ud835\udc64\ud835\udc4eanchor boxes need to be classified. This often makes\nclassification with fully connected layers infeasible due to likely heavy parametrization\ncosts. Recall how we used channels of convolutional layers to predict classes in Section\n8.3. Single-shot multibox detection uses the same technique to reduce model complex-\nity.\nSpecifically, the class prediction layer uses a convolutional layer without altering width\nor height of feature maps. In this way, there can be a one-to-one correspondence between\noutputsandinputsatthesamespatialdimensions(widthandheight)offeaturemaps. More\nconcretely, channelsof the output feature maps at anyspatial position ( \ud835\udc65,\ud835\udc66) represent class\npredictionsforalltheanchorboxescenteredon( \ud835\udc65,\ud835\udc66)oftheinputfeaturemaps. Toproduce\nvalidpredictions,theremustbe \ud835\udc4e\u00b9\ud835\udc5e\u00b81\u00baoutputchannels,whereforthesamespatialposition", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2146, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42e10955-9842-4acd-8f2b-5889104376e3": {"__data__": {"id_": "42e10955-9842-4acd-8f2b-5889104376e3", "embedding": null, "metadata": {"page_label": "632", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d146a532-2f3c-49dc-85d3-b25e34d6a592", "node_type": "4", "metadata": {"page_label": "632", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "58b9aa90a3f6ad100732dda0b215d0e6346776950d587a5164119bce9c2c62ff", "class_name": "RelatedNodeInfo"}}, "text": "632 Computer Vision\ntheoutputchannelwithindex \ud835\udc56\u00b9\ud835\udc5e\u00b81\u00ba\u00b8\ud835\udc57representsthepredictionoftheclass \ud835\udc57(0\u0014\ud835\udc57\u0014\ud835\udc5e)\nfor the anchor box \ud835\udc56(0\u0014\ud835\udc56 <\ud835\udc4e).\nBelowwedefinesuchaclasspredictionlayer,specifying \ud835\udc4eand\ud835\udc5eviaarguments num_anchors\nandnum_classes , respectively. This layer uses a 3\u00023convolutional layer with a padding\nof 1. The width and height of the input and output of this convolutional layer remain un-\nchanged.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l\ndef cls_predictor (num_inputs, num_anchors, num_classes):\nreturn nn.Conv2d(num_inputs, num_anchors *(num_classes +1),\nkernel_size =3, padding =1)\nBoundingBoxPredictionLayer\nThe design of the bounding box prediction layer is similar to that of the class prediction\nlayer. The only difference lies in the number of outputs for each anchor box: here we need\nto predict four offsets rather than \ud835\udc5e\u00b81classes.\ndef bbox_predictor (num_inputs, num_anchors):\nreturn nn.Conv2d(num_inputs, num_anchors *4, kernel_size =3, padding =1)\nConcatenating PredictionsforMultiple Scales\nAs we mentioned, single-shot multibox detection uses multiscale feature maps to generate\nanchor boxes and predict their classes and offsets. At different scales, the shapes of feature\nmapsorthenumbersofanchorboxescenteredonthesameunitmayvary. Therefore,shapes\nof the prediction outputs at different scales may vary.\nIn the following example, we construct feature maps at two different scales, Y1andY2, for\nthe same minibatch, where the height and width of Y2are half of those of Y1. Let\u2019s take\nclass prediction as an example. Suppose that 5 and 3 anchor boxes are generated for every\nunit in Y1andY2, respectively. Suppose further that the number of object classes is 10.\nFor feature maps Y1andY2the numbers of channels in the class prediction outputs are\n5\u0002\u00b910\u00b81\u00ba=55and3\u0002\u00b910\u00b81\u00ba=33, respectively, where either output shape is (batch\nsize, number of channels, height, width).\ndef forward (x, block):\nreturn block(x)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5876cb2d-c94e-4921-9917-25941f9530c4": {"__data__": {"id_": "5876cb2d-c94e-4921-9917-25941f9530c4", "embedding": null, "metadata": {"page_label": "633", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1fe4e962-cd2d-4960-ab8e-f097c1d203c4", "node_type": "4", "metadata": {"page_label": "633", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bd43e7ab02bb58616363eefedad88c97dda04433ac5ec93a7209b9ecda6a7b51", "class_name": "RelatedNodeInfo"}}, "text": "633 Single Shot Multibox Detection\n(continued from previous page)\nY1=forward(torch .zeros(( 2,8,20,20)), cls_predictor( 8,5,10))\nY2=forward(torch .zeros(( 2,16,10,10)), cls_predictor( 16,3,10))\nY1.shape, Y2 .shape\n(torch .Size([ 2,55,20,20]), torch .Size([ 2,33,10,10]))\nAs we can see, except for the batch size dimension, the other three dimensions all have\ndifferentsizes. Toconcatenatethesetwopredictionoutputsformoreefficientcomputation,\nwe will transform these tensors into a more consistent format.\nNotethatthechanneldimensionholdsthepredictionsforanchorboxeswiththesamecenter.\nWe first move this dimension to the innermost. Since the batch size remains the same for\ndifferent scales, we can transform the prediction output into a two-dimensional tensor with\nshape (batch size, height \u0002width\u0002number of channels). Then we can concatenate such\noutputs at different scales along dimension 1.\ndef flatten_pred (pred):\nreturn torch .flatten(pred .permute( 0,2,3,1), start_dim =1)\ndef concat_preds (preds):\nreturn torch .cat([flatten_pred(p) for pinpreds], dim =1)\nIn this way, even though Y1andY2have different sizes in channels, heights, and widths,\nwe can still concatenate these two prediction outputs at two different scales for the same\nminibatch.\nconcat_preds([Y1, Y2]) .shape\ntorch .Size([ 2,25300 ])\nDownsamplingBlock\nIn order to detect objects at multiple scales, we define the following downsampling block\ndown_sample_blk thathalvestheheightandwidthofinputfeaturemaps. Infact,thisblock\napplies the design of VGG blocks in Section 8.2.1 . More concretely, each downsampling\nblock consists of two 3\u00023convolutional layers with padding of 1 followed by a 2\u00022max-\npooling layer with stride of 2. As we know, 3\u00023convolutional layers with padding of 1 do\nnot change the shape of feature maps. However, the subsequent 2\u00022max-pooling reduces\nthe height and width of input feature maps by half. For both input and output feature maps\nof this downsampling block, because 1\u00022\u00b8\u00b93\u00001\u00ba\u00b8\u00b9 3\u00001\u00ba=6, each unit in the output\nhas a 6\u00026receptive field on the input. Therefore, the downsampling block enlarges the\nreceptive field of each unit in its output feature maps.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2157, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ceaea90-bf72-4740-95f9-92d79f3c778d": {"__data__": {"id_": "2ceaea90-bf72-4740-95f9-92d79f3c778d", "embedding": null, "metadata": {"page_label": "634", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31484385-bc46-421c-9c38-baeae26bd2b4", "node_type": "4", "metadata": {"page_label": "634", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d4d60760e1341d5e0d99fcdfe779468efacc11bed58142d40636d2b5ec35be72", "class_name": "RelatedNodeInfo"}}, "text": "634 Computer Vision\ndef down_sample_blk (in_channels, out_channels):\nblk =[]\nfor _inrange (2):\nblk.append(nn .Conv2d(in_channels, out_channels,\nkernel_size =3, padding =1))\nblk.append(nn .BatchNorm2d(out_channels))\nblk.append(nn .ReLU())\nin_channels =out_channels\nblk.append(nn .MaxPool2d( 2))\nreturn nn.Sequential( *blk)\nInthefollowingexample,ourconstructeddownsamplingblockchangesthenumberofinput\nchannels and halves the height and width of the input feature maps.\nforward(torch .zeros(( 2,3,20,20)), down_sample_blk( 3,10)).shape\ntorch .Size([ 2,10,10,10])\nBaseNetworkBlock\nThe base network block is used to extract features from input images. For simplicity, we\nconstruct a small base network consisting of three downsampling blocks that double the\nnumber of channels at each block. Given a 256\u0002256input image, this base network block\noutputs 32\u000232feature maps ( 256\u009d23=32).\ndef base_net ():\nblk =[]\nnum_filters =[3,16,32,64]\nfor iinrange (len(num_filters) -1):\nblk.append(down_sample_blk(num_filters[i], num_filters[i +1]))\nreturn nn.Sequential( *blk)\nforward(torch .zeros(( 2,3,256,256)), base_net()) .shape\ntorch .Size([ 2,64,32,32])\nTheCompleteModel\nThe complete single shot multibox detection model consists of five blocks. The feature\nmapsproducedbyeachblockareusedforboth(i)generatinganchorboxesand(ii)predict-\ning classes and offsets of these anchor boxes. Among these five blocks, the first one is the\nbase network block, the second to the fourth are downsampling blocks, and the last block\nuses global max-pooling to reduce both the height and width to 1. Technically, the second\nto the fifth blocks are all those multiscale feature map blocks in Fig. 14.7.1 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2153a5a5-1056-43bf-9a7c-0eecc3551d8e": {"__data__": {"id_": "2153a5a5-1056-43bf-9a7c-0eecc3551d8e", "embedding": null, "metadata": {"page_label": "635", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31601c6f-72a7-41be-be94-382d9fc8b16f", "node_type": "4", "metadata": {"page_label": "635", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "10779f461aac69ff81fc81f7a77adc06b5830510861f2e544f2213927b1f7edd", "class_name": "RelatedNodeInfo"}}, "text": "635 Single Shot Multibox Detection\ndef get_blk (i):\nifi==0:\nblk =base_net()\nelif i==1:\nblk =down_sample_blk( 64,128)\nelif i==4:\nblk =nn.AdaptiveMaxPool2d(( 1,1))\nelse :\nblk =down_sample_blk( 128,128)\nreturn blk\nNow we define the forward propagation for each block. Different from in image classifica-\ntion tasks, outputs here include (i) CNN feature maps Y, (ii) anchor boxes generated using\nYat the current scale, and (iii) classes and offsets predicted (based on Y) for these anchor\nboxes.\ndef blk_forward (X, blk, size, ratio, cls_predictor, bbox_predictor):\nY=blk(X)\nanchors =d2l.multibox_prior(Y, sizes =size, ratios =ratio)\ncls_preds =cls_predictor(Y)\nbbox_preds =bbox_predictor(Y)\nreturn (Y, anchors, cls_preds, bbox_preds)\nRecall that in Fig. 14.7.1 a multiscale feature map block that is closer to the top is for\ndetectinglargerobjects;thus,itneedstogeneratelargeranchorboxes. Intheaboveforward\npropagation,ateachmultiscalefeaturemapblockwepassinalistoftwoscalevaluesviathe\nsizesargument of the invoked multibox_prior function (described in Section 14.4 ). In\nthefollowing,theintervalbetween0.2and1.05issplitevenlyintofivesectionstodetermine\nthe smaller scale values at the five blocks: 0.2, 0.37, 0.54, 0.71, and 0.88. Then their larger\nscale values are given byp\n0.2\u00020.37=0.272,p\n0.37\u00020.54=0.447, and so on.\nsizes =[[0.2,0.272 ], [ 0.37 ,0.447 ], [ 0.54 ,0.619 ], [ 0.71 ,0.79 ],\n[0.88 ,0.961 ]]\nratios =[[1,2,0.5]]*5\nnum_anchors =len(sizes[ 0])+len(ratios[ 0])-1\nNow we can define the complete model TinySSD as follows.\nclass TinySSD (nn.Module):\ndef __init__ (self , num_classes, **kwargs):\nsuper (TinySSD, self ).__init__ (**kwargs)\nself .num_classes =num_classes\nidx_to_in_channels =[64,128,128,128,128]\nfor iinrange (5):\n# Equivalent to the assignment statement `self.blk_i = get_blk(i)`\nsetattr (self ,f'blk_ {i}', get_blk(i))\nsetattr (self ,f'cls_ {i}', cls_predictor(idx_to_in_channels[i],\nnum_anchors, num_classes))\nsetattr (self ,f'bbox_ {i}', bbox_predictor(idx_to_in_channels[i],\nnum_anchors))\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2041, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84710840-6eb9-4cd3-8442-7923e08d24e0": {"__data__": {"id_": "84710840-6eb9-4cd3-8442-7923e08d24e0", "embedding": null, "metadata": {"page_label": "636", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d794c4e-8b37-4952-b371-8471e23c54cc", "node_type": "4", "metadata": {"page_label": "636", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0e72653f11d3e15c77c617cce27e3cc57c1485420d260153821de0ff9119b3c0", "class_name": "RelatedNodeInfo"}}, "text": "636 Computer Vision\n(continued from previous page)\ndef forward (self , X):\nanchors, cls_preds, bbox_preds =[None ]*5, [None ]*5, [None ]*5\nfor iinrange (5):\n# Here `getattr(self, 'blk_%d' % i)` accesses `self.blk_i`\nX, anchors[i], cls_preds[i], bbox_preds[i] =blk_forward(\nX,getattr (self ,f'blk_ {i}'), sizes[i], ratios[i],\ngetattr (self ,f'cls_ {i}'),getattr (self ,f'bbox_ {i}'))\nanchors =torch .cat(anchors, dim =1)\ncls_preds =concat_preds(cls_preds)\ncls_preds =cls_preds .reshape(\ncls_preds .shape[ 0],-1,self .num_classes +1)\nbbox_preds =concat_preds(bbox_preds)\nreturn anchors, cls_preds, bbox_preds\nWe create a model instance and use it to perform forward propagation on a minibatch of\n256\u0002256images X.\nAsshownearlierinthissection,thefirstblockoutputs 32\u000232featuremaps. Recallthatthe\nsecond to fourth downsampling blocks halve the height and width and the fifth block uses\nglobal pooling. Since 4 anchor boxes are generated for each unit along spatial dimensions\nof feature maps, at all the five scales a total of \u00b9322\u00b8162\u00b882\u00b842\u00b81\u00ba\u00024=5444anchor\nboxes are generated for each image.\nnet =TinySSD(num_classes =1)\nX=torch .zeros(( 32,3,256,256))\nanchors, cls_preds, bbox_preds =net(X)\nprint ('output anchors: ', anchors .shape)\nprint ('output class preds: ', cls_preds .shape)\nprint ('output bbox preds: ', bbox_preds .shape)\noutput anchors: torch .Size([ 1,5444 ,4])\noutput class preds : torch .Size([ 32,5444 ,2])\noutput bbox preds: torch .Size([ 32,21776 ])\n14.7.2Training\nNow we will explain how to train the single shot multibox detection model for object de-\ntection.\nReadingthe Datasetand Initializing the Model\nTo begin with, let\u2019s read the banana detection dataset described in Section 14.6 .\nbatch_size =32\ntrain_iter, _ =d2l.load_data_bananas(batch_size)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1769, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3381bf6e-09d0-4bc7-b056-1614b131ed1e": {"__data__": {"id_": "3381bf6e-09d0-4bc7-b056-1614b131ed1e", "embedding": null, "metadata": {"page_label": "637", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6cec1e5e-b4cb-4c62-aca7-10654095f57a", "node_type": "4", "metadata": {"page_label": "637", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f9abcfa16535d7c72ab32c78fb807b7702aa030d2bcd4c63d908cc74f558b263", "class_name": "RelatedNodeInfo"}}, "text": "637 Single Shot Multibox Detection\nread 1000 training examples\nread 100 validation examples\nThere is only one class in the banana detection dataset. After defining the model, we need\nto initialize its parameters and define the optimization algorithm.\ndevice, net =d2l.try_gpu(), TinySSD(num_classes =1)\ntrainer =torch .optim .SGD(net .parameters(), lr =0.2, weight_decay =5e-4 )\nDefiningLoss and EvaluationFunctions\nObject detection has two types of losses. The first loss concerns classes of anchor boxes:\nits computation can simply reuse the cross-entropy loss function that we used for image\nclassification. Thesecondlossconcernsoffsetsofpositive(non-background)anchorboxes:\nthis is a regression problem. For this regression problem, however, here we do not use the\nsquared loss described in Section 3.1.3 . Instead, we use the \u21131norm loss, the absolute\nvalue of the difference between the prediction and the ground-truth. The mask variable\nbbox_masks filters out negative anchor boxes and illegal (padded) anchor boxes in the loss\ncalculation. In the end, we sum up the anchor box class loss and the anchor box offset loss\nto obtain the loss function for the model.\ncls_loss =nn.CrossEntropyLoss(reduction ='none ')\nbbox_loss =nn.L1Loss(reduction ='none ')\ndef calc_loss (cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\nbatch_size, num_classes =cls_preds .shape[ 0], cls_preds .shape[ 2]\ncls =cls_loss(cls_preds .reshape( -1, num_classes),\ncls_labels .reshape( -1)).reshape(batch_size, -1).mean(dim =1)\nbbox =bbox_loss(bbox_preds *bbox_masks,\nbbox_labels *bbox_masks) .mean(dim =1)\nreturn cls +bbox\nWe can use accuracy to evaluate the classification results. Due to the used \u21131norm loss\nfor the offsets, we use the mean absolute error to evaluate the predicted bounding boxes.\nThese prediction results are obtained from the generated anchor boxes and the predicted\noffsets for them.\ndef cls_eval (cls_preds, cls_labels):\n# Because the class prediction results are on the final dimension,\n# `argmax` needs to specify this dimension\nreturn float ((cls_preds .argmax(dim =-1).type(\ncls_labels .dtype) ==cls_labels) .sum())\ndef bbox_eval (bbox_preds, bbox_labels, bbox_masks):\nreturn float ((torch .abs((bbox_labels -bbox_preds) *bbox_masks)) .sum())", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b29dfb1-6729-4bf3-954b-0f6b3fe879a4": {"__data__": {"id_": "8b29dfb1-6729-4bf3-954b-0f6b3fe879a4", "embedding": null, "metadata": {"page_label": "638", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "95884c02-e73a-45dd-a107-37f379deb437", "node_type": "4", "metadata": {"page_label": "638", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ecbbc7016f6120dbc3e44978c6c376968db8967f14da8f60e5dd3835835ba281", "class_name": "RelatedNodeInfo"}}, "text": "638 Computer Vision\nTrainingthe Model\nWhen training the model, we need to generate multiscale anchor boxes ( anchors ) and pre-\ndict their classes ( cls_preds ) and offsets ( bbox_preds ) in the forward propagation. Then\nwe label the classes ( cls_labels ) and offsets ( bbox_labels ) of such generated anchor\nboxes based on the label information Y. Finally, we calculate the loss function using the\npredicted and labeled values of the classes and offsets. For concise implementations, eval-\nuation of the test dataset is omitted here.\nnum_epochs, timer =20, d2l .Timer()\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],\nlegend =['class error ','bbox mae '])\nnet =net.to(device)\nfor epoch inrange (num_epochs):\n# Sum of training accuracy, no. of examples in sum of training accuracy,\n# Sum of absolute error, no. of examples in sum of absolute error\nmetric =d2l.Accumulator( 4)\nnet.train()\nfor features, target intrain_iter:\ntimer .start()\ntrainer .zero_grad()\nX, Y =features .to(device), target .to(device)\n# Generate multiscale anchor boxes and predict their classes and\n# offsets\nanchors, cls_preds, bbox_preds =net(X)\n# Label the classes and offsets of these anchor boxes\nbbox_labels, bbox_masks, cls_labels =d2l.multibox_target(anchors, Y)\n# Calculate the loss function using the predicted and labeled values\n# of the classes and offsets\nl=calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\nbbox_masks)\nl.mean() .backward()\ntrainer .step()\nmetric .add(cls_eval(cls_preds, cls_labels), cls_labels .numel(),\nbbox_eval(bbox_preds, bbox_labels, bbox_masks),\nbbox_labels .numel())\ncls_err, bbox_mae =1-metric[ 0]/metric[ 1], metric[ 2]/metric[ 3]\nanimator .add(epoch +1, (cls_err, bbox_mae))\nprint (f'class err {cls_err :.2e}, bbox mae {bbox_mae :.2e}')\nprint (f'{len(train_iter .dataset) /timer .stop() :.1f}examples/sec on '\nf'{str(device) }')\nclass err 3.27e-03 , bbox mae 3.08e-03\n4279.7 examples /sec on cuda: 0\n14.7.3Prediction\nDuring prediction, the goal is to detect all the objects of interest on the image. Below we\nread and resize a test image, converting it to a four-dimensional tensor that is required by\nconvolutional layers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee0f81b7-5b9a-44bf-97b7-e89f9fd49920": {"__data__": {"id_": "ee0f81b7-5b9a-44bf-97b7-e89f9fd49920", "embedding": null, "metadata": {"page_label": "639", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "abfce39d-eb99-4e71-88a0-ceb5722cef89", "node_type": "4", "metadata": {"page_label": "639", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a16b1de259b82572c5f08611151d7ba738d2218736a0b58abaf832d450a0e100", "class_name": "RelatedNodeInfo"}}, "text": "639 Single Shot Multibox Detection\nX=torchvision .io.read_image( '../img/banana.jpg ').unsqueeze( 0).float()\nimg =X.squeeze( 0).permute( 1,2,0).long()\nUsingthe multibox_detection functionbelow,thepredictedboundingboxesareobtained\nfromtheanchorboxesandtheirpredictedoffsets. Thennon-maximumsuppressionisused\nto remove similar predicted bounding boxes.\ndef predict (X):\nnet.eval()\nanchors, cls_preds, bbox_preds =net(X .to(device))\ncls_probs =F.softmax(cls_preds, dim =2).permute( 0,2,1)\noutput =d2l.multibox_detection(cls_probs, bbox_preds, anchors)\nidx =[ifor i, row inenumerate (output[ 0])ifrow[ 0]!=-1]\nreturn output[ 0, idx]\noutput =predict(X)\nFinally, we display all the predicted bounding boxes with confidence 0.9 or above as out-\nput.\ndef display (img, output, threshold):\nd2l.set_figsize(( 5,5))\nfig =d2l.plt.imshow(img)\nfor row inoutput:\nscore =float (row[ 1])\nifscore <threshold:\ncontinue\nh, w =img.shape[: 2]\nbbox =[row[ 2:6]*torch .tensor((w, h, w, h), device =row.device)]\nd2l.show_bboxes(fig .axes, bbox, '%.2f '%score, 'w')\ndisplay(img, output .cpu(), threshold =0.9)\n14.7.4Summary\n\u000fSingle shot multibox detection is a multiscale object detection model. Via its base net-\nwork and several multiscale feature map blocks, single-shot multibox detection gen-\nerates a varying number of anchor boxes with different sizes, and detects varying-size", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c7f3259-2c7f-4f68-9823-235bda659090": {"__data__": {"id_": "7c7f3259-2c7f-4f68-9823-235bda659090", "embedding": null, "metadata": {"page_label": "640", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8296b80-18fe-4a90-a4e6-0d05ab4481e6", "node_type": "4", "metadata": {"page_label": "640", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fead4917e9638b6f967726e44549cc1ce809741dab7e6d3d16bf747dc04ab81d", "class_name": "RelatedNodeInfo"}}, "text": "640 Computer Vision\nobjects by predicting classes and offsets of these anchor boxes (thus the bounding\nboxes).\n\u000fWhen training the single-shot multibox detection model, the loss function is calculated\nbased on the predicted and labeled values of the anchor box classes and offsets.\n14.7.5Exercises\n1.Canyouimprovethesingle-shotmultiboxdetectionbyimprovingthelossfunction? For\nexample, replace \u21131norm loss with smooth \u21131norm loss for the predicted offsets. This\nloss function uses a square function around zero for smoothness, which is controlled by\nthe hyperparameter \ud835\udf0e:\n\ud835\udc53\u00b9\ud835\udc65\u00ba=(\n\u00b9\ud835\udf0e\ud835\udc65\u00ba2\u009d2,ifj\ud835\udc65j<1\u009d\ud835\udf0e2\nj\ud835\udc65j\u00000.5\u009d\ud835\udf0e2,otherwise(14.7.1)\nWhen\ud835\udf0eis very large, this loss is similar to the \u21131norm loss. When its value is smaller, the\nloss function is smoother.\ndef smooth_l1 (data, scalar):\nout =[]\nfor iindata:\nifabs(i) <1/(scalar **2):\nout.append(((scalar *i)**2)/2)\nelse :\nout.append( abs(i) -0.5 /(scalar **2))\nreturn torch .tensor(out)\nsigmas =[10,1,0.5]\nlines =['-','--','-.']\nx=torch .arange( -2,2,0.1)\nd2l.set_figsize()\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3ceb45b-6da4-4b25-a6b6-b518c2fb52ac": {"__data__": {"id_": "a3ceb45b-6da4-4b25-a6b6-b518c2fb52ac", "embedding": null, "metadata": {"page_label": "641", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de5c0c3b-b9b7-4b29-b428-67b61e26490a", "node_type": "4", "metadata": {"page_label": "641", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ef59ffc2e5c921ec25175cc6aca17efd102c7c1523b6ffa066f7f4e8247bc34a", "class_name": "RelatedNodeInfo"}}, "text": "641 Single Shot Multibox Detection\n(continued from previous page)\nfor l, s inzip(lines, sigmas):\ny=smooth_l1(x, scalar =s)\nd2l.plt.plot(x, y, l, label ='sigma= %.1f '%s)\nd2l.plt.legend();\nBesides, in the experiment we used cross-entropy loss for class prediction: denoting by \ud835\udc5d\ud835\udc57\nthepredictedprobabilityfortheground-truthclass \ud835\udc57,thecross-entropylossis \u0000log\ud835\udc5d\ud835\udc57. We\ncan also use the focal loss ( Linet al., 2017): given hyperparameters \ud835\udefe > 0and\ud835\udefc> 0, this\nloss is defined as:\n\u0000\ud835\udefc\u00b91\u0000\ud835\udc5d\ud835\udc57\u00ba\ud835\udefelog\ud835\udc5d\ud835\udc57. (14.7.2)\nAs we can see, increasing \ud835\udefecan effectively reduce the relative loss for well-classified ex-\namples (e.g., \ud835\udc5d\ud835\udc57>0.5) so the training can focus more on those difficult examples that are\nmisclassified.\ndef focal_loss (gamma, x):\nreturn -(1-x)**gamma *torch .log(x)\nx=torch .arange( 0.01 ,1,0.01 )\nfor l, gamma inzip(lines, [ 0,1,5]):\ny=d2l.plt.plot(x, focal_loss(gamma, x), l, label ='gamma= %.1f '%gamma)\nd2l.plt.legend();\n2.Due to space limitations, we have omitted some implementation details of the single\nshotmultiboxdetectionmodelinthissection. Canyoufurtherimprovethemodelinthe\nfollowing aspects:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "948d6ee0-2aad-45e7-a939-220a3aa4ee22": {"__data__": {"id_": "948d6ee0-2aad-45e7-a939-220a3aa4ee22", "embedding": null, "metadata": {"page_label": "642", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d4e2e04-ae67-4b1e-9fd0-6de65f487f74", "node_type": "4", "metadata": {"page_label": "642", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4d6462cafe3f9ebcaa06d1ac1a552d04c48fb7027f58fc5d6ee57dc4b0298db4", "class_name": "RelatedNodeInfo"}}, "text": "642 Computer Vision\n2181.When an object is much smaller compared with the image, the model could resize\nthe input image bigger.\n2.There are typically a vast number of negative anchor boxes. To make the class dis-\ntribution more balanced, we could downsample negative anchor boxes.\n3.Inthelossfunction,assigndifferentweighthyperparameterstotheclasslossandthe\noffset loss.\n4.Useothermethodstoevaluatetheobjectdetectionmodel, suchasthoseinthesingle\nshot multibox detection paper ( Liuetal., 2016).\nDiscussions218.\n14.8Region-basedCNNs (R-CNNs)\nBesides single shot multibox detection described in Section 14.7 , region-based CNNs or\nregions with CNN features (R-CNNs) are also among many pioneering approaches of ap-\nplying deep learning to object detection ( Girshick et al., 2014). In this section, we will\nintroducetheR-CNNanditsseriesofimprovements: thefastR-CNN( Girshick,2015 ),the\nfaster R-CNN ( Renet al., 2015), and the mask R-CNN ( Heet al., 2017). Due to limited\nspace, we will only focus on the design of these models.\n14.8.1R-CNNs\nTheR-CNNfirstextractsmany(e.g.,2000) regionproposals fromtheinputimage(e.g.,an-\nchorboxescanalsobeconsideredasregionproposals),labelingtheirclassesandbounding\nboxes (e.g., offsets).\n(Girshicketal., 2014)\nThen a CNN is used to perform forward propagation on each region proposal to extract\nits features. Next, features of each region proposal are used for predicting the class and\nbounding box of this region proposal.\ntFig. 14.8.1 The R-CNN model.\nFig.14.8.1 showstheR-CNNmodel. Moreconcretely,theR-CNNconsistsofthefollowing\nfour steps:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b4dbab65-0c5d-4328-9ffb-5163360f8500": {"__data__": {"id_": "b4dbab65-0c5d-4328-9ffb-5163360f8500", "embedding": null, "metadata": {"page_label": "643", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f47de9d4-04d6-4492-8f9e-fada8643d096", "node_type": "4", "metadata": {"page_label": "643", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1bd0713c256b6dce77da9885ec8f5e671433c3754186533f6306a1bcad4a514a", "class_name": "RelatedNodeInfo"}}, "text": "643 Region-based CNNs (R-CNNs)\n1.Performselective search to extract multiple high-quality region proposals on the input\nimage (Uijlingset al., 2013). These proposed regions are usually selected at multiple\nscales with different shapes and sizes. Each region proposal will be labeled with a class\nand a ground-truth bounding box.\n2.Choose a pretrained CNN and truncate it before the output layer. Resize each region\nproposal to the input size required by the network, and output the extracted features for\nthe region proposal through forward propagation.\n3.Take the extracted features and labeled class of each region proposal as an example.\nTrain multiple support vector machines to classify objects, where each support vector\nmachine individually determines whether the example contains a specific class.\n4.Take the extracted features and labeled bounding box of each region proposal as an\nexample. Train a linear regression model to predict the ground-truth bounding box.\nAlthough the R-CNN model uses pretrained CNNs to effectively extract image features, it\nis slow. Imagine that we select thousands of region proposals from a single input image:\nthis requires thousands of CNN forward propagations to perform object detection. This\nmassive computing load makes it infeasible to widely use R-CNNs in real-world applica-\ntions.\n14.8.2FastR-CNN\nThemainperformancebottleneckofanR-CNNliesintheindependentCNNforwardprop-\nagationforeachregionproposal,withoutsharingcomputation. Sincetheseregionsusually\nhave overlaps, independent feature extractions lead to much repeated computation. One\nof the major improvements of the fast R-CNN from the R-CNN is that the CNN forward\npropagation is only performed on the entire image ( Girshick, 2015 ).\ntFig. 14.8.2 The fast R-CNN model.\nFig.14.8.2 describesthefastR-CNNmodel. Itsmajorcomputationsareasfollows:\n1.Compared with the R-CNN, in the fast R-CNN the input of the CNN for feature extrac-\ntion is the entire image, rather than individual region proposals. Moreover, this CNN is\ntrainable. Given an input image, let the shape of the CNN output be 1\u0002\ud835\udc50\u0002\u210e1\u0002\ud835\udc641.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59ce14e2-9c6a-4e15-a9e4-c763084ddb9c": {"__data__": {"id_": "59ce14e2-9c6a-4e15-a9e4-c763084ddb9c", "embedding": null, "metadata": {"page_label": "644", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "448cb869-e223-48a6-9466-f465b6af6234", "node_type": "4", "metadata": {"page_label": "644", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5207b83108523771fa57dad5655afc69c4ab2b601bf0a59533059223d454a967", "class_name": "RelatedNodeInfo"}}, "text": "644 Computer Vision\n2.Suppose that selective search generates \ud835\udc5bregion proposals. These region proposals (of\ndifferentshapes)markregionsofinterest(ofdifferentshapes)ontheCNNoutput. Then\nthese regions of interest further extract features of the same shape (say height \u210e2and\nwidth\ud835\udc642are specified) in order to be easily concatenated. To achieve this, the fast R-\nCNN introduces the region of interest (RoI) pooling layer: the CNN output and region\nproposalsareinputintothislayer,outputtingconcatenatedfeaturesofshape \ud835\udc5b\u0002\ud835\udc50\u0002\u210e2\u0002\n\ud835\udc642that are further extracted for all the region proposals.\n3.Using a fully connected layer, transform the concatenated features into an output of\nshape\ud835\udc5b\u0002\ud835\udc51, where\ud835\udc51depends on the model design.\n4.Predict the class and bounding box for each of the \ud835\udc5bregion proposals. More concretely,\nin class and bounding box prediction, transform the fully connected layer output into\nan output of shape \ud835\udc5b\u0002\ud835\udc5e(\ud835\udc5eis the number of classes) and an output of shape \ud835\udc5b\u00024,\nrespectively. The class prediction uses softmax regression.\nTheregionofinterestpoolinglayerproposedinthefastR-CNNisdifferentfromthepooling\nlayer introduced in Section 7.5 . In the pooling layer, we indirectly control the output shape\nbyspecifyingsizesofthepoolingwindow, padding, andstride. Incontrast, wecandirectly\nspecify the output shape in the region of interest pooling layer.\nFor example, let\u2019s specify the output height and width for each region as \u210e2and\ud835\udc642, re-\nspectively. For any region of interest window of shape \u210e\u0002\ud835\udc64, this window is divided\ninto a\u210e2\u0002\ud835\udc642grid of subwindows, where the shape of each subwindow is approximately\n\u00b9\u210e\u009d\u210e2\u00ba\u0002\u00b9\ud835\udc64\u009d\ud835\udc642\u00ba. Inpractice, theheightandwidthofanysubwindowshallberoundedup,\nand the largest element shall be used as output of the subwindow. Therefore, the region of\ninterest pooling layer can extract features of the same shape even when regions of interest\nhave different shapes.\nAs an illustrative example, in Fig. 14.8.3 , the upper-left 3\u00023region of interest is selected\nona 4\u00024input. Forthisregionofinterest,weusea 2\u00022regionofinterestpoolinglayerto\nobtain a 2\u00022output. Note that each of the four divided subwindows contains elements 0,\n1, 4, and 5 (5 is the maximum); 2 and 6 (6 is the maximum); 8 and 9 (9 is the maximum);\nand 10.\ntFig. 14.8.3 A 2\u00022 region of interest pooling layer.\nBelow we demonstrate the computation of the region of interest pooling layer. Suppose\nthat the height and width of the CNN-extracted features Xare both 4, and there is only a\nsingle channel.\nimport torch\nimport torchvision\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6edccddd-adc5-4b4f-8c73-7cb85e9f18c7": {"__data__": {"id_": "6edccddd-adc5-4b4f-8c73-7cb85e9f18c7", "embedding": null, "metadata": {"page_label": "645", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c8b6da6-1589-449e-ad61-3103f05c0bc9", "node_type": "4", "metadata": {"page_label": "645", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6e68015906bd3e9136fe35e5a4db3294178b3dd5074efb372f699e655e988cd0", "class_name": "RelatedNodeInfo"}}, "text": "645 Region-based CNNs (R-CNNs)\n(continued from previous page)\nX=torch .arange( 16.).reshape( 1,1,4,4)\nX\ntensor([[[[ 0.,1.,2.,3.],\n[4.,5.,6.,7.],\n[8.,9.,10.,11.],\n[12.,13.,14.,15.]]]])\nLet\u2019s further suppose that the height and width of the input image are both 40 pixels and\nthat selective search generates two region proposals on this image. Each region proposal is\nexpressed as five elements: its object class followed by the \u00b9\ud835\udc65,\ud835\udc66\u00ba-coordinates of its upper-\nleft and lower-right corners.\nrois =torch .Tensor([[ 0,0,0,20,20], [ 0,0,10,30,30]])\nBecause the height and width of Xare1\u009d10of the height and width of the input image,\nthe coordinates of the tworegion proposals are multiplied by0.1 according to the specified\nspatial_scale argument. Then the two regions of interest are marked on XasX[:, :,\n0:3, 0:3] andX[:, :, 1:4, 0:4] , respectively. Finally in the 2\u00022region of interest\npooling, each region of interest is divided into a grid of sub-windows to further extract\nfeatures of the same shape 2\u00022.\ntorchvision .ops.roi_pool(X, rois, output_size =(2,2), spatial_scale =0.1)\ntensor([[[[ 5.,6.],\n[9.,10.]]],\n[[[ 9.,11.],\n[13.,15.]]]])\n14.8.3FasterR-CNN\nTo be more accurate in object detection, the fast R-CNN model usually has to generate\na lot of region proposals in selective search. To reduce region proposals without loss of\naccuracy, the faster R-CNN proposes to replace selective search with a region proposal\nnetwork (Renetal., 2015).\nFig. 14.8.4 shows the faster R-CNN model. Compared with the fast R-CNN, the faster R-\nCNN only changes the region proposal method from selective search to a region proposal\nnetwork. The rest of the model remain unchanged. The region proposal network works in\nthe following steps:\n1.Use a 3\u00023convolutional layer with padding of 1 to transform the CNN output to a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74f0acb7-8822-41f1-b284-cae932aa3acd": {"__data__": {"id_": "74f0acb7-8822-41f1-b284-cae932aa3acd", "embedding": null, "metadata": {"page_label": "646", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ddf90a3c-417a-4f55-9e18-7c210bd23161", "node_type": "4", "metadata": {"page_label": "646", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d251088e555db0ab7322cf89526fe5e4f9b0326780e0e08704551212cb47754a", "class_name": "RelatedNodeInfo"}}, "text": "646 Computer Vision\ntFig. 14.8.4 The faster R-CNN model.\nnew output with \ud835\udc50channels. In this way, each unit along the spatial dimensions of the\nCNN-extracted feature maps gets a new feature vector of length \ud835\udc50.\n2.Centered on each pixel of the feature maps, generate multiple anchor boxes of different\nscales and aspect ratios and label them.\n3.Using the length- \ud835\udc50feature vector at the center of each anchor box, predict the binary\nclass (background or objects) and bounding box for this anchor box.\n4.Consider those predicted bounding boxes whose predicted classes are objects. Remove\noverlappedresultsusingnon-maximumsuppression. Theremainingpredictedbounding\nboxesforobjectsaretheregionproposalsrequiredbytheregionofinterestpoolinglayer.\nIt is worth noting that, as part of the faster R-CNN model, the region proposal network\nis jointly trained with the rest of the model. In other words, the objective function of the\nfaster R-CNN includes not only the class and bounding box prediction in object detection,\nbutalsothebinaryclassandboundingboxpredictionofanchorboxesintheregionproposal\nnetwork. As a result of the end-to-end training, the region proposal network learns how to\ngenerate high-quality region proposals, so as to stay accurate in object detection with a\nreduced number of region proposals that are learned from data.\n14.8.4Mask R-CNN\nIn the training dataset, if pixel-level positions of object are also labeled on images, the\nmask R-CNN can effectively leverage such detailed labels to further improve the accuracy\nof object detection ( Heetal., 2017).\nAs shown in Fig. 14.8.5 , the mask R-CNN is modified based on the faster R-CNN. Specif-\nically, the mask R-CNN replaces the region of interest pooling layer with the region of\ninterest (RoI) alignment layer. This region of interest alignment layer uses bilinear inter-\npolation to preserve the spatial information on the feature maps, which is more suitable for\npixel-level prediction. The output of this layer contains feature maps of the same shape for\nall the regions of interest. They are used to predict not only the class and bounding box\nfor each region of interest, but also the pixel-level position of the object through an addi-\ntional fully convolutional network. More details on using a fully convolutional network to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6ddd1b3-a9de-4253-a39b-4b90e3ad4446": {"__data__": {"id_": "d6ddd1b3-a9de-4253-a39b-4b90e3ad4446", "embedding": null, "metadata": {"page_label": "647", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "927e5ccc-9654-4efa-b67d-336e58e045a0", "node_type": "4", "metadata": {"page_label": "647", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5a845cefc2aec2d8a42ebb4921a7d66a5154f2aead6c0a3c8bf72265e0545b9d", "class_name": "RelatedNodeInfo"}}, "text": "647 Region-based CNNs (R-CNNs)\ntFig. 14.8.5 The mask R-CNN model.\n219predict pixel-level semantics of an image will be provided in subsequent sections of this\nchapter.\n14.8.5Summary\n\u000fTheR-CNNextractsmanyregionproposalsfromtheinputimage,usesaCNNtoperform\nforward propagation on each region proposal to extract its features, then uses these\nfeatures to predict the class and bounding box of this region proposal.\n\u000fOne of the majorimprovements of the fastR-CNN from the R-CNNis that the CNN for-\nward propagation is only performed on the entire image. It also introduces the region\nof interest pooling layer, so that features of the same shape can be further extracted\nfor regions of interest that have different shapes.\n\u000fThe faster R-CNN replaces the selective search used in the fast R-CNN with a jointly\ntrained region proposal network, so that the former can stay accurate in object detec-\ntion with a reduced number of region proposals.\n\u000fBased on the faster R-CNN, the mask R-CNN additionally introduces a fully convolu-\ntional network, so as to leverage pixel-level labels to further improve the accuracy of\nobject detection.\n14.8.6Exercises\n1.Canweframeobjectdetectionasasingleregressionproblem,suchaspredictingbound-\ning boxes and class probabilities? You may refer to the design of the YOLO model\n(Redmonetal., 2016).\n2.Compare single shot multibox detection with the methods introduced in this section.\nWhat are their major differences? You may refer to Figure 2 of Zhao etal.(2019).\nDiscussions219.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1506, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c0199ad-7f2f-402f-82ea-41f2f139dd68": {"__data__": {"id_": "8c0199ad-7f2f-402f-82ea-41f2f139dd68", "embedding": null, "metadata": {"page_label": "648", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "adff3466-6057-4fc7-af37-89d64ecc8d78", "node_type": "4", "metadata": {"page_label": "648", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c2db4014d0c1927812e08c4f3110fe30a20596d5a57b245e8107ba9611a85dc5", "class_name": "RelatedNodeInfo"}}, "text": "648 Computer Vision\n22014.9Semantic Segmentation and the Dataset\nWhen discussing object detection tasks in Section 14.3 \u2013Section 14.8 , rectangular bound-\ning boxes are used to label and predict objects in images. This section will discuss the\nproblem of semantic segmentation , which focuses on how to divide an image into regions\nbelonging to different semantic classes. Different from object detection, semantic seg-\nmentation recognizes and understands what are in images in pixel level: its labeling and\nprediction of semantic regions are in pixel level. Fig. 14.9.1 shows the labels of the dog,\ncat, and background of the image in semantic segmentation. Compared with in object de-\ntection, the pixel-level borders labeled in semantic segmentation are obviously more fine-\ngrained.\ntFig. 14.9.1 Labels of the dog, cat, and background of the image in semantic segmentation.\n14.9.1ImageSegmentation and Instance Segmentation\nTherearealsotwoimportanttasksinthefieldofcomputervisionthataresimilartoseman-\ntic segmentation, namely image segmentation and instance segmentation. We will briefly\ndistinguish them from semantic segmentation as follows.\n\u000fImage segmentation divides an image into several constituent regions. The methods for\nthis type of problem usually make use of the correlation between pixels in the image.\nIt does not need label information about image pixels during training, and it cannot\nguarantee that the segmented regions will have the semantics that we hope to obtain\nduring prediction. Taking the image in Fig. 14.9.1 as input, image segmentation may\ndivide the dog into two regions: one covers the mouth and eyes which are mainly\nblack, and the other covers the rest of the body which is mainly yellow.\n\u000fInstancesegmentation isalsocalled simultaneousdetectionandsegmentation . Itstudies\nhow to recognize the pixel-level regions of each object instance in an image. Differ-\nent from semantic segmentation, instance segmentation needs to distinguish not only\nsemantics, but also different object instances. For example, if there are two dogs in\nthe image, instance segmentation needs to distinguish which of the two dogs a pixel\nbelongs to.\n14.9.2The PascalVOC2012Semantic Segmentation Dataset\nOn of the most important semantic segmentation dataset is Pascal VOC2012220. In the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "88f772ee-d8f6-42c0-b44c-8d67c53723af": {"__data__": {"id_": "88f772ee-d8f6-42c0-b44c-8d67c53723af", "embedding": null, "metadata": {"page_label": "649", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf288c58-6c63-41c0-b8e9-5923b61e79e7", "node_type": "4", "metadata": {"page_label": "649", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b53e7447d7f57000b8b2135568b41e900bc3fdd018feb85e7ee56cf43444f3d1", "class_name": "RelatedNodeInfo"}}, "text": "649 Semantic Segmentation and the Dataset\nfollowing, we will take a look at this dataset.\n%matplotlib inline\nimport os\nimport torch\nimport torchvision\nfrom d2l import torch asd2l\nThe tar file of the dataset is about 2 GB, so it may take a while to download the file. The\nextracted dataset is located at ../data/VOCdevkit/VOC2012 .\n#@save\nd2l.DATA_HUB[ 'voc2012 ']=(d2l .DATA_URL +'VOCtrainval_11-May-2012.tar ',\n'4e443f8a2eca6b1dac8a6c57641b67dd40621a49 ')\nvoc_dir =d2l.download_extract( 'voc2012 ','VOCdevkit/VOC2012 ')\nDownloading ../data /VOCtrainval_11 -May-2012. tar from http ://d2l-data .s3-\n\u21a9!accelerate .amazonaws .com/VOCtrainval_11 -May-2012. tar...\nAfter entering the path ../data/VOCdevkit/VOC2012 , we can see the different compo-\nnents of the dataset. The ImageSets/Segmentation path contains text files that specify\ntraining and test samples, while the JPEGImages andSegmentationClass paths store the\ninput image and label for each example, respectively. The label here is also in the im-\nage format, with the same size as its labeled input image. Besides, pixels with the same\ncolor in any label image belong to the same semantic class. The following defines the\nread_voc_images functiontoreadalltheinputimagesandlabelsintothememory.\n#@save\ndef read_voc_images (voc_dir, is_train =True ):\n\"\"\"Read all VOC feature and label images.\"\"\"\ntxt_fname =os.path .join(voc_dir, 'ImageSets ','Segmentation ',\n'train.txt 'ifis_train else 'val.txt ')\nmode =torchvision .io.image .ImageReadMode .RGB\nwith open (txt_fname, 'r')asf:\nimages =f.read() .split()\nfeatures, labels =[], []\nfor i, fname inenumerate (images):\nfeatures .append(torchvision .io.read_image(os .path .join(\nvoc_dir, 'JPEGImages ',f'{fname }.jpg ')))\nlabels .append(torchvision .io.read_image(os .path .join(\nvoc_dir, 'SegmentationClass ',f'{fname }.png '), mode))\nreturn features, labels\ntrain_features, train_labels =read_voc_images(voc_dir, True )\nWedrawthefirstfiveinputimagesandtheirlabels. Inthelabelimages,whiteandblackrep-\nresent borders and background, respectively, while the other colors correspond to different\nclasses.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2a3f180-9fdf-4c08-bb2e-89ed1dcdc1b2": {"__data__": {"id_": "a2a3f180-9fdf-4c08-bb2e-89ed1dcdc1b2", "embedding": null, "metadata": {"page_label": "650", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad17cb25-0e69-4f5a-a218-697c597f70db", "node_type": "4", "metadata": {"page_label": "650", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "23cba1fc556d191a63da13e23ac070f59f7a80715ee17850f070c9b3c19aa962", "class_name": "RelatedNodeInfo"}}, "text": "650 Computer Vision\nn=5\nimgs =train_features[:n] +train_labels[:n]\nimgs =[img .permute( 1,2,0)for img inimgs]\nd2l.show_images(imgs, 2, n);\nNext,weenumeratetheRGBcolorvaluesandclassnamesforallthelabelsinthisdataset.\n#@save\nVOC_COLORMAP =[[0,0,0], [ 128,0,0], [ 0,128,0], [ 128,128,0],\n[0,0,128], [ 128,0,128], [ 0,128,128], [ 128,128,128],\n[64,0,0], [ 192,0,0], [ 64,128,0], [ 192,128,0],\n[64,0,128], [ 192,0,128], [ 64,128,128], [ 192,128,128],\n[0,64,0], [ 128,64,0], [ 0,192,0], [ 128,192,0],\n[0,64,128]]\n#@save\nVOC_CLASSES =['background ','aeroplane ','bicycle ','bird ','boat ',\n'bottle ','bus','car','cat','chair ','cow',\n'diningtable ','dog','horse ','motorbike ','person ',\n'potted plant ','sheep ','sofa ','train ','tv/monitor ']\nWith the two constants defined above, we can conveniently find the class index for each\npixel in a label. We define the voc_colormap2label function to build the mapping from\nthe above RGB color values to class indices, and the voc_label_indices function to map\nany RGB values to their class indices in this Pascal VOC2012 dataset.\n#@save\ndef voc_colormap2label ():\n\"\"\"Build the mapping from RGB to class indices for VOC labels.\"\"\"\ncolormap2label =torch .zeros( 256 **3, dtype =torch .long)\nfor i, colormap inenumerate (VOC_COLORMAP):\ncolormap2label[\n(colormap[ 0]*256 +colormap[ 1])*256 +colormap[ 2]]=i\nreturn colormap2label\n#@save\ndef voc_label_indices (colormap, colormap2label):\n\"\"\"Map any RGB values in VOC labels to their class indices.\"\"\"\ncolormap =colormap .permute( 1,2,0).numpy() .astype( 'int32 ')\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ccf0433-a8d8-4945-b0dd-b6da5f6376fc": {"__data__": {"id_": "6ccf0433-a8d8-4945-b0dd-b6da5f6376fc", "embedding": null, "metadata": {"page_label": "651", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee24f0f3-22e0-4115-ba51-42484dc95d81", "node_type": "4", "metadata": {"page_label": "651", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6e9e85b404f08493dfd6739ef2830c27038d5c857f44e56f03ec899ede2fca91", "class_name": "RelatedNodeInfo"}}, "text": "651 Semantic Segmentation and the Dataset\n(continued from previous page)\nidx =((colormap[:, :, 0]*256 +colormap[:, :, 1])*256\n+colormap[:, :, 2])\nreturn colormap2label[idx]\nFor example, in the first example image, the class index for the front part of the airplane is\n1, while the background index is 0.\ny=voc_label_indices(train_labels[ 0], voc_colormap2label())\ny[105:115,130:140], VOC_CLASSES[ 1]\n(tensor([[ 0,0,0,0,0,0,0,0,0,1],\n[0,0,0,0,0,0,0,1,1,1],\n[0,0,0,0,0,0,1,1,1,1],\n[0,0,0,0,0,1,1,1,1,1],\n[0,0,0,0,0,1,1,1,1,1],\n[0,0,0,0,1,1,1,1,1,1],\n[0,0,0,0,0,1,1,1,1,1],\n[0,0,0,0,0,1,1,1,1,1],\n[0,0,0,0,0,0,1,1,1,1],\n[0,0,0,0,0,0,0,0,1,1]]),\n'aeroplane ')\nData Preprocessing\nIn previous experiments such as in Section 8.1 \u2013Section 8.4 , images are rescaled to fit the\nmodel\u2019srequiredinputshape. However,insemanticsegmentation,doingsorequiresrescal-\ningthepredictedpixelclassesbacktotheoriginalshapeoftheinputimage. Suchrescaling\nmay be inaccurate, especially for segmented regions with different classes. To avoid this\nissue, we crop the image to a fixedshape instead of rescaling. Specifically, using random\ncropping from image augmentation, we crop the same area of the input image and the la-\nbel.\n#@save\ndef voc_rand_crop (feature, label, height, width):\n\"\"\"Randomly crop both feature and label images.\"\"\"\nrect =torchvision .transforms .RandomCrop .get_params(\nfeature, (height, width))\nfeature =torchvision .transforms .functional .crop(feature, *rect)\nlabel =torchvision .transforms .functional .crop(label, *rect)\nreturn feature, label\nimgs =[]\nfor _inrange (n):\nimgs +=voc_rand_crop(train_features[ 0], train_labels[ 0],200,300)\nimgs =[img .permute( 1,2,0)for img inimgs]\nd2l.show_images(imgs[:: 2]+imgs[ 1::2],2, n);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1725, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11b22650-43e3-4840-aea7-2345b75b4cc7": {"__data__": {"id_": "11b22650-43e3-4840-aea7-2345b75b4cc7", "embedding": null, "metadata": {"page_label": "652", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee8ce06d-e9fb-4f43-87c2-e25b8f16142a", "node_type": "4", "metadata": {"page_label": "652", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f453ada22bc3d7c41ee8bc21c052f4f0a77372a7d8997cf39a7920a9c0f2cc39", "class_name": "RelatedNodeInfo"}}, "text": "652 Computer Vision\nCustom Semantic Segmentation DatasetClass\nWe define a custom semantic segmentation dataset class VOCSegDataset by inheriting the\nDataset class provided by high-level APIs. By implementing the __getitem__ function,\nwe can arbitrarily access the input image indexed as idxin the dataset and the class index\nof each pixel in this image. Since some images in the dataset have a smaller size than\nthe output size of random cropping, these examples are filtered out by a custom filter\nfunction. In addition, we also define the normalize_image function to standardize the\nvalues of the three RGB channels of input images.\n#@save\nclass VOCSegDataset (torch .utils .data .Dataset):\n\"\"\"A customized dataset to load the VOC dataset.\"\"\"\ndef __init__ (self , is_train, crop_size, voc_dir):\nself .transform =torchvision .transforms .Normalize(\nmean =[0.485 ,0.456 ,0.406 ], std =[0.229 ,0.224 ,0.225 ])\nself .crop_size =crop_size\nfeatures, labels =read_voc_images(voc_dir, is_train =is_train)\nself .features =[self .normalize_image(feature)\nfor feature inself .filter(features)]\nself .labels =self .filter(labels)\nself .colormap2label =voc_colormap2label()\nprint ('read '+str(len(self .features)) +'examples ')\ndef normalize_image (self , img):\nreturn self .transform(img .float() /255)\ndef filter (self , imgs):\nreturn [img for img inimgs if(\nimg.shape[ 1]>=self .crop_size[ 0]and\nimg.shape[ 2]>=self .crop_size[ 1])]\ndef __getitem__ (self , idx):\nfeature, label =voc_rand_crop( self .features[idx], self .labels[idx],\n*self .crop_size)\nreturn (feature, voc_label_indices(label, self .colormap2label))\ndef __len__ (self ):\nreturn len(self .features)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5979a0e-a171-4043-8a90-254604770c94": {"__data__": {"id_": "e5979a0e-a171-4043-8a90-254604770c94", "embedding": null, "metadata": {"page_label": "653", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f6433652-fc14-40b7-ae20-cf5bd618af00", "node_type": "4", "metadata": {"page_label": "653", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "453a52ff8e52fe646a538259c27021a0bb8229a7a4ac05051d4e5ea995104f24", "class_name": "RelatedNodeInfo"}}, "text": "653 Semantic Segmentation and the Dataset\nReadingthe Dataset\nWe use the custom VOCSegDatase t class to create instances of the training set and test set,\nrespectively. Suppose that we specify that the output shape of randomly cropped images is\n320\u0002480. Below we can view the number of examples that are retained in the training set\nand test set.\ncrop_size =(320,480)\nvoc_train =VOCSegDataset( True , crop_size, voc_dir)\nvoc_test =VOCSegDataset( False , crop_size, voc_dir)\nread 1114 examples\nread 1078 examples\nSetting the batch size to 64, we define the data iterator for the training set. Let\u2019s print\nthe shape of the first minibatch. Different from in image classification or object detection,\nlabels here are three-dimensional tensors.\nbatch_size =64\ntrain_iter =torch .utils .data .DataLoader(voc_train, batch_size, shuffle =True ,\ndrop_last =True ,\nnum_workers =d2l.get_dataloader_workers())\nfor X, Y intrain_iter:\nprint (X.shape)\nprint (Y.shape)\nbreak\ntorch .Size([ 64,3,320,480])\ntorch .Size([ 64,320,480])\nPuttingIt All Together\nFinally, we define the following load_data_voc function to download and read the Pascal\nVOC2012 semantic segmentation dataset. It returns data iterators for both the training and\ntest datasets.\n#@save\ndef load_data_voc (batch_size, crop_size):\n\"\"\"Load the VOC semantic segmentation dataset.\"\"\"\nvoc_dir =d2l.download_extract( 'voc2012 ', os .path .join(\n'VOCdevkit ','VOC2012 '))\nnum_workers =d2l.get_dataloader_workers()\ntrain_iter =torch .utils .data .DataLoader(\nVOCSegDataset( True , crop_size, voc_dir), batch_size,\nshuffle =True , drop_last =True , num_workers =num_workers)\ntest_iter =torch .utils .data .DataLoader(\nVOCSegDataset( False , crop_size, voc_dir), batch_size,\ndrop_last =True , num_workers =num_workers)\nreturn train_iter, test_iter", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bc1cabc-b262-42f1-a20a-d1cafbfa15b2": {"__data__": {"id_": "9bc1cabc-b262-42f1-a20a-d1cafbfa15b2", "embedding": null, "metadata": {"page_label": "654", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b466cfa-d99a-46a1-83ee-6d3c084760f2", "node_type": "4", "metadata": {"page_label": "654", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d0a6756446bbabfc61cb46e20da96a031561d20dddc8378d137bd50bb384609a", "class_name": "RelatedNodeInfo"}}, "text": "654 Computer Vision\n22114.9.3Summary\n\u000fSemantic segmentation recognizes and understands what are in an image in pixel level\nby dividing the image into regions belonging to different semantic classes.\n\u000fOne of the most important semantic segmentation dataset is Pascal VOC2012.\n\u000fIn semantic segmentation, since the input image and label correspond one-to-one on the\npixel, the input image is randomly cropped to a fixed shape rather than rescaled.\n14.9.4Exercises\n1.How can semantic segmentation be applied in autonomous vehicles and medical image\ndiagnostics? Can you think of other applications?\n2.Recall the descriptions of data augmentation in Section 14.1 . Which of the image aug-\nmentation methods used in image classification would be infeasible to be applied in\nsemantic segmentation?\nDiscussions221.\n14.10TransposedConvolution\nThe CNN layers we have seen so far, such as convolutional layers ( Section 7.2 ) and pool-\ning layers ( Section 7.5 ), typically reduce (downsample) the spatial dimensions (height and\nwidth) of the input, or keep them unchanged. In semantic segmentation that classifies at\npixel-level, it will be convenient if the spatial dimensions of the input and output are the\nsame. For example, the channel dimension at one output pixel can hold the classification\nresults for the input pixel at the same spatial position.\nTo achieve this, especially after the spatial dimensions are reduced by CNN layers, we\ncan use another type of CNN layers that can increase (upsample) the spatial dimensions of\nintermediatefeaturemaps. Inthissection,wewillintroduce transposedconvolution ,which\nis also called fractionally-strided convolution (Dumoulin and Visin, 2016 ), for reversing\ndownsampling operations by the convolution.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n14.10.1Basic Operation\nIgnoringchannelsfornow,let\u2019sbeginwiththebasictransposedconvolutionoperationwith\nstride of1 andno padding. Supposethat weare givena \ud835\udc5b\u210e\u0002\ud835\udc5b\ud835\udc64inputtensorand a \ud835\udc58\u210e\u0002\ud835\udc58\ud835\udc64\nkernel. Sliding the kernel window with stride of 1 for \ud835\udc5b\ud835\udc64times in each row and \ud835\udc5b\u210etimes", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7f995636-794e-411a-bbe0-d907fb5dfd88": {"__data__": {"id_": "7f995636-794e-411a-bbe0-d907fb5dfd88", "embedding": null, "metadata": {"page_label": "655", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c05c809-20e5-4c2c-8f55-119d4bc65ddf", "node_type": "4", "metadata": {"page_label": "655", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fd0834fdd1790b069359fb921008a62bd9b064187af69dc46e8e5486171caa86", "class_name": "RelatedNodeInfo"}}, "text": "655 Transposed Convolution\nin each column yields a total of \ud835\udc5b\u210e\ud835\udc5b\ud835\udc64intermediate results. Each intermediate result is\na\u00b9\ud835\udc5b\u210e\u00b8\ud835\udc58\u210e\u00001\u00ba\u0002\u00b9\ud835\udc5b\ud835\udc64\u00b8\ud835\udc58\ud835\udc64\u00001\u00batensor that are initialized as zeros. To compute each\nintermediate tensor, each element in the input tensor is multiplied by the kernel so that\nthe resulting \ud835\udc58\u210e\u0002\ud835\udc58\ud835\udc64tensor replaces a portion in each intermediate tensor. Note that the\nposition of the replaced portion in each intermediate tensor corresponds to the position of\nthe element in the input tensor used for the computation. In the end, all the intermediate\nresults are summed over to produce the output.\nAs an example, Fig. 14.10.1 illustrates how transposed convolution with a 2\u00022kernel is\ncomputed for a 2\u00022input tensor.\ntFig. 14.10.1 Transposed convolution with a 2 \u00022 kernel. The shaded portions are a portion of an\nintermediate tensor as well as the input and kernel tensor elements used for the\ncomputation.\nWe can implement this basic transposed convolution operation trans_conv for a input\nmatrix Xand a kernel matrix K.\ndef trans_conv (X, K):\nh, w =K.shape\nY=torch .zeros((X .shape[ 0]+h-1, X.shape[ 1]+w-1))\nfor iinrange (X.shape[ 0]):\nfor jinrange (X.shape[ 1]):\nY[i: i +h, j: j +w]+=X[i, j] *K\nreturn Y\nIn contrast to the regular convolution (in Section 7.2 ) thatreduces input elements via the\nkernel, the transposed convolution broadcasts input elements via the kernel, thereby pro-\nducing an output that is larger than the input. We can construct the input tensor Xand the\nkernel tensor KfromFig. 14.10.1 to validate the output of the above implementation of the\nbasic two-dimensional transposed convolution operation.\nX=torch .tensor([[ 0.0,1.0], [ 2.0,3.0]])\nK=torch .tensor([[ 0.0,1.0], [ 2.0,3.0]])\ntrans_conv(X, K)\ntensor([[ 0.,0.,1.],\n[0.,4.,6.],\n[4.,12.,9.]])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6fda905-21ad-4f80-b0c3-f8e3a61e7bac": {"__data__": {"id_": "e6fda905-21ad-4f80-b0c3-f8e3a61e7bac", "embedding": null, "metadata": {"page_label": "656", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cad1de93-88ca-4e08-8174-8fb3f4ac4477", "node_type": "4", "metadata": {"page_label": "656", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "972f049eff383c776f6a2f42d175761b78bafdead608c751e035caef8b34ff37", "class_name": "RelatedNodeInfo"}}, "text": "656 Computer Vision\nAlternatively, when the input Xand kernel Kare both four-dimensional tensors, we can use\nhigh-level APIs to obtain the same results.\nX, K =X.reshape( 1,1,2,2), K .reshape( 1,1,2,2)\ntconv =nn.ConvTranspose2d( 1,1, kernel_size =2, bias =False )\ntconv .weight .data =K\ntconv(X)\ntensor([[[[ 0.,0.,1.],\n[0.,4.,6.],\n[4.,12.,9.]]]], grad_fn =<ConvolutionBackward0 >)\n14.10.2Padding,Strides, and Multiple Channels\nDifferent from in the regular convolution where padding is applied to input, it is applied to\noutput in the transposed convolution. For example, when specifying the padding number\non either side of the height and width as 1, the first and last rows and columns will be\nremoved from the transposed convolution output.\ntconv =nn.ConvTranspose2d( 1,1, kernel_size =2, padding =1, bias =False )\ntconv .weight .data =K\ntconv(X)\ntensor([[[[ 4.]]]], grad_fn =<ConvolutionBackward0 >)\nIn the transposed convolution, strides are specified for intermediate results (thus output),\nnot for input. Using the same input and kernel tensors from Fig. 14.10.1 , changing the\nstride from 1 to 2 increases both the height and weight of intermediate tensors, hence the\noutput tensor in Fig. 14.10.2 .\nThe following code snippet can validate the transposed convolution output for stride of 2\ninFig. 14.10.2 .\ntconv =nn.ConvTranspose2d( 1,1, kernel_size =2, stride =2, bias =False )\ntconv .weight .data =K\ntconv(X)\ntensor([[[[ 0.,0.,0.,1.],\n[0.,0.,2.,3.],\n[0.,2.,0.,3.],\n[4.,6.,6.,9.]]]], grad_fn =<ConvolutionBackward0 >)\nFor multiple input and output channels, the transposed convolution works in the same way\nas the regular convolution. Suppose that the input has \ud835\udc50\ud835\udc56channels, and that the transposed\nconvolution assigns a \ud835\udc58\u210e\u0002\ud835\udc58\ud835\udc64kernel tensor to each input channel. When multiple output\nchannels are specified, we will have a \ud835\udc50\ud835\udc56\u0002\ud835\udc58\u210e\u0002\ud835\udc58\ud835\udc64kernel for each output channel.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6162a03f-03d6-4b5f-ab67-504eea69f401": {"__data__": {"id_": "6162a03f-03d6-4b5f-ab67-504eea69f401", "embedding": null, "metadata": {"page_label": "657", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75b5f002-9500-448f-9455-87f3442f42ed", "node_type": "4", "metadata": {"page_label": "657", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "de8eaa39ea60583f18448eeeb7bb4bfe232e85e58c7da2f357a3ef3a701ebbc9", "class_name": "RelatedNodeInfo"}}, "text": "657 Transposed Convolution\ntFig. 14.10.2 Transposed convolution with a 2 \u00022 kernel with stride of 2. The shaded portions are a\nportion of an intermediate tensor as well as the input and kernel tensor elements used for\nthe computation.\nAs in all, if we feed Xinto a convolutional layer \ud835\udc53to output Y=\ud835\udc53\u00b9X\u00baand create a trans-\nposed convolutional layer \ud835\udc54with the same hyperparameters as \ud835\udc53except for the number of\noutput channels being the number of channels in X, then\ud835\udc54\u00b9\ud835\udc4c\u00bawill have the same shape as\nX. This can be illustrated in the following example.\nX=torch .rand(size =(1,10,16,16))\nconv =nn.Conv2d( 10,20, kernel_size =5, padding =2, stride =3)\ntconv =nn.ConvTranspose2d( 20,10, kernel_size =5, padding =2, stride =3)\ntconv(conv(X)) .shape ==X.shape\nTrue\n14.10.3Connectionto Matrix Transposition\nThe transposed convolution is named after the matrix transposition. To explain, let\u2019s first\nseehowtoimplementconvolutionsusingmatrixmultiplications. Intheexamplebelow,we\ndefine a 3\u00023input Xand a 2\u00022convolution kernel K, and then use the corr2dfunction\nto compute the convolution output Y.\nX=torch .arange( 9.0).reshape( 3,3)\nK=torch .tensor([[ 1.0,2.0], [ 3.0,4.0]])\nY=d2l.corr2d(X, K)\nY\ntensor([[ 27.,37.],\n[57.,67.]])\nNext, we rewrite the convolution kernel Kas a sparse weight matrix Wcontaining a lot of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d680c7f-75c2-4c0f-a2dd-9eddf2700330": {"__data__": {"id_": "6d680c7f-75c2-4c0f-a2dd-9eddf2700330", "embedding": null, "metadata": {"page_label": "658", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5155a425-bbba-4e0d-b81c-bd73509a22e9", "node_type": "4", "metadata": {"page_label": "658", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b368f0a0a02a64f177698913efb6544f81574afa2b1c124b9056680983836e05", "class_name": "RelatedNodeInfo"}}, "text": "658 Computer Vision\nzeros. The shape of the weight matrix is ( 4,9), where the non-zero elements come from\nthe convolution kernel K.\ndef kernel2matrix (K):\nk, W =torch .zeros( 5), torch .zeros(( 4,9))\nk[:2], k[ 3:5]=K[0, :], K[ 1, :]\nW[0, :5], W[ 1,1:6], W[ 2,3:8], W[ 3,4:]=k, k, k, k\nreturn W\nW=kernel2matrix(K)\nW\ntensor([[ 1.,2.,0.,3.,4.,0.,0.,0.,0.],\n[0.,1.,2.,0.,3.,4.,0.,0.,0.],\n[0.,0.,0.,1.,2.,0.,3.,4.,0.],\n[0.,0.,0.,0.,1.,2.,0.,3.,4.]])\nConcatenate the input Xrow by row to get a vector of length 9. Then the matrix multiplica-\ntion of Wand the vectorized Xgives a vector of length 4. After reshaping it, we can obtain\nthesameresult Yfromtheoriginalconvolutionoperationabove: wejustimplementedcon-\nvolutions using matrix multiplications.\nY==torch .matmul(W, X .reshape( -1)).reshape( 2,2)\ntensor([[ True ,True ],\n[True ,True ]])\nLikewise, we can implement transposed convolutions using matrix multiplications. In the\nfollowing example, we take the 2\u00022output Yfrom the above regular convolution as input\nto the transposed convolution. To implement this operation by multiplying matrices, we\nonly need to transpose the weight matrix Wwith the new shape \u00b99,4\u00ba.\nZ=trans_conv(Y, K)\nZ==torch .matmul(W .T, Y .reshape( -1)).reshape( 3,3)\ntensor([[ True ,True ,True ],\n[True ,True ,True ],\n[True ,True ,True ]])\nConsider implementing the convolution by multiplying matrices. Given an input vector\nxand a weight matrix W, the forward propagation function of the convolution can be\nimplemented by multiplying its input with the weight matrix and outputting a vector y=\nWx. Since backpropagation follows the chain rule and rxy=W>, the backpropagation\nfunctionoftheconvolutioncanbeimplementedbymultiplyingitsinputwiththetransposed\nweight matrix W>. Therefore, the transposed convolutional layer can just exchange the\nforward propagation function and the backpropagation function of the convolutional layer:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22addc01-0df5-4ced-8aab-c2f59392b1db": {"__data__": {"id_": "22addc01-0df5-4ced-8aab-c2f59392b1db", "embedding": null, "metadata": {"page_label": "659", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9b3c287-44aa-4e4f-b1b2-e4708b5f00d5", "node_type": "4", "metadata": {"page_label": "659", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "839304d5147c44c65e3ed0ba1755af0b87815314fe46dcfb4a6a1adb842d3f82", "class_name": "RelatedNodeInfo"}}, "text": "659 Fully Convolutional Networks\n222itsforwardpropagationandbackpropagationfunctionsmultiplytheirinputvectorwith W>\nandW, respectively.\n14.10.4Summary\n\u000fIn contrast to the regular convolution that reduces input elements via the kernel, the\ntransposed convolution broadcasts input elements via the kernel, thereby producing\nan output that is larger than the input.\n\u000fIf we feed Xinto a convolutional layer \ud835\udc53to output Y=\ud835\udc53\u00b9X\u00baand create a transposed\nconvolutional layer \ud835\udc54with the same hyperparameters as \ud835\udc53except for the number of\noutputchannelsbeingthenumberofchannelsin X,then\ud835\udc54\u00b9\ud835\udc4c\u00bawillhavethesameshape\nasX.\n\u000fWe can implement convolutions using matrix multiplications. The transposed convolu-\ntional layer can just exchange the forward propagation function and the backpropaga-\ntion function of the convolutional layer.\n14.10.5Exercises\n1.InSection14.10.3 ,theconvolutioninput Xandthetransposedconvolutionoutput Zhave\nthe same shape. Do they have the same value? Why?\n2.Is it efficient to use matrix multiplications to implement convolutions? Why?\nDiscussions222.\n14.11FullyConvolutional Networks\nAs discussed in Section 14.9 , semantic segmentation classifies images in pixel level. A\nfullyconvolutionalnetwork(FCN)usesaconvolutionalneuralnetworktotransformimage\npixels to pixel classes ( Longet al., 2015). Unlike the CNNs that we encountered earlier\nfor image classification or object detection, a fully convolutional network transforms the\nheight and width of intermediate feature maps back to those of the input image: this is\nachievedbythetransposedconvolutionallayerintroducedin Section14.10 . Asaresult,the\nclassification output and the input image have a one-to-one correspondence in pixel level:\nthe channel dimension at any output pixel holds the classification results for the input pixel\nat the same spatial position.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e4f4147-16e1-4f8e-a89e-980befb4a411": {"__data__": {"id_": "5e4f4147-16e1-4f8e-a89e-980befb4a411", "embedding": null, "metadata": {"page_label": "660", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2d42b5d-8155-42e0-a13b-da6a6d12d2e3", "node_type": "4", "metadata": {"page_label": "660", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "02c25eb40b55aca228d639d9e444c629856bba92b4ebd4e6539d39fe2fc70e6a", "class_name": "RelatedNodeInfo"}}, "text": "660 Computer Vision\n14.11.1The Model\nHere we describe the basic design of the fully convolutional network model. As shown\ninFig. 14.11.1 , this model first uses a CNN to extract image features, then transforms the\nnumber of channels into the number of classes via a 1\u00021convolutional layer, and finally\ntransforms the height and width of the feature maps to those of the input image via the\ntransposed convolution introduced in Section 14.10 . As a result, the model output has the\nsame height and width as the input image, where the output channel contains the predicted\nclasses for the input pixel at the same spatial position.\ntFig. 14.11.1 Fully convolutional network.\nBelow, we use a ResNet-18 model pretrained on the ImageNet dataset to extract image\nfeatures and denote the model instance as pretrained_net . The last few layers of this\nmodel include a global average pooling layer and a fully connected layer: they are not\nneeded in the fully convolutional network.\npretrained_net =torchvision .models .resnet18(pretrained =True )\nlist (pretrained_net .children())[ -3:]\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /\n\u21a9!home/ci/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff| 44.7M/44.7M [00:00<00:00, 56.3MB/s]\n[Sequential(\n(0): BasicBlock(\n(conv1): Conv2d( 256,512, kernel_size =(3,3), stride =(2,2), padding =(1,\u2423\n\u21a9!1), bias =False )\n(bn1): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_\n\u21a9!running_stats =True )\n(relu): ReLU(inplace =True )\n(conv2): Conv2d( 512,512, kernel_size =(3,3), stride =(1,1), padding =(1,\u2423\n\u21a9!1), bias =False )\n(bn2): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1717, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17c90ec9-8504-479e-98cd-df51239686af": {"__data__": {"id_": "17c90ec9-8504-479e-98cd-df51239686af", "embedding": null, "metadata": {"page_label": "661", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cb244484-4e89-46d9-843c-d9357ffe6105", "node_type": "4", "metadata": {"page_label": "661", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c3c933a67387c97cdbbb5c7fa4dcd57debeab92845a38a4444830e288e518550", "class_name": "RelatedNodeInfo"}}, "text": "661 Fully Convolutional Networks\n(continued from previous page)\n\u21a9!running_stats =True )\n(downsample): Sequential(\n(0): Conv2d( 256,512, kernel_size =(1,1), stride =(2,2), bias =False )\n(1): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_\n\u21a9!running_stats =True )\n)\n)\n(1): BasicBlock(\n(conv1): Conv2d( 512,512, kernel_size =(3,3), stride =(1,1), padding =(1,\u2423\n\u21a9!1), bias =False )\n(bn1): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_\n\u21a9!running_stats =True )\n(relu): ReLU(inplace =True )\n(conv2): Conv2d( 512,512, kernel_size =(3,3), stride =(1,1), padding =(1,\u2423\n\u21a9!1), bias =False )\n(bn2): BatchNorm2d( 512, eps =1e-05 , momentum =0.1, affine =True , track_\n\u21a9!running_stats =True )\n)\n),\nAdaptiveAvgPool2d(output_size =(1,1)),\nLinear(in_features =512, out_features =1000 , bias =True )]\nNext, we create the fully convolutional network instance net. It copies all the pretrained\nlayers in the ResNet-18 except for the final global average pooling layer and the fully con-\nnected layer that are closest to the output.\nnet =nn.Sequential( *list (pretrained_net .children())[: -2])\nGiven an input with height and width of 320 and 480 respectively, the forward propagation\nofnetreducestheinputheightandwidthto1/32oftheoriginal,namely10and15.\nX=torch .rand(size =(1,3,320,480))\nnet(X) .shape\ntorch .Size([ 1,512,10,15])\nNext, we use a 1\u00021convolutional layer to transform the number of output channels into\nthenumberofclasses(21)ofthePascalVOC2012dataset. Finally, weneedtoincreasethe\nheight and width of the feature maps by 32 times to change them back to the height and\nwidth of the input image. Recall how to calculate the output shape of a convolutional layer\ninSection7.3 . Since\u00b9320\u000064\u00b816\u00022\u00b832\u00ba\u009d32=10and\u00b9480\u000064\u00b816\u00022\u00b832\u00ba\u009d32=15,\nweconstructatransposedconvolutionallayerwithstrideof 32,settingtheheightandwidth\nof the kernel to 64, the padding to 16. In general, we can see that for stride \ud835\udc60, padding\ud835\udc60\u009d2\n(assuming\ud835\udc60\u009d2is an integer), and the height and width of the kernel 2\ud835\udc60, the transposed\nconvolution will increase the height and width of the input by \ud835\udc60times.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b752b76d-bef6-42e9-868f-2e710bde9440": {"__data__": {"id_": "b752b76d-bef6-42e9-868f-2e710bde9440", "embedding": null, "metadata": {"page_label": "662", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a91fe56b-a7a9-48b3-9dda-24383d6d1ff2", "node_type": "4", "metadata": {"page_label": "662", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "43ae52504862d55b7d3e77f5f7134da86ac401f4b65b460401257be61b02cbcd", "class_name": "RelatedNodeInfo"}}, "text": "662 Computer Vision\nnum_classes =21\nnet.add_module( 'final_conv ', nn .Conv2d( 512, num_classes, kernel_size =1))\nnet.add_module( 'transpose_conv ', nn .ConvTranspose2d(num_classes, num_classes,\nkernel_size =64, padding =16, stride =32))\n14.11.2InitializingTransposedConvolutionalLayers\nWe already know that transposed convolutional layers can increase the height and width of\nfeature maps. In image processing, we may need to scale up an image, i.e., upsampling .\nBilinearinterpolation is one of the commonly used upsampling techniques. It is also often\nused for initializing transposed convolutional layers.\nTo explain bilinear interpolation, say that given an input image we want to calculate each\npixel of the upsampled output image. In order to calculate the pixel of the output image\nat coordinate\u00b9\ud835\udc65,\ud835\udc66\u00ba, first map\u00b9\ud835\udc65,\ud835\udc66\u00bato coordinate\u00b9\ud835\udc650,\ud835\udc660\u00baon the input image, for example,\naccordingtotheratiooftheinputsizetotheoutputsize. Notethatthemapped \ud835\udc650and\ud835\udc660are\nreal numbers. Then, find the four pixels closest to coordinate \u00b9\ud835\udc650,\ud835\udc660\u00baon the input image.\nFinally, the pixel of the output image at coordinate \u00b9\ud835\udc65,\ud835\udc66\u00bais calculated based on these four\nclosest pixels on the input image and their relative distance from \u00b9\ud835\udc650,\ud835\udc660\u00ba.\nUpsampling of bilinear interpolation can be implemented by the transposed convolutional\nlayer with the kernel constructed by the following bilinear_kernel function. Due to\nspace limitations, we only provide the implementation of the bilinear_kernel function\nbelow without discussions on its algorithm design.\ndef bilinear_kernel (in_channels, out_channels, kernel_size):\nfactor =(kernel_size +1)//2\nifkernel_size %2==1:\ncenter =factor -1\nelse :\ncenter =factor -0.5\nog=(torch .arange(kernel_size) .reshape( -1,1),\ntorch .arange(kernel_size) .reshape( 1,-1))\nfilt =(1-torch .abs(og[ 0]-center) /factor) *\\\n(1-torch .abs(og[ 1]-center) /factor)\nweight =torch .zeros((in_channels, out_channels,\nkernel_size, kernel_size))\nweight[ range (in_channels), range (out_channels), :, :] =filt\nreturn weight\nLet\u2019s experiment with upsampling of bilinear interpolation that is implemented by a trans-\nposed convolutional layer. We construct a transposed convolutional layer that doubles the\nheight and weight, and initialize its kernel with the bilinear_kernel function.\nconv_trans =nn.ConvTranspose2d( 3,3, kernel_size =4, padding =1, stride =2,\nbias =False )\nconv_trans .weight .data .copy_(bilinear_kernel( 3,3,4));", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2417, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f04cef2-36aa-4512-8bd9-c0e50fb6e61e": {"__data__": {"id_": "0f04cef2-36aa-4512-8bd9-c0e50fb6e61e", "embedding": null, "metadata": {"page_label": "663", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2ead6c59-9aad-4c3c-853b-5977a71b2f6a", "node_type": "4", "metadata": {"page_label": "663", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9553833f18559f2abc0c10eab1233914d2e9246800427cce90884e4d63f70a5a", "class_name": "RelatedNodeInfo"}}, "text": "663 Fully Convolutional Networks\nRead the image Xand assign the upsampling output to Y. In order to print the image, we\nneed to adjust the position of the channel dimension.\nimg =torchvision .transforms .ToTensor()(d2l .Image .open( '../img/catdog.jpg '))\nX=img.unsqueeze( 0)\nY=conv_trans(X)\nout_img =Y[0].permute( 1,2,0).detach()\nAs we can see, the transposed convolutional layer increases both the height and width of\nthe image by a factor of two. Except for the different scales in coordinates, the image\nscaled up by bilinear interpolation and the original image printed in Section 14.3 look the\nsame.\nd2l.set_figsize()\nprint ('input image shape: ', img .permute( 1,2,0).shape)\nd2l.plt.imshow(img .permute( 1,2,0));\nprint ('output image shape: ', out_img .shape)\nd2l.plt.imshow(out_img);\ninput image shape: torch .Size([ 561,728,3])\noutput image shape: torch .Size([ 1122 ,1456 ,3])\nIn a fully convolutional network, we initialize the transposed convolutional layer with up-\nsampling of bilinear interpolation. For the 1\u00021convolutional layer, we use Xavier initial-\nization.\nW=bilinear_kernel(num_classes, num_classes, 64)\nnet.transpose_conv .weight .data .copy_(W);\n14.11.3Readingthe Dataset\nWereadthesemanticsegmentationdatasetasintroducedin Section14.9 . Theoutputimage\nshapeofrandomcroppingisspecifiedas 320\u0002480: boththeheightandwidtharedivisible\nby32.\nbatch_size, crop_size =32, (320,480)\ntrain_iter, test_iter =d2l.load_data_voc(batch_size, crop_size)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b885727c-93c2-4f33-8965-bcc04203a70b": {"__data__": {"id_": "b885727c-93c2-4f33-8965-bcc04203a70b", "embedding": null, "metadata": {"page_label": "664", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16e4b73a-02cb-45de-b908-9428488e8b02", "node_type": "4", "metadata": {"page_label": "664", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0a46739da1b5140e43dfb3ce0b44239d4c690b3202068d5117ca73869db4e4e1", "class_name": "RelatedNodeInfo"}}, "text": "664 Computer Vision\nread 1114 examples\nread 1078 examples\n14.11.4Training\nNow we can train our constructed fully convolutional network. The loss function and ac-\ncuracy calculation here are not essentially different from those in image classification of\nearlier chapters. Because we use the output channel of the transposed convolutional layer\ntopredicttheclassforeachpixel,thechanneldimensionisspecifiedinthelosscalculation.\nInaddition,theaccuracyiscalculatedbasedoncorrectnessofthepredictedclassforallthe\npixels.\ndef loss (inputs, targets):\nreturn F.cross_entropy(inputs, targets, reduction ='none ').mean( 1).mean( 1)\nnum_epochs, lr, wd, devices =5,0.001 ,1e-3 , d2l .try_all_gpus()\ntrainer =torch .optim .SGD(net .parameters(), lr =lr, weight_decay =wd)\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.449 , train acc 0.861 , test acc 0.852\n226.7 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]\n14.11.5Prediction\nWhenpredicting,weneedtostandardizetheinputimageineachchannelandtransformthe\nimage into the four-dimensional input format required by the CNN.\ndef predict (img):\nX=test_iter .dataset .normalize_image(img) .unsqueeze( 0)\npred =net(X .to(devices[ 0])).argmax(dim =1)\nreturn pred .reshape(pred .shape[ 1], pred .shape[ 2])\nTo visualize the predicted class of each pixel, we map the predicted class back to its label\ncolor in the dataset.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1429, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a1d3429-2942-4e2d-bf9f-2e9aa99466c8": {"__data__": {"id_": "4a1d3429-2942-4e2d-bf9f-2e9aa99466c8", "embedding": null, "metadata": {"page_label": "665", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b5e44a5-e136-438a-b707-13c9677b9d52", "node_type": "4", "metadata": {"page_label": "665", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7e442ce0c8e3b9e812d488cf6c85b3dcffd0babadd8cf9fdd6573fdd2bdb8b00", "class_name": "RelatedNodeInfo"}}, "text": "665 Fully Convolutional Networks\ndef label2image (pred):\ncolormap =torch .tensor(d2l .VOC_COLORMAP, device =devices[ 0])\nX=pred .long()\nreturn colormap[X, :]\nImages in the test dataset vary in size and shape. Since the model uses a transposed con-\nvolutional layer with stride of 32, when the height or width of an input image is indivisible\nby 32, the output height or width of the transposed convolutional layer will deviate from\nthe shape of the input image. In order to address this issue, we can crop multiple rectangu-\nlar areas with height and width that are integer multiples of 32 in the image, and perform\nforward propagation on the pixels in these areas separately. Note that the union of these\nrectangular areas needs to completely cover the input image. When a pixel is covered by\nmultiple rectangular areas, the average of the transposed convolution outputs in separate\nareas for this same pixel can be input to the softmax operation to predict the class.\nForsimplicity,weonlyreadafewlargertestimages,andcropa 320\u0002480areaforprediction\nstartingfromtheupper-leftcornerofanimage. Forthesetestimages,weprinttheircropped\nareas, prediction results, and ground-truth row by row.\nvoc_dir =d2l.download_extract( 'voc2012 ','VOCdevkit/VOC2012 ')\ntest_images, test_labels =d2l.read_voc_images(voc_dir, False )\nn, imgs =4, []\nfor iinrange (n):\ncrop_rect =(0,0,320,480)\nX=torchvision .transforms .functional .crop(test_images[i], *crop_rect)\npred =label2image(predict(X))\nimgs +=[X.permute( 1,2,0), pred .cpu(),\ntorchvision .transforms .functional .crop(\ntest_labels[i], *crop_rect) .permute( 1,2,0)]\nd2l.show_images(imgs[:: 3]+imgs[ 1::3]+imgs[ 2::3],3, n, scale =2);", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce5c7886-f7af-4317-9a1c-e721c3a0caed": {"__data__": {"id_": "ce5c7886-f7af-4317-9a1c-e721c3a0caed", "embedding": null, "metadata": {"page_label": "666", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3000855a-a40f-42dd-b782-0e2f718f848d", "node_type": "4", "metadata": {"page_label": "666", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6b0e5a904da95e7d01bc1fc161fcab3689cfab499b654e1d5ea52f8dad79053c", "class_name": "RelatedNodeInfo"}}, "text": "666 Computer Vision\n22314.11.6Summary\n\u000fThe fully convolutional network first uses a CNN to extract image features, then trans-\nforms the number of channels into the number of classes via a 1\u00021convolutional\nlayer, and finally transforms the height and width of the feature maps to those of the\ninput image via the transposed convolution.\n\u000fIn a fully convolutional network, we can use upsampling of bilinear interpolation to\ninitialize the transposed convolutional layer.\n14.11.7Exercises\n1.If we use Xavier initialization for the transposed convolutional layer in the experiment,\nhow does the result change?\n2.Can you further improve the accuracy of the model by tuning the hyperparameters?\n3.Predict the classes of all pixels in test images.\n4.The original fully convolutional network paper also uses outputs of some intermediate\nCNN layers ( Longetal., 2015). Try to implement this idea.\nDiscussions223.\n14.12NeuralStyleTransfer\nIf you are a photography enthusiast, you may be familiar with the filter. It can change\nthe color style of photos so that landscape photos become sharper or portrait photos have\nwhitenedskins. However, onefilterusuallyonlychangesoneaspectofthephoto. Toapply\nan ideal style to a photo, you probably need to try many different filter combinations. This\nprocess is as complex as tuning the hyperparameters of a model.\nIn this section, we will leverage layerwise representations of a CNN to automatically apply\nthe style of one image to another image, i.e., style transfer (Gatyset al., 2016). This task\nneeds two input images: one is the content image and the other is the style image . We will\nuseneuralnetworkstomodifythecontentimagetomakeitclosetothestyleimageinstyle.\nFor example, the content image in Fig. 14.12.1 is a landscape photo taken by us in Mount\nRainierNationalParkinthesuburbsofSeattle,whilethestyleimageisanoilpaintingwith\nthe theme of autumn oak trees. In the output synthesized image, the oil brush strokes of\nthe style image are applied, leading to more vivid colors, while preserving the main shape\nof the objects in the content image.\n14.12.1Method\nFig. 14.12.2 illustrates the CNN-based style transfer method with a simplified example.\nFirst, we initialize the synthesized image, for example, into the content image. This syn-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2276, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "52783869-7f39-449e-8fa2-c04c6872afe7": {"__data__": {"id_": "52783869-7f39-449e-8fa2-c04c6872afe7", "embedding": null, "metadata": {"page_label": "667", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38742736-69f8-4178-929a-21d9f6fcb7c7", "node_type": "4", "metadata": {"page_label": "667", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "80f701e555788677bc669fa8f4c61723ce938f23327f716b4152220490a7160f", "class_name": "RelatedNodeInfo"}}, "text": "667 Neural Style Transfer\ntFig. 14.12.1 Given content and style images, style transfer outputs a synthesized image.\nthesizedimageistheonlyvariablethatneedstobeupdatedduringthestyletransferprocess,\ni.e.,themodelparameterstobeupdatedduringtraining. ThenwechooseapretrainedCNN\nto extract image features and freeze its model parameters during training. This deep CNN\nuses multiple layers to extract hierarchical features for images. We can choose the output\nof some of these layers as content features or style features. Take Fig. 14.12.2 as an exam-\nple. The pretrained neural network here has 3 convolutional layers, where the second layer\noutputs the content features, and the first and third layers output the style features.\ntFig. 14.12.2 CNN-based style transfer process. Solid lines show the direction of forward propagation\nand dotted lines show backward propagation.\nNext, we calculate the loss function of style transfer through forward propagation (direc-\ntion of solid arrows), and update the model parameters (the synthesized image for output)\nthrough backpropagation (direction of dashed arrows). The loss function commonly used\nin style transfer consists of three parts: (i) content loss makes the synthesized image and\nthe content image close in content features; (ii) styleloss makes the synthesized image and\nstyle image close in style features; and (iii) total variation loss helps to reduce the noise\nin the synthesized image. Finally, when the model training is over, we output the model\nparameters of the style transfer to generate the final synthesized image.\nIn the following, we will explain the technical details of style transfer via a concrete exper-\niment.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb713ab9-c16e-4cd7-a3a8-6b17312384df": {"__data__": {"id_": "bb713ab9-c16e-4cd7-a3a8-6b17312384df", "embedding": null, "metadata": {"page_label": "668", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42e697c4-af99-42a5-be6f-8b715043aeaf", "node_type": "4", "metadata": {"page_label": "668", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "919dd89eab7038b5a3a5f2329ab46c4d1d430b98193c40460f804c03bd11edf8", "class_name": "RelatedNodeInfo"}}, "text": "668 Computer Vision\n14.12.2Readingthe Content and StyleImages\nFirst, we read the content and style images. From their printed coordinate axes, we can tell\nthat these images have different sizes.\n%matplotlib inline\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\nd2l.set_figsize()\ncontent_img =d2l.Image .open( '../img/rainier.jpg ')\nd2l.plt.imshow(content_img);\nstyle_img =d2l.Image .open( '../img/autumn-oak.jpg ')\nd2l.plt.imshow(style_img);\n14.12.3Preprocessingand Postprocessing\nBelow, we define two functions for preprocessing and postprocessing images. The pre-\nprocess function standardizes each of the three RGB channels of the input image and\ntransforms the results into the CNN input format. The postprocess function restores the\npixel values in the output image to their original values before standardization. Since the\nimage printing function requires that each pixel has a floating point value from 0 to 1, we\nreplace any value smaller than 0 or greater than 1 with 0 or 1, respectively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1033, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2a9a17ed-518b-4da6-81ca-2f1b3d71b462": {"__data__": {"id_": "2a9a17ed-518b-4da6-81ca-2f1b3d71b462", "embedding": null, "metadata": {"page_label": "669", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c34cf53-539e-4fe6-bc12-8744b8ab45b1", "node_type": "4", "metadata": {"page_label": "669", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3aab5143b17a9b288501f573ab2f46e011c010457374aac529a3c19d4bf01b85", "class_name": "RelatedNodeInfo"}}, "text": "669 Neural Style Transfer\nrgb_mean =torch .tensor([ 0.485 ,0.456 ,0.406 ])\nrgb_std =torch .tensor([ 0.229 ,0.224 ,0.225 ])\ndef preprocess (img, image_shape):\ntransforms =torchvision .transforms .Compose([\ntorchvision .transforms .Resize(image_shape),\ntorchvision .transforms .ToTensor(),\ntorchvision .transforms .Normalize(mean =rgb_mean, std =rgb_std)])\nreturn transforms(img) .unsqueeze( 0)\ndef postprocess (img):\nimg =img[ 0].to(rgb_std .device)\nimg =torch .clamp(img .permute( 1,2,0)*rgb_std +rgb_mean, 0,1)\nreturn torchvision .transforms .ToPILImage()(img .permute( 2,0,1))\n14.12.4Extracting Features\nWe use the VGG-19 model pretrained on the ImageNet dataset to extract image features\n(Gatysetal., 2016).\npretrained_net =torchvision .models .vgg19(pretrained =True )\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /home/\n\u21a9!ci/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff| 548M/548M [00:02<00:00, 213MB/s]\nIn order to extract the content features and style features of the image, we can select the\noutput of certain layers in the VGG network. Generally speaking, the closer to the input\nlayer, the easier to extract details of the image, and vice versa, the easier to extract the\nglobal information of the image. In order to avoid excessively retaining the details of the\ncontent image in the synthesized image, we choose a VGG layer that is closer to the output\nas thecontent layer to output the content features of the image. We also select the output\nof different VGG layers for extracting local and global style features. These layers are also\ncalledstyle layers . As mentioned in Section 8.2 , the VGG network uses 5 convolutional\nblocks. Intheexperiment,wechoosethelastconvolutionallayerofthefourthconvolutional\nblock as the content layer, and the first convolutional layer of each convolutional block as\nthestylelayer. Theindicesoftheselayerscanbeobtainedbyprintingthe pretrained_net\ninstance.\nstyle_layers, content_layers =[0,5,10,19,28], [ 25]\nWhen extracting features using VGG layers, we only need to use all those from the input\nlayer to the content layer or style layer that is closest to the output layer. Let\u2019s construct\na new network instance net, which only retains all the VGG layers to be used for feature\nextraction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2286, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d2e1faf-6ed8-49eb-8db6-2d7d3cb5b03f": {"__data__": {"id_": "1d2e1faf-6ed8-49eb-8db6-2d7d3cb5b03f", "embedding": null, "metadata": {"page_label": "670", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ba56652-f7fa-436e-bac1-929f49062fb6", "node_type": "4", "metadata": {"page_label": "670", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0c891520d8fbed6288c7953c4708ba891dcbcc0fb52340bf8f0da7240d20d926", "class_name": "RelatedNodeInfo"}}, "text": "670 Computer Vision\nnet =nn.Sequential( *[pretrained_net .features[i] for iin\nrange (max(content_layers +style_layers) +1)])\nGiven the input X, if we simply invoke the forward propagation net(X), we can only get\nthe output of the last layer. Since we also need the outputs of intermediate layers, we need\nto perform layer-by-layer computation and keep the content and style layer outputs.\ndef extract_features (X, content_layers, style_layers):\ncontents =[]\nstyles =[]\nfor iinrange (len(net)):\nX=net[i](X)\nifiinstyle_layers:\nstyles .append(X)\nifiincontent_layers:\ncontents .append(X)\nreturn contents, styles\nTwo functions are defined below: the get_contents function extracts content features\nfrom the content image, and the get_styles function extracts style features from the style\nimage. SincethereisnoneedtoupdatethemodelparametersofthepretrainedVGGduring\ntraining, we can extract the content and the style features even before the training starts.\nSince the synthesized image is a set of model parameters to be updated for style transfer,\nwe can only extract the content and style features of the synthesized image by calling the\nextract_features function during training.\ndef get_contents (image_shape, device):\ncontent_X =preprocess(content_img, image_shape) .to(device)\ncontents_Y, _ =extract_features(content_X, content_layers, style_layers)\nreturn content_X, contents_Y\ndef get_styles (image_shape, device):\nstyle_X =preprocess(style_img, image_shape) .to(device)\n_, styles_Y =extract_features(style_X, content_layers, style_layers)\nreturn style_X, styles_Y\n14.12.5Definingthe Loss Function\nNow we will describe the loss function for style transfer. The loss function consists of the\ncontent loss, style loss, and total variation loss.\nContentLoss\nSimilar to the loss function in linear regression, the content loss measures the difference in\ncontent features between the synthesized image and the content image via the squared loss\nfunction. The two inputs of the squared loss function are both outputs of the content layer\ncomputed by the extract_features function.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08f62a96-ebae-4d52-b107-cb1d1c30dba8": {"__data__": {"id_": "08f62a96-ebae-4d52-b107-cb1d1c30dba8", "embedding": null, "metadata": {"page_label": "671", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8291bf50-0ba7-49a6-8c13-6c06e12e8af6", "node_type": "4", "metadata": {"page_label": "671", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "22292e08b62d693334de1ce910247bf2ff52b2d1baf4360d8b87cf4d53fa7c0c", "class_name": "RelatedNodeInfo"}}, "text": "671 Neural Style Transfer\ndef content_loss (Y_hat, Y):\n# We detach the target content from the tree used to dynamically compute\n# the gradient: this is a stated value, not a variable. Otherwise the loss\n# will throw an error.\nreturn torch .square(Y_hat -Y.detach()) .mean()\nStyleLoss\nStyle loss, similar to content loss, also uses the squared loss function to measure the dif-\nference in style between the synthesized image and the style image. To express the style\noutput of any style layer, we first use the extract_features function to compute the style\nlayer output. Suppose that the output has 1 example, \ud835\udc50channels, height \u210e, and width\ud835\udc64, we\ncan transform this output into matrix Xwith\ud835\udc50rows and\u210e\ud835\udc64columns. This matrix can be\nthought of as the concatenation of \ud835\udc50vectors x1,...,x\ud835\udc50, each of which has a length of \u210e\ud835\udc64.\nHere, vector x\ud835\udc56represents the style feature of channel \ud835\udc56.\nIn theGram matrix of these vectors XX>2R\ud835\udc50\u0002\ud835\udc50, element\ud835\udc65\ud835\udc56\ud835\udc57in row\ud835\udc56and column \ud835\udc57is\nthe dot product of vectors x\ud835\udc56andx\ud835\udc57. It represents the correlation of the style features of\nchannels\ud835\udc56and\ud835\udc57. We use this Gram matrix to represent the style output of any style layer.\nNote that when the value of \u210e\ud835\udc64is larger, it likely leads to larger values in the Gram matrix.\nNote also that the height and width of the Gram matrix are both the number of channels \ud835\udc50.\nTo allow style loss not to be affected by these values, the gramfunction below divides the\nGram matrix by the number of its elements, i.e., \ud835\udc50\u210e\ud835\udc64.\ndef gram (X):\nnum_channels, n =X.shape[ 1], X .numel() //X.shape[ 1]\nX=X.reshape((num_channels, n))\nreturn torch .matmul(X, X .T)/(num_channels *n)\nObviously, the two Gram matrix inputs of the squared loss function for style loss are based\non the style layer outputs for the synthesized image and the style image. It is assumed here\nthat the Gram matrix gram_Ybased on the style image has been precomputed.\ndef style_loss (Y_hat, gram_Y):\nreturn torch .square(gram(Y_hat) -gram_Y .detach()) .mean()\nTotal VariationLoss\nSometimes, the learned synthesized image has a lot of high-frequency noise, i.e., particu-\nlarly bright or dark pixels. One common noise reduction method is total variation denois-\ning. Denoteby\ud835\udc65\ud835\udc56,\ud835\udc57thepixelvalueatcoordinate \u00b9\ud835\udc56,\ud835\udc57\u00ba. Reducingtotalvariationloss\n\u00d5\n\ud835\udc56,\ud835\udc57\f\f\ud835\udc65\ud835\udc56,\ud835\udc57\u0000\ud835\udc65\ud835\udc56\u00b81,\ud835\udc57\f\f\u00b8\f\f\ud835\udc65\ud835\udc56,\ud835\udc57\u0000\ud835\udc65\ud835\udc56,\ud835\udc57\u00b81\f\f(14.12.1)\nmakes values of neighboring pixels on the synthesized image closer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0780a937-ec8c-4ed9-8486-7bb7155b732c": {"__data__": {"id_": "0780a937-ec8c-4ed9-8486-7bb7155b732c", "embedding": null, "metadata": {"page_label": "672", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df2bc6c0-5550-4a18-98d1-ca0f8acd5c09", "node_type": "4", "metadata": {"page_label": "672", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5f22ac09d293e01ff5557ad26ca09413fc32318b7da30764a11fd651c19f3c4a", "class_name": "RelatedNodeInfo"}}, "text": "672 Computer Vision\ndef tv_loss (Y_hat):\nreturn 0.5 *(torch .abs(Y_hat[:, :, 1:, :] -Y_hat[:, :, : -1, :]) .mean() +\ntorch .abs(Y_hat[:, :, :, 1:]-Y_hat[:, :, :, : -1]).mean())\nLoss Function\nThe loss function of style transfer is the weighted sum of content loss, style loss, and total\nvariation loss. By adjusting these weight hyperparameters, we can balance among content\nretention, style transfer, and noise reduction on the synthesized image.\ncontent_weight, style_weight, tv_weight =1,1e4,10\ndef compute_loss (X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n# Calculate the content, style, and total variance losses respectively\ncontents_l =[content_loss(Y_hat, Y) *content_weight for Y_hat, Y inzip(\ncontents_Y_hat, contents_Y)]\nstyles_l =[style_loss(Y_hat, Y) *style_weight for Y_hat, Y inzip(\nstyles_Y_hat, styles_Y_gram)]\ntv_l =tv_loss(X) *tv_weight\n# Add up all the losses\nl=sum(styles_l +contents_l +[tv_l])\nreturn contents_l, styles_l, tv_l, l\n14.12.6Initializing the Synthesized Image\nIn style transfer, the synthesized image is the only variable that needs to be updated during\ntraining. Thus, we can define a simple model, SynthesizedImage , and treat the synthe-\nsized image as the model parameters. In this model, forward propagation just returns the\nmodel parameters.\nclass SynthesizedImage (nn.Module):\ndef __init__ (self , img_shape, **kwargs):\nsuper (SynthesizedImage, self ).__init__ (**kwargs)\nself .weight =nn.Parameter(torch .rand( *img_shape))\ndef forward (self ):\nreturn self .weight\nNext, we define the get_inits function. This function creates a synthesized image model\ninstance and initializes it to the image X. Gram matrices for the style image at various style\nlayers, styles_Y_gram , are computed prior to training.\ndef get_inits (X, device, lr, styles_Y):\ngen_img =SynthesizedImage(X .shape) .to(device)\ngen_img .weight .data .copy_(X .data)\ntrainer =torch .optim .Adam(gen_img .parameters(), lr =lr)\nstyles_Y_gram =[gram(Y) for Yinstyles_Y]\nreturn gen_img(), styles_Y_gram, trainer", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ddf2788b-bea6-4ace-bcfa-ddeddff86082": {"__data__": {"id_": "ddf2788b-bea6-4ace-bcfa-ddeddff86082", "embedding": null, "metadata": {"page_label": "673", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47485790-4ffe-4c22-af58-6e528b760ead", "node_type": "4", "metadata": {"page_label": "673", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ecd9306c45d6b2913f0137d5f62bb7cabf28f76b1d7f270d3b41baefd388ca0a", "class_name": "RelatedNodeInfo"}}, "text": "673 Neural Style Transfer\n14.12.7Training\nWhen training the model for style transfer, we continuously extract content features and\nstyle features of the synthesized image, and calculate the loss function. Below defines the\ntraining loop.\ndef train (X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\nX, styles_Y_gram, trainer =get_inits(X, device, lr, styles_Y)\nscheduler =torch .optim .lr_scheduler .StepLR(trainer, lr_decay_epoch, 0.8)\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[10, num_epochs],\nlegend =['content ','style ','TV'],\nncols =2, figsize =(7,2.5))\nfor epoch inrange (num_epochs):\ntrainer .zero_grad()\ncontents_Y_hat, styles_Y_hat =extract_features(\nX, content_layers, style_layers)\ncontents_l, styles_l, tv_l, l =compute_loss(\nX, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\nl.backward()\ntrainer .step()\nscheduler .step()\nif(epoch +1)%10==0:\nanimator .axes[ 1].imshow(postprocess(X))\nanimator .add(epoch +1, [float (sum(contents_l)),\nfloat (sum(styles_l)), float (tv_l)])\nreturn X\nNow we start to train the model. We rescale the height and width of the content and style\nimages to 300 by 450 pixels. We use the content image to initialize the synthesized im-\nage.\ndevice, image_shape =d2l.try_gpu(), ( 300,450)# PIL Image (h, w)\nnet =net.to(device)\ncontent_X, contents_Y =get_contents(image_shape, device)\n_, styles_Y =get_styles(image_shape, device)\noutput =train(content_X, contents_Y, styles_Y, device, 0.3,500,50)\nWecanseethatthesynthesizedimageretainsthesceneryandobjectsofthecontentimage,\nand transfers the color of the style image at the same time. For example, the synthesized", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68321a47-9b7a-4ef3-a774-6b5ee5595047": {"__data__": {"id_": "68321a47-9b7a-4ef3-a774-6b5ee5595047", "embedding": null, "metadata": {"page_label": "674", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11215cf3-b9d2-4731-b9fe-7f036853eb80", "node_type": "4", "metadata": {"page_label": "674", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3352954e2928462e21782a396e3502b9db4c447aa0d94f1739bf1a01a7941bd4", "class_name": "RelatedNodeInfo"}}, "text": "674 Computer Vision\n224image has blocks of color like those in the style image. Some of these blocks even have the\nsubtle texture of brush strokes.\n14.12.8Summary\n\u000fThelossfunctioncommonlyusedinstyletransferconsistsofthreeparts: (i)contentloss\nmakesthesynthesizedimageandthecontentimagecloseincontentfeatures; (ii)style\nlossmakesthesynthesizedimageandstyleimagecloseinstylefeatures; and(iii)total\nvariation loss helps to reduce the noise in the synthesized image.\n\u000fWe can use a pretrained CNN to extract image features and minimize the loss function\nto continuously update the synthesized image as model parameters during training.\n\u000fWe use Gram matrices to represent the style outputs from the style layers.\n14.12.9Exercises\n1.How does the output change when you select different content and style layers?\n2.Adjust the weight hyperparameters in the loss function. Does the output retain more\ncontent or have less noise?\n3.Use different content and style images. Can you create more interesting synthesized\nimages?\n4.Can we apply style transfer for text? Hint: you may refer to the survey paper by Hu et\nal.(2022).\nDiscussions224.\n14.13ImageClassification (CIFAR-10)on Kaggle\nSo far, we have been using high-level APIs of deep learning frameworks to directly obtain\nimage datasets in tensor format. However, custom image datasets often come in the form\nof image files. In this section, we will start from raw image files, and organize, read, then\ntransform them into tensor format step by step.\nWeexperimentedwiththeCIFAR-10datasetin Section14.1 ,whichisanimportantdataset\nin computer vision. In this section, we will apply the knowledge we learned in previous\nsections to practice the Kaggle competition of CIFAR-10 image classification. The web\naddress of the competition is https://www.kaggle.com/c/cifar-10\nFig. 14.13.1 shows the information on the competition\u2019s webpage. In order to submit the\nresults, you need to register a Kaggle account.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1944, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9d75b1a-2b3a-4167-8533-0b043c9d95b0": {"__data__": {"id_": "d9d75b1a-2b3a-4167-8533-0b043c9d95b0", "embedding": null, "metadata": {"page_label": "675", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8e817ba-e86b-48d4-ac4e-a1aeffbd1d71", "node_type": "4", "metadata": {"page_label": "675", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "59a0fc56d85812992712321fb1bd12ebbc7730254433a30e6dbc1455cb6bc529", "class_name": "RelatedNodeInfo"}}, "text": "675 Image Classi\ufb01cation (CIFAR-10) on Kaggle\ntFig. 14.13.1 CIFAR-10 image classi\ufb01cation competition webpage information. The competition\ndataset can be obtained by clicking the \u201cData\u201d tab.\nimport collections\nimport math\nimport os\nimport shutil\nimport pandas aspd\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\n14.13.1Obtaining and Organizingthe Dataset\nThe competition dataset is divided into a training set and a test set, which contain 50000\nand 300000 images, respectively. In the test set, 10000 images will be used for evaluation,\nwhile the remaining 290000 images will not be evaluated: they are included just to make\nit hard to cheat with manually labeled results of the test set. The images in this dataset\nare all png color (RGB channels) image files, whose height and width are both 32 pixels.\nThe images cover a total of 10 categories, namely airplanes, cars, birds, cats, deer, dogs,\nfrogs, horses, boats, and trucks. The upper-left corner of Fig. 14.13.1 shows some images\nof airplanes, cars, and birds in the dataset.\nDownloadingthe Dataset\nAfter logging in to Kaggle, we can click the \u201cData\u201d tab on the CIFAR-10 image classifi-\ncation competition webpage shown in Fig. 14.13.1 and download the dataset by clicking\nthe \u201cDownload All\u201d button. After unzipping the downloaded file in ../data , and un-\nzipping train.7z andtest.7z inside it, you will find the entire dataset in the following\npaths:\n\u000f../data/cifar-10/train/[1-50000].png\n\u000f../data/cifar-10/test/[1-300000].png", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1518, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5cb1281a-9a85-493f-bdd6-37fd180c1e9f": {"__data__": {"id_": "5cb1281a-9a85-493f-bdd6-37fd180c1e9f", "embedding": null, "metadata": {"page_label": "676", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b787ca16-3662-4fc8-be72-4eecca88405f", "node_type": "4", "metadata": {"page_label": "676", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "907485a36c918b8da77c9cf5015ebbc4aa391bca0fef73d7d49d034939a21b12", "class_name": "RelatedNodeInfo"}}, "text": "676 Computer Vision\n\u000f../data/cifar-10/trainLabels.csv\n\u000f../data/cifar-10/sampleSubmission.csv\nwherethe trainandtestdirectories containthe trainingand testingimages, respectively,\ntrainLabels.csv provides labels for the training images, and sample_submission.csv\nis a sample submission file.\nTomakeiteasiertogetstarted,weprovideasmall-scalesampleofthedatasetthatcontains\nthe first 1000 training images and 5 random testing images. To use the full dataset of the\nKaggle competition, you need to set the following demovariable to False.\n#@save\nd2l.DATA_HUB[ 'cifar10_tiny ']=(d2l .DATA_URL +'kaggle_cifar10_tiny.zip ',\n'2068874e4b9a9f0fb07ebe0ad2b29754449ccacd ')\n# If you use the full dataset downloaded for the Kaggle competition, set\n# `demo` to False\ndemo =True\nifdemo:\ndata_dir =d2l.download_extract( 'cifar10_tiny ')\nelse :\ndata_dir ='../data/cifar-10/ '\nDownloading ../data /kaggle_cifar10_tiny .zip from http ://d2l-data .s3-accelerate .\n\u21a9!amazonaws .com/kaggle_cifar10_tiny .zip...\nOrganizingthe Dataset\nWe need to organize datasets to facilitate model training and testing. Let\u2019s first read the\nlabels from the csv file. The following function returns a dictionary that maps the non-\nextension part of the filename to its label.\n#@save\ndef read_csv_labels (fname):\n\"\"\"Read `fname` to return a filename to label dictionary.\"\"\"\nwith open (fname, 'r')asf:\n# Skip the file header line (column name)\nlines =f.readlines()[ 1:]\ntokens =[l.rstrip() .split( ',')for linlines]\nreturn dict (((name, label) for name, label intokens))\nlabels =read_csv_labels(os .path .join(data_dir, 'trainLabels.csv '))\nprint ('# training examples: ',len(labels))\nprint ('# classes: ',len(set(labels .values())))\n# training examples: 1000\n# classes: 10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1730, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd163e78-b195-42d2-a50e-5bc7c06f99d8": {"__data__": {"id_": "dd163e78-b195-42d2-a50e-5bc7c06f99d8", "embedding": null, "metadata": {"page_label": "677", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c28d313-2532-4c9f-8243-daf2347b7c11", "node_type": "4", "metadata": {"page_label": "677", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4d119c32f234b98d4f520a665facaab4007cb7e543f763a5cc006693a35d1216", "class_name": "RelatedNodeInfo"}}, "text": "677 Image Classi\ufb01cation (CIFAR-10) on Kaggle\nNext,wedefinethe reorg_train_valid functiontosplitthevalidationsetoutoftheorig-\ninal training set. The argument valid_ratio in this function is the ratio of the number\nof examples in the validation set to the number of examples in the original training set.\nMore concretely, let \ud835\udc5bbe the number of images of the class with the least examples, and\n\ud835\udc5fbe the ratio. The validation set will split out max\u00b9b\ud835\udc5b\ud835\udc5fc,1\u00baimages for each class. Let\u2019s\nusevalid_ratio=0.1 as an example. Since the original training set has 50000 images,\nthere will be 45000 images used for training in the path train_valid_test/train , while\nthe other 5000 images will be split out as validation set in the path train_valid_test/\nvalid. Afterorganizingthedataset,imagesofthesameclasswillbeplacedunderthesame\nfolder.\n#@save\ndef copyfile (filename, target_dir):\n\"\"\"Copy a file into a target directory.\"\"\"\nos.makedirs(target_dir, exist_ok =True )\nshutil .copy(filename, target_dir)\n#@save\ndef reorg_train_valid (data_dir, labels, valid_ratio):\n\"\"\"Split the validation set out of the original training set.\"\"\"\n# The number of examples of the class that has the fewest examples in the\n# training dataset\nn=collections .Counter(labels .values()) .most_common()[ -1][1]\n# The number of examples per class for the validation set\nn_valid_per_label =max(1, math .floor(n *valid_ratio))\nlabel_count ={}\nfor train_file inos.listdir(os .path .join(data_dir, 'train ')):\nlabel =labels[train_file .split( '.')[0]]\nfname =os.path .join(data_dir, 'train ', train_file)\ncopyfile(fname, os .path .join(data_dir, 'train_valid_test ',\n'train_valid ', label))\niflabel not inlabel_count orlabel_count[label] <n_valid_per_label:\ncopyfile(fname, os .path .join(data_dir, 'train_valid_test ',\n'valid ', label))\nlabel_count[label] =label_count .get(label, 0)+1\nelse :\ncopyfile(fname, os .path .join(data_dir, 'train_valid_test ',\n'train ', label))\nreturn n_valid_per_label\nThereorg_test function below organizes the testing set for data loading during predic-\ntion.\n#@save\ndef reorg_test (data_dir):\n\"\"\"Organize the testing set for data loading during prediction.\"\"\"\nfor test_file inos.listdir(os .path .join(data_dir, 'test ')):\ncopyfile(os .path .join(data_dir, 'test ', test_file),\nos.path .join(data_dir, 'train_valid_test ','test ',\n'unknown '))", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2333, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab21cab0-2221-4684-9657-338b2519d8b6": {"__data__": {"id_": "ab21cab0-2221-4684-9657-338b2519d8b6", "embedding": null, "metadata": {"page_label": "678", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "267bec77-a109-448a-9ce2-c9dab2a59cf0", "node_type": "4", "metadata": {"page_label": "678", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "34dd1fcbcc457a3fe0b462e8539a1579a632871b02324b13649a8712112a8c4b", "class_name": "RelatedNodeInfo"}}, "text": "678 Computer Vision\nFinally, we use a function to invoke the read_csv_labels ,reorg_train_valid , and re-\norg_test functions defined above.\ndef reorg_cifar10_data (data_dir, valid_ratio):\nlabels =read_csv_labels(os .path .join(data_dir, 'trainLabels.csv '))\nreorg_train_valid(data_dir, labels, valid_ratio)\nreorg_test(data_dir)\nHere we only set the batch size to 32 for the small-scale sample of the dataset. When\ntraining and testing the complete dataset of the Kaggle competition, batch_size should\nbe set to a larger integer, such as 128. We split out 10% of the training examples as the\nvalidation set for tuning hyperparameters.\nbatch_size =32ifdemo else 128\nvalid_ratio =0.1\nreorg_cifar10_data(data_dir, valid_ratio)\n14.13.2ImageAugmentation\nWeuseimageaugmentationtoaddressoverfitting. Forexample,imagescanbeflippedhor-\nizontallyatrandomduringtraining. WecanalsoperformstandardizationforthethreeRGB\nchannels of color images. Below lists some of these operations that you can tweak.\ntransform_train =torchvision .transforms .Compose([\n# Scale the image up to a square of 40 pixels in both height and width\ntorchvision .transforms .Resize( 40),\n# Randomly crop a square image of 40 pixels in both height and width to\n# produce a small square of 0.64 to 1 times the area of the original\n# image, and then scale it to a square of 32 pixels in both height and\n# width\ntorchvision .transforms .RandomResizedCrop( 32, scale =(0.64 ,1.0),\nratio =(1.0,1.0)),\ntorchvision .transforms .RandomHorizontalFlip(),\ntorchvision .transforms .ToTensor(),\n# Standardize each channel of the image\ntorchvision .transforms .Normalize([ 0.4914 ,0.4822 ,0.4465 ],\n[0.2023 ,0.1994 ,0.2010 ])])\nDuring testing, we only perform standardization on images so as to remove randomness in\nthe evaluation results.\ntransform_test =torchvision .transforms .Compose([\ntorchvision .transforms .ToTensor(),\ntorchvision .transforms .Normalize([ 0.4914 ,0.4822 ,0.4465 ],\n[0.2023 ,0.1994 ,0.2010 ])])\n14.13.3Readingthe Dataset\nNext, we read the organized dataset consisting of raw image files. Each example includes\nan image and a label.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95390684-5560-4fc4-ad8c-8ec02dc575a1": {"__data__": {"id_": "95390684-5560-4fc4-ad8c-8ec02dc575a1", "embedding": null, "metadata": {"page_label": "679", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a299c54e-0947-4772-a46f-d5d9a57a719a", "node_type": "4", "metadata": {"page_label": "679", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e8074d918756cb6dd157a8d581da35bec774890ec11720eb2bd119c7790f2bc7", "class_name": "RelatedNodeInfo"}}, "text": "679 Image Classi\ufb01cation (CIFAR-10) on Kaggle\ntrain_ds, train_valid_ds =[torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'train_valid_test ', folder),\ntransform =transform_train) for folder in['train ','train_valid ']]\nvalid_ds, test_ds =[torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'train_valid_test ', folder),\ntransform =transform_test) for folder in['valid ','test ']]\nDuring training, we need to specify all the image augmentation operations defined above.\nWhen the validation set is used for model evaluation during hyperparameter tuning, no\nrandomness from image augmentation should be introduced. Before final prediction, we\ntrain the model on the combined training set and validation set to make full use of all the\nlabeled data.\ntrain_iter, train_valid_iter =[torch .utils .data .DataLoader(\ndataset, batch_size, shuffle =True , drop_last =True )\nfor dataset in(train_ds, train_valid_ds)]\nvalid_iter =torch .utils .data .DataLoader(valid_ds, batch_size, shuffle =False ,\ndrop_last =True )\ntest_iter =torch .utils .data .DataLoader(test_ds, batch_size, shuffle =False ,\ndrop_last =False )\n14.13.4Defining the Model\nWe define the ResNet-18 model described in Section 8.6 .\ndef get_net ():\nnum_classes =10\nnet =d2l.resnet18(num_classes, 3)\nreturn net\nloss =nn.CrossEntropyLoss(reduction =\"none \")\n14.13.5Defining the TrainingFunction\nWe will select models and tune hyperparameters according to the model\u2019s performance on\nthe validation set. In the following, we define the model training function train.\ndef train (net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay):\ntrainer =torch .optim .SGD(net .parameters(), lr =lr, momentum =0.9,\nweight_decay =wd)\nscheduler =torch .optim .lr_scheduler .StepLR(trainer, lr_period, lr_decay)\nnum_batches, timer =len(train_iter), d2l .Timer()\nlegend =['train loss ','train acc ']\nifvalid_iter isnot None :\nlegend .append( 'valid acc ')\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4bd82f63-bb4c-4e84-bf44-6a87939cf60b": {"__data__": {"id_": "4bd82f63-bb4c-4e84-bf44-6a87939cf60b", "embedding": null, "metadata": {"page_label": "680", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "424a7c3f-4cbe-4b51-873f-cb880a91533f", "node_type": "4", "metadata": {"page_label": "680", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "687ee93feb9ab64121223d24ae6425b3942aeb1c01e420a3b324ced90a94e69e", "class_name": "RelatedNodeInfo"}}, "text": "680 Computer Vision\n(continued from previous page)\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],\nlegend =legend)\nnet =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])\nfor epoch inrange (num_epochs):\nnet.train()\nmetric =d2l.Accumulator( 3)\nfor i, (features, labels) inenumerate (train_iter):\ntimer .start()\nl, acc =d2l.train_batch_ch13(net, features, labels,\nloss, trainer, devices)\nmetric .add(l, acc, labels .shape[ 0])\ntimer .stop()\nif(i+1)%(num_batches //5)==0ori==num_batches -1:\nanimator .add(epoch +(i+1)/num_batches,\n(metric[ 0]/metric[ 2], metric[ 1]/metric[ 2],\nNone ))\nifvalid_iter isnot None :\nvalid_acc =d2l.evaluate_accuracy_gpu(net, valid_iter)\nanimator .add(epoch +1, (None ,None , valid_acc))\nscheduler .step()\nmeasures =(f'train loss {metric[ 0]/metric[ 2]:.3f},'\nf'train acc {metric[ 1]/metric[ 2]:.3f}')\nifvalid_iter isnot None :\nmeasures +=f', valid acc {valid_acc :.3f}'\nprint (measures +f'\\n{metric[ 2]*num_epochs /timer .sum() :.1f}'\nf'examples/sec on {str(devices) }')\n14.13.6Trainingand Validatingthe Model\nNow, wecantrainandvalidatethemodel. Allthefollowinghyperparameterscanbetuned.\nFor example, we can increase the number of epochs. When lr_period andlr_decay\nare set to 4 and 0.9, respectively, the learning rate of the optimization algorithm will be\nmultiplied by 0.9 after every 4 epochs. Just for ease of demonstration, we only train 20\nepochs here.\ndevices, num_epochs, lr, wd =d2l.try_all_gpus(), 20,2e-4 ,5e-4\nlr_period, lr_decay, net =4,0.9, get_net()\nnet( next (iter (train_iter))[ 0])\ntrain(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay)\ntrain loss 0.654 , train acc 0.789 , valid acc 0.438\n958.1 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]\n14.13.7Classifying the TestingSetand Submitting Resultson Kaggle\nAfter obtaining a promising model with hyperparameters, we use all the labeled data (in-\ncluding the validation set) to retrain the model and classify the testing set.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b78e44ce-8841-43b1-9356-95b875a27223": {"__data__": {"id_": "b78e44ce-8841-43b1-9356-95b875a27223", "embedding": null, "metadata": {"page_label": "681", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9644e0d-7967-4a02-aca2-837028f12e99", "node_type": "4", "metadata": {"page_label": "681", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cb7e9599a77db782e4b3c296d5ef55e0b89365373db6d3dc3bf7b2163f1fd8f4", "class_name": "RelatedNodeInfo"}}, "text": "681 Image Classi\ufb01cation (CIFAR-10) on Kaggle\nnet, preds =get_net(), []\nnet( next (iter (train_valid_iter))[ 0])\ntrain(net, train_valid_iter, None , num_epochs, lr, wd, devices, lr_period,\nlr_decay)\nfor X, _ intest_iter:\ny_hat =net(X .to(devices[ 0]))\npreds .extend(y_hat .argmax(dim =1).type(torch .int32) .cpu() .numpy())\nsorted_ids =list (range (1,len(test_ds) +1))\nsorted_ids .sort(key =lambda x:str(x))\ndf=pd.DataFrame({ 'id': sorted_ids, 'label ': preds})\ndf['label ']=df['label '].apply( lambda x: train_valid_ds .classes[x])\ndf.to_csv( 'submission.csv ', index =False )\ntrain loss 0.608 , train acc 0.786\n1040.8 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]\nThe abovecode will generatea submission.csv file, whose format meets the requirement\nof the Kaggle competition. The method for submitting results to Kaggle is similar to that\ninSection 5.7 .\n14.13.8Summary\n\u000fWe can read datasets containing raw image files after organizing them into the required\nformat.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1010, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9f41ad85-ff2f-4e67-b93d-383341400f98": {"__data__": {"id_": "9f41ad85-ff2f-4e67-b93d-383341400f98", "embedding": null, "metadata": {"page_label": "682", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51214602-82da-427b-af0c-be8b40887e07", "node_type": "4", "metadata": {"page_label": "682", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c632a3cfab4b948e861619d2e8c6a46ec1653fc81ae402125ab461890f6ff8b5", "class_name": "RelatedNodeInfo"}}, "text": "682 Computer Vision\n225\u000fWe can use convolutional neural networks and image augmentation in an image classi-\nfication competition.\n14.13.9Exercises\n1.Use the complete CIFAR-10 dataset for this Kaggle competition. Set hyperparameters\nasbatch_size = 128 ,num_epochs = 100 ,lr = 0.1 ,lr_period = 50 ,and lr_decay\n= 0.1. See what accuracy and ranking you can achieve in this competition. Can you\nfurther improve them?\n2.What accuracy can you get when not using image augmentation?\nDiscussions225.\n14.14Dog BreedIdentification (ImageNetDogs) on\nKaggle\nIn this section, we will practice the dog breed identification problem on Kaggle. The web\naddress of this competition is https://www.kaggle.com/c/dog-breed-identification\nIn this competition, 120 different breeds of dogs will be recognized. In fact, the dataset for\nthis competition is a subset of the ImageNet dataset. Unlike the images in the CIFAR-10\ndataset in Section 14.13 , the images in the ImageNet dataset are both higher and wider in\nvarying dimensions. Fig. 14.14.1 shows the information on the competition\u2019s webpage.\nYou need a Kaggle account to submit your results.\nimport os\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\n14.14.1Obtaining and Organizingthe Dataset\nThe competition dataset is divided into a training set and a test set, which contain 10222\nand 10357 JPEG images of three RGB (color) channels, respectively. Among the training\ndataset, there are 120 breeds of dogs such as Labradors, Poodles, Dachshunds, Samoyeds,\nHuskies, Chihuahuas, and Yorkshire Terriers.\nDownloadingthe Dataset\nAfter logging into Kaggle, you can click on the \u201cData\u201d tab on the competition webpage\nshown in Fig. 14.14.1 and download the dataset by clicking the \u201cDownload All\u201d button.\nAfter unzipping the downloaded file in ../data , you will find the entire dataset in the\nfollowing paths:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e121fb3b-03ff-42f3-84c9-a5db85f241d8": {"__data__": {"id_": "e121fb3b-03ff-42f3-84c9-a5db85f241d8", "embedding": null, "metadata": {"page_label": "683", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c805a67-0a63-4054-9ca6-d9cef8ab4d91", "node_type": "4", "metadata": {"page_label": "683", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7c3e735d907bfcab4d13aeb74b5867021410e9485ddf8a9a8605083937543488", "class_name": "RelatedNodeInfo"}}, "text": "683 Dog Breed Identi\ufb01cation (ImageNet Dogs) on Kaggle\ntFig. 14.14.1 The dog breed identi\ufb01cation competition website. The competition dataset can be\nobtained by clicking the \u201cData\u201d tab.\n\u000f../data/dog-breed-identification/labels.csv\n\u000f../data/dog-breed-identification/sample_submission.csv\n\u000f../data/dog-breed-identification/train\n\u000f../data/dog-breed-identification/test\nYoumayhavenoticedthattheabovestructureissimilartothatoftheCIFAR-10competition\ninSection14.13 ,wherefolders train/andtest/containtrainingandtestingdogimages,\nrespectively, and labels.csv contains the labels for the training images. Similarly, to\nmake it easier to get started, we provide a small sample of the dataset mentioned above:\ntrain_valid_test_tiny.zip . If you are going to use the full dataset for the Kaggle\ncompetition, you need to change the demovariable below to False.\n#@save\nd2l.DATA_HUB[ 'dog_tiny ']=(d2l .DATA_URL +'kaggle_dog_tiny.zip ',\n'0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d ')\n# If you use the full dataset downloaded for the Kaggle competition, change\n# the variable below to `False`\ndemo =True\nifdemo:\ndata_dir =d2l.download_extract( 'dog_tiny ')\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1163, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f422671-7ff5-4473-acdf-c1a5300b06c4": {"__data__": {"id_": "8f422671-7ff5-4473-acdf-c1a5300b06c4", "embedding": null, "metadata": {"page_label": "684", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "153f8743-c109-4227-8d0e-3cb67514c743", "node_type": "4", "metadata": {"page_label": "684", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4823f4978780b0f69f3f0527eaa8d364273f591d726afaf3b4c8c9f4f6c7f19a", "class_name": "RelatedNodeInfo"}}, "text": "684 Computer Vision\n(continued from previous page)\nelse :\ndata_dir =os.path .join( '..','data ','dog-breed-identification ')\nDownloading ../data /kaggle_dog_tiny .zip from http ://d2l-data .s3-accelerate .\n\u21a9!amazonaws .com/kaggle_dog_tiny .zip...\nOrganizingthe Dataset\nWe can organize the dataset similarly to what we did in Section 14.13 , namely splitting out\na validation set from the original training set, and moving images into subfolders grouped\nby labels.\nThereorg_dog_data functionbelowreadsthetrainingdatalabels,splitsoutthevalidation\nset, and organizes the training set.\ndef reorg_dog_data (data_dir, valid_ratio):\nlabels =d2l.read_csv_labels(os .path .join(data_dir, 'labels.csv '))\nd2l.reorg_train_valid(data_dir, labels, valid_ratio)\nd2l.reorg_test(data_dir)\nbatch_size =32ifdemo else 128\nvalid_ratio =0.1\nreorg_dog_data(data_dir, valid_ratio)\n14.14.2ImageAugmentation\nRecall that this dog breed dataset is a subset of the ImageNet dataset, whose images are\nlargerthanthoseoftheCIFAR-10datasetin Section14.13 . Thefollowinglistsafewimage\naugmentation operations that might be useful for relatively larger images.\ntransform_train =torchvision .transforms .Compose([\n# Randomly crop the image to obtain an image with an area of 0.08 to 1 of\n# the original area and height-to-width ratio between 3/4 and 4/3. Then,\n# scale the image to create a new 224 x 224 image\ntorchvision .transforms .RandomResizedCrop( 224, scale =(0.08 ,1.0),\nratio =(3.0/4.0,4.0/3.0)),\ntorchvision .transforms .RandomHorizontalFlip(),\n# Randomly change the brightness, contrast, and saturation\ntorchvision .transforms .ColorJitter(brightness =0.4,\ncontrast =0.4,\nsaturation =0.4),\n# Add random noise\ntorchvision .transforms .ToTensor(),\n# Standardize each channel of the image\ntorchvision .transforms .Normalize([ 0.485 ,0.456 ,0.406 ],\n[0.229 ,0.224 ,0.225 ])])\nDuringprediction,weonlyuseimagepreprocessingoperationswithoutrandomness.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2eae8a1f-7c00-4aff-8c35-1941ffa24651": {"__data__": {"id_": "2eae8a1f-7c00-4aff-8c35-1941ffa24651", "embedding": null, "metadata": {"page_label": "685", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce71b5d1-0b1b-40a9-a00a-26fba5fb8b4b", "node_type": "4", "metadata": {"page_label": "685", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fd8b46b1674151bcd3095dd33fc3e9ac2bf00c294bd69c93830d7c271bf90d64", "class_name": "RelatedNodeInfo"}}, "text": "685 Dog Breed Identi\ufb01cation (ImageNet Dogs) on Kaggle\ntransform_test =torchvision .transforms .Compose([\ntorchvision .transforms .Resize( 256),\n# Crop a 224 x 224 square area from the center of the image\ntorchvision .transforms .CenterCrop( 224),\ntorchvision .transforms .ToTensor(),\ntorchvision .transforms .Normalize([ 0.485 ,0.456 ,0.406 ],\n[0.229 ,0.224 ,0.225 ])])\n14.14.3Readingthe Dataset\nAsinSection14.13 ,wecanreadtheorganizeddatasetconsistingofrawimagefiles.\ntrain_ds, train_valid_ds =[torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'train_valid_test ', folder),\ntransform =transform_train) for folder in['train ','train_valid ']]\nvalid_ds, test_ds =[torchvision .datasets .ImageFolder(\nos.path .join(data_dir, 'train_valid_test ', folder),\ntransform =transform_test) for folder in['valid ','test ']]\nBelow we create data iterator instances the same way as in Section 14.13 .\ntrain_iter, train_valid_iter =[torch .utils .data .DataLoader(\ndataset, batch_size, shuffle =True , drop_last =True )\nfor dataset in(train_ds, train_valid_ds)]\nvalid_iter =torch .utils .data .DataLoader(valid_ds, batch_size, shuffle =False ,\ndrop_last =True )\ntest_iter =torch .utils .data .DataLoader(test_ds, batch_size, shuffle =False ,\ndrop_last =False )\n14.14.4Fine-Tuninga PretrainedModel\nAgain, the dataset for this competition is a subset of the ImageNet dataset. Therefore, we\ncan use the approach discussed in Section 14.2 to select a model pretrained on the full\nImageNet dataset and use it to extract image features to be fed into a custom small-scale\noutput network. High-level APIs of deep learning frameworks provide a wide range of\nmodelspretrainedontheImageNetdataset. Here,wechooseapretrainedResNet-34model,\nwhere we simply reuse the input of this model\u2019s output layer (i.e., the extracted features).\nThen we can replace the original output layer with a small custom output network that can\nbe trained, such as stacking two fully connected layers. Different from the experiment in\nSection14.2 ,thefollowingdoesnotretrainthepretrainedmodelusedforfeatureextraction.\nThis reduces training time and memory for storing gradients.\nRecall that we standardized images using the means and standard deviations of the three\nRGB channels for the full ImageNet dataset. In fact, this is also consistent with the stan-\ndardization operation by the pretrained model on ImageNet.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2380, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee6eeb25-c9b5-4a0a-ac6e-07b6973a45c4": {"__data__": {"id_": "ee6eeb25-c9b5-4a0a-ac6e-07b6973a45c4", "embedding": null, "metadata": {"page_label": "686", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f88ba83-e254-4fe7-8620-46afd37b9966", "node_type": "4", "metadata": {"page_label": "686", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9eea84aab677304cb1fa646581dce8ca964fbd428b6f81d526f31e89ce15326b", "class_name": "RelatedNodeInfo"}}, "text": "686 Computer Vision\ndef get_net (devices):\nfinetune_net =nn.Sequential()\nfinetune_net .features =torchvision .models .resnet34(pretrained =True )\n# Define a new output network (there are 120 output categories)\nfinetune_net .output_new =nn.Sequential(nn .Linear( 1000 ,256),\nnn.ReLU(),\nnn.Linear( 256,120))\n# Move the model to devices\nfinetune_net =finetune_net .to(devices[ 0])\n# Freeze parameters of feature layers\nfor param infinetune_net .features .parameters():\nparam .requires_grad =False\nreturn finetune_net\nBefore calculating the loss, we first obtain the input of the pretrained model\u2019s output layer,\ni.e., the extracted feature. Then we use this feature as input for our small custom output\nnetwork to calculate the loss.\nloss =nn.CrossEntropyLoss(reduction ='none ')\ndef evaluate_loss (data_iter, net, devices):\nl_sum, n =0.0,0\nfor features, labels indata_iter:\nfeatures, labels =features .to(devices[ 0]), labels .to(devices[ 0])\noutputs =net(features)\nl=loss(outputs, labels)\nl_sum +=l.sum()\nn+=labels .numel()\nreturn l_sum /n\n14.14.5Defining the TrainingFunction\nWe will select the model and tune hyperparameters according to the model\u2019s performance\non the validation set. The model training function trainonly iterates parameters of the\nsmall custom output network.\ndef train (net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay):\n# Only train the small custom output network\nnet =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])\ntrainer =torch .optim .SGD((param for param innet.parameters()\nifparam .requires_grad), lr =lr,\nmomentum =0.9, weight_decay =wd)\nscheduler =torch .optim .lr_scheduler .StepLR(trainer, lr_period, lr_decay)\nnum_batches, timer =len(train_iter), d2l .Timer()\nlegend =['train loss ']\nifvalid_iter isnot None :\nlegend .append( 'valid loss ')\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],\nlegend =legend)\nfor epoch inrange (num_epochs):\nmetric =d2l.Accumulator( 2)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "105adfb8-c69e-43ab-91ad-21a7dc27f40d": {"__data__": {"id_": "105adfb8-c69e-43ab-91ad-21a7dc27f40d", "embedding": null, "metadata": {"page_label": "687", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85b8a2aa-3b2a-416f-8590-f7f3959d1cf8", "node_type": "4", "metadata": {"page_label": "687", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f0c1789b97bc2ff1470b60dcb69db07b66bfd185629768ef578eee708820858e", "class_name": "RelatedNodeInfo"}}, "text": "687 Dog Breed Identi\ufb01cation (ImageNet Dogs) on Kaggle\n(continued from previous page)\nfor i, (features, labels) inenumerate (train_iter):\ntimer .start()\nfeatures, labels =features .to(devices[ 0]), labels .to(devices[ 0])\ntrainer .zero_grad()\noutput =net(features)\nl=loss(output, labels) .sum()\nl.backward()\ntrainer .step()\nmetric .add(l, labels .shape[ 0])\ntimer .stop()\nif(i+1)%(num_batches //5)==0ori==num_batches -1:\nanimator .add(epoch +(i+1)/num_batches,\n(metric[ 0]/metric[ 1],None ))\nmeasures =f'train loss {metric[ 0]/metric[ 1]:.3f}'\nifvalid_iter isnot None :\nvalid_loss =evaluate_loss(valid_iter, net, devices)\nanimator .add(epoch +1, (None , valid_loss .detach() .cpu()))\nscheduler .step()\nifvalid_iter isnot None :\nmeasures +=f', valid loss {valid_loss :.3f}'\nprint (measures +f'\\n{metric[ 1]*num_epochs /timer .sum() :.1f}'\nf'examples/sec on {str(devices) }')\n14.14.6Trainingand Validatingthe Model\nNow we can train and validate the model. The following hyperparameters are all tunable.\nFor example, the number of epochs can be increased. Because lr_period andlr_decay\nare set to 2 and 0.9, respectively, the learning rate of the optimization algorithm will be\nmultiplied by 0.9 after every 2 epochs.\ndevices, num_epochs, lr, wd =d2l.try_all_gpus(), 10,1e-4 ,1e-4\nlr_period, lr_decay, net =2,0.9, get_net(devices)\ntrain(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,\nlr_decay)\ntrain loss 1.240 , valid loss 1.545\n577.5 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5b35ed3-7cda-4f62-a004-03a375713c5f": {"__data__": {"id_": "e5b35ed3-7cda-4f62-a004-03a375713c5f", "embedding": null, "metadata": {"page_label": "688", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8176dbfb-831d-43b7-bddc-dd2a10529fd8", "node_type": "4", "metadata": {"page_label": "688", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1077cfacf5e1682429ecb6b24dcbc49b0bab6e6b4eb144482676bed81c2430db", "class_name": "RelatedNodeInfo"}}, "text": "688 Computer Vision\n14.14.7Classifying the TestingSetand Submitting Resultson Kaggle\nSimilar to the final step in Section 14.13 , in the end all the labeled data (including the\nvalidation set) are used for training the model and classifying the testing set. We will use\nthe trained custom output network for classification.\nnet =get_net(devices)\ntrain(net, train_valid_iter, None , num_epochs, lr, wd, devices, lr_period,\nlr_decay)\npreds =[]\nfor data, label intest_iter:\noutput =torch .nn.functional .softmax(net(data .to(devices[ 0])), dim =1)\npreds .extend(output .cpu() .detach() .numpy())\nids =sorted (os.listdir(\nos.path .join(data_dir, 'train_valid_test ','test ','unknown ')))\nwith open ('submission.csv ','w')asf:\nf.write( 'id,'+','.join(train_valid_ds .classes) +'\\n')\nfor i, output inzip(ids, preds):\nf.write(i .split( '.')[0]+','+','.join(\n[str(num) for num inoutput]) +'\\n')\ntrain loss 1.217\n742.7 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]\nTheabovecodewillgeneratea submission.csv filetobesubmittedtoKaggleinthesame\nway described in Section 5.7 .\n14.14.8Summary\n\u000fImagesintheImageNetdatasetarelarger(withvaryingdimensions)thanCIFAR-10im-\nages. We may modify image augmentation operations for tasks on a different dataset.\n\u000fTo classify a subset of the ImageNet dataset, we can leverage pre-trained models on the\nfull ImageNet dataset to extract features and only train a custom small-scale output\nnetwork. This will lead to less computational time and memory cost.\n14.14.9Exercises", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "520eb548-6559-433f-9948-6e73e1b43b30": {"__data__": {"id_": "520eb548-6559-433f-9948-6e73e1b43b30", "embedding": null, "metadata": {"page_label": "689", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be5a3c9b-efc9-4b99-a413-3d2c72c0ce96", "node_type": "4", "metadata": {"page_label": "689", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "34b6e1d951e257d010e4cbea714021118b7aec3c683fa47729c9ff451b5a23fc", "class_name": "RelatedNodeInfo"}}, "text": "689 Dog Breed Identi\ufb01cation (ImageNet Dogs) on Kaggle\n2261.WhenusingthefullKagglecompetitiondataset,whatresultscanyouachievewhenyou\nincrease batch_size (batch size) and num_epochs (number of epochs) while setting\nsome other hyperparameters as lr = 0.01 ,lr_period = 10 , and lr_decay = 0.1 ?\n2.Do you get better results if you use a deeper pretrained model? How do you tune hyper-\nparameters? Can you further improve the results?\nDiscussions226.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 445, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00a0c6da-b58a-4d7e-ae2d-14e3ebdcd07e": {"__data__": {"id_": "00a0c6da-b58a-4d7e-ae2d-14e3ebdcd07e", "embedding": null, "metadata": {"page_label": "690", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b69b97b-be75-40ee-a84d-807ffa98292b", "node_type": "4", "metadata": {"page_label": "690", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d3a3af1f3619a15502d5557ce5426a9d9c821fcddeb5d981745a21d76af842ed", "class_name": "RelatedNodeInfo"}}, "text": "15Natural Language Processing: Pretraining\nHumansneedtocommunicate. Outofthisbasicneedofthehumancondition,avastamount\nof written text has been generated on an everyday basis. Given rich text in social media,\nchat apps, emails, product reviews, news articles, research papers, and books, it becomes\nvital to enable computers to understand them to offer assistance or make decisions based\non human languages.\nNatural language processing studies interactions between computers and humans using\nnatural languages. In practice, it is very common to use natural language processing tech-\nniquestoprocessandanalyzetext(humannaturallanguage)data, suchaslanguagemodels\ninSection 9.3 and machine translation models in Section 10.5 .\nTo understand text, we can begin by learning its representations. Leveraging the existing\ntext sequences from large corpora, self-supervised learning has been extensively used to\npretrain text representations, such as by predicting some hidden part of the text using some\nother part of their surrounding text. In this way, models learn through supervision from\nmassive text data without expensive labeling efforts!\nAs we will see in this chapter, when treating each word or subword as an individual token,\nthe representation of each token can be pretrained using word2vec, GloVe, or subword\nembedding models on large corpora. After pretraining, representation of each token can\nbe a vector, however, it remains the same no matter what the context is. For instance,\nthe vector representation of \u201cbank\u201d is the same in both \u201cgo to the bank to deposit some\nmoney\u201d and \u201cgo to the bank to sit down\u201d. Thus, many more recent pretraining models\nadaptrepresentationofthesametokentodifferentcontexts. AmongthemisBERT,amuch\ndeeper self-supervised model based on the Transformer encoder. In this chapter, we will\nfocus on how to pretrain such representations for text, as highlighted in Fig. 15.1 .\nFor sight of the big picture, Fig. 15.1 shows that the pretrained text representations can be\nfed to a variety of deep learning architectures for different downstream natural language\nprocessing applications. We will cover them in Chapter 16 .\n690", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2156, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c33684d7-9377-4261-b531-3a085915a23e": {"__data__": {"id_": "c33684d7-9377-4261-b531-3a085915a23e", "embedding": null, "metadata": {"page_label": "691", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88c57ea9-bd30-4bef-8e83-b2b5a37ab8d8", "node_type": "4", "metadata": {"page_label": "691", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ebdd2aa49a3cdd0719ffbb73558c706e7be6f7a8ebe38f44b986dd38808fba5f", "class_name": "RelatedNodeInfo"}}, "text": "691 Word Embedding (word2vec)\ntFig. 15.1 Pretrained text representations can be fed to various deep learning architectures for\ndifferent downstream natural language processing applications. This chapter focuses on\nthe upstream text representation pretraining.\n22715.1WordEmbedding (word2vec)\nNatural language is a complex system used to express meanings. In this system, words\nare the basic unit of the meaning. As the name implies, word vectors are vectors used to\nrepresent words, and can also be considered as feature vectors or representations of words.\nThe technique of mapping words to real vectors is called wordembedding . In recent years,\nword embedding has gradually become the basic knowledge of natural language process-\ning.\n15.1.1One-HotVectorsArea Bad Choice\nWe used one-hot vectorsto represent words(characters are words)in Section 9.5 . Suppose\nthat the number of different words in the dictionary (the dictionary size) is \ud835\udc41, and each\nword corresponds to a different integer (index) from 0to\ud835\udc41\u00001. To obtain the one-hot\nvectorrepresentationforanywordwithindex \ud835\udc56, wecreatealength- \ud835\udc41vectorwithall0sand\nset the element at position \ud835\udc56to 1. In this way, each word is represented as a vector of length\n\ud835\udc41, and it can be used directly by neural networks.\nAlthoughone-hot wordvectorsare easyto construct, theyare usuallynot a goodchoice. A\nmain reason is that one-hot word vectors cannot accurately express the similarity between\ndifferentwords,suchasthe cosinesimilarity thatweoftenuse. Forvectors x,y2R\ud835\udc51,their\ncosine similarity is the cosine of the angle between them:\nx>y\nkxkkyk2\u00bb\u0000 1,1\u00bc. (15.1.1)\nSincethecosinesimilaritybetweenone-hotvectorsofanytwodifferentwordsis0,one-hot\nvectors cannot encode similarities among words.\n15.1.2Self-Supervisedword2vec\nTheword2vec227tool was proposed to address the above issue. It maps each word to a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e63113c5-e2a9-498a-a0d6-d6bbb19f7875": {"__data__": {"id_": "e63113c5-e2a9-498a-a0d6-d6bbb19f7875", "embedding": null, "metadata": {"page_label": "692", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be8fba07-60f5-4216-8222-bd8a3c56d0e9", "node_type": "4", "metadata": {"page_label": "692", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3f21562262110e8de4d7ae28a977d12835253bf1959658cdce59baeb6114abe3", "class_name": "RelatedNodeInfo"}}, "text": "692 Natural Language Processing: Pretraining\nfixed-lengthvector,andthesevectorscanbetterexpressthesimilarityandanalogyrelation-\nship among different words. The word2vec tool contains two models, namely skip-gram\n(Mikolovet al., 2013) andcontinuous bag of words (CBOW) ( Mikolovet al., 2013). For\nsemantically meaningful representations, their training relies on conditional probabilities\nthatcanbeviewedaspredictingsomewordsusingsomeoftheirsurroundingwordsincor-\npora. Sincesupervisioncomesfromthedatawithoutlabels,bothskip-gramandcontinuous\nbag of words are self-supervised models.\nIn the following, we will introduce these two models and their training methods.\n15.1.3TheSkip-Gram Model\nTheskip-gram modelassumesthatawordcanbeusedtogenerateitssurroundingwordsin\na text sequence. Take the text sequence \u201cthe\u201d, \u201cman\u201d, \u201cloves\u201d, \u201chis\u201d, \u201cson\u201d as an example.\nLet\u2019s choose \u201cloves\u201d as the centerword and set the context window size to 2. As shown in\nFig. 15.1.1 , given the center word \u201cloves\u201d, the skip-gram model considers the conditional\nprobability for generating the contextwords : \u201cthe\u201d, \u201cman\u201d, \u201chis\u201d, and \u201cson\u201d, which are no\nmore than 2 words away from the center word:\n\ud835\udc43\u00b9\u201dthe\u201d,\u201dman\u201d,\u201dhis\u201d,\u201dson\u201dj\u201dloves\u201d\u00ba. (15.1.2)\nAssume that the context words are independently generated given the center word (i.e.,\nconditional independence). In this case, the above conditional probability can be rewritten\nas\n\ud835\udc43\u00b9\u201dthe\u201dj\u201dloves\u201d\u00ba\u0001\ud835\udc43\u00b9\u201dman\u201dj\u201dloves\u201d\u00ba\u0001\ud835\udc43\u00b9\u201dhis\u201dj\u201dloves\u201d\u00ba\u0001\ud835\udc43\u00b9\u201dson\u201dj\u201dloves\u201d\u00ba.\n(15.1.3)\ntFig. 15.1.1 The skip-gram model considers the conditional probability of generating the surrounding\ncontext words given a center word.\nIn the skip-gram model, each word has two \ud835\udc51-dimensional-vector representations for cal-\nculating conditional probabilities. More concretely, for any word with index \ud835\udc56in the dic-\ntionary, denote by v\ud835\udc562R\ud835\udc51andu\ud835\udc562R\ud835\udc51its two vectors when used as a centerword and a\ncontextword, respectively. The conditional probability of generating any context word \ud835\udc64\ud835\udc5c\n(with index\ud835\udc5cin the dictionary) given the center word \ud835\udc64\ud835\udc50(with index\ud835\udc50in the dictionary)\ncan be modeled by a softmax operation on vector dot products:\n\ud835\udc43\u00b9\ud835\udc64\ud835\udc5cj\ud835\udc64\ud835\udc50\u00ba=exp\u00b9u>\n\ud835\udc5cv\ud835\udc50\u00ba\u00cd\n\ud835\udc562Vexp\u00b9u>\n\ud835\udc56v\ud835\udc50\u00ba, (15.1.4)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2153, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60087b53-0045-45fe-80a3-94536fc4e761": {"__data__": {"id_": "60087b53-0045-45fe-80a3-94536fc4e761", "embedding": null, "metadata": {"page_label": "693", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9697af5-4d69-4aff-a3f5-538429ef7550", "node_type": "4", "metadata": {"page_label": "693", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3d5551b5cd75d33ee4d763fa525d27f2cc181e4447d703971271d898019421d7", "class_name": "RelatedNodeInfo"}}, "text": "693 Word Embedding (word2vec)\nwhere the vocabulary index set V=f0,1,...,jVj\u0000 1g. Given a text sequence of length\n\ud835\udc47, where the word at time step \ud835\udc61is denoted as \ud835\udc64\u00b9\ud835\udc61\u00ba. Assume that context words are in-\ndependently generated given any center word. For context window size \ud835\udc5a, the likelihood\nfunction of the skip-gram model is the probability of generating all context words given\nany center word:\n\ud835\udc47\u00d6\n\ud835\udc61=1\u00d6\n\u0000\ud835\udc5a\u0014\ud835\udc57\u0014\ud835\udc5a, \ud835\udc57\u22600\ud835\udc43\u00b9\ud835\udc64\u00b9\ud835\udc61\u00b8\ud835\udc57\u00baj\ud835\udc64\u00b9\ud835\udc61\u00ba\u00ba, (15.1.5)\nwhere any time step that is less than 1or greater than \ud835\udc47can be omitted.\nTraining\nThe skip-gram model parameters are the center word vector and context word vector for\neachwordinthevocabulary. Intraining,welearnthemodelparametersbymaximizingthe\nlikelihoodfunction(i.e.,maximumlikelihoodestimation). Thisisequivalenttominimizing\nthe following loss function:\n\u0000\ud835\udc47\u00d5\n\ud835\udc61=1\u00d5\n\u0000\ud835\udc5a\u0014\ud835\udc57\u0014\ud835\udc5a, \ud835\udc57\u22600log\ud835\udc43\u00b9\ud835\udc64\u00b9\ud835\udc61\u00b8\ud835\udc57\u00baj\ud835\udc64\u00b9\ud835\udc61\u00ba\u00ba. (15.1.6)\nWhen using stochastic gradient descent to minimize the loss, in each iteration we can ran-\ndomly sample a shorter subsequence to calculate the (stochastic) gradient for this subse-\nquence to update the model parameters. To calculate this (stochastic) gradient, we need\nto obtain the gradients of the log conditional probability with respect to the center word\nvector and the context word vector. In general, according to (15.1.4 )the log conditional\nprobability involving any pair of the center word \ud835\udc64\ud835\udc50and the context word \ud835\udc64\ud835\udc5cis\nlog\ud835\udc43\u00b9\ud835\udc64\ud835\udc5cj\ud835\udc64\ud835\udc50\u00ba=u>\n\ud835\udc5cv\ud835\udc50\u0000log \u00d5\n\ud835\udc562Vexp\u00b9u>\n\ud835\udc56v\ud835\udc50\u00ba!\n. (15.1.7)\nThrough differentiation, we can obtain its gradient with respect to the center word vector\nv\ud835\udc50as\n\ud835\udf15log\ud835\udc43\u00b9\ud835\udc64\ud835\udc5cj\ud835\udc64\ud835\udc50\u00ba\n\ud835\udf15v\ud835\udc50=u\ud835\udc5c\u0000\u00cd\n\ud835\udc572Vexp\u00b9u>\n\ud835\udc57v\ud835\udc50\u00bau\ud835\udc57\u00cd\n\ud835\udc562Vexp\u00b9u>\n\ud835\udc56v\ud835\udc50\u00ba\n=u\ud835\udc5c\u0000\u00d5\n\ud835\udc572V exp\u00b9u>\n\ud835\udc57v\ud835\udc50\u00ba\n\u00cd\n\ud835\udc562Vexp\u00b9u>\n\ud835\udc56v\ud835\udc50\u00ba!\nu\ud835\udc57\n=u\ud835\udc5c\u0000\u00d5\n\ud835\udc572V\ud835\udc43\u00b9\ud835\udc64\ud835\udc57j\ud835\udc64\ud835\udc50\u00bau\ud835\udc57.(15.1.8)\nNote that the calculation in (15.1.8 )requires the conditional probabilities of all words in\nthe dictionary with \ud835\udc64\ud835\udc50as the center word. The gradients for the other word vectors can be\nobtained in the same way.\nAfter training, for any word with index \ud835\udc56in the dictionary, we obtain both word vectors", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67d091fe-dc9b-46ae-b5ba-2df989b15fe4": {"__data__": {"id_": "67d091fe-dc9b-46ae-b5ba-2df989b15fe4", "embedding": null, "metadata": {"page_label": "694", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aafd4e9e-d5fa-43c2-9eca-b06a98e65060", "node_type": "4", "metadata": {"page_label": "694", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "93b9bb06fdd9bd22c074875c841bbd60d1cb5cfceb28d3cc1b5537fb57444a4e", "class_name": "RelatedNodeInfo"}}, "text": "694 Natural Language Processing: Pretraining\nv\ud835\udc56(as the center word) and u\ud835\udc56(as the context word). In natural language processing ap-\nplications, the center word vectors of the skip-gram model are typically used as the word\nrepresentations.\n15.1.4TheContinuous Bag of Words(CBOW)Model\nThecontinuousbagofwords (CBOW)modelissimilartotheskip-grammodel. Themajor\ndifferencefromtheskip-grammodelisthatthecontinuousbagofwordsmodelassumesthat\nacenterwordisgeneratedbasedonitssurroundingcontextwordsinthetextsequence. For\nexample,inthesametextsequence\u201cthe\u201d,\u201cman\u201d,\u201cloves\u201d,\u201chis\u201d,and\u201cson\u201d,with\u201cloves\u201das\nthe center word and the context window size being 2, the continuous bag of words model\nconsiders the conditional probability of generating the center word \u201cloves\u201d based on the\ncontext words \u201cthe\u201d, \u201cman\u201d, \u201chis\u201d and \u201cson\u201d (as shown in Fig. 15.1.2 ), which is\n\ud835\udc43\u00b9\u201dloves\u201dj\u201dthe\u201d,\u201dman\u201d,\u201dhis\u201d,\u201dson\u201d\u00ba. (15.1.9)\ntFig. 15.1.2 The continuous bag of words model considers the conditional probability of generating\nthe center word given its surrounding context words.\nSincetherearemultiplecontextwordsinthecontinuousbagofwordsmodel, thesecontext\nword vectorsare averagedin the calculation of the conditional probability. Specifically, for\nany word with index \ud835\udc56in the dictionary, denote by v\ud835\udc562R\ud835\udc51andu\ud835\udc562R\ud835\udc51its two vectors\nwhen used as a contextword and a centerword (meanings are switched in the skip-gram\nmodel), respectively. The conditional probability of generating any center word \ud835\udc64\ud835\udc50(with\nindex\ud835\udc50in the dictionary) given its surrounding context words \ud835\udc64\ud835\udc5c1,...,\ud835\udc64\ud835\udc5c2\ud835\udc5a(with index\n\ud835\udc5c1,...,\ud835\udc5c 2\ud835\udc5ain the dictionary) can be modeled by\n\ud835\udc43\u00b9\ud835\udc64\ud835\udc50j\ud835\udc64\ud835\udc5c1,...,\ud835\udc64\ud835\udc5c2\ud835\udc5a\u00ba=exp\u0010\n1\n2\ud835\udc5au>\n\ud835\udc50\u00b9v\ud835\udc5c1\u00b8...\u00b8v\ud835\udc5c2\ud835\udc5a\u00ba\u0011\n\u00cd\n\ud835\udc562Vexp\u0010\n1\n2\ud835\udc5au>\n\ud835\udc56\u00b9v\ud835\udc5c1\u00b8...\u00b8v\ud835\udc5c2\ud835\udc5a\u00ba\u0011. (15.1.10)\nForbrevity,letW\ud835\udc5c=f\ud835\udc64\ud835\udc5c1,...,\ud835\udc64\ud835\udc5c2\ud835\udc5agand \u00afv\ud835\udc5c=\u0000v\ud835\udc5c1\u00b8...\u00b8v\ud835\udc5c2\ud835\udc5a\u0001\u009d\u00b92\ud835\udc5a\u00ba. Then (15.1.10 )\ncan be simplified as\n\ud835\udc43\u00b9\ud835\udc64\ud835\udc50jW\ud835\udc5c\u00ba=exp\u0000u>\n\ud835\udc50\u00afv\ud835\udc5c\u0001\n\u00cd\n\ud835\udc562Vexp\u0000u>\n\ud835\udc56\u00afv\ud835\udc5c\u0001. (15.1.11)\nGiven a text sequence of length \ud835\udc47, where the word at time step \ud835\udc61is denoted as \ud835\udc64\u00b9\ud835\udc61\u00ba. For", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c34ef9b-041e-4cb1-baa3-ce1ba0f59151": {"__data__": {"id_": "9c34ef9b-041e-4cb1-baa3-ce1ba0f59151", "embedding": null, "metadata": {"page_label": "695", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4b87256a-30e7-44a9-91ff-f96bf683ae27", "node_type": "4", "metadata": {"page_label": "695", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b4d298338c0cc9b4d9a0c35e374d085249de4230d3267e18da48739337381dd7", "class_name": "RelatedNodeInfo"}}, "text": "695 Word Embedding (word2vec)\ncontext window size \ud835\udc5a, the likelihood function of the continuous bag of words model is\nthe probability of generating all center words given their context words:\n\ud835\udc47\u00d6\n\ud835\udc61=1\ud835\udc43\u00b9\ud835\udc64\u00b9\ud835\udc61\u00baj\ud835\udc64\u00b9\ud835\udc61\u0000\ud835\udc5a\u00ba,...,\ud835\udc64\u00b9\ud835\udc61\u00001\u00ba,\ud835\udc64\u00b9\ud835\udc61\u00b81\u00ba,...,\ud835\udc64\u00b9\ud835\udc61\u00b8\ud835\udc5a\u00ba\u00ba. (15.1.12)\nTraining\nTrainingcontinuousbagofwordsmodelsisalmostthesameastrainingskip-grammodels.\nThe maximum likelihood estimation of the continuous bag of words model is equivalent to\nminimizing the following loss function:\n\u0000\ud835\udc47\u00d5\n\ud835\udc61=1log\ud835\udc43\u00b9\ud835\udc64\u00b9\ud835\udc61\u00baj\ud835\udc64\u00b9\ud835\udc61\u0000\ud835\udc5a\u00ba,...,\ud835\udc64\u00b9\ud835\udc61\u00001\u00ba,\ud835\udc64\u00b9\ud835\udc61\u00b81\u00ba,...,\ud835\udc64\u00b9\ud835\udc61\u00b8\ud835\udc5a\u00ba\u00ba. (15.1.13)\nNotice that\nlog\ud835\udc43\u00b9\ud835\udc64\ud835\udc50jW\ud835\udc5c\u00ba=u>\n\ud835\udc50\u00afv\ud835\udc5c\u0000log \u00d5\n\ud835\udc562Vexp\u0000u>\n\ud835\udc56\u00afv\ud835\udc5c\u0001!\n. (15.1.14)\nThrough differentiation, we can obtain its gradient with respect to any context word vector\nv\ud835\udc5c\ud835\udc56(\ud835\udc56=1,..., 2\ud835\udc5a) as\n\ud835\udf15log\ud835\udc43\u00b9\ud835\udc64\ud835\udc50jW\ud835\udc5c\u00ba\n\ud835\udf15v\ud835\udc5c\ud835\udc56=1\n2\ud835\udc5a\u00a9\u00ad\n\u00abu\ud835\udc50\u0000\u00d5\n\ud835\udc572Vexp\u00b9u>\n\ud835\udc57\u00afv\ud835\udc5c\u00bau\ud835\udc57\u00cd\n\ud835\udc562Vexp\u00b9u>\n\ud835\udc56\u00afv\ud835\udc5c\u00ba\u00aa\u00ae\n\u00ac=1\n2\ud835\udc5a\u00a9\u00ad\n\u00abu\ud835\udc50\u0000\u00d5\n\ud835\udc572V\ud835\udc43\u00b9\ud835\udc64\ud835\udc57jW\ud835\udc5c\u00bau\ud835\udc57\u00aa\u00ae\n\u00ac.\n(15.1.15)\nThe gradients for the other word vectors can be obtained in the same way. Unlike the skip-\ngram model, the continuous bag of words model typically uses context word vectors as the\nword representations.\n15.1.5Summary\n\u000fWord vectors are vectors used to represent words, and can also be considered as feature\nvectors or representations of words. The technique of mapping words to real vectors\nis called word embedding.\n\u000fThe word2vec tool contains both the skip-gram and continuous bag of words models.\n\u000fTheskip-grammodelassumesthatawordcanbeusedtogenerateitssurroundingwords\nin a text sequence; while the continuous bag of words model assumes that a center\nword is generated based on its surrounding context words.\n15.1.6Exercises\n1.Whatis the computational complexityforcalculating eachgradient? What could be the\nissue if the dictionary size is huge?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20ab309f-ff31-4269-9fba-6d4909b01a86": {"__data__": {"id_": "20ab309f-ff31-4269-9fba-6d4909b01a86", "embedding": null, "metadata": {"page_label": "696", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed46322f-6583-4922-9e4a-01e75987ef46", "node_type": "4", "metadata": {"page_label": "696", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8365022676f8c001f89e44c0db0ff9cb88b97ab8b6c9640522bb087bcddadd1e", "class_name": "RelatedNodeInfo"}}, "text": "696 Natural Language Processing: Pretraining\n2282.Some fixed phrases in English consist of multiple words, such as \u201cnew york\u201d. How to\ntrain their word vectors? Hint: see Section 4 in the word2vec paper ( Mikolovet al.,\n2013).\n3.Let\u2019sreflectontheword2vecdesignbytakingtheskip-grammodelasanexample. What\nis the relationship between the dot product of two word vectors in the skip-gram model\nand the cosine similarity? For a pair of words with similar semantics, why may the\ncosine similarity of their word vectors (trained by the skip-gram model) be high?\nDiscussions228.\n15.2ApproximateTraining\nRecall our discussions in Section 15.1 . The main idea of the skip-gram model is using\nsoftmax operations to calculate the conditional probability of generating a context word\n\ud835\udc64\ud835\udc5cbased on the given center word \ud835\udc64\ud835\udc50in(15.1.4 ), whose corresponding logarithmic loss\nis given by the opposite of (15.1.7 ).\nDue to the nature of the softmax operation, since a context word may be anyone in the\ndictionaryV, the opposite of (15.1.7 )contains the summation of items as many as the\nentire size of the vocabulary. Consequently, the gradient calculation for the skip-gram\nmodelin (15.1.8 )andthatforthecontinuousbag-of-wordsmodelin (15.1.15 )bothcontain\nthe summation. Unfortunately, the computational cost for such gradients that sum over a\nlarge dictionary (often with hundreds of thousands or millions of words) is huge!\nInordertoreducetheaforementionedcomputationalcomplexity,thissectionwillintroduce\ntwoapproximatetrainingmethods: negativesampling andhierarchicalsoftmax . Duetothe\nsimilarity between the skip-gram model and the continuous bag of words model, we will\njust take the skip-gram model as an example to describe these two approximate training\nmethods.\n15.2.1NegativeSampling\nNegative sampling modifies the original objective function. Given the context window of\na center word \ud835\udc64\ud835\udc50, the fact that any (context) word \ud835\udc64\ud835\udc5ccomes from this context window is\nconsidered as an event with the probability modeled by\n\ud835\udc43\u00b9\ud835\udc37=1j\ud835\udc64\ud835\udc50,\ud835\udc64\ud835\udc5c\u00ba=\ud835\udf0e\u00b9u>\n\ud835\udc5cv\ud835\udc50\u00ba, (15.2.1)\nwhere\ud835\udf0euses the definition of the sigmoid activation function:\n\ud835\udf0e\u00b9\ud835\udc65\u00ba=1\n1\u00b8exp\u00b9\u0000\ud835\udc65\u00ba. (15.2.2)\nLet\u2019s begin by maximizing the joint probability of all such events in text sequences to train\nword embeddings. Specifically, given a text sequence of length \ud835\udc47, denote by\ud835\udc64\u00b9\ud835\udc61\u00bathe word", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2306, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "603bbae9-6235-40dd-84d0-f939c4110e04": {"__data__": {"id_": "603bbae9-6235-40dd-84d0-f939c4110e04", "embedding": null, "metadata": {"page_label": "697", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dbde8267-f37d-4e57-a99b-5ef58957fcfd", "node_type": "4", "metadata": {"page_label": "697", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d0b1d69692658ba1c2a630824a6a3cf6f104a55d2eb2c473d8ff0a19257cb19c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0d5bbe9-09f1-4854-9c8d-7cf6ea109deb", "node_type": "1", "metadata": {}, "hash": "89b9ef280c53906a4cdde4319ad94119292a75182aa74bcccd718bbeea847cf7", "class_name": "RelatedNodeInfo"}}, "text": "697 Approximate Training\nat time step\ud835\udc61and let the context window size be \ud835\udc5a, consider maximizing the joint proba-\nbility\n\ud835\udc47\u00d6\n\ud835\udc61=1\u00d6\n\u0000\ud835\udc5a\u0014\ud835\udc57\u0014\ud835\udc5a, \ud835\udc57\u22600\ud835\udc43\u00b9\ud835\udc37=1j\ud835\udc64\u00b9\ud835\udc61\u00ba,\ud835\udc64\u00b9\ud835\udc61\u00b8\ud835\udc57\u00ba\u00ba. (15.2.3)\nHowever, (15.2.3 )only considers those events that involve positive examples. As a result,\nthe joint probability in (15.2.3 )is maximized to 1 only if all the word vectors are equal\nto infinity. Of course, such results are meaningless. To make the objective function more\nmeaningful, negative sampling adds negative examples sampled from a predefined distri-\nbution.\nDenote by\ud835\udc46the event that a context word \ud835\udc64\ud835\udc5ccomes from the context window of a cen-\nter word\ud835\udc64\ud835\udc50. For this event involving \ud835\udc64\ud835\udc5c, from a predefined distribution \ud835\udc43\u00b9\ud835\udc64\u00basample\ud835\udc3e\nnoise words that are not from this context window. Denote by \ud835\udc41\ud835\udc58the event that a noise\nword\ud835\udc64\ud835\udc58(\ud835\udc58=1,...,\ud835\udc3e) does not come from the context window of \ud835\udc64\ud835\udc50. Assume that these\neventsinvolvingboththepositiveexampleandnegativeexamples \ud835\udc46,\ud835\udc41 1,...,\ud835\udc41\ud835\udc3earemutu-\nally independent. Negative sampling rewrites the joint probability (involving only positive\nexamples) in (15.2.3 )as\n\ud835\udc47\u00d6\n\ud835\udc61=1\u00d6\n\u0000\ud835\udc5a\u0014\ud835\udc57\u0014\ud835\udc5a, \ud835\udc57\u22600\ud835\udc43\u00b9\ud835\udc64\u00b9\ud835\udc61\u00b8\ud835\udc57\u00baj\ud835\udc64\u00b9\ud835\udc61\u00ba\u00ba, (15.2.4)\nwhere the conditional probability is approximated through events \ud835\udc46,\ud835\udc41 1,...,\ud835\udc41\ud835\udc3e:\n\ud835\udc43\u00b9\ud835\udc64\u00b9\ud835\udc61\u00b8\ud835\udc57\u00baj\ud835\udc64\u00b9\ud835\udc61\u00ba\u00ba=\ud835\udc43\u00b9\ud835\udc37=1j\ud835\udc64\u00b9\ud835\udc61\u00ba,\ud835\udc64\u00b9\ud835\udc61\u00b8\ud835\udc57\u00ba\u00ba\ud835\udc3e\u00d6\n\ud835\udc58=1, \ud835\udc64\ud835\udc58\u0018\ud835\udc43\u00b9\ud835\udc64\u00ba\ud835\udc43\u00b9\ud835\udc37=0j\ud835\udc64\u00b9\ud835\udc61\u00ba,\ud835\udc64\ud835\udc58\u00ba.(15.2.5)\nDenote by\ud835\udc56\ud835\udc61and\u210e\ud835\udc58the indices of a word \ud835\udc64\u00b9\ud835\udc61\u00baat time step\ud835\udc61of a text sequence and a noise\nword\ud835\udc64\ud835\udc58, respectively. The logarithmic loss with respect to the conditional probabilities in\n(15.2.5 )is\n\u0000log\ud835\udc43\u00b9\ud835\udc64\u00b9\ud835\udc61\u00b8\ud835\udc57\u00baj\ud835\udc64\u00b9\ud835\udc61\u00ba\u00ba=\u0000log\ud835\udc43\u00b9\ud835\udc37=1j\ud835\udc64\u00b9\ud835\udc61\u00ba,\ud835\udc64\u00b9\ud835\udc61\u00b8\ud835\udc57\u00ba\u00ba\u0000\ud835\udc3e\u00d5\n\ud835\udc58=1, \ud835\udc64\ud835\udc58\u0018\ud835\udc43\u00b9\ud835\udc64\u00balog\ud835\udc43\u00b9\ud835\udc37=0j\ud835\udc64\u00b9\ud835\udc61\u00ba,\ud835\udc64\ud835\udc58\u00ba\n=\u0000log\ud835\udf0e\u0010\nu>\n\ud835\udc56\ud835\udc61\u00b8\ud835\udc57v\ud835\udc56\ud835\udc61\u0011\n\u0000\ud835\udc3e\u00d5\n\ud835\udc58=1, \ud835\udc64\ud835\udc58\u0018\ud835\udc43\u00b9\ud835\udc64\u00balog\u0010\n1\u0000\ud835\udf0e\u0010\nu>\n\u210e\ud835\udc58v\ud835\udc56\ud835\udc61\u0011\u0011\n=\u0000log\ud835\udf0e\u0010\nu>\n\ud835\udc56\ud835\udc61\u00b8\ud835\udc57v\ud835\udc56\ud835\udc61\u0011\n\u0000\ud835\udc3e\u00d5\n\ud835\udc58=1, \ud835\udc64\ud835\udc58\u0018\ud835\udc43\u00b9\ud835\udc64\u00balog\ud835\udf0e\u0010\n\u0000u>\n\u210e\ud835\udc58v\ud835\udc56\ud835\udc61\u0011\n.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1634, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0d5bbe9-09f1-4854-9c8d-7cf6ea109deb": {"__data__": {"id_": "c0d5bbe9-09f1-4854-9c8d-7cf6ea109deb", "embedding": null, "metadata": {"page_label": "697", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dbde8267-f37d-4e57-a99b-5ef58957fcfd", "node_type": "4", "metadata": {"page_label": "697", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d0b1d69692658ba1c2a630824a6a3cf6f104a55d2eb2c473d8ff0a19257cb19c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "603bbae9-6235-40dd-84d0-f939c4110e04", "node_type": "1", "metadata": {"page_label": "697", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "819fbbaec014e2ce7ca053f0fc62b1eb09abae9ac720535d0fd0a3455ababee6", "class_name": "RelatedNodeInfo"}}, "text": "(15.2.6)\nWe can see that now the computational cost for gradients at each training step has nothing\ntodowiththedictionarysize, butlinearlydependson \ud835\udc3e. Whensettingthehyperparameter\n\ud835\udc3etoasmallervalue,thecomputationalcostforgradientsateachtrainingstepwithnegative\nsampling is smaller.", "mimetype": "text/plain", "start_char_idx": 1635, "end_char_idx": 1915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b045def-dc24-4b65-86fb-555504f169f8": {"__data__": {"id_": "0b045def-dc24-4b65-86fb-555504f169f8", "embedding": null, "metadata": {"page_label": "698", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf3aec92-d71a-4240-afee-bcae965b15d0", "node_type": "4", "metadata": {"page_label": "698", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fa1abbb201bbe557879503f557427bd23ea22633955ed4ce63ad4ee85105a0ae", "class_name": "RelatedNodeInfo"}}, "text": "698 Natural Language Processing: Pretraining\n15.2.2HierarchicalSoftmax\nAsanalternativeapproximatetrainingmethod, hierarchicalsoftmax usesthebinarytree, a\ndata structure illustrated in Fig. 15.2.1 , where each leaf node of the tree represents a word\nin dictionaryV.\ntFig. 15.2.1 Hierarchical softmax for approximate training, where each leaf node of the tree represents\na word in the dictionary.\nDenote by\ud835\udc3f\u00b9\ud835\udc64\u00bathe number of nodes (including both ends) on the path from the root node\nto the leaf node representing word \ud835\udc64in the binary tree. Let \ud835\udc5b\u00b9\ud835\udc64,\ud835\udc57\u00babe the\ud835\udc57thnode on this\npath, with its context word vector being u\ud835\udc5b\u00b9\ud835\udc64,\ud835\udc57\u00ba. For example, \ud835\udc3f\u00b9\ud835\udc643\u00ba=4inFig. 15.2.1 .\nHierarchical softmax approximates the conditional probability in (15.1.4 )as\n\ud835\udc43\u00b9\ud835\udc64\ud835\udc5cj\ud835\udc64\ud835\udc50\u00ba=\ud835\udc3f\u00b9\ud835\udc64\ud835\udc5c\u00ba\u00001\u00d6\n\ud835\udc57=1\ud835\udf0e\u0010\n\u00bb\u00bb\ud835\udc5b\u00b9\ud835\udc64\ud835\udc5c,\ud835\udc57\u00b81\u00ba=leftChild\u00b9\ud835\udc5b\u00b9\ud835\udc64\ud835\udc5c,\ud835\udc57\u00ba\u00ba\u00bc\u00bc\u0001u>\n\ud835\udc5b\u00b9\ud835\udc64\ud835\udc5c,\ud835\udc57\u00bav\ud835\udc50\u0011\n,\n(15.2.7)\nwhere function \ud835\udf0eis defined in (15.2.2 ), and leftChild\u00b9\ud835\udc5b\u00bais the left child node of node \ud835\udc5b:\nif\ud835\udc65is true,\u00bb\u00bb\ud835\udc65\u00bc\u00bc=1; otherwise\u00bb\u00bb\ud835\udc65\u00bc\u00bc=\u00001.\nTo illustrate, let\u2019s calculate the conditional probability of generating word \ud835\udc643given word\n\ud835\udc64\ud835\udc50inFig. 15.2.1 . This requires dot products between the word vector v\ud835\udc50of\ud835\udc64\ud835\udc50and non-\nleaf node vectors on the path (the path in bold in Fig. 15.2.1 ) from the root to \ud835\udc643, which is\ntraversed left, right, then left:\n\ud835\udc43\u00b9\ud835\udc643j\ud835\udc64\ud835\udc50\u00ba=\ud835\udf0e\u00b9u>\n\ud835\udc5b\u00b9\ud835\udc643,1\u00bav\ud835\udc50\u00ba\u0001\ud835\udf0e\u00b9\u0000u>\n\ud835\udc5b\u00b9\ud835\udc643,2\u00bav\ud835\udc50\u00ba\u0001\ud835\udf0e\u00b9u>\n\ud835\udc5b\u00b9\ud835\udc643,3\u00bav\ud835\udc50\u00ba. (15.2.8)\nSince\ud835\udf0e\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf0e\u00b9\u0000\ud835\udc65\u00ba=1, it holds that the conditional probabilities of generating all the\nwords in dictionary Vbased on any word \ud835\udc64\ud835\udc50sum up to one:\n\u00d5\n\ud835\udc642V\ud835\udc43\u00b9\ud835\udc64j\ud835\udc64\ud835\udc50\u00ba=1.(15.2.9)\nFortunately, since \ud835\udc3f\u00b9\ud835\udc64\ud835\udc5c\u00ba\u00001is on the order ofO\u00b9log2jVj\u00badue to the binary tree struc-\nture, when the dictionary size Vis huge, the computational cost for each training step us-\ning hierarchical softmax is significantly reduced compared with that without approximate\ntraining.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a62ae3d-f907-4b3c-98a5-d8adea9cf3b1": {"__data__": {"id_": "7a62ae3d-f907-4b3c-98a5-d8adea9cf3b1", "embedding": null, "metadata": {"page_label": "699", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "764f70f0-b2b0-4ce2-9594-5cc7e4b35650", "node_type": "4", "metadata": {"page_label": "699", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6dd41ea2d14a63bbabb2512d9623f697d7785db9b88b4788641d955fcfe4440b", "class_name": "RelatedNodeInfo"}}, "text": "699 The Dataset for Pretraining Word Embeddings\n229\n23015.2.3Summary\n\u000fNegative sampling constructs the loss function by considering mutually independent\nevents that involve both positive and negative examples. The computational cost for\ntraining is linearly dependent on the number of noise words at each step.\n\u000fHierarchical softmax constructs the loss function using the path from the root node to\nthe leaf node in the binary tree. The computational cost for training is dependent on\nthe logarithm of the dictionary size at each step.\n15.2.4Exercises\n1.How can we sample noise words in negative sampling?\n2.Verify that (15.2.9 )holds.\n3.Howtotrainthecontinuousbagofwordsmodelusingnegativesamplingandhierarchi-\ncal softmax, respectively?\nDiscussions229.\n15.3The DatasetforPretrainingWordEmbeddings\nNow that we know the technical details of the word2vec models and approximate training\nmethods, let\u2019s walk through their implementations. Specifically, we will take the skip-\ngram model in Section 15.1 and negative sampling in Section 15.2 as an example. In this\nsection, we begin with the dataset for pretraining the word embedding model: the original\nformat of the data will be transformed into minibatches that can be iterated over during\ntraining.\nimport collections\nimport math\nimport os\nimport random\nimport torch\nfrom d2l import torch asd2l\n15.3.1Readingthe Dataset\nThedatasetthatweusehereis PennTreeBank(PTB)230.ThiscorpusissampledfromWall\nStreet Journal articles, split into training, validation, and test sets. In the original format,\neach line of the text file represents a sentence of words that are separated by spaces. Here\nwe treat each word as a token.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b7c982d-6e6f-4233-acf0-7a0e66280f28": {"__data__": {"id_": "4b7c982d-6e6f-4233-acf0-7a0e66280f28", "embedding": null, "metadata": {"page_label": "700", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1e645a1-7aac-412a-b4c7-0d1e163e8716", "node_type": "4", "metadata": {"page_label": "700", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a35fb655af732752b4c2e575704bfda2717c9b85ddef8e0d05b5634c1b1abb16", "class_name": "RelatedNodeInfo"}}, "text": "700 Natural Language Processing: Pretraining\n#@save\nd2l.DATA_HUB[ 'ptb']=(d2l .DATA_URL +'ptb.zip ',\n'319d85e578af0cdc590547f26231e4e31cdf1e42 ')\n#@save\ndef read_ptb ():\n\"\"\"Load the PTB dataset into a list of text lines.\"\"\"\ndata_dir =d2l.download_extract( 'ptb')\n# Read the training set\nwith open (os.path .join(data_dir, 'ptb.train.txt '))asf:\nraw_text =f.read()\nreturn [line .split() for line inraw_text .split( '\\n')]\nsentences =read_ptb()\nf'# sentences: {len(sentences) }'\nDownloading ../data /ptb.zip from http ://d2l-data .s3-accelerate .amazonaws .com/\n\u21a9!ptb.zip...\n'# sentences: 42069 '\nAfter reading the training set, we build a vocabulary for the corpus, where any word that\nappears less than 10 times is replaced by the \u201c<unk>\u201d token. Note that the original dataset\nalso contains \u201c<unk>\u201d tokens that represent rare (unknown) words.\nvocab =d2l.Vocab(sentences, min_freq =10)\nf'vocab size: {len(vocab) }'\n'vocab size: 6719 '\n15.3.2Subsampling\nText data typically have high-frequency words such as \u201cthe\u201d, \u201ca\u201d, and \u201cin\u201d: they may even\noccur billions of times in very large corpora. However, these words often co-occur with\nmany different words in context windows, providing little useful signals. For instance,\nconsider the word \u201cchip\u201d in a context window: intuitively its co-occurrence with a low-\nfrequency word \u201cintel\u201d is more useful in training than the co-occurrence with a high-\nfrequency word \u201ca\u201d. Moreover, training with vast amounts of (high-frequency) words is\nslow. Thus, when training word embedding models, high-frequency words can be sub-\nsampled (Mikolovet al., 2013). Specifically, each indexed word \ud835\udc64\ud835\udc56in the dataset will be\ndiscarded with probability\n\ud835\udc43\u00b9\ud835\udc64\ud835\udc56\u00ba=max\u0012\n1\u0000r\ud835\udc61\n\ud835\udc53\u00b9\ud835\udc64\ud835\udc56\u00ba,0\u0013\n, (15.3.1)\nwhere\ud835\udc53\u00b9\ud835\udc64\ud835\udc56\u00bais the ratio of the number of words \ud835\udc64\ud835\udc56to the total number of words in the\ndataset, and the constant \ud835\udc61is a hyperparameter ( 10\u00004in the experiment). We can see that", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1883, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c08bd1c-1a63-4776-a9f2-d5856aa28fcb": {"__data__": {"id_": "9c08bd1c-1a63-4776-a9f2-d5856aa28fcb", "embedding": null, "metadata": {"page_label": "701", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e29889e9-a394-44d3-b666-4cc65390941e", "node_type": "4", "metadata": {"page_label": "701", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "809bce1c554a7eea989a9685fb70ed9e1154075967fd34cad4643581e01841f4", "class_name": "RelatedNodeInfo"}}, "text": "701 The Dataset for Pretraining Word Embeddings\nonlywhentherelativefrequency \ud835\udc53\u00b9\ud835\udc64\ud835\udc56\u00ba>\ud835\udc61canthe(high-frequency)word \ud835\udc64\ud835\udc56bediscarded,\nand the higher the relative frequency of the word, the greater the probability of being dis-\ncarded.\n#@save\ndef subsample (sentences, vocab):\n\"\"\"Subsample high-frequency words.\"\"\"\n# Exclude unknown tokens ('<unk>')\nsentences =[[token for token inline ifvocab[token] !=vocab .unk]\nfor line insentences]\ncounter =collections .Counter([\ntoken for line insentences for token inline])\nnum_tokens =sum(counter .values())\n# Return True if `token` is kept during subsampling\ndef keep (token):\nreturn (random .uniform( 0,1)<\nmath .sqrt( 1e-4 /counter[token] *num_tokens))\nreturn ([[token for token inline ifkeep(token)] for line insentences],\ncounter)\nsubsampled, counter =subsample(sentences, vocab)\nThe following code snippet plots the histogram of the number of tokens per sentence be-\nfore and after subsampling. As expected, subsampling significantly shortens sentences by\ndropping high-frequency words, which will lead to training speedup.\nd2l.show_list_len_pair_hist([ 'origin ','subsampled '],'# tokens per sentence ',\n'count ', sentences, subsampled);\nFor individual tokens, the sampling rate of the high-frequency word \u201cthe\u201d is less than\n1/20.\ndef compare_counts (token):\nreturn (f'# of \"{token }\":'\nf'before= {sum([l.count(token) for linsentences]) },'\nf'after= {sum([l.count(token) for linsubsampled]) }')\ncompare_counts( 'the')", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6c9566e-8751-4664-8782-d7b72ba64434": {"__data__": {"id_": "e6c9566e-8751-4664-8782-d7b72ba64434", "embedding": null, "metadata": {"page_label": "702", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d635aefd-96b4-49b6-8dfe-b7e606bb749f", "node_type": "4", "metadata": {"page_label": "702", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cc3db62fef8ed762d34a42320df1cfcc099cfb2ca5adebda609d0d57dfb83379", "class_name": "RelatedNodeInfo"}}, "text": "702 Natural Language Processing: Pretraining\n'# of \"the\": before=50770, after=2010 '\nIn contrast, low-frequency words \u201cjoin\u201d are completely kept.\ncompare_counts( 'join ')\n'# of \"join \": before=45, after=45 '\nAfter subsampling, we map tokens to their indices for the corpus.\ncorpus =[vocab[line] for line insubsampled]\ncorpus[: 3]\n[[], [ 4127 ,3228 ,1773 ], [ 3922 ,1922 ,4743 ,2696 ]]\n15.3.3Extracting Center Wordsand ContextWords\nThefollowing get_centers_and_contexts functionextractsallthecenterwordsandtheir\ncontextwordsfrom corpus. Ituniformlysamplesanintegerbetween1and max_window_size\nat random as the context window size. For any center word, those words whose distance\nfrom it does not exceed the sampled context window size are its context words.\n#@save\ndef get_centers_and_contexts (corpus, max_window_size):\n\"\"\"Return center words and context words in skip-gram.\"\"\"\ncenters, contexts =[], []\nfor line incorpus:\n# To form a \"center word--context word\" pair, each sentence needs to\n# have at least 2 words\niflen(line) <2:\ncontinue\ncenters +=line\nfor iinrange (len(line)): # Context window centered at `i`\nwindow_size =random .randint( 1, max_window_size)\nindices =list (range (max(0, i -window_size),\nmin(len(line), i +1+window_size)))\n# Exclude the center word from the context words\nindices .remove(i)\ncontexts .append([line[idx] for idx inindices])\nreturn centers, contexts\nNext,wecreateanartificialdatasetcontainingtwosentencesof7and3words,respectively.\nLet the maximum context window size be 2 and print all the center words and their context\nwords.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c3a1e48-6d17-4190-b9ff-02cc831d3ed5": {"__data__": {"id_": "8c3a1e48-6d17-4190-b9ff-02cc831d3ed5", "embedding": null, "metadata": {"page_label": "703", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2cbd9874-e1d4-423b-992c-8b2db529cd7f", "node_type": "4", "metadata": {"page_label": "703", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b609a3d9b3ed52f3a97ad90d3d42c0b54a835c9f6289af740b5a43e6cee4d551", "class_name": "RelatedNodeInfo"}}, "text": "703 The Dataset for Pretraining Word Embeddings\ntiny_dataset =[list (range (7)), list (range (7,10))]\nprint ('dataset ', tiny_dataset)\nfor center, context inzip(*get_centers_and_contexts(tiny_dataset, 2)):\nprint ('center ', center, 'has contexts ', context)\ndataset [[ 0,1,2,3,4,5,6], [ 7,8,9]]\ncenter 0has contexts [ 1]\ncenter 1has contexts [ 0,2]\ncenter 2has contexts [ 0,1,3,4]\ncenter 3has contexts [ 1,2,4,5]\ncenter 4has contexts [ 2,3,5,6]\ncenter 5has contexts [ 3,4,6]\ncenter 6has contexts [ 5]\ncenter 7has contexts [ 8,9]\ncenter 8has contexts [ 7,9]\ncenter 9has contexts [ 7,8]\nWhen training on the PTB dataset, we set the maximum context window size to 5. The\nfollowing extracts all the center words and their context words in the dataset.\nall_centers, all_contexts =get_centers_and_contexts(corpus, 5)\nf'# center-context pairs: {sum([len(contexts) for contexts inall_contexts]) }'\n'# center-context pairs: 1503420 '\n15.3.4NegativeSampling\nWe use negative sampling for approximate training. To sample noise words according to a\npredefineddistribution,wedefinethefollowing RandomGenerator class,wherethe(possi-\nblyunnormalized)samplingdistributionispassedviatheargument sampling_weights .\n#@save\nclass RandomGenerator :\n\"\"\"Randomly draw among {1, ..., n} according to n sampling weights.\"\"\"\ndef __init__ (self , sampling_weights):\n# Exclude\nself .population =list (range (1,len(sampling_weights) +1))\nself .sampling_weights =sampling_weights\nself .candidates =[]\nself .i=0\ndef draw (self ):\nifself .i==len(self .candidates):\n# Cache `k` random sampling results\nself .candidates =random .choices(\nself .population, self .sampling_weights, k =10000 )\nself .i=0\nself .i+=1\nreturn self .candidates[ self .i-1]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1712, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d101822-629a-4eb0-8767-d1a4449ce12f": {"__data__": {"id_": "5d101822-629a-4eb0-8767-d1a4449ce12f", "embedding": null, "metadata": {"page_label": "704", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "188863b3-daac-4441-a607-fe109b0766dd", "node_type": "4", "metadata": {"page_label": "704", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f9372bcd79152e549d8fa712faa997a17ec704f5ea7ed053ef92126b91eb6f82", "class_name": "RelatedNodeInfo"}}, "text": "704 Natural Language Processing: Pretraining\nFor example, we can draw 10 random variables \ud835\udc4bamong indices 1, 2, and 3 with sampling\nprobabilities \ud835\udc43\u00b9\ud835\udc4b=1\u00ba=2\u009d9,\ud835\udc43\u00b9\ud835\udc4b=2\u00ba=3\u009d9, and\ud835\udc43\u00b9\ud835\udc4b=3\u00ba=4\u009d9as follows.\nFor a pair of center word and context word, we randomly sample K(5 in the experiment)\nnoisewords. Accordingtothesuggestionsintheword2vecpaper, thesamplingprobability\n\ud835\udc43\u00b9\ud835\udc64\u00baof a noise word \ud835\udc64is set to its relative frequency in the dictionary raised to the power\nof 0.75 ( Mikolovetal., 2013).\n#@save\ndef get_negatives (all_contexts, vocab, counter, K):\n\"\"\"Return noise words in negative sampling.\"\"\"\n# Sampling weights for words with indices 1, 2, ... (index 0 is the\n# excluded unknown token) in the vocabulary\nsampling_weights =[counter[vocab .to_tokens(i)] **0.75\nfor iinrange (1,len(vocab))]\nall_negatives, generator =[], RandomGenerator(sampling_weights)\nfor contexts inall_contexts:\nnegatives =[]\nwhile len(negatives) <len(contexts) *K:\nneg =generator .draw()\n# Noise words cannot be context words\nifneg not incontexts:\nnegatives .append(neg)\nall_negatives .append(negatives)\nreturn all_negatives\nall_negatives =get_negatives(all_contexts, vocab, counter, 5)\n15.3.5LoadingTrainingExamplesin Minibatches\nAfter all the center words together with their context words and sampled noise words are\nextracted, they will be transformed into minibatches of examples that can be iteratively\nloaded during training.\nInaminibatch,the \ud835\udc56thexampleincludesacenterwordandits \ud835\udc5b\ud835\udc56contextwordsand \ud835\udc5a\ud835\udc56noise\nwords. Due to varying context window sizes, \ud835\udc5b\ud835\udc56\u00b8\ud835\udc5a\ud835\udc56varies for different \ud835\udc56. Thus, for each\nexample we concatenate its context words and noise words in the contexts_negatives\nvariable, and pad zeros until the concatenation length reaches max\ud835\udc56\ud835\udc5b\ud835\udc56\u00b8\ud835\udc5a\ud835\udc56(max_len ). To\nexcludepaddingsinthecalculationoftheloss,wedefineamaskvariable masks. Thereisa\none-to-onecorrespondencebetweenelementsin masksandelementsin contexts_negatives ,\nwherezeros(otherwiseones)in maskscorrespondtopaddingsin contexts_negatives .\nTo distinguish between positive and negative examples, we separate context words from\nnoisewordsin contexts_negatives viaa labelsvariable. Similarto masks,thereisalso\naone-to-onecorrespondencebetweenelementsin labelsandelementsin contexts_negatives ,\nwhere ones (otherwise zeros) in labelscorrespond to context words (positive examples)\nincontexts_negatives .\nThe above idea is implemented in the following batchify function. Its input datais a\nlist with length equal to the batch size, where each element is an example consisting of\nthe center word center, its context words context , and its noise words negative . This", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa31036c-f851-4ec4-ac18-ced0c21ded84": {"__data__": {"id_": "aa31036c-f851-4ec4-ac18-ced0c21ded84", "embedding": null, "metadata": {"page_label": "705", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf955db7-1e7a-45a0-aef1-39cf22c1baea", "node_type": "4", "metadata": {"page_label": "705", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "116ab48de408c73e83b8fd61a88bda5301b62fd89923f1b0b2ae335e3d8dbe8a", "class_name": "RelatedNodeInfo"}}, "text": "705 The Dataset for Pretraining Word Embeddings\nfunction returns a minibatch that can be loaded for calculations during training, such as\nincluding the mask variable.\n#@save\ndef batchify (data):\n\"\"\"Return a minibatch of examples for skip-gram with negative sampling.\"\"\"\nmax_len =max(len(c) +len(n) for _, c, n indata)\ncenters, contexts_negatives, masks, labels =[], [], [], []\nfor center, context, negative indata:\ncur_len =len(context) +len(negative)\ncenters +=[center]\ncontexts_negatives +=[context +negative +[0]*(max_len -cur_len)]\nmasks +=[[1]*cur_len +[0]*(max_len -cur_len)]\nlabels +=[[1]*len(context) +[0]*(max_len -len(context))]\nreturn (torch .tensor(centers) .reshape(( -1,1)), torch .tensor(\ncontexts_negatives), torch .tensor(masks), torch .tensor(labels))\nLet\u2019s test this function using a minibatch of two examples.\nx_1 =(1, [2,2], [ 3,3,3,3])\nx_2 =(1, [2,2,2], [ 3,3])\nbatch =batchify((x_1, x_2))\nnames =['centers ','contexts_negatives ','masks ','labels ']\nfor name, data inzip(names, batch):\nprint (name, '=', data)\ncenters =tensor([[ 1],\n[1]])\ncontexts_negatives =tensor([[ 2,2,3,3,3,3],\n[2,2,2,3,3,0]])\nmasks =tensor([[ 1,1,1,1,1,1],\n[1,1,1,1,1,0]])\nlabels =tensor([[ 1,1,0,0,0,0],\n[1,1,1,0,0,0]])\n15.3.6PuttingIt All Together\nLast,wedefinethe load_data_ptb functionthatreadsthePTBdatasetandreturnsthedata\niterator and the vocabulary.\n#@save\ndef load_data_ptb (batch_size, max_window_size, num_noise_words):\n\"\"\"Download the PTB dataset and then load it into memory.\"\"\"\nnum_workers =d2l.get_dataloader_workers()\nsentences =read_ptb()\nvocab =d2l.Vocab(sentences, min_freq =10)\nsubsampled, counter =subsample(sentences, vocab)\ncorpus =[vocab[line] for line insubsampled]\nall_centers, all_contexts =get_centers_and_contexts(\ncorpus, max_window_size)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1789, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "656ee8c3-a454-49f4-b21f-08c2f6aac575": {"__data__": {"id_": "656ee8c3-a454-49f4-b21f-08c2f6aac575", "embedding": null, "metadata": {"page_label": "706", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f8bbbc3-3ad3-42b7-bc4d-fd4f73f1e647", "node_type": "4", "metadata": {"page_label": "706", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1cc637421f9257dd61cb9173800cd5653e250d6000da4df1ea2d7e163f45e8d3", "class_name": "RelatedNodeInfo"}}, "text": "706 Natural Language Processing: Pretraining\n(continued from previous page)\nall_negatives =get_negatives(\nall_contexts, vocab, counter, num_noise_words)\nclass PTBDataset (torch .utils .data .Dataset):\ndef __init__ (self , centers, contexts, negatives):\nassert len(centers) ==len(contexts) ==len(negatives)\nself .centers =centers\nself .contexts =contexts\nself .negatives =negatives\ndef __getitem__ (self , index):\nreturn (self .centers[index], self .contexts[index],\nself .negatives[index])\ndef __len__ (self ):\nreturn len(self .centers)\ndataset =PTBDataset(all_centers, all_contexts, all_negatives)\ndata_iter =torch .utils .data .DataLoader(dataset, batch_size, shuffle =True ,\ncollate_fn =batchify,\nnum_workers =num_workers)\nreturn data_iter, vocab\nLet\u2019s print the first minibatch of the data iterator.\ndata_iter, vocab =load_data_ptb( 512,5,5)\nfor batch indata_iter:\nfor name, data inzip(names, batch):\nprint (name, 'shape: ', data .shape)\nbreak\ncenters shape: torch .Size([ 512,1])\ncontexts_negatives shape: torch .Size([ 512,60])\nmasks shape: torch .Size([ 512,60])\nlabels shape: torch .Size([ 512,60])\n15.3.7Summary\n\u000fHigh-frequency words may not be so useful in training. We can subsample them for\nspeedup in training.\n\u000fFor computational efficiency, we load examples in minibatches. We can define other\nvariablestodistinguishpaddingsfromnon-paddings,andpositiveexamplesfromneg-\native ones.\n15.3.8Exercises\n1.How does the running time of code in this section changes if not using subsampling?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b39e720b-d514-49c1-a485-eb73286fbb7c": {"__data__": {"id_": "b39e720b-d514-49c1-a485-eb73286fbb7c", "embedding": null, "metadata": {"page_label": "707", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2b965a9-1a4f-4ee9-a1a5-21e15307ae2d", "node_type": "4", "metadata": {"page_label": "707", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "750ee11242e6d331fd704031631cd14f2f08c13755c6d9e0e0198f614c366813", "class_name": "RelatedNodeInfo"}}, "text": "707 Pretraining word2vec\n2312.TheRandomGenerator class caches krandom sampling results. Set kto other values\nand see how it affects the data loading speed.\n3.What other hyperparameters in the code of this section may affect the data loading\nspeed?\nDiscussions231.\n15.4Pretrainingword2vec\nWegoontoimplementtheskip-grammodeldefinedin Section15.1 . Thenwewillpretrain\nword2vec using negative sampling on the PTB dataset. First of all, let\u2019s obtain the data\niterator and the vocabulary for this dataset by calling the d2l.load_data_ptb function,\nwhich was described in Section 15.3\nimport math\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\nbatch_size, max_window_size, num_noise_words =512,5,5\ndata_iter, vocab =d2l.load_data_ptb(batch_size, max_window_size,\nnum_noise_words)\n15.4.1TheSkip-Gram Model\nWe implement the skip-gram model by using embedding layers and batch matrix multipli-\ncations. First, let\u2019s review how embedding layers work.\nEmbedding Layer\nAs described in Section 10.7 , an embedding layer maps a token\u2019s index to its feature vec-\ntor. The weight of this layer is a matrix whose number of rows equals to the dictio-\nnary size ( input_dim ) and number of columns equals to the vector dimension for each\ntoken ( output_dim ). After a word embedding model is trained, this weight is what we\nneed.\nembed =nn.Embedding(num_embeddings =20, embedding_dim =4)\nprint (f'Parameter embedding_weight ( {embed .weight .shape },'\nf'dtype= {embed .weight .dtype })')\nParameter embedding_weight (torch .Size([ 20,4]), dtype =torch .float32)\nThe input of an embedding layer is the index of a token (word). For any token index \ud835\udc56, its", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1646, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b671dbb3-d1cb-4682-98b8-898df2326874": {"__data__": {"id_": "b671dbb3-d1cb-4682-98b8-898df2326874", "embedding": null, "metadata": {"page_label": "708", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8b394317-0275-45f1-a6f5-ef641760a114", "node_type": "4", "metadata": {"page_label": "708", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2896b159e497e92496e4c0c785334293154081336ff573ea066702e52a094714", "class_name": "RelatedNodeInfo"}}, "text": "708 Natural Language Processing: Pretraining\nvectorrepresentationcanbeobtainedfromthe \ud835\udc56throwoftheweightmatrixintheembedding\nlayer. Since the vector dimension ( output_dim ) was set to 4, the embedding layer returns\nvectors with shape (2, 3, 4) for a minibatch of token indices with shape (2, 3).\nx=torch .tensor([[ 1,2,3], [ 4,5,6]])\nembed(x)\ntensor([[[ 0.7606 ,0.3872 ,-0.1864 ,1.1732 ],\n[1.5035 ,2.3623 ,-1.7542 ,-1.4990 ],\n[-1.2639 ,-1.5313 ,2.1719 ,0.4151 ]],\n[[-1.9079 ,0.2434 ,1.5395 ,1.2990 ],\n[0.7470 ,1.0129 ,0.4039 ,0.0591 ],\n[-0.6293 ,-0.1814 ,-0.4782 ,-0.5289 ]]], grad_fn =<EmbeddingBackward0 >)\nDefining the ForwardPropagation\nIn the forward propagation, the input of the skip-gram model includes the center word\nindices centerofshape(batchsize,1)andtheconcatenatedcontextandnoisewordindices\ncontexts_and_negatives of shape (batch size, max_len ), where max_len is defined in\nSection15.3.5 . Thesetwovariablesarefirsttransformedfromthetokenindicesintovectors\nviatheembeddinglayer,thentheirbatchmatrixmultiplication(describedin Section11.3.2 )\nreturns an output of shape (batch size, 1, max_len ). Each element in the output is the dot\nproduct of a center word vector and a context or noise word vector.\ndef skip_gram (center, contexts_and_negatives, embed_v, embed_u):\nv=embed_v(center)\nu=embed_u(contexts_and_negatives)\npred =torch .bmm(v, u .permute( 0,2,1))\nreturn pred\nLet\u2019s print the output shape of this skip_gram function for some example inputs.\nskip_gram(torch .ones(( 2,1), dtype =torch .long),\ntorch .ones(( 2,4), dtype =torch .long), embed, embed) .shape\ntorch .Size([ 2,1,4])\n15.4.2Training\nBefore training the skip-gram model with negative sampling, let\u2019s first define its loss func-\ntion.\nBinaryCross-EntropyLoss\nAccording to the definition of the loss function for negative sampling in Section 15.2.1 , we\nwill use the binary cross-entropy loss.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e5f1d46-3a11-4ff0-8151-b97cbf1ef3bf": {"__data__": {"id_": "1e5f1d46-3a11-4ff0-8151-b97cbf1ef3bf", "embedding": null, "metadata": {"page_label": "709", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6395cf6a-89c4-485e-8465-5c8f2253d8b7", "node_type": "4", "metadata": {"page_label": "709", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "93d2127f3a5989415abc276bb772e2ee4da2d522515adc3667bfcef89c920344", "class_name": "RelatedNodeInfo"}}, "text": "709 Pretraining word2vec\nclass SigmoidBCELoss (nn.Module):\n# Binary cross-entropy loss with masking\ndef __init__ (self ):\nsuper ().__init__ ()\ndef forward (self , inputs, target, mask =None ):\nout =nn.functional .binary_cross_entropy_with_logits(\ninputs, target, weight =mask, reduction =\"none \")\nreturn out.mean(dim =1)\nloss =SigmoidBCELoss()\nRecall our descriptions of the mask variable and the label variable in Section 15.3.5 . The\nfollowing calculates the binary cross-entropy loss for the given variables.\npred =torch .tensor([[ 1.1,-2.2,3.3,-4.4]]*2)\nlabel =torch .tensor([[ 1.0,0.0,0.0,0.0], [ 0.0,1.0,0.0,0.0]])\nmask =torch .tensor([[ 1,1,1,1], [ 1,1,0,0]])\nloss(pred, label, mask) *mask .shape[ 1]/mask .sum(axis =1)\ntensor([ 0.9352 ,1.8462 ])\nBelowshowshowtheaboveresultsarecalculated(inalessefficientway)usingthesigmoid\nactivationfunctioninthebinarycross-entropyloss. Wecanconsiderthetwooutputsastwo\nnormalized losses that are averaged over non-masked predictions.\ndef sigmd (x):\nreturn -math .log( 1/(1+math .exp( -x)))\nprint (f'{(sigmd( 1.1)+sigmd( 2.2)+sigmd( -3.3)+sigmd( 4.4))/4:.4f}')\nprint (f'{(sigmd( -1.1)+sigmd( -2.2))/2:.4f}')\n0.9352\n1.8462\nInitializingModel Parameters\nWe define two embedding layers for all the words in the vocabulary when they are used as\ncenter words and context words, respectively. The word vector dimension embed_size is\nset to 100.\nembed_size =100\nnet =nn.Sequential(nn .Embedding(num_embeddings =len(vocab),\nembedding_dim =embed_size),\nnn.Embedding(num_embeddings =len(vocab),\nembedding_dim =embed_size))", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "472660aa-0262-41e3-b4e3-4990581fa8d6": {"__data__": {"id_": "472660aa-0262-41e3-b4e3-4990581fa8d6", "embedding": null, "metadata": {"page_label": "710", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "434751c2-f69d-4511-99ae-c9a2ffd49eb2", "node_type": "4", "metadata": {"page_label": "710", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "732f94880753b5caa77435fc5ad871b991cdcb49c1d5a47fe5214659585010c9", "class_name": "RelatedNodeInfo"}}, "text": "710 Natural Language Processing: Pretraining\nDefining the TrainingLoop\nThe training loop is defined below. Because of the existence of padding, the calculation of\nthe loss function is slightly different compared to the previous training functions.\ndef train (net, data_iter, lr, num_epochs, device =d2l.try_gpu()):\ndef init_weights (module):\niftype (module) ==nn.Embedding:\nnn.init .xavier_uniform_(module .weight)\nnet.apply(init_weights)\nnet =net.to(device)\noptimizer =torch .optim .Adam(net .parameters(), lr =lr)\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[1, num_epochs])\n# Sum of normalized losses, no. of normalized losses\nmetric =d2l.Accumulator( 2)\nfor epoch inrange (num_epochs):\ntimer, num_batches =d2l.Timer(), len(data_iter)\nfor i, batch inenumerate (data_iter):\noptimizer .zero_grad()\ncenter, context_negative, mask, label =[\ndata .to(device) for data inbatch]\npred =skip_gram(center, context_negative, net[ 0], net[ 1])\nl=(loss(pred .reshape(label .shape) .float(), label .float(), mask)\n/mask .sum(axis =1)*mask .shape[ 1])\nl.sum() .backward()\noptimizer .step()\nmetric .add(l .sum(), l .numel())\nif(i+1)%(num_batches //5)==0ori==num_batches -1:\nanimator .add(epoch +(i+1)/num_batches,\n(metric[ 0]/metric[ 1],))\nprint (f'loss {metric[ 0]/metric[ 1]:.3f},'\nf'{metric[ 1]/timer .stop() :.1f}tokens/sec on {str(device) }')\nNow we can train a skip-gram model using negative sampling.\nlr, num_epochs =0.002 ,5\ntrain(net, data_iter, lr, num_epochs)\nloss 0.410 ,223485.0 tokens /sec on cuda: 0", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5811d56-5146-4e71-bbfd-decd52533389": {"__data__": {"id_": "e5811d56-5146-4e71-bbfd-decd52533389", "embedding": null, "metadata": {"page_label": "711", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da05a50d-7e02-4334-b061-2bdfe03432b9", "node_type": "4", "metadata": {"page_label": "711", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e0bc8b7da9e9d1270351ce7213fd6ba4e24e6eb057814c0a74f31c614ac3e4f3", "class_name": "RelatedNodeInfo"}}, "text": "711 Word Embedding with Global Vectors (GloVe)\n23215.4.3ApplyingWordEmbeddings\nAfter training the word2vec model, we can use the cosine similarity of word vectors from\nthe trained model to find words from the dictionary that are most semantically similar to\nan input word.\ndef get_similar_tokens (query_token, k, embed):\nW=embed .weight .data\nx=W[vocab[query_token]]\n# Compute the cosine similarity. Add 1e-9 for numerical stability\ncos =torch .mv(W, x) /torch .sqrt(torch .sum(W *W, dim =1)*\ntorch .sum(x *x)+1e-9 )\ntopk =torch .topk(cos, k =k+1)[1].cpu() .numpy() .astype( 'int32 ')\nfor iintopk[ 1:]: # Remove the input words\nprint (f'cosine sim= {float (cos[i]) :.3f}:{vocab .to_tokens(i) }')\nget_similar_tokens( 'chip ',3, net[ 0])\ncosine sim =0.702 : microprocessor\ncosine sim =0.649 : mips\ncosine sim =0.643 : intel\n15.4.4Summary\n\u000fWe can train a skip-gram model with negative sampling using embedding layers and the\nbinary cross-entropy loss.\n\u000fApplicationsofwordembeddingsincludefindingsemanticallysimilarwordsforagiven\nword based on the cosine similarity of word vectors.\n15.4.5Exercises\n1.Usingthetrainedmodel,findsemanticallysimilarwordsforotherinputwords. Canyou\nimprove the results by tuning hyperparameters?\n2.When a training corpus is huge, we often sample context words and noise words for\nthe center words in the current minibatch when updating model parameters . In other\nwords,thesamecenterwordmayhavedifferentcontextwordsornoisewordsindifferent\ntraining epochs. What are the benefits of this method? Try to implement this training\nmethod.\nDiscussions232.\n15.5WordEmbedding with Global Vectors (GloVe)\nWord-word co-occurrences within context windows may carry rich semantic information.\nFor example, in a large corpus word \u201csolid\u201d is more likely to co-occur with \u201cice\u201d than", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b732d77-f709-42c0-a81b-3f2538c64755": {"__data__": {"id_": "9b732d77-f709-42c0-a81b-3f2538c64755", "embedding": null, "metadata": {"page_label": "712", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd801881-de1c-4f9f-8585-8503650f0e76", "node_type": "4", "metadata": {"page_label": "712", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "177cf72496507da8d95505722343c89c76a0cc09d2b5788fc2dea4084a084491", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "64801aeb-2037-4cc7-b06d-b822589788d2", "node_type": "1", "metadata": {}, "hash": "42b35102af25da5e62decb695aada200e1c6efebcc68d92a8b22280192c2fd19", "class_name": "RelatedNodeInfo"}}, "text": "712 Natural Language Processing: Pretraining\n\u201csteam\u201d, but word \u201cgas\u201d probably co-occurs with \u201csteam\u201d more frequently than \u201cice\u201d. Be-\nsides, global corpus statistics of such co-occurrences can be precomputed: this can lead\nto more efficient training. To leverage statistical information in the entire corpus for word\nembedding,let\u2019sfirstrevisittheskip-grammodelin Section15.1.3 ,butinterpretingitusing\nglobal corpus statistics such as co-occurrence counts.\n15.5.1Skip-Gramwith Global Corpus Statistics\nDenoting by \ud835\udc5e\ud835\udc56\ud835\udc57the conditional probability \ud835\udc43\u00b9\ud835\udc64\ud835\udc57j\ud835\udc64\ud835\udc56\u00baof word\ud835\udc64\ud835\udc57given word\ud835\udc64\ud835\udc56in the\nskip-gram model, we have\n\ud835\udc5e\ud835\udc56\ud835\udc57=exp\u00b9u>\n\ud835\udc57v\ud835\udc56\u00ba\n\u00cd\n\ud835\udc582Vexp\u00b9u>\n\ud835\udc58v\ud835\udc56\u00ba, (15.5.1)\nwhere for any index \ud835\udc56vectors v\ud835\udc56andu\ud835\udc56represent word \ud835\udc64\ud835\udc56as the center word and context\nword, respectively, and V=f0,1,...,jVj\u0000 1gis the index set of the vocabulary.\nConsider word \ud835\udc64\ud835\udc56that may occur multiple times in the corpus. In the entire corpus, all the\ncontext words wherever \ud835\udc64\ud835\udc56is taken as their center word form a multisetC\ud835\udc56of word indices\nthatallows for multiple instances of the same element . For any element, its number of in-\nstancesiscalledits multiplicity . Toillustratewithanexample,supposethatword \ud835\udc64\ud835\udc56occurs\ntwice in the corpus and indices of the context words that take \ud835\udc64\ud835\udc56as their center word in the\ntwocontextwindowsare \ud835\udc58,\ud835\udc57,\ud835\udc5a,\ud835\udc58 and\ud835\udc58,\ud835\udc59,\ud835\udc58,\ud835\udc57. Thus,multisetC\ud835\udc56=f\ud835\udc57,\ud835\udc57,\ud835\udc58,\ud835\udc58,\ud835\udc58,\ud835\udc58,\ud835\udc59,\ud835\udc5ag,\nwhere multiplicities of elements \ud835\udc57,\ud835\udc58,\ud835\udc59,\ud835\udc5a are 2, 4, 1, 1, respectively.\nNow let\u2019s denote the multiplicity of element \ud835\udc57in multisetC\ud835\udc56as\ud835\udc65\ud835\udc56\ud835\udc57. This is the global co-\noccurrence count of word \ud835\udc64\ud835\udc57(as the context word) and word \ud835\udc64\ud835\udc56(as the center word) in\nthe same context window in the entire corpus. Using such global corpus statistics, the loss\nfunction of the skip-gram model is equivalent to\n\u0000\u00d5\n\ud835\udc562V\u00d5\n\ud835\udc572V\ud835\udc65\ud835\udc56\ud835\udc57log\ud835\udc5e\ud835\udc56\ud835\udc57.(15.5.2)\nWe further denote by \ud835\udc65\ud835\udc56the number of all the context words in the context windows where\n\ud835\udc64\ud835\udc56occurs as their center word, which is equivalent to jC\ud835\udc56j. Letting\ud835\udc5d\ud835\udc56\ud835\udc57be the conditional\nprobability\ud835\udc65\ud835\udc56\ud835\udc57\u009d\ud835\udc65\ud835\udc56for generating context word \ud835\udc64\ud835\udc57given center word \ud835\udc64\ud835\udc56,(15.5.2 )can be\nrewritten as\n\u0000\u00d5\n\ud835\udc562V\ud835\udc65\ud835\udc56\u00d5\n\ud835\udc572V\ud835\udc5d\ud835\udc56\ud835\udc57log\ud835\udc5e\ud835\udc56\ud835\udc57.(15.5.3)\nIn(15.5.3 ),\u0000\u00cd\n\ud835\udc572V\ud835\udc5d\ud835\udc56\ud835\udc57log\ud835\udc5e\ud835\udc56\ud835\udc57calculatesthecross-entropyoftheconditionaldistribution\n\ud835\udc5d\ud835\udc56\ud835\udc57ofglobalcorpusstatisticsandtheconditionaldistribution \ud835\udc5e\ud835\udc56\ud835\udc57ofmodelpredictions. This\nloss is also weighted by \ud835\udc65\ud835\udc56as explained above. Minimizing the loss function in (15.5.3 )\nwill allow the predicted conditional distribution to get close to the conditional distribution\nfrom the global corpus statistics.\nThoughbeingcommonlyusedformeasuringthedistancebetweenprobabilitydistributions,\nthe cross-entropy loss function may not be a good choice here.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64801aeb-2037-4cc7-b06d-b822589788d2": {"__data__": {"id_": "64801aeb-2037-4cc7-b06d-b822589788d2", "embedding": null, "metadata": {"page_label": "712", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd801881-de1c-4f9f-8585-8503650f0e76", "node_type": "4", "metadata": {"page_label": "712", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "177cf72496507da8d95505722343c89c76a0cc09d2b5788fc2dea4084a084491", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9b732d77-f709-42c0-a81b-3f2538c64755", "node_type": "1", "metadata": {"page_label": "712", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b00d91125b09056a5f6d98a153ebb1ee349c000abe758b7ff58a02c4640eeb74", "class_name": "RelatedNodeInfo"}}, "text": "(15.5.3)\nIn(15.5.3 ),\u0000\u00cd\n\ud835\udc572V\ud835\udc5d\ud835\udc56\ud835\udc57log\ud835\udc5e\ud835\udc56\ud835\udc57calculatesthecross-entropyoftheconditionaldistribution\n\ud835\udc5d\ud835\udc56\ud835\udc57ofglobalcorpusstatisticsandtheconditionaldistribution \ud835\udc5e\ud835\udc56\ud835\udc57ofmodelpredictions. This\nloss is also weighted by \ud835\udc65\ud835\udc56as explained above. Minimizing the loss function in (15.5.3 )\nwill allow the predicted conditional distribution to get close to the conditional distribution\nfrom the global corpus statistics.\nThoughbeingcommonlyusedformeasuringthedistancebetweenprobabilitydistributions,\nthe cross-entropy loss function may not be a good choice here. On the one hand, as we\nmentioned in Section 15.2 , the cost of properly normalizing \ud835\udc5e\ud835\udc56\ud835\udc57results in the sum over", "mimetype": "text/plain", "start_char_idx": 2046, "end_char_idx": 2693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7db79abb-0693-4028-a608-3deae8cf3ed6": {"__data__": {"id_": "7db79abb-0693-4028-a608-3deae8cf3ed6", "embedding": null, "metadata": {"page_label": "713", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c5a2a59-6761-45d6-9e72-91a950acd176", "node_type": "4", "metadata": {"page_label": "713", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "354e87389a1f1a9107bf1bbe562f05387c467bdd8b112144ee30ed08b02888f7", "class_name": "RelatedNodeInfo"}}, "text": "713 Word Embedding with Global Vectors (GloVe)\nthe entire vocabulary, which can be computationally expensive. On the other hand, a large\nnumber of rare events from a large corpus are often modeled by the cross-entropy loss to\nbe assigned with too much weight.\n15.5.2The GloVeModel\nIn view of this, the GloVemodel makes three changes to the skip-gram model based on\nsquared loss ( Pennington etal., 2014):\n1.Use variables \ud835\udc5d0\n\ud835\udc56\ud835\udc57=\ud835\udc65\ud835\udc56\ud835\udc57and\ud835\udc5e0\n\ud835\udc56\ud835\udc57=exp\u00b9u>\n\ud835\udc57v\ud835\udc56\u00bathat are not probability distributions\nand take the logarithm of both, so the squared loss term is\u0010\nlog\ud835\udc5d0\n\ud835\udc56\ud835\udc57\u0000log\ud835\udc5e0\n\ud835\udc56\ud835\udc57\u00112\n=\n\u0010\nu>\n\ud835\udc57v\ud835\udc56\u0000log\ud835\udc65\ud835\udc56\ud835\udc57\u00112\n.\n2.Add two scalar model parameters for each word \ud835\udc64\ud835\udc56: the center word bias \ud835\udc4f\ud835\udc56and the\ncontext word bias \ud835\udc50\ud835\udc56.\n3.Replace the weight of each loss term with the weight function \u210e\u00b9\ud835\udc65\ud835\udc56\ud835\udc57\u00ba, where\u210e\u00b9\ud835\udc65\u00bais\nincreasing in the interval of \u00bb0,1\u00bc.\nPuttingallthingstogether,trainingGloVeistominimizethefollowinglossfunction:\n\u00d5\n\ud835\udc562V\u00d5\n\ud835\udc572V\u210e\u00b9\ud835\udc65\ud835\udc56\ud835\udc57\u00ba\u0010\nu>\n\ud835\udc57v\ud835\udc56\u00b8\ud835\udc4f\ud835\udc56\u00b8\ud835\udc50\ud835\udc57\u0000log\ud835\udc65\ud835\udc56\ud835\udc57\u00112\n. (15.5.4)\nFor the weight function, a suggested choice is: \u210e\u00b9\ud835\udc65\u00ba=\u00b9\ud835\udc65\u009d\ud835\udc50\u00ba\ud835\udefc(e.g\ud835\udefc=0.75) if\ud835\udc65 <\ud835\udc50(e.g.,\n\ud835\udc50=100); otherwise \u210e\u00b9\ud835\udc65\u00ba=1. In this case, because \u210e\u00b90\u00ba=0, the squared loss term for any\n\ud835\udc65\ud835\udc56\ud835\udc57=0can be omitted for computational efficiency. For example, when using minibatch\nstochastic gradient descent for training, at each iteration we randomly sample a minibatch\nofnon-zero\ud835\udc65\ud835\udc56\ud835\udc57to calculate gradients and update the model parameters. Note that these\nnon-zero\ud835\udc65\ud835\udc56\ud835\udc57are precomputed global corpus statistics; thus, the model is called GloVe for\nGlobalVectors .\nIt should be emphasized that if word \ud835\udc64\ud835\udc56appears in the context window of word \ud835\udc64\ud835\udc57, then\nvice versa . Therefore, \ud835\udc65\ud835\udc56\ud835\udc57=\ud835\udc65\ud835\udc57\ud835\udc56. Unlike word2vec that fits the asymmetric conditional\nprobability\ud835\udc5d\ud835\udc56\ud835\udc57, GloVe fits the symmetric log\ud835\udc65\ud835\udc56\ud835\udc57. Therefore, the center word vector and\nthe context word vector of any word are mathematically equivalent in the GloVe model.\nHowever in practice, owing to different initialization values, the same word may still get\ndifferent values in these two vectors after training: GloVe sums them up as the output\nvector.\n15.5.3InterpretingGloVefromthe Ratio of Co-occurrence\nProbabilities\nWe can also interpret the GloVe model from another perspective. Using the same notation\ninSection 15.5.1 , let\ud835\udc5d\ud835\udc56\ud835\udc57def=\ud835\udc43\u00b9\ud835\udc64\ud835\udc57j\ud835\udc64\ud835\udc56\u00babe the conditional probability of generating the\ncontext word \ud835\udc64\ud835\udc57given\ud835\udc64\ud835\udc56as the center word in the corpus. tab_glove lists several co-\noccurrence probabilities given words \u201cice\u201d and \u201csteam\u201d and their ratios based on statistics\nfrom a large corpus.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2449, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d77d70d-92a4-41d5-9174-5172e5d3eee1": {"__data__": {"id_": "4d77d70d-92a4-41d5-9174-5172e5d3eee1", "embedding": null, "metadata": {"page_label": "714", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c85bc06f-c06a-4756-9e85-6ae6bd17e2a7", "node_type": "4", "metadata": {"page_label": "714", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "940ae905a284904fc4db82286210f165a7764f3ad9f26d7e94ac26f1baf3bf5b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c11f807e-f02c-4039-87b5-b7912935b536", "node_type": "1", "metadata": {}, "hash": "4ee01c3a176b26de3345f35b49fa9c2dbf983d6005f490acfa3d735613a45563", "class_name": "RelatedNodeInfo"}}, "text": "714 Natural Language Processing: Pretraining\n:Word-word co-occurrence probabilities and their ratios from a large corpus (adapted from\nTable 1 in Pennington etal.(2014))\nTable 15.5.1: label: tab_glove\n\ud835\udc64\ud835\udc58= solid gas water fashion\n\ud835\udc5d1=\ud835\udc43\u00b9\ud835\udc64\ud835\udc58jice\u00ba0.00019 0.000066 0.003 0.000017\n\ud835\udc5d2=\ud835\udc43\u00b9\ud835\udc64\ud835\udc58jsteam\u00ba0.000022 0.00078 0.0022 0.000018\n\ud835\udc5d1\u009d\ud835\udc5d2 8.9 0.085 1.36 0.96\nWe can observe the following from tab_glove :\n\u000fFor a word\ud835\udc64\ud835\udc58that is related to \u201cice\u201d but unrelated to \u201csteam\u201d, such as \ud835\udc64\ud835\udc58=solid, we\nexpect a larger ratio of co-occurence probabilities, such as 8.9.\n\u000fFor a word\ud835\udc64\ud835\udc58that is related to \u201csteam\u201d but unrelated to \u201cice\u201d, such as \ud835\udc64\ud835\udc58=gas, we\nexpect a smaller ratio of co-occurence probabilities, such as 0.085.\n\u000fFor a word\ud835\udc64\ud835\udc58that is related to both \u201cice\u201d and \u201csteam\u201d, such as \ud835\udc64\ud835\udc58=water, we expect\na ratio of co-occurence probabilities that is close to 1, such as 1.36.\n\u000fFor a word\ud835\udc64\ud835\udc58that is unrelated to both \u201cice\u201d and \u201csteam\u201d, such as \ud835\udc64\ud835\udc58=fashion, we\nexpect a ratio of co-occurence probabilities that is close to 1, such as 0.96.\nIt can be seen that the ratio of co-occurrence probabilities can intuitively express the rela-\ntionship between words. Thus, we can design a function of three word vectors to fit this\nratio. Fortheratioofco-occurrenceprobabilities \ud835\udc5d\ud835\udc56\ud835\udc57\u009d\ud835\udc5d\ud835\udc56\ud835\udc58with\ud835\udc64\ud835\udc56beingthecenterwordand\n\ud835\udc64\ud835\udc57and\ud835\udc64\ud835\udc58being the context words, we want to fit this ratio using some function \ud835\udc53:\n\ud835\udc53\u00b9u\ud835\udc57,u\ud835\udc58,v\ud835\udc56\u00ba\u0019\ud835\udc5d\ud835\udc56\ud835\udc57\n\ud835\udc5d\ud835\udc56\ud835\udc58. (15.5.5)\nAmong many possible designs for \ud835\udc53, we only pick a reasonable choice in the following.\nSince the ratio of co-occurrence probabilities is a scalar, we require that \ud835\udc53be a scalar\nfunction, such as \ud835\udc53\u00b9u\ud835\udc57,u\ud835\udc58,v\ud835\udc56\u00ba=\ud835\udc53\u0000\u00b9u\ud835\udc57\u0000u\ud835\udc58\u00ba>v\ud835\udc56\u0001. Switching word indices \ud835\udc57and\ud835\udc58in\n(15.5.5 ), it must hold that \ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc53\u00b9\u0000\ud835\udc65\u00ba=1, so one possibility is \ud835\udc53\u00b9\ud835\udc65\u00ba=exp\u00b9\ud835\udc65\u00ba, i.e.,\n\ud835\udc53\u00b9u\ud835\udc57,u\ud835\udc58,v\ud835\udc56\u00ba=exp\u0010\nu>\n\ud835\udc57v\ud835\udc56\u0011\nexp\u0010\nu>\n\ud835\udc58v\ud835\udc56\u0011\u0019\ud835\udc5d\ud835\udc56\ud835\udc57\n\ud835\udc5d\ud835\udc56\ud835\udc58. (15.5.6)\nNow let\u2019s pick exp\u0010\nu>\n\ud835\udc57v\ud835\udc56\u0011\n\u0019\ud835\udefc\ud835\udc5d\ud835\udc56\ud835\udc57, where\ud835\udefcis a constant. Since \ud835\udc5d\ud835\udc56\ud835\udc57=\ud835\udc65\ud835\udc56\ud835\udc57\u009d\ud835\udc65\ud835\udc56, after taking\nthe logarithm on both sides we get u>\n\ud835\udc57v\ud835\udc56\u0019log\ud835\udefc\u00b8log\ud835\udc65\ud835\udc56\ud835\udc57\u0000log\ud835\udc65\ud835\udc56. We may use additional\nbias terms to fit\u0000log\ud835\udefc\u00b8log\ud835\udc65\ud835\udc56, such as the center word bias \ud835\udc4f\ud835\udc56and the context word bias\n\ud835\udc50\ud835\udc57:\nu>\n\ud835\udc57v\ud835\udc56\u00b8\ud835\udc4f\ud835\udc56\u00b8\ud835\udc50\ud835\udc57\u0019log\ud835\udc65\ud835\udc56\ud835\udc57.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c11f807e-f02c-4039-87b5-b7912935b536": {"__data__": {"id_": "c11f807e-f02c-4039-87b5-b7912935b536", "embedding": null, "metadata": {"page_label": "714", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c85bc06f-c06a-4756-9e85-6ae6bd17e2a7", "node_type": "4", "metadata": {"page_label": "714", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "940ae905a284904fc4db82286210f165a7764f3ad9f26d7e94ac26f1baf3bf5b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d77d70d-92a4-41d5-9174-5172e5d3eee1", "node_type": "1", "metadata": {"page_label": "714", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e7bf0189fc60eda41fef6ec5dcbf2aef49e165caaeea39b5b6adb40344e28287", "class_name": "RelatedNodeInfo"}}, "text": "Since \ud835\udc5d\ud835\udc56\ud835\udc57=\ud835\udc65\ud835\udc56\ud835\udc57\u009d\ud835\udc65\ud835\udc56, after taking\nthe logarithm on both sides we get u>\n\ud835\udc57v\ud835\udc56\u0019log\ud835\udefc\u00b8log\ud835\udc65\ud835\udc56\ud835\udc57\u0000log\ud835\udc65\ud835\udc56. We may use additional\nbias terms to fit\u0000log\ud835\udefc\u00b8log\ud835\udc65\ud835\udc56, such as the center word bias \ud835\udc4f\ud835\udc56and the context word bias\n\ud835\udc50\ud835\udc57:\nu>\n\ud835\udc57v\ud835\udc56\u00b8\ud835\udc4f\ud835\udc56\u00b8\ud835\udc50\ud835\udc57\u0019log\ud835\udc65\ud835\udc56\ud835\udc57. (15.5.7)\nMeasuring the squared error of (15.5.7 )with weights, the GloVe loss function in (15.5.4 )\nis obtained.", "mimetype": "text/plain", "start_char_idx": 1834, "end_char_idx": 2172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac79371d-1e1d-4af1-842c-d0843ae4f7f5": {"__data__": {"id_": "ac79371d-1e1d-4af1-842c-d0843ae4f7f5", "embedding": null, "metadata": {"page_label": "715", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dee01888-6c66-454e-ac69-9613290d207f", "node_type": "4", "metadata": {"page_label": "715", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "173da62f5abec82f7a1660c22efd580447e9a0999fd306f74f72ff26eb6df2b7", "class_name": "RelatedNodeInfo"}}, "text": "715 Subword Embedding\n23315.5.4Summary\n\u000fTheskip-grammodelcanbeinterpretedusingglobalcorpusstatisticssuchasword-word\nco-occurrence counts.\n\u000fThe cross-entropy loss may not be a good choice for measuring the difference of two\nprobability distributions, especially for a large corpus. GloVe uses squared loss to fit\nprecomputed global corpus statistics.\n\u000fThe center word vector and the context word vector are mathematically equivalent for\nany word in GloVe.\n\u000fGloVe can be interpreted from the ratio of word-word co-occurrence probabilities.\n15.5.5Exercises\n1.Ifwords\ud835\udc64\ud835\udc56and\ud835\udc64\ud835\udc57co-occurinthesamecontextwindow,howcanweusetheirdistance\nin the text sequence to redesign the method for calculating the conditional probability\n\ud835\udc5d\ud835\udc56\ud835\udc57? Hint: see Section 4.2 of the GloVe paper ( Pennington etal., 2014).\n2.For any word, are its center word bias and context word bias mathematically equivalent\nin GloVe? Why?\nDiscussions233.\n15.6SubwordEmbedding\nIn English, words such as \u201chelps\u201d, \u201chelped\u201d, and \u201chelping\u201d are inflected forms of the same\nword\u201chelp\u201d. Therelationshipbetween\u201cdog\u201d and\u201cdogs\u201dis thesameas thatbetween\u201ccat\u201d\nand\u201ccats\u201d,andtherelationshipbetween\u201cboy\u201dand\u201cboyfriend\u201disthesameasthatbetween\n\u201cgirl\u201d and \u201cgirlfriend\u201d. In other languages such as French and Spanish, many verbs have\nover 40 inflected forms, while in Finnish, a noun may have up to 15 cases. In linguistics,\nmorphologystudieswordformationandwordrelationships. However,theinternalstructure\nof words was neither explored in word2vec nor in GloVe.\n15.6.1ThefastTextModel\nRecall how words are represented in word2vec. In both the skip-gram model and the con-\ntinuous bag-of-words model, different inflected forms of the same word are directly repre-\nsented by different vectors without shared parameters. To use morphological information,\nthefastTextmodel proposed a subwordembedding approach, where a subword is a charac-\nter\ud835\udc5b-gram(Bojanowski etal.,2017). Insteadoflearningword-levelvectorrepresentations,\nfastText can be considered as the subword-level skip-gram, where each centerword is rep-\nresented by the sum of its subword vectors.\nLet\u2019s illustrate how to obtain subwords for each center word in fastText using the word", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35b9af45-a378-4aaf-aa1e-c19782fb6315": {"__data__": {"id_": "35b9af45-a378-4aaf-aa1e-c19782fb6315", "embedding": null, "metadata": {"page_label": "716", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "043e995c-ebc6-41aa-963e-b051cfb39023", "node_type": "4", "metadata": {"page_label": "716", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c3b389310493f58bce65da6344514f2f62cacbf5ed0584a2d87e2793494647dc", "class_name": "RelatedNodeInfo"}}, "text": "716 Natural Language Processing: Pretraining\n\u201cwhere\u201d. First, add special characters \u201c<\u201d and \u201c>\u201d at the beginning and end of the word\nto distinguish prefixes and suffixes from other subwords. Then, extract character \ud835\udc5b-grams\nfrom the word. For example, when \ud835\udc5b=3, we obtain all subwords of length 3: \u201c<wh\u201d,\n\u201cwhe\u201d, \u201cher\u201d, \u201cere\u201d, \u201cre>\u201d, and the special subword \u201c<where>\u201d.\nIn fastText, for any word \ud835\udc64, denote byG\ud835\udc64the union of all its subwords of length between\n3 and 6 and its special subword. The vocabulary is the union of the subwords of all words.\nLetting z\ud835\udc54bethevectorofsubword \ud835\udc54inthedictionary,thevector v\ud835\udc64forword\ud835\udc64asacenter\nword in the skip-gram model is the sum of its subword vectors:\nv\ud835\udc64=\u00d5\n\ud835\udc542G\ud835\udc64z\ud835\udc54.(15.6.1)\nThe rest of fastText is the same as the skip-gram model. Compared with the skip-gram\nmodel, the vocabulary in fastText is larger, resulting in more model parameters. Besides,\ntocalculatetherepresentationofaword,allitssubwordvectorshavetobesummed,leading\nto higher computational complexity. However, thanks to shared parameters from subwords\namong words with similar structures, rare words and even out-of-vocabulary words may\nobtain better vector representations in fastText.\n15.6.2BytePairEncoding\nIn fastText, all the extracted subwords have to be of the specified lengths, such as 3to6,\nthus the vocabulary size cannot be predefined. To allow for variable-length subwords in\na fixed-size vocabulary, we can apply a compression algorithm called byte pair encoding\n(BPE) to extract subwords ( Sennrich etal., 2015).\nByte pair encoding performs a statistical analysis of the training dataset to discover com-\nmon symbols within a word, such as consecutive characters of arbitrary length. Starting\nfrom symbols of length 1, byte pair encoding iteratively merges the most frequent pair of\nconsecutive symbols to produce new longer symbols. Note that for efficiency, pairs cross-\ning word boundaries are not considered. In the end, we can use such symbols as subwords\nto segment words. Byte pair encoding and its variants has been used for input representa-\ntions in popular natural language processing pretraining models such as GPT-2 ( Radford\net al., 2019) and RoBERTa ( Liuet al., 2019). In the following, we will illustrate how byte\npair encoding works.\nFirst, we initialize the vocabulary of symbols as all the English lowercase characters, a\nspecial end-of-word symbol '_', and a special unknown symbol '[UNK]' .\nimport collections\nsymbols =['a','b','c','d','e','f','g','h','i','j','k','l','m',\n'n','o','p','q','r','s','t','u','v','w','x','y','z',\n'_','[UNK] ']\nSince we do not consider symbol pairs that cross boundaries of words, we only need a\ndictionary raw_token_freqs thatmapswordstotheirfrequencies(numberofoccurrences)\nin a dataset. Note that the special symbol '_'is appended to each word so that we can", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9fadcd69-b57d-49c1-aa39-8b52110a6b72": {"__data__": {"id_": "9fadcd69-b57d-49c1-aa39-8b52110a6b72", "embedding": null, "metadata": {"page_label": "717", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c88e89d-1572-498c-a162-4b2399be03e4", "node_type": "4", "metadata": {"page_label": "717", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d9b9b2c559f05b15d823a2103ed5c46dbe76ca9cf0b293e5adfdcca14be61675", "class_name": "RelatedNodeInfo"}}, "text": "717 Subword Embedding\neasily recover a word sequence (e.g., \u201ca taller man\u201d) from a sequence of output symbols\n( e.g., \u201ca_ tall er_ man\u201d). Since we start the merging process from a vocabulary of only\nsingle characters and special symbols, space is inserted between every pair of consecutive\ncharacters within each word (keys of the dictionary token_freqs ). In other words, space\nis the delimiter between symbols within a word.\nraw_token_freqs ={'fast_ ':4,'faster_ ':3,'tall_ ':5,'taller_ ':4}\ntoken_freqs ={}\nfor token, freq inraw_token_freqs .items():\ntoken_freqs[ ''.join( list (token))] =raw_token_freqs[token]\ntoken_freqs\n{'f a s t _ ':4,'f a s t e r _ ':3,'t a l l _ ':5,'t a l l e r _ ':4}\nWe define the following get_max_freq_pair function that returns the most frequent pair\nofconsecutivesymbolswithinaword,wherewordscomefromkeysoftheinputdictionary\ntoken_freqs .\ndef get_max_freq_pair (token_freqs):\npairs =collections .defaultdict( int)\nfor token, freq intoken_freqs .items():\nsymbols =token .split()\nfor iinrange (len(symbols) -1):\n# Key of `pairs` is a tuple of two consecutive symbols\npairs[symbols[i], symbols[i +1]]+=freq\nreturn max(pairs, key =pairs .get) # Key of `pairs` with the max value\nAs a greedy approach based on frequency of consecutive symbols, byte pair encoding will\nuse the following merge_symbols function to merge the most frequent pair of consecutive\nsymbols to produce new symbols.\ndef merge_symbols (max_freq_pair, token_freqs, symbols):\nsymbols .append( ''.join(max_freq_pair))\nnew_token_freqs =dict ()\nfor token, freq intoken_freqs .items():\nnew_token =token .replace( ''.join(max_freq_pair),\n''.join(max_freq_pair))\nnew_token_freqs[new_token] =token_freqs[token]\nreturn new_token_freqs\nNowweiterativelyperformthebytepairencodingalgorithmoverthekeysofthedictionary\ntoken_freqs . In the first iteration, the most frequent pair of consecutive symbols are 't'\nand'a', thus byte pair encoding merges them to produce a new symbol 'ta'. In the\nsecond iteration, byte pair encoding continues to merge 'ta'and'l'to result in another\nnew symbol 'tal'.\nnum_merges =10\nfor iinrange (num_merges):\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72cf49ee-9743-46cd-afc1-11315b197351": {"__data__": {"id_": "72cf49ee-9743-46cd-afc1-11315b197351", "embedding": null, "metadata": {"page_label": "718", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18e53e85-30c9-4f79-a6ac-cbb67c90bb20", "node_type": "4", "metadata": {"page_label": "718", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a712f4193f69a6567e12facddc9a47fc73759fa1c71dd9f38dd1a32cb47523c1", "class_name": "RelatedNodeInfo"}}, "text": "718 Natural Language Processing: Pretraining\n(continued from previous page)\nmax_freq_pair =get_max_freq_pair(token_freqs)\ntoken_freqs =merge_symbols(max_freq_pair, token_freqs, symbols)\nprint (f'merge # {i+1}:', max_freq_pair)\nmerge #1: ('t', 'a')\nmerge #2: ('ta', 'l')\nmerge #3: ('tal', 'l')\nmerge #4: ('f', 'a')\nmerge #5: ('fa', 's')\nmerge #6: ('fas', 't')\nmerge #7: ('e', 'r')\nmerge #8: ('er', '_')\nmerge #9: ('tall', '_')\nmerge #10: ('fast', '_')\nAfter 10 iterations of byte pair encoding, we can see that list symbols now contains 10\nmore symbols that are iteratively merged from other symbols.\nprint (symbols)\n['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p\n\u21a9!','q','r','s','t','u','v','w','x','y','z','_','[UNK] ','ta','tal\n\u21a9!','tall ','fa','fas','fast ','er','er_','tall_ ','fast_ ']\nFor the same dataset specified in the keys of the dictionary raw_token_freqs , each word\nin the dataset is now segmented by subwords \u201cfast_\u201d, \u201cfast\u201d, \u201cer_\u201d, \u201ctall_\u201d, and \u201ctall\u201d as a\nresult of the byte pair encoding algorithm. For instance, words \u201cfaster_\u201d and \u201ctaller_\u201d are\nsegmented as \u201cfast er_\u201d and \u201ctall er_\u201d, respectively.\nprint (list (token_freqs .keys()))\n['fast_ ','fast er_ ','tall_ ','tall er_ ']\nNote that the result of byte pair encoding depends on the dataset being used. We can also\nuse the subwords learned from one dataset to segment words of another dataset. As a\ngreedyapproach, thefollowing segment_BPE functiontriestobreakwordsintothelongest\npossible subwords from the input argument symbols .\ndef segment_BPE (tokens, symbols):\noutputs =[]\nfor token intokens:\nstart, end =0,len(token)\ncur_output =[]\n# Segment token with the longest possible subwords from symbols\nwhile start <len(token) and start <end:\niftoken[start: end] insymbols:\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1788, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36b1e70d-2fec-476d-8ca0-aa589ecd16c5": {"__data__": {"id_": "36b1e70d-2fec-476d-8ca0-aa589ecd16c5", "embedding": null, "metadata": {"page_label": "719", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf3a0ad1-69a9-4a77-a836-004f10a7feb1", "node_type": "4", "metadata": {"page_label": "719", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b6eef9bd818859b3affbaed5a4f7aa0663293fde7b9745b756c6669c2c5ea67a", "class_name": "RelatedNodeInfo"}}, "text": "719 Subword Embedding\n234(continued from previous page)\ncur_output .append(token[start: end])\nstart =end\nend =len(token)\nelse :\nend -=1\nifstart <len(token):\ncur_output .append( '[UNK] ')\noutputs .append( ''.join(cur_output))\nreturn outputs\nInthefollowing, weusethesubwordsinlist symbols , whichislearnedfromtheaforemen-\ntioned dataset, to segment tokensthat represent another dataset.\ntokens =['tallest_ ','fatter_ ']\nprint (segment_BPE(tokens, symbols))\n['tall e s t _ ','fa t t er_ ']\n15.6.3Summary\n\u000fThe fastText model proposes a subword embedding approach. Based on the skip-gram\nmodel in word2vec, it represents a center word as the sum of its subword vectors.\n\u000fBytepairencodingperformsastatisticalanalysisofthetrainingdatasettodiscovercom-\nmon symbols within a word. As a greedy approach, byte pair encoding iteratively\nmerges the most frequent pair of consecutive symbols.\n\u000fSubword embedding may improve the quality of representations of rare words and out-\nof-dictionary words.\n15.6.4Exercises\n1.As an example, there are about 3\u0002108possible 6-grams in English. What is the issue\nwhen there are too many subwords? How to address the issue? Hint: refer to the end of\nSection 3.2 of the fastText paper ( Bojanowski etal., 2017).\n2.How to design a subword embedding model based on the continuous bag-of-words\nmodel?\n3.Togetavocabularyofsize \ud835\udc5a,howmanymergingoperationsareneededwhentheinitial\nsymbol vocabulary size is \ud835\udc5b?\n4.How to extend the idea of byte pair encoding to extract phrases?\nDiscussions234.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7283e245-0259-4ec4-9fea-80cf06e8cdc3": {"__data__": {"id_": "7283e245-0259-4ec4-9fea-80cf06e8cdc3", "embedding": null, "metadata": {"page_label": "720", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f51033c6-0578-46b9-80c0-b8b9a7736f85", "node_type": "4", "metadata": {"page_label": "720", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4adbcfc41a7e7f64d0b9290a52696444660a3103c6f693d527d1d10981a12b1d", "class_name": "RelatedNodeInfo"}}, "text": "720 Natural Language Processing: Pretraining\n235\n23615.7WordSimilarity and Analogy\nInSection 15.4 , we trained a word2vec model on a small dataset, and applied it to find\nsemantically similar words for an input word. In practice, word vectors that are pretrained\non large corpora can be applied to downstream natural language processing tasks, which\nwill be covered later in Chapter 16 . To demonstrate semantics of pretrained word vectors\nfrom large corpora in a straightforward way, let\u2019s apply them in the word similarity and\nanalogy tasks.\nimport os\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n15.7.1LoadingPretrainedWordVectors\nBelow lists pretrained GloVe embeddings of dimension 50, 100, and 300, which can be\ndownloaded from the GloVe website235. The pretrained fastText embeddings are available\nin multiple languages. Here we consider one English version (300-dimensional \u201cwiki.en\u201d)\nthat can be downloaded from the fastText website236.\n#@save\nd2l.DATA_HUB[ 'glove.6b.50d ']=(d2l .DATA_URL +'glove.6B.50d.zip ',\n'0b8703943ccdb6eb788e6f091b8946e82231bc4d ')\n#@save\nd2l.DATA_HUB[ 'glove.6b.100d ']=(d2l .DATA_URL +'glove.6B.100d.zip ',\n'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a ')\n#@save\nd2l.DATA_HUB[ 'glove.42b.300d ']=(d2l .DATA_URL +'glove.42B.300d.zip ',\n'b5116e234e9eb9076672cfeabf5469f3eec904fa ')\n#@save\nd2l.DATA_HUB[ 'wiki.en ']=(d2l .DATA_URL +'wiki.en.zip ',\n'c1816da3821ae9f43899be655002f6c723e91b88 ')\nTo load these pretrained GloVe and fastText embeddings, we define the following Token-\nEmbedding class.\n#@save\nclass TokenEmbedding :\n\"\"\"Token Embedding.\"\"\"\ndef __init__ (self , embedding_name):\nself .idx_to_token, self .idx_to_vec =self ._load_embedding(\nembedding_name)\nself .unknown_idx =0\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1756, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95e79758-4a3b-47a1-abaf-a1033d83c3e9": {"__data__": {"id_": "95e79758-4a3b-47a1-abaf-a1033d83c3e9", "embedding": null, "metadata": {"page_label": "721", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f7be9653-0593-489d-abc6-4164e0769f78", "node_type": "4", "metadata": {"page_label": "721", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "74aa0260ba29f98b2e241fe581d19141657cf21aac2720d63131e1eeda864df2", "class_name": "RelatedNodeInfo"}}, "text": "721 Word Similarity and Analogy\n(continued from previous page)\nself .token_to_idx ={token: idx for idx, token in\nenumerate (self .idx_to_token)}\ndef _load_embedding (self , embedding_name):\nidx_to_token, idx_to_vec =['<unk> '], []\ndata_dir =d2l.download_extract(embedding_name)\n# GloVe website: https://nlp.stanford.edu/projects/glove/\n# fastText website: https://fasttext.cc/\nwith open (os.path .join(data_dir, 'vec.txt '),'r')asf:\nfor line inf:\nelems =line .rstrip() .split( '')\ntoken, elems =elems[ 0], [ float (elem) for elem inelems[ 1:]]\n# Skip header information, such as the top row in fastText\niflen(elems) >1:\nidx_to_token .append(token)\nidx_to_vec .append(elems)\nidx_to_vec =[[0]*len(idx_to_vec[ 0])] +idx_to_vec\nreturn idx_to_token, torch .tensor(idx_to_vec)\ndef __getitem__ (self , tokens):\nindices =[self .token_to_idx .get(token, self .unknown_idx)\nfor token intokens]\nvecs =self .idx_to_vec[torch .tensor(indices)]\nreturn vecs\ndef __len__ (self ):\nreturn len(self .idx_to_token)\nBelow we load the 50-dimensional GloVe embeddings (pretrained on a Wikipedia sub-\nset). When creating the TokenEmbedding instance, the specified embedding file has to be\ndownloaded if it was not yet.\nglove_6b50d =TokenEmbedding( 'glove.6b.50d ')\nDownloading ../data /glove .6B.50d.zip from http ://d2l-data .s3-accelerate .\n\u21a9!amazonaws .com/glove .6B.50d.zip...\nOutput the vocabulary size. The vocabulary contains 400000 words (tokens) and a special\nunknown token.\nlen(glove_6b50d)\n400001\nWe can get the index of a word in the vocabulary, and vice versa.\nglove_6b50d .token_to_idx[ 'beautiful '], glove_6b50d .idx_to_token[ 3367 ]", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1625, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aebfe93c-afb2-4d1f-bc1a-f0236e83d72a": {"__data__": {"id_": "aebfe93c-afb2-4d1f-bc1a-f0236e83d72a", "embedding": null, "metadata": {"page_label": "722", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b49817c-3133-4373-a1c8-9d4f5f8d908f", "node_type": "4", "metadata": {"page_label": "722", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f819a38c2fcab070d347bb4e461116d176cfda6c38a9042cb4cc446f1fa710b9", "class_name": "RelatedNodeInfo"}}, "text": "722 Natural Language Processing: Pretraining\n(3367 ,'beautiful ')\n15.7.2ApplyingPretrained WordVectors\nUsing the loaded GloVe vectors, we will demonstrate their semantics by applying them in\nthe following word similarity and analogy tasks.\nWordSimilarity\nSimilar to Section 15.4.3 , in order to find semantically similar words for an input word\nbased on cosine similarities between word vectors, we implement the following knn(\ud835\udc58-\nnearest neighbors) function.\ndef knn(W, x, k):\n# Add 1e-9 for numerical stability\ncos =torch .mv(W, x .reshape( -1,)) /(\ntorch .sqrt(torch .sum(W *W, axis =1)+1e-9 )*\ntorch .sqrt((x *x).sum()))\n_, topk =torch .topk(cos, k =k)\nreturn topk, [cos[ int(i)] for iintopk]\nThen, we search for similar words using the pretrained word vectors from the TokenEm-\nbedding instance embed.\ndef get_similar_tokens (query_token, k, embed):\ntopk, cos =knn(embed .idx_to_vec, embed[[query_token]], k +1)\nfor i, c inzip(topk[ 1:], cos[ 1:]): # Exclude the input word\nprint (f'cosine sim= {float (c):.3f}:{embed .idx_to_token[ int(i)] }')\nThe vocabulary of the pretrained word vectors in glove_6b50d contains 400000 words\nand a special unknown token. Excluding the input word and unknown token, among this\nvocabulary let\u2019s find three most semantically similar words to word \u201cchip\u201d.\nget_similar_tokens( 'chip ',3, glove_6b50d)\ncosine sim =0.856 : chips\ncosine sim =0.749 : intel\ncosine sim =0.749 : electronics\nBelow outputs similar words to \u201cbaby\u201d and \u201cbeautiful\u201d.\nget_similar_tokens( 'baby ',3, glove_6b50d)\ncosine sim =0.839 : babies\ncosine sim =0.800 : boy\ncosine sim =0.792 : girl", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1594, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6ed486b-ecb4-48db-9d74-50f0a167625b": {"__data__": {"id_": "c6ed486b-ecb4-48db-9d74-50f0a167625b", "embedding": null, "metadata": {"page_label": "723", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b56ba533-2859-4fc1-8842-0876c945c994", "node_type": "4", "metadata": {"page_label": "723", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a501221594e64a0df68f9f80e85ce95ec7b0c1f90291ea389f153682c5e8223c", "class_name": "RelatedNodeInfo"}}, "text": "723 Word Similarity and Analogy\nget_similar_tokens( 'beautiful ',3, glove_6b50d)\ncosine sim =0.921 : lovely\ncosine sim =0.893 : gorgeous\ncosine sim =0.830 : wonderful\nWordAnalogy\nBesides finding similar words, we can also apply word vectors to word analogy tasks. For\nexample, \u201cman\u201d:\u201cwoman\u201d::\u201cson\u201d:\u201cdaughter\u201d is the form of a word analogy: \u201cman\u201d is to\n\u201cwoman\u201d as \u201cson\u201d is to \u201cdaughter\u201d. Specifically, the word analogy completion task can be\ndefined as: for a word analogy \ud835\udc4e:\ud835\udc4f::\ud835\udc50:\ud835\udc51, given the first three words \ud835\udc4e,\ud835\udc4fand\ud835\udc50, find\ud835\udc51.\nDenote the vector of word \ud835\udc64by vec\u00b9\ud835\udc64\u00ba. To complete the analogy, we will find the word\nwhose vector is most similar to the result of vec \u00b9\ud835\udc50\u00ba\u00b8vec\u00b9\ud835\udc4f\u00ba\u0000vec\u00b9\ud835\udc4e\u00ba.\ndef get_analogy (token_a, token_b, token_c, embed):\nvecs =embed[[token_a, token_b, token_c]]\nx=vecs[ 1]-vecs[ 0]+vecs[ 2]\ntopk, cos =knn(embed .idx_to_vec, x, 1)\nreturn embed .idx_to_token[ int(topk[ 0])] # Remove unknown words\nLet\u2019s verify the \u201cmale-female\u201d analogy using the loaded word vectors.\nget_analogy( 'man','woman ','son', glove_6b50d)\n'daughter '\nBelow completes a \u201ccapital-country\u201d analogy: \u201cbeijing\u201d:\u201cchina\u201d::\u201ctokyo\u201d:\u201cjapan\u201d. This\ndemonstrates semantics in the pretrained word vectors.\nget_analogy( 'beijing ','china ','tokyo ', glove_6b50d)\n'japan '\nFor the \u201cadjective-superlative adjective\u201d analogy such as \u201cbad\u201d:\u201cworst\u201d::\u201cbig\u201d:\u201cbiggest\u201d,\nwe can see that the pretrained word vectors may capture the syntactic information.\nget_analogy( 'bad','worst ','big', glove_6b50d)\n'biggest '\nTo show the captured notion of past tense in the pretrained word vectors, we can test the\nsyntax using the \u201cpresent tense-past tense\u201d analogy: \u201cdo\u201d:\u201cdid\u201d::\u201cgo\u201d:\u201cwent\u201d.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1643, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f51ee76-46d4-4678-af84-8ed3a4b10d74": {"__data__": {"id_": "8f51ee76-46d4-4678-af84-8ed3a4b10d74", "embedding": null, "metadata": {"page_label": "724", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c70ebe84-e40e-4547-b42f-69e1036482e8", "node_type": "4", "metadata": {"page_label": "724", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "91b16744a6e4cd9d837aee1c11bbdcd88ee862244db06654ed1c2cd1d49632b8", "class_name": "RelatedNodeInfo"}}, "text": "724 Natural Language Processing: Pretraining\n237get_analogy( 'do','did','go', glove_6b50d)\n'went '\n15.7.3Summary\n\u000fIn practice, word vectors that are pretrained on large corpora can be applied to down-\nstream natural language processing tasks.\n\u000fPretrained word vectors can be applied to the word similarity and analogy tasks.\n15.7.4Exercises\n1.Test the fastText results using TokenEmbedding('wiki.en') .\n2.When the vocabulary is extremely large, how can we find similar words or complete a\nword analogy faster?\nDiscussions237.\n15.8BidirectionalEncoder Representations from\nTransformers(BERT)\nWe have introduced several word embedding models for natural language understanding.\nAfter pretraining, the output can be thought of as a matrix where each row is a vector that\nrepresents a word of a predefined vocabulary. In fact, these word embedding models are\nallcontext-independent . Let\u2019s begin by illustrating this property.\n15.8.1FromContext-Independent to Context-Sensitive\nRecalltheexperimentsin Section15.4 andSection15.7 . Forinstance,word2vecandGloVe\nboth assign the same pretrained vector to the same word regardless of the context of the\nword (if any). Formally, a context-independent representation of any token \ud835\udc65is a func-\ntion\ud835\udc53\u00b9\ud835\udc65\u00bathat only takes \ud835\udc65as its input. Given the abundance of polysemy and complex\nsemantics in natural languages, context-independent representations have obvious limita-\ntions. For instance, the word \u201ccrane\u201d in contexts \u201ca crane is flying\u201d and \u201ca crane driver\ncame\u201d has completely different meanings; thus, the same word may be assigned different\nrepresentations depending on contexts.\nThismotivatesthedevelopmentof context-sensitive wordrepresentations, whererepresen-\ntations of words depend on their contexts. Hence, a context-sensitive representation of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1790, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c30ea178-e97c-4de9-900a-f8b318d7b678": {"__data__": {"id_": "c30ea178-e97c-4de9-900a-f8b318d7b678", "embedding": null, "metadata": {"page_label": "725", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1921b58-16da-468f-afbd-15aa161c8103", "node_type": "4", "metadata": {"page_label": "725", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f347b99d66c6792c4974d7c6909c8f1f6ccd8ef12af2349da1990e9172925853", "class_name": "RelatedNodeInfo"}}, "text": "725 Bidirectional Encoder Representations from Transformers (BERT)\ntoken\ud835\udc65is a function \ud835\udc53\u00b9\ud835\udc65,\ud835\udc50\u00b9\ud835\udc65\u00ba\u00badepending on both \ud835\udc65and its context \ud835\udc50\u00b9\ud835\udc65\u00ba. Popular context-\nsensitiverepresentationsincludeTagLM(language-model-augmentedsequencetagger)( Pe-\ntersetal., 2017), CoVe (Context Vectors) ( McCannetal., 2017), and ELMo (Embeddings\nfrom Language Models) ( Petersetal., 2018).\nFor example, by taking the entire sequence as input, ELMo is a function that assigns a rep-\nresentation to each word from the input sequence. Specifically, ELMo combines all the\nintermediate layer representations from pretrained bidirectional LSTM as the output rep-\nresentation. Then the ELMo representation will be added to a downstream task\u2019s existing\nsupervised model as additional features, such as by concatenating ELMo representation\nand the original representation (e.g., GloVe) of tokens in the existing model. On the one\nhand, all the weights in the pretrained bidirectional LSTM model are frozen after ELMo\nrepresentations are added. On the other hand, the existing supervised model is specifically\ncustomized for a given task. Leveraging different best models for different tasks at that\ntime, adding ELMo improved the state of the art across six natural language processing\ntasks: sentiment analysis, natural language inference, semantic role labeling, coreference\nresolution, named entity recognition, and question answering.\n15.8.2FromTask-Specificto Task-Agnostic\nAlthough ELMo has significantly improved solutions to a diverse set of natural language\nprocessing tasks, each solution still hinges on a task-specific architecture. However, it is\npractically non-trivial to craft a specific architecture for every natural language processing\ntask. The GPT (Generative Pre-Training) model represents an effort in designing a general\ntask-agnostic model for context-sensitive representations ( Radfordet al., 2018). Built on\na Transformer decoder, GPT pretrains a language model that will be used to represent text\nsequences. When applying GPT to a downstream task, the output of the language model\nwillbefedintoanaddedlinearoutputlayertopredictthelabelofthetask. Insharpcontrast\nto ELMo that freezes parameters of the pretrained model, GPT fine-tunes allthe parame-\nters in the pretrained Transformer decoder during supervised learning of the downstream\ntask. GPT was evaluated on twelve tasks of natural language inference, question answer-\ning, sentencesimilarity, andclassification, andimprovedthestateoftheartinnineofthem\nwith minimal changes to the model architecture.\nHowever, due to the autoregressive nature of language models, GPT only looks forward\n(left-to-right). In contexts \u201ci went to the bank to deposit cash\u201d and \u201ci went to the bank\nto sit down\u201d, as \u201cbank\u201d is sensitive to the context to its left, GPT will return the same\nrepresentation for \u201cbank\u201d, though it has different meanings.\n15.8.3BERT:Combining the Best of BothWorlds\nAswehaveseen,ELMoencodescontextbidirectionallybutusestask-specificarchitectures;\nwhile GPT is task-agnostic but encodes context left-to-right. Combining the best of both\nworlds, BERT (Bidirectional Encoder Representations from Transformers) encodes con-\ntext bidirectionally and requires minimal architecture changes for a wide range of natural\nlanguage processing tasks ( Devlinet al., 2018). Using a pretrained Transformer encoder,\nBERT is able to represent any token based on its bidirectional context. During supervised", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f772bae-4358-47cd-9437-784c5f917b75": {"__data__": {"id_": "1f772bae-4358-47cd-9437-784c5f917b75", "embedding": null, "metadata": {"page_label": "726", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "711bd499-e9af-47bf-af3d-d868d7ae49bb", "node_type": "4", "metadata": {"page_label": "726", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "63b066061120b52f232b50632ba7ea5e835c0ac139461ba1e0760653c0334787", "class_name": "RelatedNodeInfo"}}, "text": "726 Natural Language Processing: Pretraining\nlearning of downstream tasks, BERT is similar to GPT in two aspects. First, BERT rep-\nresentations will be fed into an added output layer, with minimal changes to the model\narchitecture depending on nature of tasks, such as predicting for every token vs. predicting\nfor the entire sequence. Second, all the parameters of the pretrained Transformer encoder\nare fine-tuned, while the additional output layer will be trained from scratch. Fig. 15.8.1\ndepicts the differences among ELMo, GPT, and BERT.\ntFig. 15.8.1 A comparison of ELMo, GPT, and BERT.\nBERT further improved the state of the art on eleven natural language processing tasks\nunderbroadcategoriesof(i)singletextclassification(e.g.,sentimentanalysis),(ii)textpair\nclassification (e.g., natural language inference), (iii) question answering, (iv) text tagging\n(e.g., named entity recognition). All proposed in 2018, from context-sensitive ELMo to\ntask-agnostic GPT and BERT, conceptually simple yet empirically powerful pretraining of\ndeep representations for natural languages have revolutionized solutions to various natural\nlanguage processing tasks.\nIntherestofthischapter,wewilldiveintothepretrainingofBERT.Whennaturallanguage\nprocessingapplicationsareexplainedin Chapter16 , wewillillustratefine-tuningofBERT\nfor downstream applications.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n15.8.4InputRepresentation\nIn natural language processing, some tasks (e.g., sentiment analysis) take single text as\ninput, while in some other tasks (e.g., natural language inference), the input is a pair of\ntext sequences. The BERT input sequence unambiguously represents both single text and\ntext pairs. In the former, the BERT input sequence is the concatenation of the special\nclassification token \u201c<cls>\u201d, tokens of a text sequence, and the special separation token\n\u201c<sep>\u201d. In the latter, the BERT input sequence is the concatenation of \u201c<cls>\u201d, tokens\nof the first text sequence, \u201c<sep>\u201d, tokens of the second text sequence, and \u201c<sep>\u201d. We\nwill consistently distinguish the terminology \u201cBERT input sequence\u201d from other types of", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e026d2e-9a10-4d60-a3cd-e0873f78b389": {"__data__": {"id_": "2e026d2e-9a10-4d60-a3cd-e0873f78b389", "embedding": null, "metadata": {"page_label": "727", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a691f552-13c7-470e-91b4-835659309171", "node_type": "4", "metadata": {"page_label": "727", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "76f4b030f405295c125c7e820901edf1996d71d10497dff06d9fe8dbdb913653", "class_name": "RelatedNodeInfo"}}, "text": "727 Bidirectional Encoder Representations from Transformers (BERT)\n\u201csequences\u201d. For instance, one BERTinputsequence may include either one textsequence\nor twotextsequences .\nTo distinguish text pairs, the learned segment embeddings e\ud835\udc34ande\ud835\udc35are added to the\ntoken embeddings of the first sequence and the second sequence, respectively. For single\ntext inputs, only e\ud835\udc34is used.\nThe following get_tokens_and_segments takes either one sentence or two sentences as\ninput, then returns tokens of the BERT input sequence and their corresponding segment\nIDs.\n#@save\ndef get_tokens_and_segments (tokens_a, tokens_b =None ):\n\"\"\"Get tokens of the BERT input sequence and their segment IDs.\"\"\"\ntokens =['<cls> ']+tokens_a +['<sep> ']\n# 0 and 1 are marking segment A and B, respectively\nsegments =[0]*(len(tokens_a) +2)\niftokens_b isnot None :\ntokens +=tokens_b +['<sep> ']\nsegments +=[1]*(len(tokens_b) +1)\nreturn tokens, segments\nBERT chooses the Transformer encoder as its bidirectional architecture. Common in the\nTransformerencoder,positionalembeddingsareaddedateverypositionoftheBERTinput\nsequence. However,differentfromtheoriginalTransformerencoder,BERTuses learnable\npositional embeddings. To sum up, Fig. 15.8.2 shows that the embeddings of the BERT\ninput sequence are the sum of the token embeddings, segment embeddings, and positional\nembeddings.\ntFig. 15.8.2 The embeddings of the BERT input sequence are the sum of the token embeddings,\nsegment embeddings, and positional embeddings.\nThe following BERTEncoder class is similar to the TransformerEncoder class as imple-\nmentedin Section11.7 . Differentfrom TransformerEncoder ,BERTEncoder usessegment\nembeddings and learnable positional embeddings.\n#@save\nclass BERTEncoder (nn.Module):\n\"\"\"BERT encoder.\"\"\"\ndef __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout, max_len =1000 ,**kwargs):\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1897, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4af72307-fad6-45ea-a56e-c9243718a6da": {"__data__": {"id_": "4af72307-fad6-45ea-a56e-c9243718a6da", "embedding": null, "metadata": {"page_label": "728", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24ae88e3-bf25-43d3-868d-11372ed0bb7e", "node_type": "4", "metadata": {"page_label": "728", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "187c0485612334a7d07d12c808eaf7835eb08d49352da6dbb86092c989fc110b", "class_name": "RelatedNodeInfo"}}, "text": "728 Natural Language Processing: Pretraining\n(continued from previous page)\nsuper (BERTEncoder, self ).__init__ (**kwargs)\nself .token_embedding =nn.Embedding(vocab_size, num_hiddens)\nself .segment_embedding =nn.Embedding( 2, num_hiddens)\nself .blks =nn.Sequential()\nfor iinrange (num_blks):\nself .blks .add_module( f\"{i}\", d2l .TransformerEncoderBlock(\nnum_hiddens, ffn_num_hiddens, num_heads, dropout, True ))\n# In BERT, positional embeddings are learnable, thus we create a\n# parameter of positional embeddings that are long enough\nself .pos_embedding =nn.Parameter(torch .randn( 1, max_len,\nnum_hiddens))\ndef forward (self , tokens, segments, valid_lens):\n# Shape of `X` remains unchanged in the following code snippet:\n# (batch size, max sequence length, `num_hiddens`)\nX=self .token_embedding(tokens) +self .segment_embedding(segments)\nX=X+self .pos_embedding[:, :X .shape[ 1], :]\nfor blk inself .blks:\nX=blk(X, valid_lens)\nreturn X\nSuppose that the vocabulary size is 10000. To demonstrate forward inference of BERTEn-\ncoder, let\u2019s create an instance of it and initialize its parameters.\nvocab_size, num_hiddens, ffn_num_hiddens, num_heads =10000 ,768,1024 ,4\nffn_num_input, num_blks, dropout =768,2,0.2\nencoder =BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens, num_heads,\nnum_blks, dropout)\nWe define tokensto be 2 BERT input sequences of length 8, where each token is an index\nof the vocabulary. The forward inference of BERTEncoder with the input tokensreturns\nthe encoded result where each token is represented by a vector whose length is predefined\nby the hyperparameter num_hiddens . This hyperparameter is usually referred to as the\nhiddensize (number of hidden units) of the Transformer encoder.\ntokens =torch .randint( 0, vocab_size, ( 2,8))\nsegments =torch .tensor([[ 0,0,0,0,1,1,1,1], [ 0,0,0,1,1,1,1,1]])\nencoded_X =encoder(tokens, segments, None )\nencoded_X .shape\ntorch .Size([ 2,8,768])\n15.8.5PretrainingTasks\nThe forward inference of BERTEncoder gives the BERT representation of each token of\nthe input text and the inserted special tokens \u201c<cls>\u201d and \u201c<seq>\u201d. Next, we will use\nthese representations to compute the loss function for pretraining BERT. The pretraining\nis composed of the following two tasks: masked language modeling and next sentence\nprediction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7df5a440-856e-405e-99bd-e3943da3ce15": {"__data__": {"id_": "7df5a440-856e-405e-99bd-e3943da3ce15", "embedding": null, "metadata": {"page_label": "729", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ea7bc45-6623-4d87-b6ae-d410bf58478f", "node_type": "4", "metadata": {"page_label": "729", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "aa488a83c92f57b2f40ca86346fd7a8ab1e1a1ce5f215ba7b8b337a0687a6d8e", "class_name": "RelatedNodeInfo"}}, "text": "729 Bidirectional Encoder Representations from Transformers (BERT)\nMaskedLanguage Modeling\nAs illustrated in Section 9.3 , a language model predicts a token using the context on its\nleft. To encode context bidirectionally for representing each token, BERT randomly masks\ntokens and uses tokens from the bidirectional context to predict the masked tokens in a\nself-supervised fashion. This task is referred to as a masked languagemodel .\nIn this pretraining task, 15% of tokens will be selected at random as the masked tokens for\nprediction. To predict a masked token without cheating by using the label, one straight-\nforward approach is to always replace it with a special \u201c<mask>\u201d token in the BERT input\nsequence. However, the artificial special token \u201c<mask>\u201d will never appear in fine-tuning.\nTo avoid such a mismatch between pretraining and fine-tuning, if a token is masked for\nprediction (e.g., \u201cgreat\u201d is selected to be masked and predicted in \u201cthis movie is great\u201d), in\nthe input it will be replaced with:\n\u000fa special \u201c<mask>\u201d token for 80% of the time (e.g., \u201cthis movie is great\u201d becomes \u201cthis\nmovie is <mask>\u201d);\n\u000fa random token for 10% of the time (e.g., \u201cthis movie is great\u201d becomes \u201cthis movie is\ndrink\u201d);\n\u000fthe unchanged label token for 10% of the time (e.g., \u201cthis movie is great\u201d becomes \u201cthis\nmovie is great\u201d).\nNotethatfor10%of15%timearandomtokenisinserted. Thisoccasionalnoiseencourages\nBERTtobelessbiasedtowardsthemaskedtoken(especiallywhenthelabeltokenremains\nunchanged) in its bidirectional context encoding.\nWeimplementthefollowing MaskLMclasstopredictmaskedtokensinthemaskedlanguage\nmodeltaskofBERTpretraining. Thepredictionusesaone-hidden-layerMLP( self.mlp ).\nIn forward inference, it takes two inputs: the encoded result of BERTEncoder and the token\npositions for prediction. The output is the prediction results at these positions.\n#@save\nclass MaskLM (nn.Module):\n\"\"\"The masked language model task of BERT.\"\"\"\ndef __init__ (self , vocab_size, num_hiddens, **kwargs):\nsuper (MaskLM, self ).__init__ (**kwargs)\nself .mlp =nn.Sequential(nn .LazyLinear(num_hiddens),\nnn.ReLU(),\nnn.LayerNorm(num_hiddens),\nnn.LazyLinear(vocab_size))\ndef forward (self , X, pred_positions):\nnum_pred_positions =pred_positions .shape[ 1]\npred_positions =pred_positions .reshape( -1)\nbatch_size =X.shape[ 0]\nbatch_idx =torch .arange( 0, batch_size)\n# Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n# `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\nbatch_idx =torch .repeat_interleave(batch_idx, num_pred_positions)\nmasked_X =X[batch_idx, pred_positions]\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2599, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0b93bdc-e656-4ea0-8ee8-abc701f10e68": {"__data__": {"id_": "d0b93bdc-e656-4ea0-8ee8-abc701f10e68", "embedding": null, "metadata": {"page_label": "730", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84dc5fb7-ecba-479b-9c9d-b186d8d29622", "node_type": "4", "metadata": {"page_label": "730", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "76a11f432a66c9deec7296fa1b020443126efa8cfa48d32b008bbf5b4eb1d95d", "class_name": "RelatedNodeInfo"}}, "text": "730 Natural Language Processing: Pretraining\n(continued from previous page)\nmasked_X =masked_X .reshape((batch_size, num_pred_positions, -1))\nmlm_Y_hat =self .mlp(masked_X)\nreturn mlm_Y_hat\nTo demonstrate the forward inference of MaskLM, we create its instance mlmand initialize\nit. Recall that encoded_X from the forward inference of BERTEncoder represents 2 BERT\ninputsequences. Wedefine mlm_positions asthe3indicestopredictineitherBERTinput\nsequenceof encoded_X .Theforwardinferenceof mlmreturnspredictionresults mlm_Y_hat\nat all the masked positions mlm_positions ofencoded_X . For each prediction, the size of\nthe result is equal to the vocabulary size.\nmlm =MaskLM(vocab_size, num_hiddens)\nmlm_positions =torch .tensor([[ 1,5,2], [ 6,1,5]])\nmlm_Y_hat =mlm(encoded_X, mlm_positions)\nmlm_Y_hat .shape\ntorch .Size([ 2,3,10000 ])\nWith the ground truth labels mlm_Yof the predicted tokens mlm_Y_hat under masks, we\ncan calculate the cross-entropy loss of the masked language model task in BERT pretrain-\ning.\nmlm_Y =torch .tensor([[ 7,8,9], [ 10,20,30]])\nloss =nn.CrossEntropyLoss(reduction ='none ')\nmlm_l =loss(mlm_Y_hat .reshape(( -1, vocab_size)), mlm_Y .reshape( -1))\nmlm_l .shape\ntorch .Size([ 6])\nNextSentence Prediction\nAlthough masked language modeling is able to encode bidirectional context for represent-\ning words, it does not explicitly model the logical relationship between text pairs. To help\nunderstand the relationship between two text sequences, BERT considers a binary classi-\nfication task, next sentence prediction , in its pretraining. When generating sentence pairs\nfor pretraining, for half of the time they are indeed consecutive sentences with the label\n\u201cTrue\u201d; while for the other half of the time the second sentence is randomly sampled from\nthe corpus with the label \u201cFalse\u201d.\nThe following NextSentencePred class uses a one-hidden-layer MLP to predict whether\nthe second sentence is the next sentence of the first in the BERT input sequence. Due to\nself-attention in the Transformer encoder, the BERT representation of the special token\n\u201c<cls>\u201d encodes both the two sentences from the input. Hence, the output layer ( self.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e32bed8e-2fdd-481d-8e4f-16d5f79d0974": {"__data__": {"id_": "e32bed8e-2fdd-481d-8e4f-16d5f79d0974", "embedding": null, "metadata": {"page_label": "731", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9791e908-04ad-48b0-a9ea-7d90973d0f83", "node_type": "4", "metadata": {"page_label": "731", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4c912603d6663a1b1491fe737c26e441a01c4790503d2a0e222668fc03c533e6", "class_name": "RelatedNodeInfo"}}, "text": "731 Bidirectional Encoder Representations from Transformers (BERT)\noutput) of the MLP classifier takes Xas input, where Xis the output of the MLP hidden\nlayer whose input is the encoded \u201c<cls>\u201d token.\n#@save\nclass NextSentencePred (nn.Module):\n\"\"\"The next sentence prediction task of BERT.\"\"\"\ndef __init__ (self ,**kwargs):\nsuper (NextSentencePred, self ).__init__ (**kwargs)\nself .output =nn.LazyLinear( 2)\ndef forward (self , X):\n# `X` shape: (batch size, `num_hiddens`)\nreturn self .output(X)\nWe can see that the forward inference of an NextSentencePred instance returns binary\npredictions for each BERT input sequence.\n# PyTorch by default will not flatten the tensor as seen in mxnet where, if\n# flatten=True, all but the first axis of input data are collapsed together\nencoded_X =torch .flatten(encoded_X, start_dim =1)\n# input_shape for NSP: (batch size, `num_hiddens`)\nnsp =NextSentencePred()\nnsp_Y_hat =nsp(encoded_X)\nnsp_Y_hat .shape\ntorch .Size([ 2,2])\nThe cross-entropy loss of the 2 binary classifications can also be computed.\nnsp_y =torch .tensor([ 0,1])\nnsp_l =loss(nsp_Y_hat, nsp_y)\nnsp_l .shape\ntorch .Size([ 2])\nIt is noteworthy that all the labels in both the aforementioned pretraining tasks can be triv-\nially obtained from the pretraining corpus without manual labeling effort. The original\nBERT has been pretrained on the concatenation of BookCorpus ( Zhuetal., 2015) and En-\nglish Wikipedia. These two text corpora are huge: they have 800 million words and 2.5\nbillion words, respectively.\n15.8.6PuttingIt All Together\nWhen pretraining BERT, the final loss function is a linear combination of both the loss\nfunctions for masked language modeling and next sentence prediction. Now we can de-\nfine the BERTModel class by instantiating the three classes BERTEncoder ,MaskLM, and\nNextSentencePred . TheforwardinferencereturnstheencodedBERTrepresentations en-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1bf9c5d-d6cc-4719-9d25-4f25b711a3ac": {"__data__": {"id_": "b1bf9c5d-d6cc-4719-9d25-4f25b711a3ac", "embedding": null, "metadata": {"page_label": "732", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d3f32ca-e39a-4bbb-b52c-b489c4653ef7", "node_type": "4", "metadata": {"page_label": "732", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2930f5fc75f50278452a50760ea57935ae66fece65d58a1e17dc7f5397427249", "class_name": "RelatedNodeInfo"}}, "text": "732 Natural Language Processing: Pretraining\ncoded_X , predictions of masked language modeling mlm_Y_hat , and next sentence predic-\ntions nsp_Y_hat .\n#@save\nclass BERTModel (nn.Module):\n\"\"\"The BERT model.\"\"\"\ndef __init__ (self , vocab_size, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout, max_len =1000 ):\nsuper (BERTModel, self ).__init__ ()\nself .encoder =BERTEncoder(vocab_size, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout,\nmax_len =max_len)\nself .hidden =nn.Sequential(nn .LazyLinear(num_hiddens),\nnn.Tanh())\nself .mlm =MaskLM(vocab_size, num_hiddens)\nself .nsp =NextSentencePred()\ndef forward (self , tokens, segments, valid_lens =None , pred_positions =None ):\nencoded_X =self .encoder(tokens, segments, valid_lens)\nifpred_positions isnot None :\nmlm_Y_hat =self .mlm(encoded_X, pred_positions)\nelse :\nmlm_Y_hat =None\n# The hidden layer of the MLP classifier for next sentence prediction.\n# 0 is the index of the '<cls>' token\nnsp_Y_hat =self .nsp( self .hidden(encoded_X[:, 0, :]))\nreturn encoded_X, mlm_Y_hat, nsp_Y_hat\n15.8.7Summary\n\u000fWord embedding models such as word2vec and GloVe are context-independent. They\nassign the same pretrained vector to the same word regardless of the context of the\nword (if any). It is hard for them to handle well polysemy or complex semantics in\nnatural languages.\n\u000fFor context-sensitive word representations such as ELMo and GPT, representations of\nwords depend on their contexts.\n\u000fELMo encodes context bidirectionally but uses task-specific architectures (however, it is\npractically non-trivial to craft a specific architecture for every natural language pro-\ncessing task); while GPT is task-agnostic but encodes context left-to-right.\n\u000fBERT combines the best of both worlds: it encodes context bidirectionally and requires\nminimal architecture changes for a wide range of natural language processing tasks.\n\u000fThe embeddings of the BERT input sequence are the sum of the token embeddings,\nsegment embeddings, and positional embeddings.\n\u000fPretraining BERT is composed of two tasks: masked language modeling and next sen-\ntence prediction. The former is able to encode bidirectional context for representing\nwords, while the latter explicitly models the logical relationship between text pairs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2263, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21280591-bb69-497c-a6f2-34ae2d104fc1": {"__data__": {"id_": "21280591-bb69-497c-a6f2-34ae2d104fc1", "embedding": null, "metadata": {"page_label": "733", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3474da39-a95e-4fd0-a2ab-6b7569485ea6", "node_type": "4", "metadata": {"page_label": "733", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "21027e6056488ec67dbc7b21d9e3ca642c268e0bc1fb0e644423a7fbe76f254e", "class_name": "RelatedNodeInfo"}}, "text": "733 The Dataset for Pretraining BERT\n23815.8.8Exercises\n1.All other things being equal, will a masked language model require more or fewer pre-\ntraining steps to converge than a left-to-right language model? Why?\n2.IntheoriginalimplementationofBERT,thepositionwisefeed-forwardnetworkin BERTEn-\ncoder(viad2l.TransformerEncoderBlock ) and the fully connected layer in MaskLM\nboth use the Gaussian error linear unit (GELU) ( Hendrycks and Gimpel, 2016 ) as the\nactivation function. Research into the difference between GELU and ReLU.\nDiscussions238.\n15.9TheDatasetforPretrainingBERT\nTopretraintheBERTmodelasimplementedin Section15.8 ,weneedtogeneratethedataset\nin the ideal format to facilitate the two pretraining tasks: masked language modeling and\nnext sentence prediction. On the one hand, the original BERT model is pretrained on\nthe concatenation of two huge corpora BookCorpus and English Wikipedia (see Section\n15.8.5), making it hard to run for most readers of this book. On the other hand, the off-\nthe-shelf pretrained BERT model may not fit for applications from specific domains like\nmedicine. Thus, it is getting popular to pretrain BERT on a customized dataset. To facil-\nitate the demonstration of BERT pretraining, we use a smaller corpus WikiText-2 ( Merity\netal., 2016).\nComparing with the PTB dataset used for pretraining word2vec in Section 15.3 , WikiText-\n2 (i) retains the original punctuation, making it suitable for next sentence prediction; (ii)\nretains the original case and numbers; (iii) is over twice larger.\nimport os\nimport random\nimport torch\nfrom d2l import torch asd2l\nIn the WikiText-2 dataset, each line represents a paragraph where space is inserted be-\ntween any punctuation and its preceding token. Paragraphs with at least two sentences are\nretained. To split sentences, we only use the period as the delimiter for simplicity. We\nleave discussions of more complex sentence splitting techniques in the exercises at the end\nof this section.\n#@save\nd2l.DATA_HUB[ 'wikitext-2 ']=(\n'https://s3.amazonaws.com/research.metamind.io/wikitext/ '\n'wikitext-2-v1.zip ','3c914d17d80b1459be871a5039ac23e752a53cbe ')\n#@save\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2172, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b615f598-7889-4626-a3eb-1e9d4a0375b1": {"__data__": {"id_": "b615f598-7889-4626-a3eb-1e9d4a0375b1", "embedding": null, "metadata": {"page_label": "734", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf62ba83-137b-42a0-8ae8-6e29cae4d7d5", "node_type": "4", "metadata": {"page_label": "734", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dab2cffeb60706d6d1fb6fa574bae05dc14d713dccacc47e1ab2ef45c084df6f", "class_name": "RelatedNodeInfo"}}, "text": "734 Natural Language Processing: Pretraining\n(continued from previous page)\ndef _read_wiki (data_dir):\nfile_name =os.path .join(data_dir, 'wiki.train.tokens ')\nwith open (file_name, 'r')asf:\nlines =f.readlines()\n# Uppercase letters are converted to lowercase ones\nparagraphs =[line .strip() .lower() .split( '.')\nfor line inlines iflen(line .split( '.'))>=2]\nrandom .shuffle(paragraphs)\nreturn paragraphs\n15.9.1Defining Helper Functions forPretrainingTasks\nIn the following, we begin by implementing helper functions for the two BERT pretraining\ntasks: next sentence prediction and masked language modeling. These helper functions\nwill be invoked later when transforming the raw text corpus into the dataset of the ideal\nformat to pretrain BERT.\nGenerating the NextSentence PredictionTask\nAccording to descriptions of Section 15.8.5 , the _get_next_sentence function generates\na training example for the binary classification task.\n#@save\ndef _get_next_sentence (sentence, next_sentence, paragraphs):\nifrandom .random() <0.5:\nis_next =True\nelse :\n# `paragraphs` is a list of lists of lists\nnext_sentence =random .choice(random .choice(paragraphs))\nis_next =False\nreturn sentence, next_sentence, is_next\nThe following function generates training examples for next sentence prediction from the\ninput paragraph byinvokingthe _get_next_sentence function. Here paragraph isalist\nof sentences, where each sentence is a list of tokens. The argument max_len specifies the\nmaximum length of a BERT input sequence during pretraining.\n#@save\ndef _get_nsp_data_from_paragraph (paragraph, paragraphs, vocab, max_len):\nnsp_data_from_paragraph =[]\nfor iinrange (len(paragraph) -1):\ntokens_a, tokens_b, is_next =_get_next_sentence(\nparagraph[i], paragraph[i +1], paragraphs)\n# Consider 1 '<cls>' token and 2 '<sep>' tokens\niflen(tokens_a) +len(tokens_b) +3>max_len:\ncontinue\ntokens, segments =d2l.get_tokens_and_segments(tokens_a, tokens_b)\nnsp_data_from_paragraph .append((tokens, segments, is_next))\nreturn nsp_data_from_paragraph", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2016, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "db5f2b93-137c-44ce-ba27-931f3ac5a1b5": {"__data__": {"id_": "db5f2b93-137c-44ce-ba27-931f3ac5a1b5", "embedding": null, "metadata": {"page_label": "735", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ca994dd-546a-4d4e-a130-976f64eaa4ee", "node_type": "4", "metadata": {"page_label": "735", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dcd7e5086cbfb383a5ecb5200c90dcddd18a862f7a60a41bca6278adde45676c", "class_name": "RelatedNodeInfo"}}, "text": "735 The Dataset for Pretraining BERT\nGeneratingthe MaskedLanguageModeling Task\nInordertogeneratetrainingexamplesforthemaskedlanguagemodelingtaskfromaBERT\ninputsequence,wedefinethefollowing _replace_mlm_tokens function. Initsinputs, to-\nkensisalistoftokensrepresentingaBERTinputsequence, candidate_pred_positions\nisalistoftokenindicesoftheBERTinputsequenceexcludingthoseofspecialtokens(spe-\ncial tokens are not predicted in the masked language modeling task), and num_mlm_preds\nindicates the number of predictions (recall 15% random tokens to predict). Following the\ndefinitionofthemaskedlanguagemodelingtaskin Section15.8.5 , at eachpredictionposi-\ntion, the input may be replaced by a special \u201c<mask>\u201d token or a random token, or remain\nunchanged. Intheend, thefunctionreturnstheinputtokensafterpossiblereplacement, the\ntoken indices where predictions take place and labels for these predictions.\n#@save\ndef _replace_mlm_tokens (tokens, candidate_pred_positions, num_mlm_preds,\nvocab):\n# For the input of a masked language model, make a new copy of tokens and\n# replace some of them by '<mask>' or random tokens\nmlm_input_tokens =[token for token intokens]\npred_positions_and_labels =[]\n# Shuffle for getting 15% random tokens for prediction in the masked\n# language modeling task\nrandom .shuffle(candidate_pred_positions)\nfor mlm_pred_position incandidate_pred_positions:\niflen(pred_positions_and_labels) >=num_mlm_preds:\nbreak\nmasked_token =None\n# 80% of the time: replace the word with the '<mask>' token\nifrandom .random() <0.8:\nmasked_token ='<mask> '\nelse :\n# 10% of the time: keep the word unchanged\nifrandom .random() <0.5:\nmasked_token =tokens[mlm_pred_position]\n# 10% of the time: replace the word with a random word\nelse :\nmasked_token =random .choice(vocab .idx_to_token)\nmlm_input_tokens[mlm_pred_position] =masked_token\npred_positions_and_labels .append(\n(mlm_pred_position, tokens[mlm_pred_position]))\nreturn mlm_input_tokens, pred_positions_and_labels\nBy invoking the aforementioned _replace_mlm_tokens function, the following function\ntakes a BERT input sequence ( tokens) as an input and returns indices of the input tokens\n(after possible token replacement as described in Section 15.8.5 ), the token indices where\npredictions take place, and label indices for these predictions.\n#@save\ndef _get_mlm_data_from_tokens (tokens, vocab):\ncandidate_pred_positions =[]\n# `tokens` is a list of strings\nfor i, token inenumerate (tokens):\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2474, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffffc70c-5201-4fa4-a22d-ecd5005bb512": {"__data__": {"id_": "ffffc70c-5201-4fa4-a22d-ecd5005bb512", "embedding": null, "metadata": {"page_label": "736", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "030089ec-2105-4886-ae34-1daa00b518b4", "node_type": "4", "metadata": {"page_label": "736", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c92f73f6caf6cbb584ddf749a99a84eccaf6907ccfc9145ae4ecf939d79a28e3", "class_name": "RelatedNodeInfo"}}, "text": "736 Natural Language Processing: Pretraining\n(continued from previous page)\n# Special tokens are not predicted in the masked language modeling\n# task\niftoken in['<cls> ','<sep> ']:\ncontinue\ncandidate_pred_positions .append(i)\n# 15% of random tokens are predicted in the masked language modeling task\nnum_mlm_preds =max(1,round (len(tokens) *0.15 ))\nmlm_input_tokens, pred_positions_and_labels =_replace_mlm_tokens(\ntokens, candidate_pred_positions, num_mlm_preds, vocab)\npred_positions_and_labels =sorted (pred_positions_and_labels,\nkey=lambda x: x[ 0])\npred_positions =[v[0]for vinpred_positions_and_labels]\nmlm_pred_labels =[v[1]for vinpred_positions_and_labels]\nreturn vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n15.9.2TransformingTextinto the PretrainingDataset\nNow we are almost ready to customize a Dataset class for pretraining BERT. Before that,\nwestillneedtodefineahelperfunction _pad_bert_inputs toappendthespecial\u201c<pad>\u201d\ntokens to the inputs. Its argument examples contain the outputs from the helper func-\ntions _get_nsp_data_from_paragraph and_get_mlm_data_from_tokens for the two\npretraining tasks.\n#@save\ndef _pad_bert_inputs (examples, max_len, vocab):\nmax_num_mlm_preds =round (max_len *0.15 )\nall_token_ids, all_segments, valid_lens, =[], [], []\nall_pred_positions, all_mlm_weights, all_mlm_labels =[], [], []\nnsp_labels =[]\nfor (token_ids, pred_positions, mlm_pred_label_ids, segments,\nis_next) inexamples:\nall_token_ids .append(torch .tensor(token_ids +[vocab[ '<pad> ']]*(\nmax_len -len(token_ids)), dtype =torch .long))\nall_segments .append(torch .tensor(segments +[0]*(\nmax_len -len(segments)), dtype =torch .long))\n# `valid_lens` excludes count of '<pad>' tokens\nvalid_lens .append(torch .tensor( len(token_ids), dtype =torch .float32))\nall_pred_positions .append(torch .tensor(pred_positions +[0]*(\nmax_num_mlm_preds -len(pred_positions)), dtype =torch .long))\n# Predictions of padded tokens will be filtered out in the loss via\n# multiplication of 0 weights\nall_mlm_weights .append(\ntorch .tensor([ 1.0]*len(mlm_pred_label_ids) +[0.0]*(\nmax_num_mlm_preds -len(pred_positions)),\ndtype =torch .float32))\nall_mlm_labels .append(torch .tensor(mlm_pred_label_ids +[0]*(\nmax_num_mlm_preds -len(mlm_pred_label_ids)), dtype =torch .long))\nnsp_labels .append(torch .tensor(is_next, dtype =torch .long))\nreturn (all_token_ids, all_segments, valid_lens, all_pred_positions,\nall_mlm_weights, all_mlm_labels, nsp_labels)\nPutting the helper functions for generating training examples of the two pretraining tasks,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d472ffc-d582-42c7-9a0e-5d727d4b5807": {"__data__": {"id_": "0d472ffc-d582-42c7-9a0e-5d727d4b5807", "embedding": null, "metadata": {"page_label": "737", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fe686b58-d837-432e-b3cd-a6f918c6893f", "node_type": "4", "metadata": {"page_label": "737", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "770abc58c1b1c4998b8e1c753c93f5668c82d71bf55a6595a7e99706a56dc421", "class_name": "RelatedNodeInfo"}}, "text": "737 The Dataset for Pretraining BERT\nand the helper function for padding inputs together, we customize the following _Wiki-\nTextDataset class as the WikiText-2 dataset for pretraining BERT. By implementing the\n__getitem__ function, we can arbitrarily access the pretraining (masked language model-\ning and next sentence prediction) examples generated from a pair of sentences from the\nWikiText-2 corpus.\nThe original BERT model uses WordPiece embeddings whose vocabulary size is 30000\n(Wuet al., 2016). The tokenization method of WordPiece is a slight modification of the\noriginal byte pair encoding algorithm in Section 15.6.2 . For simplicity, we use the d2l.\ntokenize function for tokenization. Infrequent tokens that appear less than five times are\nfiltered out.\n#@save\nclass _WikiTextDataset (torch .utils .data .Dataset):\ndef __init__ (self , paragraphs, max_len):\n# Input `paragraphs[i]` is a list of sentence strings representing a\n# paragraph; while output `paragraphs[i]` is a list of sentences\n# representing a paragraph, where each sentence is a list of tokens\nparagraphs =[d2l .tokenize(\nparagraph, token ='word ')for paragraph inparagraphs]\nsentences =[sentence for paragraph inparagraphs\nfor sentence inparagraph]\nself .vocab =d2l.Vocab(sentences, min_freq =5, reserved_tokens =[\n'<pad> ','<mask> ','<cls> ','<sep> '])\n# Get data for the next sentence prediction task\nexamples =[]\nfor paragraph inparagraphs:\nexamples .extend(_get_nsp_data_from_paragraph(\nparagraph, paragraphs, self .vocab, max_len))\n# Get data for the masked language model task\nexamples =[(_get_mlm_data_from_tokens(tokens, self .vocab)\n+(segments, is_next))\nfor tokens, segments, is_next inexamples]\n# Pad inputs\n(self .all_token_ids, self .all_segments, self .valid_lens,\nself .all_pred_positions, self .all_mlm_weights,\nself .all_mlm_labels, self .nsp_labels) =_pad_bert_inputs(\nexamples, max_len, self .vocab)\ndef __getitem__ (self , idx):\nreturn (self .all_token_ids[idx], self .all_segments[idx],\nself .valid_lens[idx], self .all_pred_positions[idx],\nself .all_mlm_weights[idx], self .all_mlm_labels[idx],\nself .nsp_labels[idx])\ndef __len__ (self ):\nreturn len(self .all_token_ids)\nByusingthe _read_wiki functionandthe _WikiTextDataset class,wedefinethefollow-\ningload_data_wiki to download and WikiText-2 dataset and generate pretraining exam-\nples from it.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2349, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "351c368f-bd61-4372-a13e-7b6fbc1f17b4": {"__data__": {"id_": "351c368f-bd61-4372-a13e-7b6fbc1f17b4", "embedding": null, "metadata": {"page_label": "738", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84472535-029a-4fe2-85b3-6aff6c5030d6", "node_type": "4", "metadata": {"page_label": "738", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "55b5b9abd705f24f076196e40cb43ba7d3b7a402f4d00ea96e6f1e11a963323e", "class_name": "RelatedNodeInfo"}}, "text": "738 Natural Language Processing: Pretraining\n#@save\ndef load_data_wiki (batch_size, max_len):\n\"\"\"Load the WikiText-2 dataset.\"\"\"\nnum_workers =d2l.get_dataloader_workers()\ndata_dir =d2l.download_extract( 'wikitext-2 ','wikitext-2 ')\nparagraphs =_read_wiki(data_dir)\ntrain_set =_WikiTextDataset(paragraphs, max_len)\ntrain_iter =torch .utils .data .DataLoader(train_set, batch_size,\nshuffle =True , num_workers =num_workers)\nreturn train_iter, train_set .vocab\nSetting the batch size to 512 and the maximum length of a BERT input sequence to be\n64, we print out the shapes of a minibatch of BERT pretraining examples. Note that in\neach BERT input sequence, 10(64\u00020.15) positions are predicted for the masked language\nmodeling task.\nbatch_size, max_len =512,64\ntrain_iter, vocab =load_data_wiki(batch_size, max_len)\nfor (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\nmlm_Y, nsp_y) intrain_iter:\nprint (tokens_X .shape, segments_X .shape, valid_lens_x .shape,\npred_positions_X .shape, mlm_weights_X .shape, mlm_Y .shape,\nnsp_y .shape)\nbreak\nDownloading ../data /wikitext -2-v1.zip from https ://s3.amazonaws .com/research .\n\u21a9!metamind .io/wikitext /wikitext -2-v1.zip...\ntorch .Size([ 512,64]) torch .Size([ 512,64]) torch .Size([ 512]) torch .Size([ 512,\u2423\n\u21a9!10]) torch .Size([ 512,10]) torch .Size([ 512,10]) torch .Size([ 512])\nIntheend,let\u2019stakealookatthevocabularysize. Evenafterfilteringoutinfrequenttokens,\nit is still over twice larger than that of the PTB dataset.\nlen(vocab)\n20256\n15.9.3Summary\n\u000fComparingwiththePTBdataset,theWikiText-2datesetretainstheoriginalpunctuation,\ncase and numbers, and is over twice larger.\n\u000fWe can arbitrarily access the pretraining (masked language modeling and next sentence\nprediction) examples generated from a pair of sentences from the WikiText-2 corpus.\n15.9.4Exercises", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1831, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d686143-7b0a-4530-b6f9-856b4f638512": {"__data__": {"id_": "9d686143-7b0a-4530-b6f9-856b4f638512", "embedding": null, "metadata": {"page_label": "739", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7eaf12b8-d3f1-4122-a030-afb3c65c122c", "node_type": "4", "metadata": {"page_label": "739", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8442a57c109682cbae8109203337f76200b0eabc162b109377a63c09bd134a0c", "class_name": "RelatedNodeInfo"}}, "text": "739 Pretraining BERT\n2391.For simplicity, the period is used as the only delimiter for splitting sentences. Try other\nsentence splitting techniques, such as the spaCy and NLTK. Take NLTK as an exam-\nple. You need to install NLTK first: pip install nltk . In the code, first import\nnltk. Then, download the Punkt sentence tokenizer: nltk.download('punkt') . To\nsplit sentences such as sentences = 'This is great ! Why not ?' , invok-\ningnltk.tokenize.sent_tokenize(sentences) will return a list of two sentence\nstrings: ['This is great !', 'Why not ?'] .\n2.What is the vocabulary size if we do not filter out any infrequent token?\nDiscussions239.\n15.10PretrainingBERT\nWiththeBERTmodelimplementedin Section15.8 andthepretrainingexamplesgenerated\nfrom the WikiText-2 dataset in Section 15.9 , we will pretrain BERT on the WikiText-2\ndataset in this section.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\nTostart,weloadtheWikiText-2datasetasminibatchesofpretrainingexamplesformasked\nlanguage modeling and next sentence prediction. The batch size is 512 and the maximum\nlengthofaBERTinputsequenceis64. NotethatintheoriginalBERTmodel,themaximum\nlength is 512.\nbatch_size, max_len =512,64\ntrain_iter, vocab =d2l.load_data_wiki(batch_size, max_len)\n15.10.1PretrainingBERT\nTheoriginalBERThastwoversionsofdifferentmodelsizes( Devlinetal.,2018). Thebase\nmodel (BERT BASE) uses 12 layers (Transformer encoder blocks) with 768 hidden units\n(hidden size) and 12 self-attention heads. The large model (BERT LARGE) uses 24 layers\nwith 1024 hidden units and 16 self-attention heads. Notably, the former has 110 million\nparameters while the latter has 340 million parameters. For demonstration with ease, we\ndefine a small BERT, using 2 layers, 128 hidden units, and 2 self-attention heads.\nnet =d2l.BERTModel( len(vocab), num_hiddens =128,\nffn_num_hiddens =256, num_heads =2, num_blks =2, dropout =0.2)\ndevices =d2l.try_all_gpus()\nloss =nn.CrossEntropyLoss()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f787b0c8-5ab5-44af-b1ab-63192b1b2591": {"__data__": {"id_": "f787b0c8-5ab5-44af-b1ab-63192b1b2591", "embedding": null, "metadata": {"page_label": "740", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bac9c96-5aa6-4031-b055-e7d034ca1ec2", "node_type": "4", "metadata": {"page_label": "740", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a4404f6655bffe25079718c7ad39068734cffa55359744466d00e36017f863dc", "class_name": "RelatedNodeInfo"}}, "text": "740 Natural Language Processing: Pretraining\nBefore defining the training loop, we define a helper function _get_batch_loss_bert .\nGiven the shard of training examples, this function computes the loss for both the masked\nlanguage modeling and next sentence prediction tasks. Note that the final loss of BERT\npretrainingisjustthesumofboththemaskedlanguagemodelinglossandthenextsentence\nprediction loss.\n#@save\ndef _get_batch_loss_bert (net, loss, vocab_size, tokens_X,\nsegments_X, valid_lens_x,\npred_positions_X, mlm_weights_X,\nmlm_Y, nsp_y):\n# Forward pass\n_, mlm_Y_hat, nsp_Y_hat =net(tokens_X, segments_X,\nvalid_lens_x .reshape( -1),\npred_positions_X)\n# Compute masked language model loss\nmlm_l =loss(mlm_Y_hat .reshape( -1, vocab_size), mlm_Y .reshape( -1))*\\\nmlm_weights_X .reshape( -1,1)\nmlm_l =mlm_l .sum() /(mlm_weights_X .sum() +1e-8 )\n# Compute next sentence prediction loss\nnsp_l =loss(nsp_Y_hat, nsp_y)\nl=mlm_l +nsp_l\nreturn mlm_l, nsp_l, l\nInvoking the two aforementioned helper functions, the following train_bert function de-\nfinestheproceduretopretrainBERT( net)ontheWikiText-2( train_iter )dataset. Train-\ning BERT can take very long. Instead of specifying the number of epochs for training as in\nthetrain_ch13 function(see Section14.1 ),theinput num_steps ofthefollowingfunction\nspecifies the number of iteration steps for training.\ndef train_bert (train_iter, net, loss, vocab_size, devices, num_steps):\nnet( *next (iter (train_iter))[: 4])\nnet =nn.DataParallel(net, device_ids =devices) .to(devices[ 0])\ntrainer =torch .optim .Adam(net .parameters(), lr =0.01 )\nstep, timer =0, d2l .Timer()\nanimator =d2l.Animator(xlabel ='step ', ylabel ='loss ',\nxlim =[1, num_steps], legend =['mlm','nsp'])\n# Sum of masked language modeling losses, sum of next sentence prediction\n# losses, no. of sentence pairs, count\nmetric =d2l.Accumulator( 4)\nnum_steps_reached =False\nwhile step <num_steps and not num_steps_reached:\nfor tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\nmlm_weights_X, mlm_Y, nsp_y intrain_iter:\ntokens_X =tokens_X .to(devices[ 0])\nsegments_X =segments_X .to(devices[ 0])\nvalid_lens_x =valid_lens_x .to(devices[ 0])\npred_positions_X =pred_positions_X .to(devices[ 0])\nmlm_weights_X =mlm_weights_X .to(devices[ 0])\nmlm_Y, nsp_y =mlm_Y .to(devices[ 0]), nsp_y .to(devices[ 0])\ntrainer .zero_grad()\ntimer .start()\nmlm_l, nsp_l, l =_get_batch_loss_bert(\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "425309cf-d435-40f8-95dc-b6c98b585664": {"__data__": {"id_": "425309cf-d435-40f8-95dc-b6c98b585664", "embedding": null, "metadata": {"page_label": "741", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0bed854b-0d52-4e3d-a9cc-215a62cf62b8", "node_type": "4", "metadata": {"page_label": "741", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3f02c7d6873a128335bd792433593654d400961fbdb08c1ee639ec541bd1421e", "class_name": "RelatedNodeInfo"}}, "text": "741 Pretraining BERT\n(continued from previous page)\nnet, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\npred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\nl.backward()\ntrainer .step()\nmetric .add(mlm_l, nsp_l, tokens_X .shape[ 0],1)\ntimer .stop()\nanimator .add(step +1,\n(metric[ 0]/metric[ 3], metric[ 1]/metric[ 3]))\nstep +=1\nifstep ==num_steps:\nnum_steps_reached =True\nbreak\nprint (f'MLM loss {metric[ 0]/metric[ 3]:.3f},'\nf'NSP loss {metric[ 1]/metric[ 3]:.3f}')\nprint (f'{metric[ 2]/timer .sum() :.1f}sentence pairs/sec on '\nf'{str(devices) }')\nWe can plot both the masked language modeling loss and the next sentence prediction loss\nduring BERT pretraining.\ntrain_bert(train_iter, net, loss, len(vocab), devices, 50)\nMLM loss 5.885 , NSP loss 0.760\n4413.2 sentence pairs /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\n\u21a9!index =1)]\n15.10.2RepresentingTextwith BERT\nAfter pretraining BERT, we can use it to represent single text, text pairs, or any token in\nthem. The following function returns the BERT ( net) representations for all tokens in\ntokens_a andtokens_b .\ndef get_bert_encoding (net, tokens_a, tokens_b =None ):\ntokens, segments =d2l.get_tokens_and_segments(tokens_a, tokens_b)\ntoken_ids =torch .tensor(vocab[tokens], device =devices[ 0]).unsqueeze( 0)\nsegments =torch .tensor(segments, device =devices[ 0]).unsqueeze( 0)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1383, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11ec3056-c3a7-4d3e-8b58-44d44e6d8f5b": {"__data__": {"id_": "11ec3056-c3a7-4d3e-8b58-44d44e6d8f5b", "embedding": null, "metadata": {"page_label": "742", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f776f761-4dab-423a-9127-59af5ac54afb", "node_type": "4", "metadata": {"page_label": "742", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6f8c095b5670a28518ade2871a451b63308bc641cc302c96f9e642d56f19726a", "class_name": "RelatedNodeInfo"}}, "text": "742 Natural Language Processing: Pretraining\n(continued from previous page)\nvalid_len =torch .tensor( len(tokens), device =devices[ 0]).unsqueeze( 0)\nencoded_X, _, _ =net(token_ids, segments, valid_len)\nreturn encoded_X\nConsider the sentence \u201ca crane is flying\u201d. Recall the input representation of BERT as dis-\ncussed in Section 15.8.4 . After inserting special tokens \u201c<cls>\u201d (used for classification)\nand \u201c<sep>\u201d (used for separation), the BERT input sequence has a length of six. Since\nzero is the index of the \u201c<cls>\u201d token, encoded_text[:, 0, :] is the BERT represen-\ntation of the entire input sentence. To evaluate the polysemy token \u201ccrane\u201d, we also print\nout the first three elements of the BERT representation of the token.\ntokens_a =['a','crane ','is','flying ']\nencoded_text =get_bert_encoding(net, tokens_a)\n# Tokens: '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'\nencoded_text_cls =encoded_text[:, 0, :]\nencoded_text_crane =encoded_text[:, 2, :]\nencoded_text .shape, encoded_text_cls .shape, encoded_text_crane[ 0][:3]\n(torch .Size([ 1,6,128]),\ntorch .Size([ 1,128]),\ntensor([ 0.8414 ,1.4830 ,0.8226 ], device ='cuda:0 ', grad_fn =<SliceBackward0 >))\nNowconsiderasentencepair\u201cacranedrivercame\u201dand\u201chejustleft\u201d. Similarly, encoded_pair[:,\n0, :]istheencodedresultoftheentiresentencepairfromthepretrainedBERT.Notethat\nthe first three elements of the polysemy token \u201ccrane\u201d are different from those when the\ncontext is different. This supports that BERT representations are context-sensitive.\ntokens_a, tokens_b =['a','crane ','driver ','came '], [ 'he','just ','left ']\nencoded_pair =get_bert_encoding(net, tokens_a, tokens_b)\n# Tokens: '<cls>', 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just',\n# 'left', '<sep>'\nencoded_pair_cls =encoded_pair[:, 0, :]\nencoded_pair_crane =encoded_pair[:, 2, :]\nencoded_pair .shape, encoded_pair_cls .shape, encoded_pair_crane[ 0][:3]\n(torch .Size([ 1,10,128]),\ntorch .Size([ 1,128]),\ntensor([ 0.0430 ,1.6132 ,0.0437 ], device ='cuda:0 ', grad_fn =<SliceBackward0 >))\nInChapter16 ,wewillfine-tuneapretrainedBERTmodelfordownstreamnaturallanguage\nprocessing applications.\n15.10.3Summary\n\u000fThe original BERT has two versions, where the base model has 110 million parameters\nand the large model has 340 million parameters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2266, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18bdf931-4694-4cf5-ac01-ed68f0e9d00f": {"__data__": {"id_": "18bdf931-4694-4cf5-ac01-ed68f0e9d00f", "embedding": null, "metadata": {"page_label": "743", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "85ef6810-78f9-4ee3-803b-1832f959b972", "node_type": "4", "metadata": {"page_label": "743", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dc89deea8dd47e6d61c45aea838e37f9f75c0479ba0322d18aedf943ae2291e4", "class_name": "RelatedNodeInfo"}}, "text": "743 Pretraining BERT\n240\u000fAfter pretraining BERT, we can use it to represent single text, text pairs, or any token in\nthem.\n\u000fIntheexperiment,thesametokenhasdifferentBERTrepresentationwhentheircontexts\nare different. This supports that BERT representations are context-sensitive.\n15.10.4Exercises\n1.In the experiment, we can see that the masked language modeling loss is significantly\nhigher than the next sentence prediction loss. Why?\n2.SetthemaximumlengthofaBERTinputsequencetobe512(sameastheoriginalBERT\nmodel). Use the configurations of the original BERT model such as BERT LARGE. Do\nyou encounter any error when running this section? Why?\nDiscussions240.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0b0a6ae-4ac9-44d4-a9ad-49953cf0c4a8": {"__data__": {"id_": "e0b0a6ae-4ac9-44d4-a9ad-49953cf0c4a8", "embedding": null, "metadata": {"page_label": "744", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cec06a64-d579-43f5-8eeb-7d566eefbb09", "node_type": "4", "metadata": {"page_label": "744", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ef70f5e17449113441f7b8b286ae9911266bcabe9eac30de1c9818a057087546", "class_name": "RelatedNodeInfo"}}, "text": "16Natural Language Processing:\nApplications\nWe have seen how to represent tokens in text sequences and train their representations in\nChapter 15 . Such pretrained text representations can be fed to various models for different\ndownstream natural language processing tasks.\nIn fact, earlier chapters have already discussed some natural language processing applica-\ntionswithout pretraining , just for explaining deep learning architectures. For instance, in\nChapter9 ,wehavereliedonRNNstodesignlanguagemodelstogeneratenovella-liketext.\nInChapter10 andChapter11 , wehavealsodesignedmodelsbasedonRNNsandattention\nmechanisms for machine translation.\nHowever, this book does not intend to cover all such applications in a comprehensive man-\nner. Instead, our focus is on how to apply (deep) representation learning of languages to\naddressing natural language processing problems . Given pretrained text representations,\nthis chapter will explore two popular and representative downstream natural language pro-\ncessing tasks: sentiment analysis and natural language inference, which analyze single text\nand relationships of text pairs, respectively.\ntFig. 16.1 Pretrained text representations can be fed to various deep learning architectures for\ndifferent downstream natural language processing applications. This chapter focuses on\nhow to design models for different downstream natural language processing applications.\nAsdepictedin Fig.16.1 ,thischapterfocusesondescribingthebasicideasofdesigningnat-\nural language processing models using different types of deep learning architectures, such\nas MLPs, CNNs, RNNs, and attention. Though it is possible to combine any pretrained\ntextrepresentationswithanyarchitectureforeitherapplicationin Fig.16.1 ,weselectafew\nrepresentative combinations. Specifically, we will explore popular architectures based on\nRNNs and CNNs for sentiment analysis. For natural language inference, we choose atten-\n744", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1937, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dcb30c19-f285-40f4-b2ee-9bffa1ccd1ef": {"__data__": {"id_": "dcb30c19-f285-40f4-b2ee-9bffa1ccd1ef", "embedding": null, "metadata": {"page_label": "745", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0432f765-52ac-46b7-8a64-46993358215e", "node_type": "4", "metadata": {"page_label": "745", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "da886a912f5ebb8586cbc1dd8672dd18a191f4db790281d2b1030aa1428eb7a1", "class_name": "RelatedNodeInfo"}}, "text": "745 Sentiment Analysis and the Dataset\n241tion and MLPs to demonstrate how to analyze text pairs. In the end, we introduce how to\nfine-tune a pretrained BERT model for a wide range of natural language processing appli-\ncations, such as on a sequence level (single text classification and text pair classification)\nand a token level (text tagging and question answering). As a concrete empirical case, we\nwill fine-tune BERT for natural language inference.\nAs we have introduced in Section 15.8 , BERT requires minimal architecture changes for a\nwide range of natural language processing applications. However, this benefit comes at the\ncost of fine-tuning a huge number of BERT parameters for the downstream applications.\nWhen space or time is limited, those crafted models based on MLPs, CNNs, RNNs, and\nattention are more feasible. In the following, we start by the sentiment analysis application\nand illustrate the model design based on RNNs and CNNs, respectively.\n16.1SentimentAnalysisand the Dataset\nWith the proliferation of online social media and review platforms, a plethora of opinion-\nateddatahasbeenlogged,bearinggreatpotentialforsupportingdecisionmakingprocesses.\nSentiment analysis studies people\u2019s sentiments in their produced text, such as product re-\nviews, blog comments, and forum discussions. It enjoys wide applications to fields as\ndiverse as politics (e.g., analysis of public sentiments towards policies), finance (e.g., anal-\nysisofsentimentsofthemarket), andmarketing(e.g., productresearchandbrandmanage-\nment).\nSince sentiments can be categorized as discrete polarities or scales (e.g., positive and neg-\native), we can consider sentiment analysis as a text classification task, which transforms a\nvarying-length text sequence into a fixed-length text category. In this chapter, we will use\nStanford\u2019s largemoviereviewdataset241forsentimentanalysis. Itconsistsofatrainingset\nand a testing set, either containing 25000 movie reviews downloaded from IMDb. In both\ndatasets, there are equal number of \u201cpositive\u201d and \u201cnegative\u201d labels, indicating different\nsentiment polarities.\nimport os\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n16.1.1Readingthe Dataset\nFirst,downloadandextractthisIMDbreviewdatasetinthepath ../data/aclImdb .\n#@save\nd2l.DATA_HUB[ 'aclImdb ']=(d2l .DATA_URL +'aclImdb_v1.tar.gz ',\n'01ada507287d82875905620988597833ad4e0903 ')\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2413, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0e36d0f2-76f4-4738-9a6f-b19365d0c4d1": {"__data__": {"id_": "0e36d0f2-76f4-4738-9a6f-b19365d0c4d1", "embedding": null, "metadata": {"page_label": "746", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0d3372c6-1931-4ee3-bad5-ef141ce3abd2", "node_type": "4", "metadata": {"page_label": "746", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e55127825a53217afdb668ea870a9526179dfcff2d5c8316895fa620f24e3e88", "class_name": "RelatedNodeInfo"}}, "text": "746 Natural Language Processing: Applications\n(continued from previous page)\ndata_dir =d2l.download_extract( 'aclImdb ','aclImdb ')\nDownloading ../data /aclImdb_v1 .tar.gzfrom http ://d2l-data .s3-accelerate .\n\u21a9!amazonaws .com/aclImdb_v1 .tar.gz...\nNext, read the training and test datasets. Each example is a review and its label: 1 for\n\u201cpositive\u201d and 0 for \u201cnegative\u201d.\n#@save\ndef read_imdb (data_dir, is_train):\n\"\"\"Read the IMDb review dataset text sequences and labels.\"\"\"\ndata, labels =[], []\nfor label in('pos','neg'):\nfolder_name =os.path .join(data_dir, 'train 'ifis_train else 'test ',\nlabel)\nfor file inos.listdir(folder_name):\nwith open (os.path .join(folder_name, file), 'rb')asf:\nreview =f.read() .decode( 'utf-8 ').replace( '\\n','')\ndata .append(review)\nlabels .append( 1iflabel =='pos'else 0)\nreturn data, labels\ntrain_data =read_imdb(data_dir, is_train =True )\nprint ('# trainings: ',len(train_data[ 0]))\nfor x, y inzip(train_data[ 0][:3], train_data[ 1][:3]):\nprint ('label: ', y, 'review: ', x[: 60])\n# trainings: 25000\nlabel: 1review: Zentropa has much incommon with The Third Man, another noir\nlabel: 1review: Zentropa isthe most original movie I 've seen in years. If y\nlabel: 1review: Lars Von Trier isnever backward intrying out new technique\n16.1.2Preprocessingthe Dataset\nTreating each word as a token and filtering out words that appear less than 5 times, we\ncreate a vocabulary out of the training dataset.\ntrain_tokens =d2l.tokenize(train_data[ 0], token ='word ')\nvocab =d2l.Vocab(train_tokens, min_freq =5, reserved_tokens =['<pad> '])\nAfter tokenization, let\u2019s plot the histogram of review lengths in tokens.\nd2l.set_figsize()\nd2l.plt.xlabel( '# tokens per review ')\nd2l.plt.ylabel( 'count ')\nd2l.plt.hist([ len(line) for line intrain_tokens], bins =range (0,1000 ,50));", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "065d99c3-808e-4a13-a2e5-1b237b08d3de": {"__data__": {"id_": "065d99c3-808e-4a13-a2e5-1b237b08d3de", "embedding": null, "metadata": {"page_label": "747", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "95426591-5709-442f-8cd2-8abf65a52984", "node_type": "4", "metadata": {"page_label": "747", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "00b724ecae40c65efd5afce777a30d46770bbde3f167e1e713073ca399d145ab", "class_name": "RelatedNodeInfo"}}, "text": "747 Sentiment Analysis and the Dataset\nAs we expected, the reviews have varying lengths. To process a minibatch of such reviews\nat each time, we set the length of each review to 500 with truncation and padding, which is\nsimilartothepreprocessingstepforthemachinetranslationdatasetin Section10.5 .\nnum_steps =500 # sequence length\ntrain_features =torch .tensor([d2l .truncate_pad(\nvocab[line], num_steps, vocab[ '<pad> '])for line intrain_tokens])\nprint (train_features .shape)\ntorch .Size([ 25000 ,500])\n16.1.3CreatingData Iterators\nNowwecancreatedataiterators. Ateachiteration,aminibatchofexamplesarereturned.\ntrain_iter =d2l.load_array((train_features, torch .tensor(train_data[ 1])), 64)\nfor X, y intrain_iter:\nprint ('X:', X.shape, ', y: ', y.shape)\nbreak\nprint ('# batches: ',len(train_iter))\nX: torch .Size([ 64,500]) , y: torch .Size([ 64])\n# batches: 391\n16.1.4PuttingIt All Together\nLast, we wrap up the above steps into the load_data_imdb function. It returns training\nand test data iterators and the vocabulary of the IMDb review dataset.\n#@save\ndef load_data_imdb (batch_size, num_steps =500):\n\"\"\"Return data iterators and the vocabulary of the IMDb review dataset.\"\"\"\ndata_dir =d2l.download_extract( 'aclImdb ','aclImdb ')\ntrain_data =read_imdb(data_dir, True )\ntest_data =read_imdb(data_dir, False )\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a30f9ca9-727e-46bd-a64c-1c524f941920": {"__data__": {"id_": "a30f9ca9-727e-46bd-a64c-1c524f941920", "embedding": null, "metadata": {"page_label": "748", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6065b453-9eba-4f13-916f-941a767417b9", "node_type": "4", "metadata": {"page_label": "748", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c47d064788f40d346c14ef5ae171ebfb0eabefc7d41e417991cbc51570471aa9", "class_name": "RelatedNodeInfo"}}, "text": "748 Natural Language Processing: Applications\n242\n243(continued from previous page)\ntrain_tokens =d2l.tokenize(train_data[ 0], token ='word ')\ntest_tokens =d2l.tokenize(test_data[ 0], token ='word ')\nvocab =d2l.Vocab(train_tokens, min_freq =5)\ntrain_features =torch .tensor([d2l .truncate_pad(\nvocab[line], num_steps, vocab[ '<pad> '])for line intrain_tokens])\ntest_features =torch .tensor([d2l .truncate_pad(\nvocab[line], num_steps, vocab[ '<pad> '])for line intest_tokens])\ntrain_iter =d2l.load_array((train_features, torch .tensor(train_data[ 1])),\nbatch_size)\ntest_iter =d2l.load_array((test_features, torch .tensor(test_data[ 1])),\nbatch_size,\nis_train =False )\nreturn train_iter, test_iter, vocab\n16.1.5Summary\n\u000fSentiment analysis studies people\u2019s sentiments in their produced text, which is consid-\nered as a text classification problem that transforms a varying-length text sequence\ninto a fixed-length text category.\n\u000fAfter preprocessing, we can load Stanford\u2019s large movie review dataset (IMDb review\ndataset) into data iterators with a vocabulary.\n16.1.6Exercises\n1.What hyperparameters in this section can we modify to accelerate training sentiment\nanalysis models?\n2.Can you implement a function to load the dataset of Amazon reviews242into data\niterators and labels for sentiment analysis?\nDiscussions243.\n16.2Sentiment Analysis: Using RecurrentNeural\nNetworks\nLike word similarity and analogy tasks, we can also apply pretrained word vectors to sen-\ntiment analysis. Since the IMDb review dataset in Section 16.1 is not very big, using text\nrepresentations that were pretrained on large-scale corpora may reduce overfitting of the\nmodel. As a specific example illustrated in Fig. 16.2.1 , we will represent each token using\nthe pretrained GloVe model, and feed these token representations into a multilayer bidi-\nrectional RNN to obtain the text sequence representation, which will be transformed into\nsentiment analysis outputs ( Maaset al., 2011). For the same downstream application, we\nwill consider a different architectural choice later.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2058, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51303d9a-7c64-4940-9d2e-fe8656f5aa01": {"__data__": {"id_": "51303d9a-7c64-4940-9d2e-fe8656f5aa01", "embedding": null, "metadata": {"page_label": "749", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b356567d-d8f2-4cfd-b2dd-9c9113b3039f", "node_type": "4", "metadata": {"page_label": "749", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e712048932ec9754e82d434558d5cb32db1e348a784e57487631da5813515e61", "class_name": "RelatedNodeInfo"}}, "text": "749 Sentiment Analysis: Using Recurrent Neural Networks\ntFig. 16.2.1 This section feeds pretrained GloVe to an RNN-based architecture for sentiment analysis.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\nbatch_size =64\ntrain_iter, test_iter, vocab =d2l.load_data_imdb(batch_size)\n16.2.1RepresentingSingle Textwith RNNs\nIn text classifications tasks, such as sentiment analysis, a varying-length text sequence will\nbetransformedintofixed-lengthcategories. Inthefollowing BiRNNclass,whileeachtoken\nof a text sequence gets its individual pretrained GloVe representation via the embedding\nlayer ( self.embedding ), the entire sequence is encoded by a bidirectional RNN ( self.\nencoder ). More concretely, the hidden states (at the last layer) of the bidirectional LSTM\nat both the initial and final time steps are concatenated as the representation of the text\nsequence. This single text representation is then transformed into output categories by a\nfullyconnectedlayer( self.decoder )withtwooutputs(\u201cpositive\u201dand\u201cnegative\u201d).\nclass BiRNN (nn.Module):\ndef __init__ (self , vocab_size, embed_size, num_hiddens,\nnum_layers, **kwargs):\nsuper (BiRNN, self ).__init__ (**kwargs)\nself .embedding =nn.Embedding(vocab_size, embed_size)\n# Set `bidirectional` to True to get a bidirectional RNN\nself .encoder =nn.LSTM(embed_size, num_hiddens, num_layers =num_layers,\nbidirectional =True )\nself .decoder =nn.Linear( 4*num_hiddens, 2)\ndef forward (self , inputs):\n# The shape of `inputs` is (batch size, no. of time steps). Because\n# LSTM requires its input's first dimension to be the temporal\n# dimension, the input is transposed before obtaining token\n# representations. The output shape is (no. of time steps, batch size,\n# word vector dimension)\nembeddings =self .embedding(inputs .T)\nself .encoder .flatten_parameters()\n# Returns hidden states of the last hidden layer at different time\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1917, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b3527a61-ed6e-4de8-8cbd-9cd5b4f07700": {"__data__": {"id_": "b3527a61-ed6e-4de8-8cbd-9cd5b4f07700", "embedding": null, "metadata": {"page_label": "750", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11703f22-745c-463d-b53e-eb05835c97bc", "node_type": "4", "metadata": {"page_label": "750", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d295defebef381773e24283dc5a8fac5ef1417bff2e975dd8e9fcafab8995d0d", "class_name": "RelatedNodeInfo"}}, "text": "750 Natural Language Processing: Applications\n(continued from previous page)\n# steps. The shape of `outputs` is (no. of time steps, batch size,\n# 2 * no. of hidden units)\noutputs, _ =self .encoder(embeddings)\n# Concatenate the hidden states at the initial and final time steps as\n# the input of the fully connected layer. Its shape is (batch size,\n# 4 * no. of hidden units)\nencoding =torch .cat((outputs[ 0], outputs[ -1]), dim =1)\nouts =self .decoder(encoding)\nreturn outs\nLet\u2019s construct a bidirectional RNN with two hidden layers to represent single text for sen-\ntiment analysis.\nembed_size, num_hiddens, num_layers, devices =100,100,2, d2l .try_all_gpus()\nnet =BiRNN( len(vocab), embed_size, num_hiddens, num_layers)\ndef init_weights (module):\niftype (module) ==nn.Linear:\nnn.init .xavier_uniform_(module .weight)\niftype (module) ==nn.LSTM:\nfor param inmodule ._flat_weights_names:\nif\"weight \"inparam:\nnn.init .xavier_uniform_(module ._parameters[param])\nnet.apply(init_weights);\n16.2.2LoadingPretrainedWordVectors\nBelow we load the pretrained 100-dimensional (needs to be consistent with embed_size )\nGloVe embeddings for tokens in the vocabulary.\nglove_embedding =d2l.TokenEmbedding( 'glove.6b.100d ')\nPrint the shape of the vectors for all the tokens in the vocabulary.\nembeds =glove_embedding[vocab .idx_to_token]\nembeds .shape\ntorch .Size([ 49346 ,100])\nWeusethesepretrainedwordvectorstorepresenttokensinthereviewsandwillnotupdate\nthese vectors during training.\nnet.embedding .weight .data .copy_(embeds)\nnet.embedding .weight .requires_grad =False\n16.2.3Trainingand Evaluatingthe Model", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d3dadfa-6703-41e7-971b-57a5b2ef1aa5": {"__data__": {"id_": "0d3dadfa-6703-41e7-971b-57a5b2ef1aa5", "embedding": null, "metadata": {"page_label": "751", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39c69c99-8687-4d50-a2a8-b4a5d3b50782", "node_type": "4", "metadata": {"page_label": "751", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2346899a9ab4adbcfd2b5d6b2863662befcd5cbdee19f087fe6fd490220b8e29", "class_name": "RelatedNodeInfo"}}, "text": "751 Sentiment Analysis: Using Recurrent Neural Networks\nNow we can train the bidirectional RNN for sentiment analysis.\nlr, num_epochs =0.01 ,5\ntrainer =torch .optim .Adam(net .parameters(), lr =lr)\nloss =nn.CrossEntropyLoss(reduction =\"none \")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.277 , train acc 0.884 , test acc 0.861\n2608.4 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]\nWedefinethefollowingfunctiontopredictthesentimentofatextsequenceusingthetrained\nmodel net.\n#@save\ndef predict_sentiment (net, vocab, sequence):\n\"\"\"Predict the sentiment of a text sequence.\"\"\"\nsequence =torch .tensor(vocab[sequence .split()], device =d2l.try_gpu())\nlabel =torch .argmax(net(sequence .reshape( 1,-1)), dim =1)\nreturn 'positive 'iflabel ==1else 'negative '\nFinally,let\u2019susethetrainedmodeltopredictthesentimentfortwosimplesentences.\npredict_sentiment(net, vocab, 'this movie is so great ')\n'positive '\npredict_sentiment(net, vocab, 'this movie is so bad ')\n'negative '\n16.2.4Summary\n\u000fPretrained word vectors can represent individual tokens in a text sequence.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8960da0-1db9-416d-9e30-77ca35ff721f": {"__data__": {"id_": "e8960da0-1db9-416d-9e30-77ca35ff721f", "embedding": null, "metadata": {"page_label": "752", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b109ff1f-c244-414c-b811-ceeec2cccaad", "node_type": "4", "metadata": {"page_label": "752", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "498769fae0bccb292f62dceaa71180c6e2f090c332309bffc08118964d49becd", "class_name": "RelatedNodeInfo"}}, "text": "752 Natural Language Processing: Applications\n244\u000fBidirectional RNNs can represent a text sequence, such as via the concatenation of its\nhidden states at the initial and final time steps. This single text representation can be\ntransformed into categories using a fully connected layer.\n16.2.5Exercises\n1.Increase the number of epochs. Can you improve the training and testing accuracies?\nHow about tuning other hyperparameters?\n2.Use larger pretrained word vectors, such as 300-dimensional GloVe embeddings. Does\nit improve classification accuracy?\n3.Can we improve the classification accuracy by using the spaCy tokenization? You need\nto install spaCy ( pip install spacy ) and install the English package ( python -m\nspacy download en ). In the code, first, import spaCy ( import spacy ). Then, load\nthe spaCy English package ( spacy_en = spacy.load('en') ). Finally, define the\nfunction def tokenizer(text): return [tok.text for tok in spacy_en.\ntokenizer(text)] and replace the original tokenizer function. Note the different\nforms of phrase tokens in GloVe and spaCy. For example, the phrase token \u201cnew york\u201d\ntakes the form of \u201cnew-york\u201d in GloVe and the form of \u201cnew york\u201d after the spaCy\ntokenization.\nDiscussions244.\n16.3SentimentAnalysis: Using Convolutional\nNeuralNetworks\nInChapter7 , weinvestigatedmechanismsforprocessingtwo-dimensionalimagedatawith\ntwo-dimensionalCNNs,whichwereappliedtolocalfeaturessuchasadjacentpixels. Though\noriginally designed for computer vision, CNNs are also widely used for natural language\nprocessing. Simplyput, justthinkofanytextsequenceasaone-dimensionalimage. Inthis\nway, one-dimensional CNNs can process local features such as \ud835\udc5b-grams in text.\nIn this section, we will use the textCNN model to demonstrate how to design a CNN ar-\nchitecture for representing single text ( Kim, 2014 ). Compared with Fig. 16.2.1 that uses\nan RNN architecture with GloVe pretraining for sentiment analysis, the only difference in\nFig. 16.3.1 lies in the choice of the architecture.\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\nbatch_size =64\ntrain_iter, test_iter, vocab =d2l.load_data_imdb(batch_size)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2145, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fe30fa36-116a-46af-8217-5364e41a75d9": {"__data__": {"id_": "fe30fa36-116a-46af-8217-5364e41a75d9", "embedding": null, "metadata": {"page_label": "753", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50b099ff-a2fb-4ee8-b292-12c29a73ef64", "node_type": "4", "metadata": {"page_label": "753", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1e4aafeec66cf2e7ebef3bbede47b1788d3ff20967d7d0355c24542a30bd860d", "class_name": "RelatedNodeInfo"}}, "text": "753 Sentiment Analysis: Using Convolutional Neural Networks\ntFig. 16.3.1 This section feeds pretrained GloVe to a CNN-based architecture for sentiment analysis.\n16.3.1One-DimensionalConvolutions\nBefore introducing the model, let\u2019s see how a one-dimensional convolution works. Bear\nin mind that it is just a special case of a two-dimensional convolution based on the cross-\ncorrelation operation.\ntFig. 16.3.2 One-dimensional cross-correlation operation. The shaded portions are the \ufb01rst output\nelement as well as the input and kernel tensor elements used for the output computation:\n0\u00021\u00b81\u00022=2.\nAs shown in Fig. 16.3.2 , in the one-dimensional case, the convolution window slides from\nleft to right across the input tensor. During sliding, the input subtensor (e.g., 0and 1in\nFig.16.3.2 )containedintheconvolutionwindowatacertainpositionandthekerneltensor\n(e.g., 1and2inFig. 16.3.2 ) are multiplied elementwise. The sum of these multiplications\ngives the single scalar value (e.g., 0\u00021\u00b81\u00022=2inFig. 16.3.2 ) at the corresponding\nposition of the output tensor.\nWeimplementone-dimensionalcross-correlationinthefollowing corr1dfunction. Given\nan input tensor Xand a kernel tensor K, it returns the output tensor Y.\ndef corr1d (X, K):\nw=K.shape[ 0]\nY=torch .zeros((X .shape[ 0]-w+1))\nfor iinrange (Y.shape[ 0]):\nY[i] =(X[i: i +w]*K).sum()\nreturn Y\nWe can construct the input tensor Xand the kernel tensor KfromFig. 16.3.2 to validate the\noutput of the above one-dimensional cross-correlation implementation.\nX, K =torch .tensor([ 0,1,2,3,4,5,6]), torch .tensor([ 1,2])\ncorr1d(X, K)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "443af01c-f4ea-47af-8c78-0faab5af3436": {"__data__": {"id_": "443af01c-f4ea-47af-8c78-0faab5af3436", "embedding": null, "metadata": {"page_label": "754", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5a44a25-85bf-411b-8bf2-5e06cac2476e", "node_type": "4", "metadata": {"page_label": "754", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5dd5e49b6d38ddac021c650519652b5258308ebbb60c8e73d4003ace7e1937d3", "class_name": "RelatedNodeInfo"}}, "text": "754 Natural Language Processing: Applications\ntensor([ 2.,5.,8.,11.,14.,17.])\nForanyone-dimensionalinputwithmultiplechannels,theconvolutionkernelneedstohave\nthe same number of input channels. Then for each channel, perform a cross-correlation\noperation on the one-dimensional tensor of the input and the one-dimensional tensor of\nthe convolution kernel, summing the results over all the channels to produce the one-\ndimensional output tensor. Fig. 16.3.3 shows a one-dimensional cross-correlation oper-\nation with 3 input channels.\ntFig. 16.3.3 One-dimensional cross-correlation operation with 3 input channels. The shaded portions\nare the \ufb01rst output element as well as the input and kernel tensor elements used for the\noutput computation: 0 \u00021\u00b81\u00022\u00b81\u00023\u00b82\u00024\u00b82\u0002\u00b9\u00001\u00ba\u00b83\u0002\u00b9\u00003\u00ba=2.\nWecanimplementtheone-dimensionalcross-correlationoperationformultipleinputchan-\nnels and validate the results in Fig. 16.3.3 .\ndef corr1d_multi_in (X, K):\n# First, iterate through the 0th dimension (channel dimension) of `X` and\n# `K`. Then, add them together\nreturn sum(corr1d(x, k) for x, k inzip(X, K))\nX=torch .tensor([[ 0,1,2,3,4,5,6],\n[1,2,3,4,5,6,7],\n[2,3,4,5,6,7,8]])\nK=torch .tensor([[ 1,2], [ 3,4], [ -1,-3]])\ncorr1d_multi_in(X, K)\ntensor([ 2.,8.,14.,20.,26.,32.])\nNote that multi-input-channel one-dimensional cross-correlations are equivalent to single-\ninput-channel two-dimensional cross-correlations. To illustrate, an equivalent form of the\nmulti-input-channel one-dimensional cross-correlation in Fig. 16.3.3 is the single-input-\nchannel two-dimensional cross-correlation in Fig. 16.3.4 , where the height of the convolu-\ntion kernel has to be the same as that of the input tensor.\nBoth the outputs in Fig. 16.3.2 andFig. 16.3.3 have only one channel. Same as two-\ndimensional convolutions with multiple output channels described in Section 7.4.2 , we\ncan also specify multiple output channels for one-dimensional convolutions.\n16.3.2Max-Over-Time Pooling\nSimilarly, we can use pooling to extract the highest value from sequence representations as\nthe most important feature across time steps. The max-over-timepooling used in textCNN", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89a6f371-31ee-46f2-92a5-7bb768ce96b2": {"__data__": {"id_": "89a6f371-31ee-46f2-92a5-7bb768ce96b2", "embedding": null, "metadata": {"page_label": "755", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b86d7efe-4a9e-4ee8-a6fa-b700dd73f112", "node_type": "4", "metadata": {"page_label": "755", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "230ecdf2d2cbfa68313e2205cee44f686e76b9d98260a34620b34cbc8182107b", "class_name": "RelatedNodeInfo"}}, "text": "755 Sentiment Analysis: Using Convolutional Neural Networks\ntFig. 16.3.4 Two-dimensional cross-correlation operation with a single input channel. The shaded\nportions are the \ufb01rst output element as well as the input and kernel tensor elements used\nfor the output computation: 2 \u0002\u00b9\u00001\u00ba\u00b83\u0002\u00b9\u00003\u00ba\u00b81\u00023\u00b82\u00024\u00b80\u00021\u00b81\u00022=2.\nworks like the one-dimensional global max-pooling ( Collobert et al., 2011). For a multi-\nchannel input where each channel stores values at different time steps, the output at each\nchannelisthemaximumvalueforthatchannel. Notethatthemax-over-timepoolingallows\ndifferent numbers of time steps at different channels.\n16.3.3The textCNNModel\nUsing the one-dimensional convolution and max-over-time pooling, the textCNN model\ntakes individual pretrained token representations as input, then obtains and transforms se-\nquence representations for the downstream application.\nFor a single text sequence with \ud835\udc5btokens represented by \ud835\udc51-dimensional vectors, the width,\nheight, and number of channels of the input tensor are \ud835\udc5b,1, and\ud835\udc51, respectively. The\ntextCNN model transforms the input into the output as follows:\n1.Define multiple one-dimensional convolution kernels and perform convolution opera-\ntions separately on the inputs. Convolution kernels with different widths may capture\nlocal features among different numbers of adjacent tokens.\n2.Perform max-over-time pooling on all the output channels, and then concatenate all the\nscalar pooling outputs as a vector.\n3.Transform the concatenated vector into the output categories using the fully connected\nlayer. Dropout can be used for reducing overfitting.\nFig.16.3.5 illustratesthemodelarchitectureoftextCNNwithaconcreteexample. Theinput\nis a sentence with 11 tokens, where each token is represented by a 6-dimensional vectors.\nSo we have a 6-channel input with width 11. Define two one-dimensional convolution\nkernelsofwidths2and4,with4and5outputchannels,respectively. Theyproduce4output\nchannelswithwidth 11\u00002\u00b81=10and5outputchannelswithwidth 11\u00004\u00b81=8. Despite\ndifferent widths of these 9 channels, the max-over-time pooling gives a concatenated 9-\ndimensional vector, which is finally transformed into a 2-dimensional output vector for\nbinary sentiment predictions.\nDefiningthe Model\nWe implement the textCNN model in the following class. Compared with the bidirectional\nRNN model in Section 16.2 , besides replacing recurrent layers with convolutional layers,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0191d8bd-66a8-4914-af7d-8736dce04c78": {"__data__": {"id_": "0191d8bd-66a8-4914-af7d-8736dce04c78", "embedding": null, "metadata": {"page_label": "756", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "49cb956b-4ce2-4d7b-a872-8b8f03de9b63", "node_type": "4", "metadata": {"page_label": "756", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "608c05552435c36963724aa84c548464b7e7acbe8f5a54773175f585563b4991", "class_name": "RelatedNodeInfo"}}, "text": "756 Natural Language Processing: Applications\ntFig. 16.3.5 The model architecture of textCNN.\nwe also use two embedding layers: one with trainable weights and the other with fixed\nweights.\nclass TextCNN (nn.Module):\ndef __init__ (self , vocab_size, embed_size, kernel_sizes, num_channels,\n**kwargs):\nsuper (TextCNN, self ).__init__ (**kwargs)\nself .embedding =nn.Embedding(vocab_size, embed_size)\n# The embedding layer not to be trained\nself .constant_embedding =nn.Embedding(vocab_size, embed_size)\nself .dropout =nn.Dropout( 0.5)\nself .decoder =nn.Linear( sum(num_channels), 2)\n# The max-over-time pooling layer has no parameters, so this instance\n# can be shared\nself .pool =nn.AdaptiveAvgPool1d( 1)\nself .relu =nn.ReLU()\n# Create multiple one-dimensional convolutional layers\nself .convs =nn.ModuleList()\nfor c, k inzip(num_channels, kernel_sizes):\nself .convs .append(nn .Conv1d( 2*embed_size, c, k))\ndef forward (self , inputs):\n# Concatenate two embedding layer outputs with shape (batch size, no.\n# of tokens, token vector dimension) along vectors\nembeddings =torch .cat((\nself .embedding(inputs), self .constant_embedding(inputs)), dim =2)\n# Per the input format of one-dimensional convolutional layers,\n# rearrange the tensor so that the second dimension stores channels\nembeddings =embeddings .permute( 0,2,1)\n# For each one-dimensional convolutional layer, after max-over-time\n# pooling, a tensor of shape (batch size, no. of channels, 1) is\n# obtained. Remove the last dimension and concatenate along channels\nencoding =torch .cat([\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1570, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71b1f9ca-8959-46e8-8ebf-331f0102c9cb": {"__data__": {"id_": "71b1f9ca-8959-46e8-8ebf-331f0102c9cb", "embedding": null, "metadata": {"page_label": "757", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e6cc344-e889-44d2-8e98-166083dd04ab", "node_type": "4", "metadata": {"page_label": "757", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2c9a434ea8fc6b9cb707a6b7c277ab1fb70fd42372a44d38df0268fbc6c69bdd", "class_name": "RelatedNodeInfo"}}, "text": "757 Sentiment Analysis: Using Convolutional Neural Networks\n(continued from previous page)\ntorch .squeeze( self .relu( self .pool(conv(embeddings))), dim =-1)\nfor conv inself .convs], dim =1)\noutputs =self .decoder( self .dropout(encoding))\nreturn outputs\nLet\u2019s create a textCNN instance. It has 3 convolutional layers with kernel widths of 3, 4,\nand 5, all with 100 output channels.\nembed_size, kernel_sizes, nums_channels =100, [3,4,5], [ 100,100,100]\ndevices =d2l.try_all_gpus()\nnet =TextCNN( len(vocab), embed_size, kernel_sizes, nums_channels)\ndef init_weights (module):\niftype (module) in(nn.Linear, nn .Conv1d):\nnn.init .xavier_uniform_(module .weight)\nnet.apply(init_weights);\nLoadingPretrainedWordVectors\nSame as Section 16.2 , we load pretrained 100-dimensional GloVe embeddings as the ini-\ntialized token representations. These token representations (embedding weights) will be\ntrained in embedding and fixed in constant_embedding .\nglove_embedding =d2l.TokenEmbedding( 'glove.6b.100d ')\nembeds =glove_embedding[vocab .idx_to_token]\nnet.embedding .weight .data .copy_(embeds)\nnet.constant_embedding .weight .data .copy_(embeds)\nnet.constant_embedding .weight .requires_grad =False\nTrainingand Evaluatingthe Model\nNow we can train the textCNN model for sentiment analysis.\nlr, num_epochs =0.001 ,5\ntrainer =torch .optim .Adam(net .parameters(), lr =lr)\nloss =nn.CrossEntropyLoss(reduction =\"none \")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.066 , train acc 0.979 , test acc 0.868\n4354.2 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]\nBelow we use the trained model to predict the sentiment for two simple sentences.\nd2l.predict_sentiment(net, vocab, 'this movie is so great ')", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c99654e-5608-4c86-b557-56483cc32623": {"__data__": {"id_": "0c99654e-5608-4c86-b557-56483cc32623", "embedding": null, "metadata": {"page_label": "758", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c61b74a1-7b19-490b-ab97-65784629777e", "node_type": "4", "metadata": {"page_label": "758", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "eadbc5acb38c6e4c8e39c87a6cbd00410e24b575d75d40e0f15fe01e126fb800", "class_name": "RelatedNodeInfo"}}, "text": "758 Natural Language Processing: Applications\n245'positive '\nd2l.predict_sentiment(net, vocab, 'this movie is so bad ')\n'negative '\n16.3.4Summary\n\u000fOne-dimensional CNNs can process local features such as \ud835\udc5b-grams in text.\n\u000fMulti-input-channel one-dimensional cross-correlations are equivalent to single-input-\nchannel two-dimensional cross-correlations.\n\u000fThe max-over-time pooling allows different numbers of time steps at different channels.\n\u000fThetextCNNmodeltransformsindividualtokenrepresentationsintodownstreamappli-\ncationoutputsusingone-dimensionalconvolutionallayersandmax-over-timepooling\nlayers.\n16.3.5Exercises\n1.Tune hyperparameters and compare the two architectures for sentiment analysis in Sec-\ntion 16.2 and in this section, such as in classification accuracy and computational effi-\nciency.\n2.Can you further improve the classification accuracy of the model by using the methods\nintroduced in the exercises of Section 16.2 ?\n3.Add positional encoding in the input representations. Does it improve the classification\naccuracy?\nDiscussions245.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1054, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f513bbed-6278-49a2-a71a-24b3505019ea": {"__data__": {"id_": "f513bbed-6278-49a2-a71a-24b3505019ea", "embedding": null, "metadata": {"page_label": "759", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7839cb72-b817-44da-b6b0-15003fd7aead", "node_type": "4", "metadata": {"page_label": "759", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fef3fee44136ed6deb083824a9b793cb70a3715ef75d96af388bfb88ee57442e", "class_name": "RelatedNodeInfo"}}, "text": "759 Natural Language Inference and the Dataset\n16.4Natural LanguageInferenceand the Dataset\nInSection 16.1 , we discussed the problem of sentiment analysis. This task aims to clas-\nsify a single text sequence into predefined categories, such as a set of sentiment polarities.\nHowever, when there is a need to decide whether one sentence can be inferred form an-\nother, or eliminate redundancy by identifying sentences that are semantically equivalent,\nknowing how to classify one text sequence is insufficient. Instead, we need to be able to\nreason over pairs of text sequences.\n16.4.1NaturalLanguageInference\nNatural language inference studies whether a hypothesis can be inferred from a premise,\nwhere both are a text sequence. In other words, natural language inference determines the\nlogical relationship between a pair of text sequences. Such relationships usually fall into\nthree types:\n\u000fEntailment : the hypothesis can be inferred from the premise.\n\u000fContradiction : the negation of the hypothesis can be inferred from the premise.\n\u000fNeutral: all the other cases.\nNatural language inference is also known as the recognizing textual entailment task. For\nexample, the following pair will be labeled as entailment because \u201cshowing affection\u201d in\nthe hypothesis can be inferred from \u201chugging one another\u201d in the premise.\nPremise: Two women are hugging each other.\nHypothesis: Two women are showing affection.\nThe following is an example of contradiction as \u201crunning the coding example\u201d indicates\n\u201cnot sleeping\u201d rather than \u201csleeping\u201d.\nPremise: A man is running the coding example from Dive into Deep Learning.\nHypothesis: The man is sleeping.\nThe third example shows a neutrality relationship because neither \u201cfamous\u201d nor \u201cnot fa-\nmous\u201d can be inferred from the fact that \u201care performing for us\u201d.\nPremise: The musicians are performing for us.\nHypothesis: The musicians are famous.\nNatural language inference has been a central topic for understanding natural language.\nIt enjoys wide applications ranging from information retrieval to open-domain question\nanswering. Tostudythisproblem,wewillbeginbyinvestigatingapopularnaturallanguage\ninference benchmark dataset.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b7ff5ff-9e1a-47bf-9e8a-9971c362adfa": {"__data__": {"id_": "6b7ff5ff-9e1a-47bf-9e8a-9971c362adfa", "embedding": null, "metadata": {"page_label": "760", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5db7d381-2daa-4348-ab88-dd1911f16400", "node_type": "4", "metadata": {"page_label": "760", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6d3ba003041d9ae8d346862ce0c1d43571c205f1785ba99ab1901131705947fb", "class_name": "RelatedNodeInfo"}}, "text": "760 Natural Language Processing: Applications\n16.4.2The StanfordNaturalLanguageInference(SNLI) Dataset\nStanfordNaturalLanguageInference(SNLI)Corpusisacollectionofover500000labeled\nEnglish sentence pairs ( Bowman etal., 2015). We download and store the extracted SNLI\ndataset in the path ../data/snli_1.0 .\nimport os\nimport re\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n#@save\nd2l.DATA_HUB[ 'SNLI ']=(\n'https://nlp.stanford.edu/projects/snli/snli_1.0.zip ',\n'9fcde07509c7e87ec61c640c1b2753d9041758e4 ')\ndata_dir =d2l.download_extract( 'SNLI ')\nReadingthe Dataset\nThe original SNLI dataset contains much richer information than what we really need in\nour experiments. Thus, we define a function read_snli to only extract part of the dataset,\nthen return lists of premises, hypotheses, and their labels.\n#@save\ndef read_snli (data_dir, is_train):\n\"\"\"Read the SNLI dataset into premises, hypotheses, and labels.\"\"\"\ndef extract_text (s):\n# Remove information that will not be used by us\ns=re.sub( '\\\\(','', s)\ns=re.sub( '\\\\)','', s)\n# Substitute two or more consecutive whitespace with space\ns=re.sub( '\\\\s{2,}','', s)\nreturn s.strip()\nlabel_set ={'entailment ':0,'contradiction ':1,'neutral ':2}\nfile_name =os.path .join(data_dir, 'snli_1.0_train.txt '\nifis_train else 'snli_1.0_test.txt ')\nwith open (file_name, 'r')asf:\nrows =[row .split( '\\t')for row inf.readlines()[ 1:]]\npremises =[extract_text(row[ 1])for row inrows ifrow[ 0]inlabel_set]\nhypotheses =[extract_text(row[ 2])for row inrows ifrow[ 0]inlabel_set]\nlabels =[label_set[row[ 0]]for row inrows ifrow[ 0]inlabel_set]\nreturn premises, hypotheses, labels\nNow let\u2019s print the first 3 pairs of premise and hypothesis, as well as their labels (\u201c0\u201d, \u201c1\u201d,\nand \u201c2\u201d correspond to \u201centailment\u201d, \u201ccontradiction\u201d, and \u201cneutral\u201d, respectively ).\ntrain_data =read_snli(data_dir, is_train =True )\nfor x0, x1, y inzip(train_data[ 0][:3], train_data[ 1][:3], train_data[ 2][:3]):\nprint ('premise: ', x0)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "412732c4-5b34-424a-9ded-76a11e37b52d": {"__data__": {"id_": "412732c4-5b34-424a-9ded-76a11e37b52d", "embedding": null, "metadata": {"page_label": "761", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c031c70d-2e20-48ef-bbf2-03e0041c4461", "node_type": "4", "metadata": {"page_label": "761", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "abc5cf4ebe9dd8c4a569cb52eaf26f2558e31f9b905ab9332a7382566a569b1b", "class_name": "RelatedNodeInfo"}}, "text": "761 Natural Language Inference and the Dataset\n(continued from previous page)\nprint ('hypothesis: ', x1)\nprint ('label: ', y)\npremise: A person on a horse jumps over a broken down airplane .\nhypothesis: A person istraining his horse for a competition .\nlabel: 2\npremise: A person on a horse jumps over a broken down airplane .\nhypothesis: A person isat a diner , ordering an omelette .\nlabel: 1\npremise: A person on a horse jumps over a broken down airplane .\nhypothesis: A person isoutdoors , on a horse .\nlabel: 0\nThe training set has about 550000 pairs, and the testing set has about 10000 pairs. The fol-\nlowingshowsthatthethreelabels\u201centailment\u201d,\u201ccontradiction\u201d,and\u201cneutral\u201darebalanced\nin both the training set and the testing set.\ntest_data =read_snli(data_dir, is_train =False )\nfor data in[train_data, test_data]:\nprint ([[row for row indata[ 2]].count(i) for iinrange (3)])\n[183416 ,183187 ,182764 ]\n[3368 ,3237 ,3219 ]\nDefininga Class forLoading the Dataset\nBelow we define a class for loading the SNLI dataset by inheriting from the Dataset class\nin Gluon. The argument num_steps in the class constructor specifies the length of a text\nsequence so that each minibatch of sequences will have the same shape. In other words,\ntokens after the first num_steps ones in longer sequence are trimmed, while special tokens\n\u201c<pad>\u201d will be appended to shorter sequences until their length becomes num_steps . By\nimplementingthe __getitem__ function,wecanarbitrarilyaccessthepremise,hypothesis,\nand label with the index idx.\n#@save\nclass SNLIDataset (torch .utils .data .Dataset):\n\"\"\"A customized dataset to load the SNLI dataset.\"\"\"\ndef __init__ (self , dataset, num_steps, vocab =None ):\nself .num_steps =num_steps\nall_premise_tokens =d2l.tokenize(dataset[ 0])\nall_hypothesis_tokens =d2l.tokenize(dataset[ 1])\nifvocab isNone :\nself .vocab =d2l.Vocab(all_premise_tokens +all_hypothesis_tokens,\nmin_freq =5, reserved_tokens =['<pad> '])\nelse :\nself .vocab =vocab\nself .premises =self ._pad(all_premise_tokens)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2034, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b71f7195-5add-455f-b868-adf835bbd3ad": {"__data__": {"id_": "b71f7195-5add-455f-b868-adf835bbd3ad", "embedding": null, "metadata": {"page_label": "762", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "03ba42ed-82a1-4213-b845-2f336a9ec302", "node_type": "4", "metadata": {"page_label": "762", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "35540881d7ea99313b5f70894b4ebbfdc9eaf200933e8e886096b2001e7bf4ad", "class_name": "RelatedNodeInfo"}}, "text": "762 Natural Language Processing: Applications\n(continued from previous page)\nself .hypotheses =self ._pad(all_hypothesis_tokens)\nself .labels =torch .tensor(dataset[ 2])\nprint ('read '+str(len(self .premises)) +'examples ')\ndef _pad (self , lines):\nreturn torch .tensor([d2l .truncate_pad(\nself .vocab[line], self .num_steps, self .vocab[ '<pad> '])\nfor line inlines])\ndef __getitem__ (self , idx):\nreturn (self .premises[idx], self .hypotheses[idx]), self .labels[idx]\ndef __len__ (self ):\nreturn len(self .premises)\nPuttingIt All Together\nNow we can invoke the read_snli function and the SNLIDataset class to download the\nSNLI dataset and return DataLoader instances for both training and testing sets, together\nwith the vocabulary of the training set. It is noteworthy that we must use the vocabulary\nconstructed from the training set as that of the testing set. As a result, any new token from\nthe testing set will be unknown to the model trained on the training set.\n#@save\ndef load_data_snli (batch_size, num_steps =50):\n\"\"\"Download the SNLI dataset and return data iterators and vocabulary.\"\"\"\nnum_workers =d2l.get_dataloader_workers()\ndata_dir =d2l.download_extract( 'SNLI ')\ntrain_data =read_snli(data_dir, True )\ntest_data =read_snli(data_dir, False )\ntrain_set =SNLIDataset(train_data, num_steps)\ntest_set =SNLIDataset(test_data, num_steps, train_set .vocab)\ntrain_iter =torch .utils .data .DataLoader(train_set, batch_size,\nshuffle =True ,\nnum_workers =num_workers)\ntest_iter =torch .utils .data .DataLoader(test_set, batch_size,\nshuffle =False ,\nnum_workers =num_workers)\nreturn train_iter, test_iter, train_set .vocab\nHerewesetthebatchsizeto128andsequencelengthto50,andinvokethe load_data_snli\nfunctiontogetthedataiteratorsandvocabulary. Thenweprintthevocabularysize.\ntrain_iter, test_iter, vocab =load_data_snli( 128,50)\nlen(vocab)\nread 549367 examples\nread 9824 examples", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1886, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee64bb12-3e52-413b-a725-f2056f91b46b": {"__data__": {"id_": "ee64bb12-3e52-413b-a725-f2056f91b46b", "embedding": null, "metadata": {"page_label": "763", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51c66e7f-b309-4cfb-9c21-3335b3889c8c", "node_type": "4", "metadata": {"page_label": "763", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c37a7274b6068689c784674827d88fe6da1334e90c3b8ec5e860dee32f0f9934", "class_name": "RelatedNodeInfo"}}, "text": "763 Natural Language Inference: Using Attention\n24618678\nNow we print the shape of the first minibatch. Contrary to sentiment analysis, we have two\ninputs X[0]andX[1]representing pairs of premises and hypotheses.\nfor X, Y intrain_iter:\nprint (X[0].shape)\nprint (X[1].shape)\nprint (Y.shape)\nbreak\ntorch .Size([ 128,50])\ntorch .Size([ 128,50])\ntorch .Size([ 128])\n16.4.3Summary\n\u000fNatural language inference studies whether a hypothesis can be inferred from a premise,\nwhere both are a text sequence.\n\u000fIn natural language inference, relationships between premises and hypotheses include\nentailment, contradiction, and neutral.\n\u000fStanford Natural Language Inference (SNLI) Corpus is a popular benchmark dataset of\nnatural language inference.\n16.4.4Exercises\n1.Machine translation has long been evaluated based on superficial \ud835\udc5b-gram matching be-\ntween an output translation and a ground-truth translation. Can you design a measure\nfor evaluating machine translation results by using natural language inference?\n2.How can we change hyperparameters to reduce the vocabulary size?\nDiscussions246.\n16.5NaturalLanguageInference: Using Attention\nWeintroducedthenaturallanguageinferencetaskandtheSNLIdatasetin Section16.4 . In\nviewofmanymodelsthatarebasedoncomplexanddeeparchitectures,Parikh etal.(2016)\nproposed to address natural language inference with attention mechanisms and called it a\n\u201cdecomposableattentionmodel\u201d. Thisresultsinamodelwithoutrecurrentorconvolutional\nlayers,achievingthebestresultatthetimeontheSNLIdatasetwithmuchfewerparameters.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "64a9b03a-71c8-410e-a854-4e2d32ab2337": {"__data__": {"id_": "64a9b03a-71c8-410e-a854-4e2d32ab2337", "embedding": null, "metadata": {"page_label": "764", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "588f1e82-afec-4918-b42a-2ff948c3cc10", "node_type": "4", "metadata": {"page_label": "764", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "70c6d4b544f2b5765829e8480d7a34afbb1ca5de0ee9843e889130d0756b0ded", "class_name": "RelatedNodeInfo"}}, "text": "764 Natural Language Processing: Applications\nIn this section, we will describe and implement this attention-based method (with MLPs)\nfor natural language inference, as depicted in Fig. 16.5.1 .\ntFig. 16.5.1 This section feeds pretrained GloVe to an architecture based on attention and MLPs for\nnatural language inference.\n16.5.1TheModel\nSimpler than preserving the order of tokens in premises and hypotheses, we can just align\ntokens in one text sequence to every token in the other, and vice versa, then compare and\naggregate such information to predict the logical relationships between premises and hy-\npotheses. Similar to alignment of tokens between source and target sentences in machine\ntranslation,thealignmentoftokensbetweenpremisesandhypothesescanbeneatlyaccom-\nplished by attention mechanisms.\ntFig. 16.5.2 Natural language inference using attention mechanisms.\nFig.16.5.2 depictsthenaturallanguageinferencemethodusingattentionmechanisms. Ata\nhigh level, it consists of three jointly trained steps: attending, comparing, and aggregating.\nWe will illustrate them step by step in the following.\nimport torch\nfrom torch import nn\nfrom torch .nnimport functional asF\nfrom d2l import torch asd2l", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1202, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e391b914-9ede-4c6d-84e5-bcf382a9c8d7": {"__data__": {"id_": "e391b914-9ede-4c6d-84e5-bcf382a9c8d7", "embedding": null, "metadata": {"page_label": "765", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8872df19-acec-436c-b456-b8d0c5d4b250", "node_type": "4", "metadata": {"page_label": "765", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "89d307586269d866c626b07876fa4e62b157f401114872ef33b6dae2fac17fd7", "class_name": "RelatedNodeInfo"}}, "text": "765 Natural Language Inference: Using Attention\nAttending\nThe first step is to align tokens in one text sequence to each token in the other sequence.\nSuppose that the premise is \u201ci do need sleep\u201d and the hypothesis is \u201ci am tired\u201d. Due to\nsemantical similarity, we may wish to align \u201ci\u201d in the hypothesis with \u201ci\u201d in the premise,\nand align \u201ctired\u201d in the hypothesis with \u201csleep\u201d in the premise. Likewise, we may wish\nto align \u201ci\u201d in the premise with \u201ci\u201d in the hypothesis, and align \u201cneed\u201d and \u201csleep\u201d in the\npremise with \u201ctired\u201d in the hypothesis. Note that such alignment is softusing weighted\naverage, where ideally large weights are associated with the tokens to be aligned. For ease\nof demonstration, Fig. 16.5.2 shows such alignment in a hardway.\nNow we describe the soft alignment using attention mechanisms in more detail. Denote\nbyA=\u00b9a1,...,a\ud835\udc5a\u00baandB=\u00b9b1,...,b\ud835\udc5b\u00bathe premise and hypothesis, whose number\nof tokens are \ud835\udc5aand\ud835\udc5b, respectively, where a\ud835\udc56,b\ud835\udc572R\ud835\udc51(\ud835\udc56=1,...,\ud835\udc5a,\ud835\udc57 =1,...,\ud835\udc5b) is a\n\ud835\udc51-dimensional word vector. For soft alignment, we compute the attention weights \ud835\udc52\ud835\udc56\ud835\udc572R\nas\n\ud835\udc52\ud835\udc56\ud835\udc57=\ud835\udc53\u00b9a\ud835\udc56\u00ba>\ud835\udc53\u00b9b\ud835\udc57\u00ba, (16.5.1)\nwhere the function \ud835\udc53is an MLP defined in the following mlpfunction. The output dimen-\nsion of\ud835\udc53is specified by the num_hiddens argument of mlp.\ndef mlp(num_inputs, num_hiddens, flatten):\nnet =[]\nnet.append(nn .Dropout( 0.2))\nnet.append(nn .Linear(num_inputs, num_hiddens))\nnet.append(nn .ReLU())\nifflatten:\nnet.append(nn .Flatten(start_dim =1))\nnet.append(nn .Dropout( 0.2))\nnet.append(nn .Linear(num_hiddens, num_hiddens))\nnet.append(nn .ReLU())\nifflatten:\nnet.append(nn .Flatten(start_dim =1))\nreturn nn.Sequential( *net)\nIt should be highlighted that, in (16.5.1 )\ud835\udc53takes inputs a\ud835\udc56andb\ud835\udc57separately rather than\ntakesapairofthemtogetherasinput. This decomposition trickleadstoonly \ud835\udc5a\u00b8\ud835\udc5bapplica-\ntions (linear complexity) of \ud835\udc53rather than\ud835\udc5a\ud835\udc5bapplications (quadratic complexity).\nNormalizing the attention weights in (16.5.1 ), we compute the weighted average of all\nthe token vectors in the hypothesis to obtain representation of the hypothesis that is softly\naligned with the token indexed by \ud835\udc56in the premise:\n\ud835\udf37\ud835\udc56=\ud835\udc5b\u00d5\n\ud835\udc57=1exp\u00b9\ud835\udc52\ud835\udc56\ud835\udc57\u00ba\u00cd\ud835\udc5b\n\ud835\udc58=1exp\u00b9\ud835\udc52\ud835\udc56\ud835\udc58\u00bab\ud835\udc57. (16.5.2)\nLikewise, we compute soft alignment of premise tokens for each token indexed by \ud835\udc57in the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2236, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7445ab1-1718-4f75-b8ea-3ae091754f5c": {"__data__": {"id_": "d7445ab1-1718-4f75-b8ea-3ae091754f5c", "embedding": null, "metadata": {"page_label": "766", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27cd9a90-cb1d-4ee4-9bb4-ef3c74eacb17", "node_type": "4", "metadata": {"page_label": "766", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "326bf693c8ad9d0f86c6d7fe5a5f5eaf1981d7147a26e9b05c914544395dfd50", "class_name": "RelatedNodeInfo"}}, "text": "766 Natural Language Processing: Applications\nhypothesis:\n\ud835\udf36\ud835\udc57=\ud835\udc5a\u00d5\n\ud835\udc56=1exp\u00b9\ud835\udc52\ud835\udc56\ud835\udc57\u00ba\u00cd\ud835\udc5a\n\ud835\udc58=1exp\u00b9\ud835\udc52\ud835\udc58\ud835\udc57\u00baa\ud835\udc56. (16.5.3)\nBelowwedefinethe Attendclasstocomputethesoftalignmentofhypotheses( beta)with\ninput premises Aand soft alignment of premises ( alpha) with input hypotheses B.\nclass Attend (nn.Module):\ndef __init__ (self , num_inputs, num_hiddens, **kwargs):\nsuper (Attend, self ).__init__ (**kwargs)\nself .f=mlp(num_inputs, num_hiddens, flatten =False )\ndef forward (self , A, B):\n# Shape of `A`/`B`: (`batch_size`, no. of tokens in sequence A/B,\n# `embed_size`)\n# Shape of `f_A`/`f_B`: (`batch_size`, no. of tokens in sequence A/B,\n# `num_hiddens`)\nf_A =self .f(A)\nf_B =self .f(B)\n# Shape of `e`: (`batch_size`, no. of tokens in sequence A,\n# no. of tokens in sequence B)\ne=torch .bmm(f_A, f_B .permute( 0,2,1))\n# Shape of `beta`: (`batch_size`, no. of tokens in sequence A,\n# `embed_size`), where sequence B is softly aligned with each token\n# (axis 1 of `beta`) in sequence A\nbeta =torch .bmm(F .softmax(e, dim =-1), B)\n# Shape of `alpha`: (`batch_size`, no. of tokens in sequence B,\n# `embed_size`), where sequence A is softly aligned with each token\n# (axis 1 of `alpha`) in sequence B\nalpha =torch .bmm(F .softmax(e .permute( 0,2,1), dim =-1), A)\nreturn beta, alpha\nComparing\nIn the next step, we compare a token in one sequence with the other sequence that is softly\naligned with that token. Note that in soft alignment, all the tokens from one sequence,\nthough with probably different attention weights, will be compared with a token in the\nother sequence. For easy of demonstration, Fig. 16.5.2 pairs tokens with aligned tokens\nin ahardway. For example, suppose that the attending step determines that \u201cneed\u201d and\n\u201csleep\u201d in the premise are both aligned with \u201ctired\u201d in the hypothesis, the pair \u201ctired\u2013need\nsleep\u201d will be compared.\nIn the comparing step, we feed the concatenation (operator \u00bb\u0001,\u0001\u00bc) of tokens from one se-\nquence and aligned tokens from the other sequence into a function \ud835\udc54(an MLP):\nv\ud835\udc34,\ud835\udc56=\ud835\udc54\u00b9\u00bba\ud835\udc56,\ud835\udf37\ud835\udc56\u00bc\u00ba,\ud835\udc56=1,...,\ud835\udc5a\nv\ud835\udc35,\ud835\udc57=\ud835\udc54\u00b9\u00bbb\ud835\udc57,\ud835\udf36\ud835\udc57\u00bc\u00ba,\ud835\udc57=1,...,\ud835\udc5b.(16.5.4)\nIn(16.5.4 ),v\ud835\udc34,\ud835\udc56is the comparison between token \ud835\udc56in the premise and all the hypothesis\ntokensthataresoftlyalignedwithtoken \ud835\udc56; while v\ud835\udc35,\ud835\udc57isthecomparisonbetweentoken \ud835\udc57in", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2219, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e509daa5-5acc-492d-a748-12d3957141f3": {"__data__": {"id_": "e509daa5-5acc-492d-a748-12d3957141f3", "embedding": null, "metadata": {"page_label": "767", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce9258da-39c2-4b61-83f2-89cc5f8ca5e9", "node_type": "4", "metadata": {"page_label": "767", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dc651d04d586bac40135ef84a002ed802163e6d35371fd163dd923cacc5cbeb5", "class_name": "RelatedNodeInfo"}}, "text": "767 Natural Language Inference: Using Attention\nthehypothesisandallthepremisetokensthataresoftlyalignedwithtoken \ud835\udc57. Thefollowing\nCompare class defines such as comparing step.\nclass Compare (nn.Module):\ndef __init__ (self , num_inputs, num_hiddens, **kwargs):\nsuper (Compare, self ).__init__ (**kwargs)\nself .g=mlp(num_inputs, num_hiddens, flatten =False )\ndef forward (self , A, B, beta, alpha):\nV_A =self .g(torch .cat([A, beta], dim =2))\nV_B =self .g(torch .cat([B, alpha], dim =2))\nreturn V_A, V_B\nAggregating\nWith two sets of comparison vectors v\ud835\udc34,\ud835\udc56(\ud835\udc56=1,...,\ud835\udc5a) andv\ud835\udc35,\ud835\udc57(\ud835\udc57=1,...,\ud835\udc5b) on hand,\nin the last step we will aggregate such information to infer the logical relationship. We\nbegin by summing up both sets:\nv\ud835\udc34=\ud835\udc5a\u00d5\n\ud835\udc56=1v\ud835\udc34,\ud835\udc56,v\ud835\udc35=\ud835\udc5b\u00d5\n\ud835\udc57=1v\ud835\udc35,\ud835\udc57. (16.5.5)\nNext we feed the concatenation of both summarization results into function \u210e(an MLP) to\nobtain the classification result of the logical relationship:\n\u02c6y=\u210e\u00b9\u00bbv\ud835\udc34,v\ud835\udc35\u00bc\u00ba. (16.5.6)\nThe aggregation step is defined in the following Aggregate class.\nclass Aggregate (nn.Module):\ndef __init__ (self , num_inputs, num_hiddens, num_outputs, **kwargs):\nsuper (Aggregate, self ).__init__ (**kwargs)\nself .h=mlp(num_inputs, num_hiddens, flatten =True )\nself .linear =nn.Linear(num_hiddens, num_outputs)\ndef forward (self , V_A, V_B):\n# Sum up both sets of comparison vectors\nV_A =V_A.sum(dim =1)\nV_B =V_B.sum(dim =1)\n# Feed the concatenation of both summarization results into an MLP\nY_hat =self .linear( self .h(torch .cat([V_A, V_B], dim =1)))\nreturn Y_hat\nPuttingIt All Together\nBy putting the attending, comparing, and aggregating steps together, we define the decom-\nposable attention model to jointly train these three steps.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1667, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1128b08-8bd9-4477-a44c-a0d0c7e67572": {"__data__": {"id_": "b1128b08-8bd9-4477-a44c-a0d0c7e67572", "embedding": null, "metadata": {"page_label": "768", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e309113d-9eb5-4b40-8f10-c43fe9c54ef2", "node_type": "4", "metadata": {"page_label": "768", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "196707a05643236700bf188754858d333da6ee290d667d1fdeb8a4c786230fa5", "class_name": "RelatedNodeInfo"}}, "text": "768 Natural Language Processing: Applications\nclass DecomposableAttention (nn.Module):\ndef __init__ (self , vocab, embed_size, num_hiddens, num_inputs_attend =100,\nnum_inputs_compare =200, num_inputs_agg =400,**kwargs):\nsuper (DecomposableAttention, self ).__init__ (**kwargs)\nself .embedding =nn.Embedding( len(vocab), embed_size)\nself .attend =Attend(num_inputs_attend, num_hiddens)\nself .compare =Compare(num_inputs_compare, num_hiddens)\n# There are 3 possible outputs: entailment, contradiction, and neutral\nself .aggregate =Aggregate(num_inputs_agg, num_hiddens, num_outputs =3)\ndef forward (self , X):\npremises, hypotheses =X\nA=self .embedding(premises)\nB=self .embedding(hypotheses)\nbeta, alpha =self .attend(A, B)\nV_A, V_B =self .compare(A, B, beta, alpha)\nY_hat =self .aggregate(V_A, V_B)\nreturn Y_hat\n16.5.2Trainingand Evaluatingthe Model\nNow we will train and evaluate the defined decomposable attention model on the SNLI\ndataset. We begin by reading the dataset.\nReadingthe dataset\nWe download and read the SNLI dataset using the function defined in Section 16.4 . The\nbatch size and sequence length are set to 256and50, respectively.\nbatch_size, num_steps =256,50\ntrain_iter, test_iter, vocab =d2l.load_data_snli(batch_size, num_steps)\nDownloading ../data /snli_1 .0.zip from https ://nlp.stanford .edu/projects /snli /\n\u21a9!snli_1 .0.zip...\nread 549367 examples\nread 9824 examples\nCreatingthe Model\nWe use the pretrained 100-dimensional GloVe embedding to represent the input tokens.\nThus, we predefine the dimension of vectors a\ud835\udc56andb\ud835\udc57in(16.5.1 )as 100. The output\ndimension of functions \ud835\udc53in(16.5.1 )and\ud835\udc54in(16.5.4 )is set to 200. Then we create a\nmodelinstance,initializeitsparameters,andloadtheGloVeembeddingtoinitializevectors\nof input tokens.\nembed_size, num_hiddens, devices =100,200, d2l .try_all_gpus()\nnet =DecomposableAttention(vocab, embed_size, num_hiddens)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54a5e2a2-ba94-47fb-9b3b-e9ce63feb942": {"__data__": {"id_": "54a5e2a2-ba94-47fb-9b3b-e9ce63feb942", "embedding": null, "metadata": {"page_label": "769", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "602464f7-80eb-4e19-8327-f59b328d6d78", "node_type": "4", "metadata": {"page_label": "769", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d80fbcb8bbe4ef4434bed6a999d73df9f677cba85db53646a07ce61e9adc91dd", "class_name": "RelatedNodeInfo"}}, "text": "769 Natural Language Inference: Using Attention\n(continued from previous page)\nglove_embedding =d2l.TokenEmbedding( 'glove.6b.100d ')\nembeds =glove_embedding[vocab .idx_to_token]\nnet.embedding .weight .data .copy_(embeds);\nDownloading ../data /glove .6B.100 d.zip from http ://d2l-data .s3-accelerate .\n\u21a9!amazonaws .com/glove .6B.100 d.zip...\nTrainingand Evaluatingthe Model\nIncontrasttothe split_batch functionin Section13.5 thattakessingleinputssuchastext\nsequences(orimages), wedefinea split_batch_multi_inputs functiontotakemultiple\ninputs such as premises and hypotheses in minibatches.\nNow we can train and evaluate the model on the SNLI dataset.\nlr, num_epochs =0.001 ,4\ntrainer =torch .optim .Adam(net .parameters(), lr =lr)\nloss =nn.CrossEntropyLoss(reduction =\"none \")\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.496 , train acc 0.805 , test acc 0.828\n20383.2 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]\nUsing the Model\nFinally, define the prediction function to output the logical relationship between a pair of\npremise and hypothesis.\n#@save\ndef predict_snli (net, vocab, premise, hypothesis):\n\"\"\"Predict the logical relationship between the premise and hypothesis.\"\"\"\nnet.eval()\npremise =torch .tensor(vocab[premise], device =d2l.try_gpu())\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1365, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c9f43c1-9cd6-4e28-992a-8cbc3180f8d1": {"__data__": {"id_": "9c9f43c1-9cd6-4e28-992a-8cbc3180f8d1", "embedding": null, "metadata": {"page_label": "770", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "991697e3-7fe4-4267-9fa4-feab4a729ea9", "node_type": "4", "metadata": {"page_label": "770", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "15d063f1f424cc96d9da7b0ca07080e4de7865a9d519edf45297eefd4bc7c298", "class_name": "RelatedNodeInfo"}}, "text": "770 Natural Language Processing: Applications\n247(continued from previous page)\nhypothesis =torch .tensor(vocab[hypothesis], device =d2l.try_gpu())\nlabel =torch .argmax(net([premise .reshape(( 1,-1)),\nhypothesis .reshape(( 1,-1))]), dim =1)\nreturn 'entailment 'iflabel ==0else 'contradiction 'iflabel ==1\\\nelse 'neutral '\nWe can use the trained model to obtain the natural language inference result for a sample\npair of sentences.\npredict_snli(net, vocab, [ 'he','is','good ','.'], [ 'he','is','bad','.'])\n'contradiction '\n16.5.3Summary\n\u000fThe decomposable attention model consists of three steps for predicting the logical rela-\ntionships between premises and hypotheses: attending, comparing, and aggregating.\n\u000fWith attention mechanisms, we can align tokens in one text sequence to every token in\ntheother,andviceversa. Suchalignmentissoftusingweightedaverage,whereideally\nlarge weights are associated with the tokens to be aligned.\n\u000fThedecompositiontrickleadstoamoredesirablelinearcomplexitythanquadraticcom-\nplexity when computing attention weights.\n\u000fWe can use pretrained word vectors as the input representation for downstream natural\nlanguage processing task such as natural language inference.\n16.5.4Exercises\n1.Train the model with other combinations of hyperparameters. Can you get better accu-\nracy on the test set?\n2.What are major drawbacks of the decomposable attention model for natural language\ninference?\n3.Suppose that we want to get the level of semantical similarity (e.g., a continuous value\nbetween 0 and 1) for any pair of sentences. How shall we collect and label the dataset?\nCan you design a model with attention mechanisms?\nDiscussions247.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7fad0b9-2844-4cb9-bd52-c5d9f147c21a": {"__data__": {"id_": "f7fad0b9-2844-4cb9-bd52-c5d9f147c21a", "embedding": null, "metadata": {"page_label": "771", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecff123b-7552-4248-93e0-8c289547f96d", "node_type": "4", "metadata": {"page_label": "771", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "53113eec8113ff2d1582c851b5d7579711705bb7c5304c8f8c7d329ea9c1c8f6", "class_name": "RelatedNodeInfo"}}, "text": "771 Fine-Tuning BERT for Sequence-Level and Token-Level Applications\n16.6Fine-TuningBERTforSequence-Leveland\nToken-LevelApplications\nIn the previous sections of this chapter, we have designed different models for natural lan-\nguageprocessingapplications,suchasbasedonRNNs,CNNs,attention,andMLPs. These\nmodelsarehelpfulwhenthereisspaceortimeconstraint,however,craftingaspecificmodel\nfor every natural language processing task is practically infeasible. In Section 15.8 , we in-\ntroduced a pretraining model, BERT, that requires minimal architecture changes fora wide\nrange of natural language processing tasks. On the one hand, at the time of its proposal,\nBERT improved the state of the art on various natural language processing tasks. On the\nother hand, as noted in Section 15.10 , the two versions of the original BERT model come\nwith110millionand340millionparameters. Thus,whentherearesufficientcomputational\nresources,wemayconsiderfine-tuningBERTfordownstreamnaturallanguageprocessing\napplications.\nIn the following, we generalize a subset of natural language processing applications as\nsequence-level and token-level. On the sequence level, we introduce how to transform the\nBERT representation of the text input to the output label in single text classification and\ntext pair classification or regression. On the token level, we will briefly introduce new ap-\nplications such as text tagging and question answering and shed light on how BERT can\nrepresenttheirinputsandgettransformedintooutputlabels. Duringfine-tuning,the\u201cmini-\nmalarchitecturechanges\u201drequiredbyBERTacrossdifferentapplicationsaretheextrafully\nconnected layers. During supervised learning of a downstream application, parameters of\nthe extra layers are learned from scratch while all the parameters in the pretrained BERT\nmodel are fine-tuned.\n16.6.1SingleTextClassification\nSingle text classification takes a single text sequence as input and outputs its classification\nresult. Besides sentiment analysis that we have studied in this chapter, the Corpus of Lin-\nguisticAcceptability(CoLA)isalsoadatasetforsingletextclassification,judgingwhether\na given sentence is grammatically acceptable or not ( Warstadt et al., 2019). For instance,\n\u201cI should study.\u201d is acceptable but \u201cI should studying.\u201d is not.\nSection15.8 describestheinputrepresentationofBERT.TheBERTinputsequenceunam-\nbiguously represents both single text and text pairs, where the special classification token\n\u201c<cls>\u201d is used for sequence classification and the special classification token \u201c<sep>\u201d\nmarks the end of single text or separates a pair of text. As shown in Fig. 16.6.1 , in single\ntext classification applications, the BERT representation of the special classification token\n\u201c<cls>\u201d encodes the information of the entire input text sequence. As the representation of\nthe input single text, it will be fed into a small MLP consisting of fully connected (dense)\nlayers to output the distribution of all the discrete label values.\n16.6.2TextPairClassification or Regression", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3017, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14869d58-fc89-468b-ac00-95185073a0f9": {"__data__": {"id_": "14869d58-fc89-468b-ac00-95185073a0f9", "embedding": null, "metadata": {"page_label": "772", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aaa0a4d8-6001-4888-b687-1dd9816c32d6", "node_type": "4", "metadata": {"page_label": "772", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "44e0b50ec4fb8c317d37c392d7baa115e48183ecc246341d70f57ec6aa98a7d7", "class_name": "RelatedNodeInfo"}}, "text": "772 Natural Language Processing: Applications\ntFig. 16.6.1 Fine-tuning BERT for single text classi\ufb01cation applications, such as sentiment analysis\nand testing linguistic acceptability. Suppose that the input single text has six tokens.\nWe have also examined natural language inference in this chapter. It belongs to text pair\nclassification , a type of application classifying a pair of text.\nTaking a pair of text as input but outputting a continuous value, semantictextualsimilarity\nis a popular textpairregression task. This task measures semantic similarity of sentences.\nFor instance, in the Semantic Textual Similarity Benchmark dataset, the similarity score of\na pair of sentences is an ordinal scale ranging from 0 (no meaning overlap) to 5 (meaning\nequivalence) ( Ceret al., 2017). The goal is to predict these scores. Examples from the\nSemantic Textual Similarity Benchmark dataset include (sentence 1, sentence 2, similarity\nscore):\n\u000f\u201cA plane is taking off.\u201d, \u201cAn air plane is taking off.\u201d, 5.000;\n\u000f\u201cA woman is eating something.\u201d, \u201cA woman is eating meat.\u201d, 3.000;\n\u000f\u201cA woman is dancing.\u201d, \u201cA man is talking.\u201d, 0.000.\ntFig. 16.6.2 Fine-tuning BERT for text pair classi\ufb01cation or regression applications, such as natural\nlanguage inference and semantic textual similarity. Suppose that the input text pair has\ntwo and three tokens.\nComparing with single text classification in Fig. 16.6.1 , fine-tuning BERT for text pair\nclassification in Fig. 16.6.2 is different in the input representation. For text pair regression\ntasks such as semantic textual similarity, trivial changes can be applied such as outputting", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1620, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0dd6b08d-eb79-4b64-9372-652874309ea7": {"__data__": {"id_": "0dd6b08d-eb79-4b64-9372-652874309ea7", "embedding": null, "metadata": {"page_label": "773", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "90d22edc-2f3c-4d2d-8950-77d96e519f7a", "node_type": "4", "metadata": {"page_label": "773", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2bb10dc1f5398749342d34a450e2eced0d005a8788f4c37dd2c62bcadbfe3dbe", "class_name": "RelatedNodeInfo"}}, "text": "773 Fine-Tuning BERT for Sequence-Level and Token-Level Applications\na continuous label value and using the mean squared loss: they are common for regres-\nsion.\n16.6.3TextTagging\nNow let\u2019s consider token-level tasks, such as text tagging , where each token is assigned a\nlabel. Amongtexttaggingtasks, part-of-speechtagging assignseachwordapart-of-speech\ntag (e.g., adjective and determiner) according to the role of the word in the sentence. For\nexample, according to the Penn Treebank II tag set, the sentence \u201cJohn Smith \u2019s car is\nnew\u201d should be tagged as \u201cNNP (noun, proper singular) NNP POS (possessive ending)\nNN (noun, singular or mass) VB (verb, base form) JJ (adjective)\u201d.\ntFig. 16.6.3 Fine-tuning BERT for text tagging applications, such as part-of-speech tagging. Suppose\nthat the input single text has six tokens.\nFine-tuning BERT for text tagging applications is illustrated in Fig. 16.6.3 . Comparing\nwithFig. 16.6.1 , the only distinction lies in that in text tagging, the BERT representation\nofevery token of the input text is fed into the same extra fully connected layers to output\nthe label of the token, such as a part-of-speech tag.\n16.6.4QuestionAnswering\nAsanothertoken-levelapplication, questionanswering reflectscapabilitiesofreadingcom-\nprehension. Forexample,theStanfordQuestionAnsweringDataset(SQuADv1.1)consists\nof reading passages and questions, where the answer to every question is just a segment of\ntext (text span) from the passage that the question is about ( Rajpurkar et al., 2016). To\nexplain, consider a passage \u201cSome experts report that a mask\u2019s efficacy is inconclusive.\nHowever, mask makers insist that their products, such as N95 respirator masks, can guard\nagainst the virus.\u201d and a question \u201cWho say that N95 respirator masks can guard against\nthe virus?\u201d. The answer should be the text span \u201cmask makers\u201d in the passage. Thus, the\ngoal in SQuAD v1.1 is to predict the start and end of the text span in the passage given a\npair of question and passage.\nTo fine-tune BERT for question answering, the question and passage are packed as the first\nand second text sequence, respectively, in the input of BERT. To predict the position of the\nstart of the text span, the same additional fully connected layer will transform the BERT", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c190d23c-b1d0-451d-80b7-4d2573b2d88e": {"__data__": {"id_": "c190d23c-b1d0-451d-80b7-4d2573b2d88e", "embedding": null, "metadata": {"page_label": "774", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fbcd913-a890-4651-b833-e753bca1427b", "node_type": "4", "metadata": {"page_label": "774", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fe31bcaf7670cdfd4ed7855b4b5010d09de662ffd069fedf13b9272133071636", "class_name": "RelatedNodeInfo"}}, "text": "774 Natural Language Processing: Applications\ntFig. 16.6.4 Fine-tuning BERT for question answering. Suppose that the input text pair has two and\nthree tokens.\nrepresentationofanytokenfromthepassageofposition \ud835\udc56intoascalarscore \ud835\udc60\ud835\udc56. Suchscores\nofallthepassagetokensarefurthertransformedbythesoftmaxoperationintoaprobability\ndistribution,sothateachtokenposition \ud835\udc56inthepassageisassignedaprobability \ud835\udc5d\ud835\udc56ofbeing\nthestartofthetextspan. Predictingtheendofthetextspanisthesameasabove,exceptthat\nparametersinitsadditionalfullyconnectedlayerareindependentfromthoseforpredicting\nthe start. When predicting the end, any passage token of position \ud835\udc56is transformed by the\nsame fully connected layer into a scalar score \ud835\udc52\ud835\udc56.Fig. 16.6.4 depicts fine-tuning BERT for\nquestion answering.\nForquestionanswering,thesupervisedlearning\u2019strainingobjectiveisasstraightforwardas\nmaximizing the log-likelihoods of the ground-truth start and end positions. When predict-\ning the span, we can compute the score \ud835\udc60\ud835\udc56\u00b8\ud835\udc52\ud835\udc57for a valid span from position \ud835\udc56to position\n\ud835\udc57(\ud835\udc56\u0014\ud835\udc57), and output the span with the highest score.\n16.6.5Summary\n\u000fBERTrequiresminimalarchitecturechanges(extrafullyconnectedlayers)forsequence-\nlevelandtoken-levelnaturallanguageprocessingapplications,suchassingletextclas-\nsification(e.g.,sentimentanalysisandtestinglinguisticacceptability),textpairclassi-\nficationorregression(e.g.,naturallanguageinferenceandsemantictextualsimilarity),\ntext tagging (e.g., part-of-speech tagging), and question answering.\n\u000fDuring supervised learning of a downstream application, parameters of the extra layers\nare learned from scratch while all the parameters in the pretrained BERT model are\nfine-tuned.\n16.6.6Exercises\n1.Let\u2019s design a search engine algorithm for news articles. When the system receives an\nquery (e.g., \u201coil industry during the coronavirus outbreak\u201d), it should return a ranked\nlistofnewsarticlesthataremostrelevanttothequery. Supposethatwehaveahugepool\nof news articles and a large number of queries. To simplify the problem, suppose that\nthe most relevant article has been labeled for each query. How can we apply negative\nsampling (see Section 15.2.1 ) and BERT in the algorithm design?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2170, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01bd7402-96a4-4164-870b-5c9dd0f225ef": {"__data__": {"id_": "01bd7402-96a4-4164-870b-5c9dd0f225ef", "embedding": null, "metadata": {"page_label": "775", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "249417f9-7762-49e9-aec1-a4a04edb652b", "node_type": "4", "metadata": {"page_label": "775", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a16a43c2261b6c62858e01f9f6407640a7599a5ffef5f60a0db029e3a5d63940", "class_name": "RelatedNodeInfo"}}, "text": "775 Natural Language Inference: Fine-Tuning BERT\n2482.How can we leverage BERT in training language models?\n3.Can we leverage BERT in machine translation?\nDiscussions248.\n16.7NaturalLanguageInference: Fine-Tuning\nBERT\nIn earlier sections of this chapter, we have designed an attention-based architecture (in\nSection 16.5 ) for the natural language inference task on the SNLI dataset (as described\ninSection 16.4 ). Now we revisit this task by fine-tuning BERT. As discussed in Section\n16.6, natural language inference is a sequence-level text pair classification problem, and\nfine-tuningBERTonlyrequiresanadditionalMLP-basedarchitecture,asillustratedin Fig.\n16.7.1.\ntFig. 16.7.1 This section feeds pretrained BERT to an MLP-based architecture for natural language\ninference.\nIn this section, we will download a pretrained small version of BERT, then fine-tune it for\nnatural language inference on the SNLI dataset.\nimport json\nimport multiprocessing\nimport os\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n16.7.1LoadingPretrainedBERT\nWe have explained how to pretrain BERT on the WikiText-2 dataset in Section 15.9 and\nSection 15.10 (note that the original BERT model is pretrained on much bigger corpora).\nAsdiscussedin Section15.10 ,theoriginalBERTmodelhashundredsofmillionsofparam-\neters. In the following, we provide two versions of pretrained BERT: \u201cbert.base\u201d is about", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29fd5fde-b463-4c33-b288-1189ec085985": {"__data__": {"id_": "29fd5fde-b463-4c33-b288-1189ec085985", "embedding": null, "metadata": {"page_label": "776", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2ee32b91-20d2-46d9-b8ae-688f57246466", "node_type": "4", "metadata": {"page_label": "776", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "322cd297328a52a4f50d459e245bd0e3ba1143f7b013ac2c8a25b39998e2797f", "class_name": "RelatedNodeInfo"}}, "text": "776 Natural Language Processing: Applications\nas big as the original BERT base model that requires a lot of computational resources to\nfine-tune, while \u201cbert.small\u201d is a small version to facilitate demonstration.\nd2l.DATA_HUB[ 'bert.base ']=(d2l .DATA_URL +'bert.base.torch.zip ',\n'225d66f04cae318b841a13d32af3acc165f253ac ')\nd2l.DATA_HUB[ 'bert.small ']=(d2l .DATA_URL +'bert.small.torch.zip ',\n'c72329e68a732bef0452e4b96a1c341c8910f81f ')\nEither pretrained BERT model contains a \u201cvocab.json\u201d file that defines the vocabulary set\nand a \u201cpretrained.params\u201d file of the pretrained parameters. We implement the following\nload_pretrained_model function to load pretrained BERT parameters.\ndef load_pretrained_model (pretrained_model, num_hiddens, ffn_num_hiddens,\nnum_heads, num_blks, dropout, max_len, devices):\ndata_dir =d2l.download_extract(pretrained_model)\n# Define an empty vocabulary to load the predefined vocabulary\nvocab =d2l.Vocab()\nvocab .idx_to_token =json .load( open (os.path .join(data_dir, 'vocab.json ')))\nvocab .token_to_idx ={token: idx for idx, token inenumerate (\nvocab .idx_to_token)}\nbert =d2l.BERTModel(\nlen(vocab), num_hiddens, ffn_num_hiddens =ffn_num_hiddens, num_heads =4,\nnum_blks =2, dropout =0.2, max_len =max_len)\n# Load pretrained BERT parameters\nbert .load_state_dict(torch .load(os .path .join(data_dir,\n'pretrained.params ')))\nreturn bert, vocab\nTo facilitate demonstration on most of machines, we will load and fine-tune the small ver-\nsion (\u201cbert.small\u201d) of the pretrained BERT in this section. In the exercise, we will show\nhow to fine-tune the much larger \u201cbert.base\u201d to significantly improve the testing accu-\nracy.\ndevices =d2l.try_all_gpus()\nbert, vocab =load_pretrained_model(\n'bert.small ', num_hiddens =256, ffn_num_hiddens =512, num_heads =4,\nnum_blks =2, dropout =0.1, max_len =512, devices =devices)\nDownloading ../data /bert .small .torch .zip from http ://d2l-data .s3-accelerate .\n\u21a9!amazonaws .com/bert .small .torch .zip...\n16.7.2TheDatasetforFine-TuningBERT\nFor the downstream task natural language inference on the SNLI dataset, we define a cus-\ntomized dataset class SNLIBERTDataset . In each example, the premise and hypothesis\nform a pair of text sequence and is packed into one BERT input sequence as depicted in\nFig. 16.6.2 . Recall Section 15.8.4 that segment IDs are used to distinguish the premise\nand the hypothesis in a BERT input sequence. With the predefined maximum length of a\nBERT input sequence ( max_len ), the last token of the longer of the input text pair keeps", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2534, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2b4e785-9f94-403d-a3d8-01ec399ac9eb": {"__data__": {"id_": "a2b4e785-9f94-403d-a3d8-01ec399ac9eb", "embedding": null, "metadata": {"page_label": "777", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5557427-3952-46cd-9d61-f5600faad9a4", "node_type": "4", "metadata": {"page_label": "777", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9b77fbde06a9ccb7121f39135947653a8f3c5a53630fba597e9cfbfc196c8190", "class_name": "RelatedNodeInfo"}}, "text": "777 Natural Language Inference: Fine-Tuning BERT\ngetting removed until max_len is met. To accelerate generation of the SNLI dataset for\nfine-tuning BERT, we use 4 worker processes to generate training or testing examples in\nparallel.\nclass SNLIBERTDataset (torch .utils .data .Dataset):\ndef __init__ (self , dataset, max_len, vocab =None ):\nall_premise_hypothesis_tokens =[[\np_tokens, h_tokens] for p_tokens, h_tokens inzip(\n*[d2l .tokenize([s .lower() for sinsentences])\nfor sentences indataset[: 2]])]\nself .labels =torch .tensor(dataset[ 2])\nself .vocab =vocab\nself .max_len =max_len\n(self .all_token_ids, self .all_segments,\nself .valid_lens) =self ._preprocess(all_premise_hypothesis_tokens)\nprint ('read '+str(len(self .all_token_ids)) +'examples ')\ndef _preprocess (self , all_premise_hypothesis_tokens):\npool =multiprocessing .Pool( 4)# Use 4 worker processes\nout =pool .map( self ._mp_worker, all_premise_hypothesis_tokens)\nall_token_ids =[\ntoken_ids for token_ids, segments, valid_len inout]\nall_segments =[segments for token_ids, segments, valid_len inout]\nvalid_lens =[valid_len for token_ids, segments, valid_len inout]\nreturn (torch .tensor(all_token_ids, dtype =torch .long),\ntorch .tensor(all_segments, dtype =torch .long),\ntorch .tensor(valid_lens))\ndef _mp_worker (self , premise_hypothesis_tokens):\np_tokens, h_tokens =premise_hypothesis_tokens\nself ._truncate_pair_of_tokens(p_tokens, h_tokens)\ntokens, segments =d2l.get_tokens_and_segments(p_tokens, h_tokens)\ntoken_ids =self .vocab[tokens] +[self .vocab[ '<pad> ']] \\\n*(self .max_len -len(tokens))\nsegments =segments +[0]*(self .max_len -len(segments))\nvalid_len =len(tokens)\nreturn token_ids, segments, valid_len\ndef _truncate_pair_of_tokens (self , p_tokens, h_tokens):\n# Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT\n# input\nwhile len(p_tokens) +len(h_tokens) >self .max_len -3:\niflen(p_tokens) >len(h_tokens):\np_tokens .pop()\nelse :\nh_tokens .pop()\ndef __getitem__ (self , idx):\nreturn (self .all_token_ids[idx], self .all_segments[idx],\nself .valid_lens[idx]), self .labels[idx]\ndef __len__ (self ):\nreturn len(self .all_token_ids)\nAfter downloading the SNLI dataset, we generate training and testing examples by instan-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2219, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b15b222-8000-406a-9ce9-bca780217814": {"__data__": {"id_": "0b15b222-8000-406a-9ce9-bca780217814", "embedding": null, "metadata": {"page_label": "778", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "beec423b-6d55-4e96-bcb3-b6d24cd12f22", "node_type": "4", "metadata": {"page_label": "778", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "196d0239d75c0214db4dab42d1f19cfffd43110f64503a7df06ad96d2a744d19", "class_name": "RelatedNodeInfo"}}, "text": "778 Natural Language Processing: Applications\ntiating the SNLIBERTDataset class. Such examples will be read in minibatches during\ntraining and testing of natural language inference.\n# Reduce `batch_size` if there is an out of memory error. In the original BERT\n# model, `max_len` = 512\nbatch_size, max_len, num_workers =512,128, d2l .get_dataloader_workers()\ndata_dir =d2l.download_extract( 'SNLI ')\ntrain_set =SNLIBERTDataset(d2l .read_snli(data_dir, True ), max_len, vocab)\ntest_set =SNLIBERTDataset(d2l .read_snli(data_dir, False ), max_len, vocab)\ntrain_iter =torch .utils .data .DataLoader(train_set, batch_size, shuffle =True ,\nnum_workers =num_workers)\ntest_iter =torch .utils .data .DataLoader(test_set, batch_size,\nnum_workers =num_workers)\nread 549367 examples\nread 9824 examples\n16.7.3Fine-TuningBERT\nAsFig. 16.6.2 indicates, fine-tuning BERT for natural language inference requires only an\nextra MLP consisting of two fully connected layers (see self.hidden andself.output\nin the following BERTClassifier class). This MLP transforms the BERT representation\nof the special \u201c<cls>\u201d token, which encodes the information of both the premise and the\nhypothesis, intothreeoutputsofnaturallanguageinference: entailment, contradiction, and\nneutral.\nclass BERTClassifier (nn.Module):\ndef __init__ (self , bert):\nsuper (BERTClassifier, self ).__init__ ()\nself .encoder =bert .encoder\nself .hidden =bert .hidden\nself .output =nn.LazyLinear( 3)\ndef forward (self , inputs):\ntokens_X, segments_X, valid_lens_x =inputs\nencoded_X =self .encoder(tokens_X, segments_X, valid_lens_x)\nreturn self .output( self .hidden(encoded_X[:, 0, :]))\nInthefollowing,thepretrainedBERTmodel bertisfedintothe BERTClassifier instance\nnetfor the downstream application. In common implementations of BERT fine-tuning,\nonlytheparametersoftheoutputlayeroftheadditionalMLP( net.output )willbelearned\nfrom scratch. All the parameters of the pretrained BERT encoder ( net.encoder ) and the\nhidden layer of the additional MLP ( net.hidden ) will be fine-tuned.\nnet =BERTClassifier(bert)\nRecall that in Section 15.8 both the MaskLMclass and the NextSentencePred class have\nparameters in their employed MLPs. These parameters are part of those in the pretrained\nBERT model bert, and thus part of parameters in net. However, such parameters are", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2311, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbc9e6e9-8b61-4b17-868d-55f91d1b332f": {"__data__": {"id_": "dbc9e6e9-8b61-4b17-868d-55f91d1b332f", "embedding": null, "metadata": {"page_label": "779", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3ddb4ab-7699-4279-a54e-ef343a82216b", "node_type": "4", "metadata": {"page_label": "779", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5593952bc124c4d7a9c55dd5e9164f294be5b9310b542737b53605785a39dac9", "class_name": "RelatedNodeInfo"}}, "text": "779 Natural Language Inference: Fine-Tuning BERT\nonly for computing the masked language modeling loss and the next sentence prediction\nloss during pretraining. These two loss functions are irrelevant to fine-tuning downstream\napplications,thustheparametersoftheemployedMLPsin MaskLMandNextSentencePred\nare not updated (staled) when BERT is fine-tuned.\nTo allow parameters with stale gradients, the flag ignore_stale_grad=True is set in the\nstepfunction of d2l.train_batch_ch13 . We use this function to train and evaluate the\nmodel netusing the training set ( train_iter ) and the testing set ( test_iter ) of SNLI.\nDuetothelimitedcomputationalresources,thetrainingandtestingaccuracycanbefurther\nimproved: we leave its discussions in the exercises.\nlr, num_epochs =1e-4 ,5\ntrainer =torch .optim .Adam(net .parameters(), lr =lr)\nloss =nn.CrossEntropyLoss(reduction ='none ')\nnet( next (iter (train_iter))[ 0])\nd2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)\nloss 0.520 , train acc 0.791 , test acc 0.786\n10588.8 examples /sec on [device( type ='cuda ', index =0), device( type ='cuda ',\u2423\n\u21a9!index =1)]\n16.7.4Summary\n\u000fWe can fine-tune the pretrained BERT model for downstream applications, such as nat-\nural language inference on the SNLI dataset.\n\u000fDuring fine-tuning, the BERT model becomes part of the model for the downstream\napplication. Parameters that are only related to pretraining loss will not be updated\nduring fine-tuning.\n16.7.5Exercises\n1.Fine-tuneamuchlargerpretrainedBERTmodelthatisaboutasbigastheoriginalBERT\nbasemodelifyourcomputationalresourceallows. Setargumentsinthe load_pretrained_model\nfunctionas: replacing\u2018bert.small\u2019with\u2018bert.base\u2019,increasingvaluesof num_hiddens=256 ,\nffn_num_hiddens=512 ,num_heads=4 , and num_blks=2 to 768, 3072, 12, and 12, re-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1803, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ec2393c-4a87-43ba-8734-e20e4faae9f2": {"__data__": {"id_": "5ec2393c-4a87-43ba-8734-e20e4faae9f2", "embedding": null, "metadata": {"page_label": "780", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "371143be-00ad-41e1-9555-6c54db6b5d70", "node_type": "4", "metadata": {"page_label": "780", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "80aa9c69ba6ece3225a013722c57023a4f49da80f3c340cf0430b22ef9ecf735", "class_name": "RelatedNodeInfo"}}, "text": "780 Natural Language Processing: Applications\n249spectively. By increasing fine-tuning epochs (and possibly tuning other hyperparame-\nters), can you get a testing accuracy higher than 0.86?\n2.How to truncate a pair of sequences according to their ratio of length? Compare this\npair truncation method and the one used in the SNLIBERTDataset class. What are their\npros and cons?\nDiscussions249.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1411d93-4bd6-4ad2-a665-58eab8c08e52": {"__data__": {"id_": "f1411d93-4bd6-4ad2-a665-58eab8c08e52", "embedding": null, "metadata": {"page_label": "781", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a1f5bd7d-8c87-40c0-a736-1cbb566acfcf", "node_type": "4", "metadata": {"page_label": "781", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0ce47b8be5fe2d72aff175da6cae5af3e06a4b2252b9c3e7a96e5b29cfd90732", "class_name": "RelatedNodeInfo"}}, "text": "250\n17 Reinforcement Learning\nPratik Chaudhari (University of Pennsylvania and Amazon ),Rasool Fakoor (Amazon),\nandKavoshAsadi (Amazon)\nReinforcementLearning(RL)isasuiteoftechniquesthatallowsustobuildmachinelearn-\ningsystemsthattakedecisionssequentially. Forexample,apackagecontainingnewclothes\nthat you purchased from an online retailer arrives at your doorstep after a sequence of de-\ncisions, e.g., the retailer finding the clothes in the warehouse closest to your house, putting\nthe clothes in a box, transporting the box via land or by air, and delivering it to your house\nwithin the city. There are many variables that affect the delivery of the package along the\nway, e.g., whether or not the clothes were available in the warehouse, how long it took to\ntransport the box, whether it arrived in your city before the daily delivery truck left, etc.\nThe key idea is that at each stage these variables that we do not often control affect the\nentire sequence of events in the future, e.g., if there were delays in packing the box in the\nwarehouse the retailer may need to send the package via air instead of ground to ensure a\ntimely delivery. Reinforcement Learning methods allow us to take the appropriate action\nat each stage of a sequential decision making problem in order to maximize some utility\neventually, e.g., the timely delivery of the package to you.\nSuch sequential decision making problems are seen in numerous other places, e.g., while\nplayingGo250yourcurrent move determines the next moves and the opponent\u2019s movesare\nthevariablesthatyoucannotcontrol\u2026asequenceofmoveseventuallydetermineswhether\nornotyouwin; themoviesthatNetflixrecommendstoyounowdeterminewhatyouwatch,\nwhether you like the movie or not is unknown to Netflix, eventually a sequence of movie\nrecommendations determines how satisfied you are with Netflix. Reinforcement learning\nis being used today to develop effective solutions to these problems ( Mnihet al., 2013,\nSilveretal., 2016). The key distinction between reinforcement learning and standard deep\nlearningisthatinstandarddeeplearningthepredictionofatrainedmodelononetestdatum\ndoes not affect the predictions on a future test datum; in reinforcement learning decisions\nat future instants (in RL, decisions are also called actions) are affected by what decisions\nwere made in the past.\nIn this chapter, we will develop the fundamentals of reinforcement learning and obtain\nhands-on experience in implementing some popular reinforcement learning methods. We\nwill first develop a concept called a Markov Decision Process (MDP) which allows us to\nthink of such sequential decision making problems. An algorithm called Value Iteration\nwill be our first insight into solving reinforcement learning problems under the assumption\nthat we know how the uncontrolled variables in an MDP (in RL, these controlled variables\n781", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e28091a5-55c2-4bf6-858b-5d9168151a8b": {"__data__": {"id_": "e28091a5-55c2-4bf6-858b-5d9168151a8b", "embedding": null, "metadata": {"page_label": "782", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "75f6d77b-b5fb-4a98-9181-63c2c8d6a0a0", "node_type": "4", "metadata": {"page_label": "782", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "325cc4d595bc3ea029945559dee9bba5149915c73fcd1e1e87e5210e663304b8", "class_name": "RelatedNodeInfo"}}, "text": "782 Reinforcement Learning\nare called the environment) typically behave. Using the more general version of Value\nIteration, an algorithm called Q-Learning, we will be able to take appropriate actions even\nwhen we do not necessarily have full knowledge of the environment. We will then study\nhow to use deep networks for reinforcement learning problems by imitating the actions of\nan expert. And finally, we will develop a reinforcement learning method that uses a deep\nnetworktotakeactionsinunknownenvironments. Thesetechniquesformthebasisofmore\nadvanced RL algorithms that are used today in a variety of real-world applications, some\nof which we will point to in the chapter.\ntFig. 17.1 Reinforcement Learning Structure\n17.1MarkovDecision Process(MDP)\nIn this section, we will discuss how to formulate reinforcement learning problems using\nMarkov decision processes (MDPs) and describe various components of MDPs in de-\ntail.\n17.1.1Definition of an MDP\nAMarkovdecisionprocess(MDP)( Bellman,1957 )isamodelforhowthestateofasystem\nevolves as different actions are applied to the system. A few different quantities come\ntogether to form an MDP.\n\u000fLetSbe the set of states in the MDP. As a concrete example see Fig. 17.1.1 , for a robot\nthat is navigating a gridworld. In this case, Scorresponds to the set of locations that\nthe robot can be at any given timestep.\n\u000fLetAbe the set of actions that the robot can take at each state, e.g., \u201cgo forward\u201d, \u201cturn", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0f690db0-6e9f-4ac3-bf2a-66ba64e2745c": {"__data__": {"id_": "0f690db0-6e9f-4ac3-bf2a-66ba64e2745c", "embedding": null, "metadata": {"page_label": "783", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cdc66615-e8f7-454d-adde-c492ab98fec4", "node_type": "4", "metadata": {"page_label": "783", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7eee99941d2e635bad1d176ebd73e1cd93347aea91ca65f28c5cd132ebf68ec3", "class_name": "RelatedNodeInfo"}}, "text": "783 Markov Decision Process (MDP)\ntFig. 17.1.1 A simple gridworld navigation task where the robot not only has to \ufb01nd its way to the goal\nlocation (shown as a green house) but also has to avoid trap locations (shown as red cross\nsigns).\nright\u201d, \u201cturn left\u201d, \u201cstay at the same location\u201d, etc. Actions can change the current\nstate of the robot to some other state within the set S.\n\u000fIt may happen that we do not know how the robot moves exactlybut only know it up to\nsome approximation. We model this situation in reinforcement learning as follows: if\nthe robot takes an action \u201cgo forward\u201d, there might be a small probability that it stays\nat the current state, another small probability that it \u201cturns left\u201d, etc. Mathematically,\nthis amounts to defining a \u201ctransition function\u201d \ud835\udc47:S\u0002A\u0002S!\u00bb 0,1\u00bcsuch that\n\ud835\udc47\u00b9\ud835\udc60,\ud835\udc4e,\ud835\udc600\u00ba=\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00bausing the conditional probability of reaching a state \ud835\udc600given\nthattherobotwasatstate \ud835\udc60andtookanaction \ud835\udc4e. Thetransitionfunctionisaprobability\ndistribution and we therefore have\u00cd\n\ud835\udc6002S\ud835\udc47\u00b9\ud835\udc60,\ud835\udc4e,\ud835\udc600\u00ba=1for all\ud835\udc602Sand\ud835\udc4e2A, i.e.,\nthe robot has to go to some state if it takes an action.\n\u000fWe now construct a notion of which actions are useful and which ones are not using the\nconcept of a \u201creward\u201d \ud835\udc5f:S\u0002A! R. We say that the robot gets a reward \ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00ba\nif the robot takes an action \ud835\udc4eat state\ud835\udc60. If the reward \ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00bais large, this indicates\nthat taking the action \ud835\udc4eat state\ud835\udc60is more useful to achieving the goal of the robot, i.e.,\ngoing to the green house. If the reward \ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00bais small, then action \ud835\udc4eis less useful to\nachieving this goal. It is important to note that the reward is designed by the user (the\nperson who creates the reinforcement learning algorithm) with the goal in mind.\n17.1.2Returnand Discount Factor\nThe different components above together form a Markov decision process (MDP)\nMDP :\u00b9S,A,\ud835\udc47,\ud835\udc5f\u00ba. (17.1.1)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "020beea1-5122-43c3-8c61-b94a29d840ea": {"__data__": {"id_": "020beea1-5122-43c3-8c61-b94a29d840ea", "embedding": null, "metadata": {"page_label": "784", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70d270e3-7220-4e0f-94c9-a7e8051cfc2d", "node_type": "4", "metadata": {"page_label": "784", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6d6348a203698730e869a1a4e768b7941a4df4b90e49926f880c5d65cd502b51", "class_name": "RelatedNodeInfo"}}, "text": "784 Reinforcement Learning\nLet\u2019s now consider the situation when the robot starts at a particular state \ud835\udc6002 Sand\ncontinues taking actions to result in a trajectory\n\ud835\udf0f=\u00b9\ud835\udc600,\ud835\udc4e0,\ud835\udc5f0,\ud835\udc601,\ud835\udc4e1,\ud835\udc5f1,\ud835\udc602,\ud835\udc4e2,\ud835\udc5f2,...\u00ba. (17.1.2)\nAt each time step \ud835\udc61the robot is at a state \ud835\udc60\ud835\udc61and takes an action \ud835\udc4e\ud835\udc61which results in a reward\n\ud835\udc5f\ud835\udc61=\ud835\udc5f\u00b9\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61\u00ba. Thereturnof a trajectory is the total reward obtained by the robot along\nsuch a trajectory\n\ud835\udc45\u00b9\ud835\udf0f\u00ba=\ud835\udc5f0\u00b8\ud835\udc5f1\u00b8\ud835\udc5f2\u00b8\u0001\u0001\u0001. (17.1.3)\nThe goal in reinforcement learning is to find a trajectory that has the largest return.\nThink of the situation when the robot continues to travel in the gridworld without ever\nreaching the goal location. The sequence of states and actions in a trajectory can be in-\nfinitelylonginthiscaseandthe returnofanysuchinfinitelylongtrajectorywillbeinfinite.\nIn order to keep the reinforcement learning formulation meaningful even for such trajecto-\nries, we introduce the notion of a discount factor \ud835\udefe < 1. We write the discounted return\nas\n\ud835\udc45\u00b9\ud835\udf0f\u00ba=\ud835\udc5f0\u00b8\ud835\udefe\ud835\udc5f1\u00b8\ud835\udefe2\ud835\udc5f2\u00b8\u0001\u0001\u0001=1\u00d5\n\ud835\udc61=0\ud835\udefe\ud835\udc61\ud835\udc5f\ud835\udc61. (17.1.4)\nNote that if \ud835\udefeis very small, the rewards earned by the robot in the far future, say \ud835\udc61=\n1000, are heavily discounted by the factor \ud835\udefe1000. This encourages the robot to select short\ntrajectories that achieve its goal, namely that of going to the green house in the gridwold\nexample (see Fig. 17.1.1 ). For large values of the discount factor, say \ud835\udefe=0.99, the robot\nis encouraged to exploreand then find the best trajectory to go to the goal location.\n17.1.3Discussionof the MarkovAssumption\nLet us think of a new robot where the state \ud835\udc60\ud835\udc61is the location as above but the action \ud835\udc4e\ud835\udc61is\ntheaccelerationthattherobotappliestoitswheelsinsteadofanabstractcommandlike\u201cgo\nforward\u201d. If this robot has some non-zero velocity at state \ud835\udc60\ud835\udc61, then the next location \ud835\udc60\ud835\udc61\u00b81is\na function of the past location \ud835\udc60\ud835\udc61, the acceleration \ud835\udc4e\ud835\udc61, also the velocity of the robot at time\n\ud835\udc61which is proportional to \ud835\udc60\ud835\udc61\u0000\ud835\udc60\ud835\udc61\u00001. This indicates that we should have\n\ud835\udc60\ud835\udc61\u00b81=some function\u00b9\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61,\ud835\udc60\ud835\udc61\u00001\u00ba; (17.1.5)\nthe \u201csome function\u201d in our case would be Newton\u2019s law of motion. This is quite different\nfrom our transition function that simply depends upon \ud835\udc60\ud835\udc61and\ud835\udc4e\ud835\udc61.\nMarkov systems are all systems where the next state \ud835\udc60\ud835\udc61\u00b81is only a function of the current\nstate\ud835\udc60\ud835\udc61and the action \ud835\udc4e\ud835\udc61taken at the current state. In Markov systems, the next state does\nnot depend on which actions were taken in the past or the states that the robot was at in the\npast. Forexample, thenewrobotthathasaccelerationastheactionaboveisnotMarkovian\nbecause the next location \ud835\udc60\ud835\udc61\u00b81depends upon the previous state \ud835\udc60\ud835\udc61\u00001through the velocity.\nIt may seem that Markovian nature of a system is a restrictive assumption, but it is not so.\nMarkov Decision Processes are still capable of modeling a very large class of real systems.\nFor example, for our new robot, if we chose our state \ud835\udc60\ud835\udc61to the tuple\u00b9location,velocity\u00ba", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f15aa12-9adb-46c0-9097-d722f80adf07": {"__data__": {"id_": "1f15aa12-9adb-46c0-9097-d722f80adf07", "embedding": null, "metadata": {"page_label": "785", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9faf1f0c-f20e-45c4-a5fb-f00d2b379b39", "node_type": "4", "metadata": {"page_label": "785", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ff130dc4172cda8bea9c2f4ff70513f298554168c8f870ce0f0a8ae69bc2c9c2", "class_name": "RelatedNodeInfo"}}, "text": "785 Value Iteration\n251\n252\n253then the system is Markovian because its next state \u00b9location\ud835\udc61\u00b81,velocity\ud835\udc61\u00b81\u00badepends only\nupon the current state \u00b9location\ud835\udc61,velocity\ud835\udc61\u00baand the action at the current state \ud835\udc4e\ud835\udc61.\n17.1.4Summary\nThe reinforcement learning problem is typically modeled using Markov Decision Pro-\ncesses. AMarkovdecisionprocess(MDP)isdefinedbyatupleoffourentities \u00b9S,A,\ud835\udc47,\ud835\udc5f\u00ba\nwhereSis the state space,Ais the action space, \ud835\udc47is the transition function that encodes\nthe transition probabilities of the MDP and \ud835\udc5fis the immediate reward obtained by taking\naction at a particular state.\n17.1.5Exercises\n1.Suppose that we want to design an MDP to model MountainCar251problem.\n1.What would be the set of states?\n2.What would be the set of actions?\n3.What would be the possible reward functions?\n2.How would you design an MDP for an Atari game like Pong game252?\nDiscussions253.\n17.2ValueIteration\nIn this section we will discuss how to pick the best action for the robot at each state to\nmaximize the returnof the trajectory. We will describe an algorithm called Value Iteration\nand implement it for a simulated robot that travels over a frozen lake.\n17.2.1StochasticPolicy\nA stochastic policy denoted as \ud835\udf0b\u00b9\ud835\udc4ej\ud835\udc60\u00ba(policy for short) is a conditional distribution over\nthe actions\ud835\udc4e2Agiven the state \ud835\udc602S,\ud835\udf0b\u00b9\ud835\udc4ej\ud835\udc60\u00ba\u0011\ud835\udc43\u00b9\ud835\udc4ej\ud835\udc60\u00ba. As an example, if the\nrobot has four actions A={go left, go down, go right, go up}. The policy at a state\n\ud835\udc602Sfor such a set of actions Ais a categorical distribution where the probabilities of\nthe four actions could be \u00bb0.4,0.2,0.1,0.3\u00bc; at some other state \ud835\udc6002Sthe probabilities\n\ud835\udf0b\u00b9\ud835\udc4ej\ud835\udc600\u00baof the same four actions could be \u00bb0.1,0.1,0.2,0.6\u00bc. Note that we should have\u00cd\n\ud835\udc4e\ud835\udf0b\u00b9\ud835\udc4ej\ud835\udc60\u00ba=1for any state \ud835\udc60. A deterministic policy is a special case of a stochastic\npolicy in that the distribution \ud835\udf0b\u00b9\ud835\udc4ej\ud835\udc60\u00baonly gives non-zero probability to one particular\naction, e.g.,\u00bb1,0,0,0\u00bcfor our example with four actions.\nTo make the notation less cumbersome, we will often write \ud835\udf0b\u00b9\ud835\udc60\u00baas the conditional distri-\nbution instead of \ud835\udf0b\u00b9\ud835\udc4ej\ud835\udc60\u00ba.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2018, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67cd1bd7-9233-4460-9e78-7ee08dc10107": {"__data__": {"id_": "67cd1bd7-9233-4460-9e78-7ee08dc10107", "embedding": null, "metadata": {"page_label": "786", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8bbc4af-4a25-4e78-913b-68a40eb2da3f", "node_type": "4", "metadata": {"page_label": "786", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "413366662f63ce29d9e57a1db5cd6afa46adcff12a53238ce936ac14c6f1c029", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8432446f-5322-45ee-854c-36efaa264b3c", "node_type": "1", "metadata": {}, "hash": "a1dd95fe012385e88b464d3cb958ed519512e799a49a3d45b0bbb048bee385b6", "class_name": "RelatedNodeInfo"}}, "text": "786 Reinforcement Learning\n17.2.2ValueFunction\nImagine now that the robot starts at a state \ud835\udc600and at each time instant, it first samples\nan action from the policy \ud835\udc4e\ud835\udc61\u0018\ud835\udf0b\u00b9\ud835\udc60\ud835\udc61\u00baand takes this action to result in the next state\n\ud835\udc60\ud835\udc61\u00b81. The trajectory \ud835\udf0f=\u00b9\ud835\udc600,\ud835\udc4e0,\ud835\udc5f0,\ud835\udc601,\ud835\udc4e1,\ud835\udc5f1,...\u00ba, can be different depending upon which\nparticular action \ud835\udc4e\ud835\udc61is sampled at intermediate instants. We define the average return\n\ud835\udc45\u00b9\ud835\udf0f\u00ba=\u00cd1\n\ud835\udc61=0\ud835\udefe\ud835\udc61\ud835\udc5f\u00b9\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61\u00baof all such trajectories\n\ud835\udc49\ud835\udf0b\u00b9\ud835\udc600\u00ba=\ud835\udc38\ud835\udc4e\ud835\udc61\u0018\ud835\udf0b\u00b9\ud835\udc60\ud835\udc61\u00bah\n\ud835\udc45\u00b9\ud835\udf0f\u00bai\n=\ud835\udc38\ud835\udc4e\ud835\udc61\u0018\ud835\udf0b\u00b9\ud835\udc60\ud835\udc61\u00bah1\u00d5\n\ud835\udc61=0\ud835\udefe\ud835\udc61\ud835\udc5f\u00b9\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61\u00bai\n, (17.2.1)\nwhere\ud835\udc60\ud835\udc61\u00b81\u0018\ud835\udc43\u00b9\ud835\udc60\ud835\udc61\u00b81j\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61\u00bais the next state of the robot and \ud835\udc5f\u00b9\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61\u00bais the instantaneous\nreward obtained by taking action \ud835\udc4e\ud835\udc61in state\ud835\udc60\ud835\udc61at time\ud835\udc61. This is called the \u201cvalue function\u201d\nfor the policy \ud835\udf0b. In simple words, the value of a state \ud835\udc600for a policy\ud835\udf0b, denoted by \ud835\udc49\ud835\udf0b\u00b9\ud835\udc600\u00ba,\nis the expected \ud835\udefe-discounted returnobtained by the robot if it begins at state \ud835\udc600and takes\nactions from the policy \ud835\udf0bat each time instant.\nWe next break down the trajectory into two stages (i) the first stage which corresponds to\n\ud835\udc600!\ud835\udc601upon taking the action \ud835\udc4e0, and (ii) a second stage which is the trajectory \ud835\udf0f0=\n\u00b9\ud835\udc601,\ud835\udc4e1,\ud835\udc5f1,...\u00bathereafter. The key idea behind all algorithms in reinforcement learning is\nthat the value of state \ud835\udc600can be written as the average reward obtained in the first stage\nand the value function averaged over all possible next states \ud835\udc601. This is quite intuitive and\narises from our Markov assumption: the average return from the current state is the sum\nof the average return from the next state and the average reward of going to the next state.\nMathematically, we write the two stages as\n\ud835\udc49\ud835\udf0b\u00b9\ud835\udc600\u00ba=\ud835\udc5f\u00b9\ud835\udc600,\ud835\udc4e0\u00ba\u00b8\ud835\udefe\ud835\udc38\ud835\udc4e0\u0018\ud835\udf0b\u00b9\ud835\udc600\u00bah\n\ud835\udc38\ud835\udc601\u0018\ud835\udc43\u00b9\ud835\udc601j\ud835\udc600,\ud835\udc4e0\u00bah\n\ud835\udc49\ud835\udf0b\u00b9\ud835\udc601\u00baii\n. (17.2.2)\nThis decomposition is very powerful: it is the foundation of the principle of dynamic pro-\ngramming upon which all reinforcement learning algorithms are based. Notice that the\nsecond stage gets two expectations, one over the choices of the action \ud835\udc4e0taken in the first\nstage using the stochastic policy and another over the possible states \ud835\udc601obtained from the\nchosen action. We can write (17.2.2 )using the transition probabilities in the Markov de-\ncision process (MDP) as\n\ud835\udc49\ud835\udf0b\u00b9\ud835\udc60\u00ba=\u00d5\n\ud835\udc4e2A\ud835\udf0b\u00b9\ud835\udc4ej\ud835\udc60\u00bah\n\ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00ba\u00b8\ud835\udefe\u00d5\n\ud835\udc6002S\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba\ud835\udc49\ud835\udf0b\u00b9\ud835\udc600\u00bai\n;for all\ud835\udc602S.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2185, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8432446f-5322-45ee-854c-36efaa264b3c": {"__data__": {"id_": "8432446f-5322-45ee-854c-36efaa264b3c", "embedding": null, "metadata": {"page_label": "786", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b8bbc4af-4a25-4e78-913b-68a40eb2da3f", "node_type": "4", "metadata": {"page_label": "786", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "413366662f63ce29d9e57a1db5cd6afa46adcff12a53238ce936ac14c6f1c029", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "67cd1bd7-9233-4460-9e78-7ee08dc10107", "node_type": "1", "metadata": {"page_label": "786", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "98b12ac0140b8bfca41936bffcca495ba17929712e5b54cef85dc587e368d42b", "class_name": "RelatedNodeInfo"}}, "text": "Notice that the\nsecond stage gets two expectations, one over the choices of the action \ud835\udc4e0taken in the first\nstage using the stochastic policy and another over the possible states \ud835\udc601obtained from the\nchosen action. We can write (17.2.2 )using the transition probabilities in the Markov de-\ncision process (MDP) as\n\ud835\udc49\ud835\udf0b\u00b9\ud835\udc60\u00ba=\u00d5\n\ud835\udc4e2A\ud835\udf0b\u00b9\ud835\udc4ej\ud835\udc60\u00bah\n\ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00ba\u00b8\ud835\udefe\u00d5\n\ud835\udc6002S\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba\ud835\udc49\ud835\udf0b\u00b9\ud835\udc600\u00bai\n;for all\ud835\udc602S.(17.2.3)\nAnimportantthingtonoticehereisthattheaboveidentityholdsforallstates \ud835\udc602Sbecause\nwe can think of any trajectory that begins at that state and break down the trajectory into\ntwo stages.\n17.2.3Action-ValueFunction\nIn implementations, it is often useful to maintain a quantity called the \u201caction value\u201d func-\ntion which is a closely related quantity to the value function. This is defined to be the\naveragereturnof a trajectory that begins at \ud835\udc600but when the action of the first stage is fixed", "mimetype": "text/plain", "start_char_idx": 1810, "end_char_idx": 2681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "05b8bcb2-bb0e-42cf-8d1f-108223220ffb": {"__data__": {"id_": "05b8bcb2-bb0e-42cf-8d1f-108223220ffb", "embedding": null, "metadata": {"page_label": "787", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fda93fcd-fd9b-4570-b8c8-4b013c4ffbf7", "node_type": "4", "metadata": {"page_label": "787", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dd467cdac7fc625ccf33dd72288a91f1f640233e3a90b46bf866fe225521232c", "class_name": "RelatedNodeInfo"}}, "text": "787 Value Iteration\nto be\ud835\udc4e0\n\ud835\udc44\ud835\udf0b\u00b9\ud835\udc600,\ud835\udc4e0\u00ba=\ud835\udc5f\u00b9\ud835\udc600,\ud835\udc4e0\u00ba\u00b8\ud835\udc38\ud835\udc4e\ud835\udc61\u0018\ud835\udf0b\u00b9\ud835\udc60\ud835\udc61\u00bah1\u00d5\n\ud835\udc61=1\ud835\udefe\ud835\udc61\ud835\udc5f\u00b9\ud835\udc60\ud835\udc61,\ud835\udc4e\ud835\udc61\u00bai\n, (17.2.4)\nnote that the summation inside the expectation is from \ud835\udc61=1,...,1because the reward of\nthe first stage is fixed in this case. We can again break down the trajectory into two parts\nand write\n\ud835\udc44\ud835\udf0b\u00b9\ud835\udc60,\ud835\udc4e\u00ba=\ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00ba\u00b8\ud835\udefe\u00d5\n\ud835\udc6002S\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba\u00d5\n\ud835\udc4e02A\ud835\udf0b\u00b9\ud835\udc4e0j\ud835\udc600\u00ba\ud835\udc44\ud835\udf0b\u00b9\ud835\udc600,\ud835\udc4e0\u00ba;for all\ud835\udc602S,\ud835\udc4e2A.\n(17.2.5)\nThis version is the analog of (17.2.3 )for the action value function.\n17.2.4OptimalStochasticPolicy\nBoththevaluefunctionandtheaction-valuefunctiondependuponthepolicythattherobot\nchooses. We will next think of the \u201coptimal policy\u201d that achieves the maximal average\nreturn\n\ud835\udf0b\u0003=argmax\n\ud835\udf0b\ud835\udc49\ud835\udf0b\u00b9\ud835\udc600\u00ba. (17.2.6)\nOf all possible stochastic policies that the robot could have taken, the optimal policy \ud835\udf0b\u0003\nachieves the largest average discounted returnfor trajectories starting from state \ud835\udc600. Let us\ndenote the value function and the action-value function of the optimal policy as \ud835\udc49\u0003\u0011\ud835\udc49\ud835\udf0b\u0003\nand\ud835\udc44\u0003\u0011\ud835\udc44\ud835\udf0b\u0003.\nLet us observe that for a deterministic policy where there is only one action that is possible\nunder the policy at any given state. This gives us\n\ud835\udf0b\u0003\u00b9\ud835\udc60\u00ba=argmax\n\ud835\udc4e2Ah\n\ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00ba\u00b8\ud835\udefe\u00d5\n\ud835\udc6002S\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba\ud835\udc49\u0003\u00b9\ud835\udc600\u00bai\n.(17.2.7)\nA good mnemonic to remember this is that the optimal action at state \ud835\udc60(for a deterministic\npolicy) is the one that maximizes the sum of reward \ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00bafrom the first stage and the\naveragereturnof the trajectories starting from the next sate \ud835\udc600, averaged over all possible\nnext states\ud835\udc600from the second stage.\n17.2.5Principle of Dynamic Programming\nOur developement in the previous section in (17.2.2 )or(17.2.5 )can be turned into an\nalgorithm to compute the optimal value function \ud835\udc49\u0003or the action-value function \ud835\udc44\u0003, re-\nspectively. Observe that\n\ud835\udc49\u0003\u00b9\ud835\udc60\u00ba=\u00d5\n\ud835\udc4e2A\ud835\udf0b\u0003\u00b9\ud835\udc4ej\ud835\udc60\u00bah\n\ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00ba\u00b8\ud835\udefe\u00d5\n\ud835\udc6002S\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba\ud835\udc49\u0003\u00b9\ud835\udc600\u00bai\n;for all\ud835\udc602S.(17.2.8)\nFor a deterministic optimal policy \ud835\udf0b\u0003, since there is only one action that can be taken at\nstate\ud835\udc60, we can also write\n\ud835\udc49\u0003\u00b9\ud835\udc60\u00ba=argmax\ud835\udc4e2An\n\ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00ba\u00b8\ud835\udefe\u00d5\n\ud835\udc6002S\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba\ud835\udc49\u0003\u00b9\ud835\udc600\u00bao\n(17.2.9)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ec2911f-0c0a-488c-b41a-61c8310d01a8": {"__data__": {"id_": "1ec2911f-0c0a-488c-b41a-61c8310d01a8", "embedding": null, "metadata": {"page_label": "788", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e333f705-a029-434e-b76f-96a180a3ec34", "node_type": "4", "metadata": {"page_label": "788", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4725d77b8b515f6ee551f6bc9fd595e08bbf608d1865de80dc5ff676b64da693", "class_name": "RelatedNodeInfo"}}, "text": "788 Reinforcement Learning\nfor all states \ud835\udc602S. This identity is called the \u201cprinciple of dynamic programming\u201d ( Bell-\nman, 1952 ,Bellman, 1957 ). It was formulated by Richard Bellman in 1950s and we can\nremember it as \u201cthe remainder of an optimal trajectory is also optimal\u201d.\n17.2.6ValueIteration\nWecanturntheprincipleofdynamicprogrammingintoanalgorithmforfindingtheoptimal\nvalue function called value iteration. The key idea behind value iteration is to think of this\nidentity as a set of constraints that tie together \ud835\udc49\u0003\u00b9\ud835\udc60\u00baat different states \ud835\udc602S. We initialize\nthe value function to some arbitrary values \ud835\udc490\u00b9\ud835\udc60\u00bafor all states \ud835\udc602S. At the\ud835\udc58thiteration,\nthe Value Iteration algorithm updates the value function as\n\ud835\udc49\ud835\udc58\u00b81\u00b9\ud835\udc60\u00ba=max\n\ud835\udc4e2An\n\ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00ba\u00b8\ud835\udefe\u00d5\n\ud835\udc6002S\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba\ud835\udc49\ud835\udc58\u00b9\ud835\udc600\u00bao\n;for all\ud835\udc602S.(17.2.10)\nIt turns out that as \ud835\udc58!1the value function estimated by the Value Iteration algorithm\nconverges to the optimal value function irrespective of the initialization \ud835\udc490,\n\ud835\udc49\u0003\u00b9\ud835\udc60\u00ba=lim\n\ud835\udc58!1\ud835\udc49\ud835\udc58\u00b9\ud835\udc60\u00ba;for all states \ud835\udc602S. (17.2.11)\nThesameValueIterationalgorithmcanbeequivalentlywrittenusingtheaction-valuefunc-\ntion as\n\ud835\udc44\ud835\udc58\u00b81\u00b9\ud835\udc60,\ud835\udc4e\u00ba=\ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00ba\u00b8\ud835\udefemax\n\ud835\udc4e02A\u00d5\n\ud835\udc6002S\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba\ud835\udc44\ud835\udc58\u00b9\ud835\udc600,\ud835\udc4e0\u00ba;for all\ud835\udc602S,\ud835\udc4e2A.\n(17.2.12)\nIn this case we initialize \ud835\udc440\u00b9\ud835\udc60,\ud835\udc4e\u00bato some arbitrary values for all \ud835\udc602Sand\ud835\udc4e2A. Again\nwe have\ud835\udc44\u0003\u00b9\ud835\udc60,\ud835\udc4e\u00ba=lim\ud835\udc58!1\ud835\udc44\ud835\udc58\u00b9\ud835\udc60,\ud835\udc4e\u00bafor all\ud835\udc602Sand\ud835\udc4e2A.\n17.2.7Policy Evaluation\nValue Iteration enables us to compute the optimal value function, i.e., \ud835\udc49\ud835\udf0b\u0003of the optimal\ndeterministic policy \ud835\udf0b\u0003. We can also use similar iterative updates to compute the value\nfunction associated with any other, potentially stochastic, policy \ud835\udf0b. We again initialize\n\ud835\udc49\ud835\udf0b\n0\u00b9\ud835\udc60\u00bato some arbitrary values for all states \ud835\udc602Sand at the\ud835\udc58thiteration, perform the\nupdates\n\ud835\udc49\ud835\udf0b\n\ud835\udc58\u00b81\u00b9\ud835\udc60\u00ba=\u00d5\n\ud835\udc4e2A\ud835\udf0b\u00b9\ud835\udc4ej\ud835\udc60\u00bah\n\ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00ba\u00b8\ud835\udefe\u00d5\n\ud835\udc6002S\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba\ud835\udc49\ud835\udf0b\n\ud835\udc58\u00b9\ud835\udc600\u00bai\n;for all\ud835\udc602S.(17.2.13)\nThis algorithm is known as policy evaluation and is useful to compute the value function\ngiven the policy. Again, it turns out that as \ud835\udc58!1these updates converge to the correct\nvalue function irrespective of the initialization \ud835\udc490,\n\ud835\udc49\ud835\udf0b\u00b9\ud835\udc60\u00ba=lim\n\ud835\udc58!1\ud835\udc49\ud835\udf0b\n\ud835\udc58\u00b9\ud835\udc60\u00ba;for all states \ud835\udc602S. (17.2.14)\nThe algorithm for computing the action-value function \ud835\udc44\ud835\udf0b\u00b9\ud835\udc60,\ud835\udc4e\u00baof a policy\ud835\udf0bis analo-\ngous.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2125, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68a55943-a7b8-4f96-9cd4-97fe6b5d6f60": {"__data__": {"id_": "68a55943-a7b8-4f96-9cd4-97fe6b5d6f60", "embedding": null, "metadata": {"page_label": "789", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e4bf679b-758e-4449-9934-9decdcd64c53", "node_type": "4", "metadata": {"page_label": "789", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ca37e2b893d6fb28dbacccc36990aca39279bae641b4a13836eca0bb943b05ae", "class_name": "RelatedNodeInfo"}}, "text": "789 Value Iteration\n25417.2.8Implementation of ValueIteration\nWenextshowhowtoimplementValueIterationforanavigationproblemcalledFrozenLake\nfromOpen AI Gym254. We first need to setup the enviroment as shown in the following\ncode.\n%matplotlib inline\nimport random\nimport numpy asnp\nfrom d2l import torch asd2l\nseed =0# Random number generator seed\ngamma =0.95 # Discount factor\nnum_iters =10 # Number of iterations\nrandom .seed(seed) # Set the random seed to ensure results can be reproduced\nnp.random .seed(seed)\n# Now set up the environment\nenv_info =d2l.make_env( 'FrozenLake-v1 ', seed =seed)\nIn the FrozenLake environment, the robot moves on a 4\u00024grid (these are the states) with\nactions that are \u201cup\u201d ( \"), \u201cdown\u201d (!), \u201cleft\u201d ( ), and \u201cright\u201d (!). The environment\ncontains a number of holes (H) cells and frozen (F) cells as well as a goal cell (G), all of\nwhich are unknown to the robot. To keep the problem simple, we assume the robot has\nreliable actions, i.e. \ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba=1for all\ud835\udc602S,\ud835\udc4e2A. If the robot reaches the goal, the\ntrial ends and the robot receives a reward of 1irrespective of the action; the reward at any\notherstateis 0forallactions. Theobjectiveoftherobotistolearnapolicythatreachesthe\ngoal location (G) from a given start location (S) (this is \ud835\udc600) to maximize the return.\nThe following function implements Value Iteration, where env_info contains MDP and\nenvironment related information and gammais the discount factor:\ndef value_iteration (env_info, gamma, num_iters):\nenv_desc =env_info[ 'desc ']# 2D array shows what each item means\nprob_idx =env_info[ 'trans_prob_idx ']\nnextstate_idx =env_info[ 'nextstate_idx ']\nreward_idx =env_info[ 'reward_idx ']\nnum_states =env_info[ 'num_states ']\nnum_actions =env_info[ 'num_actions ']\nmdp =env_info[ 'mdp']\nV=np.zeros((num_iters +1, num_states))\nQ=np.zeros((num_iters +1, num_states, num_actions))\npi=np.zeros((num_iters +1, num_states))\nfor kinrange (1, num_iters +1):\nfor sinrange (num_states):\nfor ainrange (num_actions):\n# Calculate \\sum_{s'} p(s'\\mid s,a) [r + \\gamma v_k(s')]\nfor pxrds inmdp[(s,a)]:\n# mdp(s,a): [(p1,next1,r1,d1),(p2,next2,r2,d2),..]\npr=pxrds[prob_idx] # p(s'\\mid s,a)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e345e3e-b3bd-40c6-97e0-9856bed35db2": {"__data__": {"id_": "2e345e3e-b3bd-40c6-97e0-9856bed35db2", "embedding": null, "metadata": {"page_label": "790", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b557cdc2-8574-43f1-a84b-ceae39550074", "node_type": "4", "metadata": {"page_label": "790", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d6b9f30b7b7474315582af24d3264a057c7f82d42cd3fb332302b92738bcb550", "class_name": "RelatedNodeInfo"}}, "text": "790 Reinforcement Learning\n(continued from previous page)\nnextstate =pxrds[nextstate_idx] # Next state\nreward =pxrds[reward_idx] # Reward\nQ[k,s,a] +=pr*(reward +gamma *V[k -1, nextstate])\n# Record max value and max action\nV[k,s] =np.max(Q[k,s,:])\npi[k,s] =np.argmax(Q[k,s,:])\nd2l.show_value_function_progress(env_desc, V[: -1], pi[: -1])\nvalue_iteration(env_info =env_info, gamma =gamma, num_iters =num_iters)\nThe above pictures show the policy (the arrow indicates the action) and value function (the\nchangeincolorshowshowthevaluefunctionchangesovertimefromtheinitialvalueshown\nby dark color to the optimal value shown by light colors.). As we see, Value Iteration finds\nthe optimal value function after 10 iterations and the goal state (G) can be reached starting\nfromanystateaslongasitisnotanHcell. Anotherinterestingaspectoftheimplementation\nis that in addition to finding the optimal value function, we also automatically found the\noptimal policy \ud835\udf0b\u0003corresponding to this value function.\n17.2.9Summary\nThe main idea behind the Value Iteration algorithm is to use the principle of dynamic pro-\ngramming to find the optimal average return obtained from a given state. Note that imple-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "984f68a1-5c59-4ab0-955e-1f4ce72d931f": {"__data__": {"id_": "984f68a1-5c59-4ab0-955e-1f4ce72d931f", "embedding": null, "metadata": {"page_label": "791", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "475a5953-2fbc-485d-9a76-a275325c2dd3", "node_type": "4", "metadata": {"page_label": "791", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d1a786bef4a9f86a4c677c83fcb1963f7cf7facea4da072c3104acf360ef7775", "class_name": "RelatedNodeInfo"}}, "text": "791 Q-Learning\n255menting the Value Iteration algorithm requires that we know the Markov decision process\n(MDP), e.g., the transition and reward functions, completely.\n17.2.10Exercises\n1.Try increasing the grid size to 8\u00028. Compared with 4\u00024grid, how many iterations\ndoes it take to find the optimal value function?\n2.What is the computational complexity of the Value Iteration algorithm?\n3.Run the Value Iteration algorithm again with \ud835\udefe(i.e. \u201cgamma\u201d in the above code) when\nit equals to 0,0.5, and 1and analyze its results.\n4.How does the value of \ud835\udefeaffect the number of iterations taken by Value Iteration to\nconverge? What happens when \ud835\udefe=1?\nDiscussions255.\n17.3Q-Learning\nIntheprevioussection,wediscussedtheValueIterationalgorithmwhichrequiresaccessing\nthe complete Markov decision process (MDP), e.g., the transition and reward functions. In\nthis section, we will look at Q-Learning ( Watkins and Dayan, 1992 ) which is an algorithm\ntolearnthevaluefunctionwithoutnecessarilyknowingtheMDP.Thisalgorithmembodies\nthe central idea behind reinforcement learning: it will enable the robot to obtain its own\ndata.\n17.3.1The Q-Learning Algorithm\nRecall that value iteration for the action-value function in Value Iteration (page 785) cor-\nresponds to the update\n\ud835\udc44\ud835\udc58\u00b81\u00b9\ud835\udc60,\ud835\udc4e\u00ba=\ud835\udc5f\u00b9\ud835\udc60,\ud835\udc4e\u00ba\u00b8\ud835\udefe\u00d5\n\ud835\udc6002S\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00bamax\n\ud835\udc4e02A\ud835\udc44\ud835\udc58\u00b9\ud835\udc600,\ud835\udc4e0\u00ba;for all\ud835\udc602Sand\ud835\udc4e2A.\n(17.3.1)\nAs we discussed, implementing this algorithm requires knowing the MDP, specifically the\ntransition function \ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba. The key idea behind Q-Learning is to replace the summa-\ntion over all \ud835\udc6002Sin the above expression by a summation over the states visited by the\nrobot. This allows us to subvert the need to know the transition function.\n17.3.2AnOptimizationProblemUnderlyingQ-Learning\nLet us imagine that the robot uses a policy \ud835\udf0b\ud835\udc52\u00b9\ud835\udc4ej\ud835\udc60\u00bato take actions. Just like the previous\nchapter,itcollectsadatasetof \ud835\udc5btrajectoriesof \ud835\udc47timestepseachf\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00ba\ud835\udc61=0,...,\ud835\udc47\u00001g\ud835\udc56=1,...,\ud835\udc5b.\nRecall that value iteration is really a set of constraints that ties together the action-value", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2000, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d12226bf-99f0-4c1b-a956-1f2e0a4e606c": {"__data__": {"id_": "d12226bf-99f0-4c1b-a956-1f2e0a4e606c", "embedding": null, "metadata": {"page_label": "792", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eeabf41f-7fa0-4681-ba2a-3676bd7f9385", "node_type": "4", "metadata": {"page_label": "792", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c2572bf0ca9fe8bb6f9f9ceb9e2dc66ec13c2888372539ff2073034b0fa91d3d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c861fe8-96e3-480c-bbba-85e31eb13593", "node_type": "1", "metadata": {}, "hash": "c78e8b82440b1eacee4364a526949ba035e23ba22bddbaf84dc65c38d7baee3b", "class_name": "RelatedNodeInfo"}}, "text": "792 Reinforcement Learning\n\ud835\udc44\u0003\u00b9\ud835\udc60,\ud835\udc4e\u00baof different states and actions to each other. We can implement an approximate\nversion of value iteration using the data that the robot has collected using \ud835\udf0b\ud835\udc52as\n\u02c6\ud835\udc44=min\n\ud835\udc441\n\ud835\udc5b\ud835\udc47\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc47\u00001\u00d5\n\ud835\udc61=0\u00b9\ud835\udc44\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00ba\u0000\ud835\udc5f\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00ba\u0000\ud835\udefemax\n\ud835\udc4e0\ud835\udc44\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61\u00b81,\ud835\udc4e0\u00ba\u00ba2\n|                                                                   {z                                                                   }\ndef=\u2113\u00b9\ud835\udc44\u00ba.\n(17.3.2)\nLet us first observe the similarities and differences between this expression and value iter-\nation above. If the robot\u2019s policy \ud835\udf0b\ud835\udc52were equal to the optimal policy \ud835\udf0b\u0003, and if it collected\nan infinite amount of data, then this optimization problem would be identical to the opti-\nmization problem underlying value iteration. But while value iteration requires us to know\n\ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba, the optimization objective does not have this term. We have not cheated: as\nthe robot uses the policy \ud835\udf0b\ud835\udc52to take an action \ud835\udc4e\ud835\udc56\n\ud835\udc61at state\ud835\udc60\ud835\udc56\n\ud835\udc61, the next state \ud835\udc60\ud835\udc56\n\ud835\udc61\u00b81is a sample\ndrawn from the transition function. So the optimization objective also has access to the\ntransition function, but implicitly in terms of the data collected by the robot.\nThe variables of our optimization problem are \ud835\udc44\u00b9\ud835\udc60,\ud835\udc4e\u00bafor all\ud835\udc602Sand\ud835\udc4e2A. We can\nminimize the objective using gradient descent. For every pair \u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00bain our dataset, we\ncan write\n\ud835\udc44\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00ba \ud835\udc44\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00ba\u0000\ud835\udefcr\ud835\udc44\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00ba\u2113\u00b9\ud835\udc44\u00ba\n=\u00b91\u0000\ud835\udefc\u00ba\ud835\udc44\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00ba\u0000\ud835\udefc\u0010\n\ud835\udc5f\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00ba\u00b8\ud835\udefemax\n\ud835\udc4e0\ud835\udc44\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61\u00b81,\ud835\udc4e0\u00ba\u0011\n,(17.3.3)\nwhere\ud835\udefcis the learning rate. Typically in real problems, when the robot reaches the goal\nlocation, the trajectories end. The value of such a terminal state is zero because the robot\ndoes not take any further actions beyond this state. We should modify our update to handle\nsuch states as\n\ud835\udc44\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00ba=\u00b91\u0000\ud835\udefc\u00ba\ud835\udc44\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00ba\u0000\ud835\udefc\u0010\n\ud835\udc5f\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61,\ud835\udc4e\ud835\udc56\n\ud835\udc61\u00ba\u00b8\ud835\udefe\u00b91\u0000\u22ae\ud835\udc60\ud835\udc56\n\ud835\udc61\u00b81is terminal\u00bamax\n\ud835\udc4e0\ud835\udc44\u00b9\ud835\udc60\ud835\udc56\n\ud835\udc61\u00b81,\ud835\udc4e0\u00ba\u0011\n.\n(17.3.4)\nwhere \u22ae\ud835\udc60\ud835\udc56\n\ud835\udc61\u00b81is terminal is an indicator variable that is one if \ud835\udc60\ud835\udc56\n\ud835\udc61\u00b81is a terminal state and zero\notherwise. The value of state-action tuples \u00b9\ud835\udc60,\ud835\udc4e\u00bathat are not a part of the dataset is set to\n\u00001.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c861fe8-96e3-480c-bbba-85e31eb13593": {"__data__": {"id_": "4c861fe8-96e3-480c-bbba-85e31eb13593", "embedding": null, "metadata": {"page_label": "792", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eeabf41f-7fa0-4681-ba2a-3676bd7f9385", "node_type": "4", "metadata": {"page_label": "792", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c2572bf0ca9fe8bb6f9f9ceb9e2dc66ec13c2888372539ff2073034b0fa91d3d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d12226bf-99f0-4c1b-a956-1f2e0a4e606c", "node_type": "1", "metadata": {"page_label": "792", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "443b3b453aa660e7ad6bde477d804a438f20aa6d0d6728df36e6aac4f8dbfb60", "class_name": "RelatedNodeInfo"}}, "text": "(17.3.4)\nwhere \u22ae\ud835\udc60\ud835\udc56\n\ud835\udc61\u00b81is terminal is an indicator variable that is one if \ud835\udc60\ud835\udc56\n\ud835\udc61\u00b81is a terminal state and zero\notherwise. The value of state-action tuples \u00b9\ud835\udc60,\ud835\udc4e\u00bathat are not a part of the dataset is set to\n\u00001. This algorithm is known as Q-Learning.\nGiven the solution of these updates \u02c6\ud835\udc44, which is an approximation of the optimal value\nfunction\ud835\udc44\u0003, we can obtain the optimal deterministic policy corresponding to this value\nfunction easily using\n\u02c6\ud835\udf0b\u00b9\ud835\udc60\u00ba=argmax\ud835\udc4e\u02c6\ud835\udc44\u00b9\ud835\udc60,\ud835\udc4e\u00ba. (17.3.5)\nThere can be situations when there are multiple deterministic policies that correspond to\nthe same optimal value function; such ties can be broken arbitrarily because they have the\nsame value function.\n17.3.3Exploration in Q-Learning", "mimetype": "text/plain", "start_char_idx": 1821, "end_char_idx": 2527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c3af813-c686-49f4-bea0-a67affde60f8": {"__data__": {"id_": "5c3af813-c686-49f4-bea0-a67affde60f8", "embedding": null, "metadata": {"page_label": "793", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d2f2c5a3-3ef3-4208-9c97-8b6f48180a05", "node_type": "4", "metadata": {"page_label": "793", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6cd039d78a6a2e8fd67a7de50b33c47b7872e8fddd80c0d900bd0b6bfcb953b9", "class_name": "RelatedNodeInfo"}}, "text": "793 Q-Learning\nThe policy used by the robot to collect data \ud835\udf0b\ud835\udc52is critical to ensure that Q-Learning works\nwell. Afterall, we have replaced the expectation over \ud835\udc600using the transition function \ud835\udc43\u00b9\ud835\udc600j\n\ud835\udc60,\ud835\udc4e\u00bausing the data collected by the robot. If the policy \ud835\udf0b\ud835\udc52does not reach diverse parts of\nthestate-actionspace,thenitiseasytoimagineourestimate \u02c6\ud835\udc44willbeapoorapproximation\nof the optimal \ud835\udc44\u0003. It is also important to note that in such a situation, the estimate of \ud835\udc44\u0003at\nallstates\ud835\udc602Swill be bad, not just the ones visited by \ud835\udf0b\ud835\udc52. This is because the Q-Learning\nobjective (or value iteration) is a constraint that ties together the value of all state-action\npairs. It is therefore critical to pick the correct policy \ud835\udf0b\ud835\udc52to collect data.\nWecanmitigatethisconcernbypickingacompletelyrandompolicy \ud835\udf0b\ud835\udc52thatsamplesactions\nuniformly randomly from A. Such a policy would visit all states, but it will take a large\nnumber of trajectories before it does so.\nWe thus arrive at the second key idea in Q-Learning, namely exploration. Typical im-\nplementations of Q-Learning tie together the current estimate of \ud835\udc44and the policy \ud835\udf0b\ud835\udc52to\nset\n\ud835\udf0b\ud835\udc52\u00b9\ud835\udc4ej\ud835\udc60\u00ba=(\nargmax\ud835\udc4e0\u02c6\ud835\udc44\u00b9\ud835\udc60,\ud835\udc4e0\u00bawith prob. 1\u0000\ud835\udf16\nuniform\u00b9A\u00ba with prob.\ud835\udf16,(17.3.6)\nwhere\ud835\udf16is called the \u201cexploration parameter\u201d and is chosen by the user. The policy \ud835\udf0b\ud835\udc52is\ncalled an exploration policy. This particular \ud835\udf0b\ud835\udc52is called an\ud835\udf16-greedy exploration policy\nbecause it chooses the optimal action (under the current estimate \u02c6\ud835\udc44) with probability 1\u0000\ud835\udf16\nbut explores randomly with the remainder probability \ud835\udf16. We can also use the so-called\nsoftmax exploration policy\n\ud835\udf0b\ud835\udc52\u00b9\ud835\udc4ej\ud835\udc60\u00ba=\ud835\udc52\u02c6\ud835\udc44\u00b9\ud835\udc60,\ud835\udc4e\u00ba\u009d\ud835\udc47\n\u00cd\n\ud835\udc4e0\ud835\udc52\u02c6\ud835\udc44\u00b9\ud835\udc60,\ud835\udc4e0\u00ba\u009d\ud835\udc47; (17.3.7)\nwhere the hyper-parameter \ud835\udc47is called temperature. A large value of \ud835\udf16in\ud835\udf16-greedy policy\nfunctions similarly to a large value of temperature \ud835\udc47for the softmax policy.\nIt is important to note that when we pick an exploration that depends upon the current\nestimate of the action-value function \u02c6\ud835\udc44, we need to resolve the optimization problem peri-\nodically. Typical implementations of Q-Learning make one mini-batch update using a few\nstate-action pairs in the collected dataset (typically the ones collected from the previous\ntimestep of the robot) after taking every action using \ud835\udf0b\ud835\udc52.\n17.3.4The \u201cSelf-correcting\u201dPropertyof Q-Learning\nThedatasetcollectedbytherobotduringQ-Learninggrowswithtime. Boththeexploration\npolicy\ud835\udf0b\ud835\udc52and the estimate \u02c6\ud835\udc44evolve as the robot collects more data. This gives us a key\ninsight into why Q-Learning works well. Consider a state \ud835\udc60: if a particular action \ud835\udc4ehas\na large value under the current estimate \u02c6\ud835\udc44\u00b9\ud835\udc60,\ud835\udc4e\u00ba, then both the \ud835\udf16-greedy and the softmax\nexplorationpolicieshavealargerprobabilityofpickingthisaction. Ifthisactionactuallyis\nnottheidealaction,thenthefuturestatesthatarisefromthisactionwillhavepoorrewards.\nThenextupdateoftheQ-Learningobjectivewillthereforereducethevalue \u02c6\ud835\udc44\u00b9\ud835\udc60,\ud835\udc4e\u00ba, which\nwill reduce the probability of picking this action the next time the robot visits state \ud835\udc60. Bad", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbcb7c17-f72b-408a-8688-5ddabe10e550": {"__data__": {"id_": "cbcb7c17-f72b-408a-8688-5ddabe10e550", "embedding": null, "metadata": {"page_label": "794", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7773128b-4279-4bd5-8144-65c100fd4481", "node_type": "4", "metadata": {"page_label": "794", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "de5fcc0f96aab71a651436374f4c585db9b7cd9d73ef838bf52df57489a016c1", "class_name": "RelatedNodeInfo"}}, "text": "794 Reinforcement Learning\n256actions, e.g., ones whose value is overestimated in \u02c6\ud835\udc44\u00b9\ud835\udc60,\ud835\udc4e\u00ba, are explored by the robot but\ntheir value is correct in the next update of the Q-Learning objective. Good actions, e.g.,\nwhose value \u02c6\ud835\udc44\u00b9\ud835\udc60,\ud835\udc4e\u00bais large, are explored more often by the robot and thereby reinforced.\nThispropertycanbeusedtoshowthatQ-Learningcanconvergetotheoptimalpolicyeven\nif it begins with a random policy \ud835\udf0b\ud835\udc52(Watkins and Dayan, 1992 ).\nThis ability to not only collect new data but also collect the right kind of data is the cen-\ntral feature of reinforcement learning algorithms, and this is what distinguishes them from\nsupervised learning. Q-Learning, using deep neural networks (which we will see in the\nDQN chapeter later), is responsible for the resurgence of reinforcement learning ( Mnihet\nal., 2013).\n17.3.5Implementation of Q-Learning\nWenowshowhowtoimplementQ-LearningonFrozenLakefrom OpenAIGym256. Note\nthis is the same setup as we consider in ValueIteration (page 785) experiment.\n%matplotlib inline\nimport random\nimport numpy asnp\nfrom d2l import torch asd2l\nseed =0# Random number generator seed\ngamma =0.95 # Discount factor\nnum_iters =256 # Number of iterations\nalpha =0.9 # Learing rate\nepsilon =0.9 # Epsilon in epsilion gready algorithm\nrandom .seed(seed) # Set the random seed\nnp.random .seed(seed)\n# Now set up the environment\nenv_info =d2l.make_env( 'FrozenLake-v1 ', seed =seed)\nIn the FrozenLake environment, the robot moves on a 4\u00024grid (these are the states) with\nactions that are \u201cup\u201d ( \"), \u201cdown\u201d (!), \u201cleft\u201d ( ), and \u201cright\u201d (!). The environment\ncontains a number of holes (H) cells and frozen (F) cells as well as a goal cell (G), all of\nwhich are unknown to the robot. To keep the problem simple, we assume the robot has\nreliable actions, i.e. \ud835\udc43\u00b9\ud835\udc600j\ud835\udc60,\ud835\udc4e\u00ba=1for all\ud835\udc602S,\ud835\udc4e2A. If the robot reaches the goal, the\ntrial ends and the robot receives a reward of 1irrespective of the action; the reward at any\notherstateis 0forallactions. Theobjectiveoftherobotistolearnapolicythatreachesthe\ngoal location (G) from a given start location (S) (this is \ud835\udc600) to maximize the return.\nWe first implement \ud835\udf16-greedy method as follows:\ndef e_greedy (env, Q, s, epsilon):\nifrandom .random() <epsilon:\nreturn env.action_space .sample()\nelse :\nreturn np.argmax(Q[s,:])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4d958a7-9457-413a-9fd3-636f479b6646": {"__data__": {"id_": "d4d958a7-9457-413a-9fd3-636f479b6646", "embedding": null, "metadata": {"page_label": "795", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0b53fdd7-cf28-4b6d-9a44-65a9f35ec628", "node_type": "4", "metadata": {"page_label": "795", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "84cb7d4c97d8f5a95e39493b496f7ef0083776db84df80856ba094b17abee7de", "class_name": "RelatedNodeInfo"}}, "text": "795 Q-Learning\nWe are now ready to implement Q-learning:\ndef q_learning (env_info, gamma, num_iters, alpha, epsilon):\nenv_desc =env_info[ 'desc ']# 2D array specifying what each grid item \u2423\n\u21a9!means\nenv =env_info[ 'env']# 2D array specifying what each grid item means\nnum_states =env_info[ 'num_states ']\nnum_actions =env_info[ 'num_actions ']\nQ=np.zeros((num_states, num_actions))\nV=np.zeros((num_iters +1, num_states))\npi=np.zeros((num_iters +1, num_states))\nfor kinrange (1, num_iters +1):\n# Reset environment\nstate, done =env.reset(), False\nwhile not done:\n# Select an action for a given state and acts in env based on \u2423\n\u21a9!selected action\naction =e_greedy(env, Q, state, epsilon)\nnext_state, reward, done, _ =env.step(action)\n# Q-update:\ny=reward +gamma *np.max(Q[next_state,:])\nQ[state, action] =Q[state, action] +alpha *(y-Q[state, \u2423\n\u21a9!action])\n# Move to the next state\nstate =next_state\n# Record max value and max action for visualization purpose only\nfor sinrange (num_states):\nV[k,s] =np.max(Q[s,:])\npi[k,s] =np.argmax(Q[s,:])\nd2l.show_Q_function_progress(env_desc, V[: -1], pi[: -1])\nq_learning(env_info =env_info, gamma =gamma, num_iters =num_iters, alpha =alpha, \u2423\n\u21a9!epsilon =epsilon)\nThisresultshowsthatQ-learningcanfindtheoptimalsolutionforthisproblemroughlyafter\n250 iterations. However, when we compare this result with the Value Iteration algorithm\u2019s\nresult(see ImplementationofValueIteration (page789)),wecanseethattheValueIteration\nalgorithm needs way fewer iterations to find the optimal solution for this problem. This\nhappens because the Value Iteration algorithm has access to the full MDP whereas Q-\nlearning does not.\n17.3.6Summary\nQ-learning is one of the most fundamental reinforcement-learning algorithms. It has been\nat the epicenter of the recent success of reinforcement learning, most notably in learning\nto play video games ( Mnihet al., 2013). Implementing Q-learning does not require that\nwe know the Markov decision process (MDP), e.g., the transition and reward functions,\ncompletely.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f7ce98de-95e4-4e73-8b1b-3feb8562c55c": {"__data__": {"id_": "f7ce98de-95e4-4e73-8b1b-3feb8562c55c", "embedding": null, "metadata": {"page_label": "796", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8a9e960d-a1d1-4c7e-b7af-2d9cd6a85549", "node_type": "4", "metadata": {"page_label": "796", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7a45d7bb8ea39d1e4a08af579a626ecbb6cbbe4413a92c32cf351410cf51d1c9", "class_name": "RelatedNodeInfo"}}, "text": "796 Reinforcement Learning\n25717.3.7Exercises\n1.Try increasing the grid size to 8\u00028. Compared with 4\u00024grid, how many iterations\ndoes it take to find the optimal value function?\n2.Run the Q-learning algorithm again with \ud835\udefe(i.e. \u201cgamma\u201d in the above code) when it\nequals to 0,0.5, and 1and analyze its results.\n3.Run the Q-learning algorithm again with \ud835\udf16(i.e. \u201cepsilon\u201d in the above code) when it\nequals to 0,0.5, and 1and analyze its results.\nDiscussions257.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb76bc25-e83e-4a8c-80ee-e1988629b130": {"__data__": {"id_": "bb76bc25-e83e-4a8c-80ee-e1988629b130", "embedding": null, "metadata": {"page_label": "797", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9708d7b-2cbb-4ce0-aff5-31fe4187ccbd", "node_type": "4", "metadata": {"page_label": "797", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "38314fff6703450545f6b16203fd7540c0ff092f7d6c85e630db4df68e13af8b", "class_name": "RelatedNodeInfo"}}, "text": "258\n18 Gaussian Processes\nAndrewGordonWilson (NewYorkUniversityand Amazon )\nGaussianprocesses(GPs)areubitiquous. Youhavealreadyencounteredmanyexamplesof\nGPs without realizing it. Any model that is linear in its parameters with a Gaussian distri-\nbutionovertheparametersisaGaussianprocess. Thisclassspansdiscretemodels, includ-\ning random walks, and autoregressive processes, as well as continuous models, including\nBayesian linear regression models, polynomials, Fourier series, radial basis functions, and\neven neural networks with an infinite number of hidden units. There is a running joke that\n\u201ceverything is a special case of a Gaussian process\u201d.\nLearning about Gaussian processes is important for three reasons: (1) they provide a func-\ntionspace perspectiveofmodelling,whichmakesunderstandingavarietyofmodelclasses,\nincluding deep neural networks, much more approachable; (2) they have an extraordinary\nrange of applications where they are state-of-the-art, including active learning, hyperpa-\nrameter learning, auto-ML, and spatiotemporal regression; (3) over the last few years,\nalgorithmic advances have made Gaussian processes increasingly scalable and relevant,\nharmonizing with deep learning through frameworks such as GPyTorch258(Gardneret\nal., 2018). Indeed, GPs and and deep neural networks are not competing approaches, but\nhighly complementary, and can be combined to great effect. These algorithmic advances\nare not just relevant to Gaussian processes, but provide a foundation in numerical methods\nthat is broadly useful in deep learning.\nIn this chapter, we introduce Gaussian processes. In the introductory notebook, we start\nby reasoning intuitively about what Gaussian processes are and how they directly model\nfunctions. In the priors notebook, we focus on how to specify Gaussian process priors.\nWedirectlyconnectthetradiationalweight-spaceapproachtomodellingtofunctionspace,\nwhich will help us reason about constructing and understanding machine learning mod-\nels, including deep neural networks. We then introduce popular covariance functions, also\nknown as kernels, which control the generalization properties of a Gaussian process. A\nGP with a given kernel defines a prior over functions. In the inference notebook, we will\nshow how to use data to infer a posterior , in order to make predictions. This notebook\ncontains from-scratch code for making predictions with a Gaussian process, as well as an\nintroduction to GPyTorch. In upcoming notebooks, we will introduce the numerics behind\nGaussianprocesses,whichisusefulforscalingGaussianprocessesbutalsoapowerfulgen-\neralfoundationfordeeplearning,andadvanceduse-casessuchashyperparametertuningin\ndeeplearning. OurexampleswillmakeuseofGPyTorch,whichmakesGaussianprocesses\nscale, and is closely integrated with deep learning functionality and PyTorch.\n797", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0d0bbe7-4717-4156-876b-1b542b2a70ca": {"__data__": {"id_": "d0d0bbe7-4717-4156-876b-1b542b2a70ca", "embedding": null, "metadata": {"page_label": "798", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "406a13f3-24ae-4bef-b893-d053451e7b5a", "node_type": "4", "metadata": {"page_label": "798", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1d5a55732af0efc221291e860cbb5c7f7e3cf66164a0ba8b336cc46651157f7a", "class_name": "RelatedNodeInfo"}}, "text": "798 Gaussian Processes\n18.1Introductionto Gaussian Processes\nIn many cases, machine learning amounts to estimating parameters from data. These pa-\nrametersareoftennumerousandrelativelyuninterpretable\u2014suchastheweightsofaneu-\nral network. Gaussian processes, by contrast, provide a mechanism for directly reasoning\nabout the high-level properties of functions that could fit our data. For example, we may\nhave a sense of whether these functions are quickly varying, periodic, involve conditional\nindependencies, or translation invariance. Gaussian processes enable us to easily incorpo-\nratethesepropertiesintoourmodel, bydirectlyspecifyingaGaussiandistributionoverthe\nfunction values that could fit our data.\nLet\u2019sgetafeelforhowGaussianprocessesoperate,bystartingwithsomeexamples.\nSuppose we observe the following dataset, of regression targets (outputs), \ud835\udc66, indexed by\ninputs,\ud835\udc65. As an example, the targets could be changes in carbon dioxide concentrations,\nand the inputs could be the times at which these targets have been recorded. What are\nsome features of the data? How quickly does it seem to varying? Do we have data points\ncollected at regular intervals, or are there missing inputs? How would you imagine filling\nin the missing regions, or forecasting up until \ud835\udc65=25?\ntFig. 18.1.1 Observed data.\nIn order to fit the data with a Gaussian process, we start by specifying a prior distribution\nover what types of functions we might believe to be reasonable. Here we show several\nsample functions from a Gaussian process. Does this prior look reasonable? Note here\nwe are not looking for functions that fit our dataset, but instead for specifying reasonable\nhigh-level properties of the solutions, such as how quickly they vary with inputs. Note that\nwe will see code for reproducing all of the plots in this notebook, in the next notebooks on\npriors and inference.\nOnce we condition on data, we can use this prior to infer a posterior distribution over func-\ntions that could fit the data. Here we show sample posterior functions.\nWe see that each of these functions are entirely consistent with our data, perfectly running\nthrough each observation. In order to use these posterior samples to make predictions, we\ncan average the values of every possible sample function from the posterior, to create the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2307, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96a53220-c096-4b50-95fe-93dcae10c664": {"__data__": {"id_": "96a53220-c096-4b50-95fe-93dcae10c664", "embedding": null, "metadata": {"page_label": "799", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7cf12f6b-4a01-4d4c-a1ec-a4527afd1d5b", "node_type": "4", "metadata": {"page_label": "799", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4141047bc33e13ba94b16a5aa73f003fb059e15c8de8ddc5ef070b86c7e17d02", "class_name": "RelatedNodeInfo"}}, "text": "799 Introduction to Gaussian Processes\ntFig. 18.1.2 Sample prior functions that we may want to represent with our model.\ntFig. 18.1.3 Sample posterior functions, once we have observed the data.\ncurve below, in thick blue. Note that we do not actually have to take an infinite number of\nsamples to compute this expectation; as we will see later, we can compute the expectation\nin closed form.\ntFig. 18.1.4 Posterior samples, alongside posterior mean, which can be used for point predictions, in\nblue.\nWe may also want a representation of uncertainty, so we know how confident we should\nbe in our predictions. Intuitively, we should have more uncertainty where there is more\nvariability in the sample posterior functions, as this tells us there are many more possible\nvaluesthetruefunctioncouldtake. Thistypeofuncertaintyiscalled epistemicuncertainty ,\nwhichisthe reducibleuncertainty associatedwithlackofinformation. Asweacquiremore\ndata, this type of uncertainty disappears, as there will be increasingly fewer solutions con-\nsistent with what we observe. Like with the posterior mean, we can compute the posterior\nvariance (the variability of these functions in the posterior) in closed form. With shade,\nwe show two times the posterior standard deviation on either side of the mean, creating a", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b2a49cf-65d9-42f6-8f7e-4ecb68364a03": {"__data__": {"id_": "5b2a49cf-65d9-42f6-8f7e-4ecb68364a03", "embedding": null, "metadata": {"page_label": "800", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f03000c-d65f-41f7-96a0-619d946bedd3", "node_type": "4", "metadata": {"page_label": "800", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2cb0c39c36784af15de7974fea9ee7076fa84e311e5757480ced38533fb3a59a", "class_name": "RelatedNodeInfo"}}, "text": "800 Gaussian Processes\ncredibleinterval that has a 95% probability of containing the true value of the function for\nany input\ud835\udc65.\ntFig. 18.1.5 Posterior samples, including 95% credible set.\nThe plot looks somewhat cleaner if we remove the posterior samples, simply visualizing\nthe data, posterior mean, and 95% credible set. Notice how the uncertainty grows away\nfrom the data, a property of epistemic uncertainty.\ntFig. 18.1.6 Point predictions, and credible set.\nThe properties of the Gaussian process that we used to fit the data are strongly controlled\nby what\u2019s called a covariance function , also known as a kernel. The covariance function\nwe used is called the RBF (Radial Basis Function)kernel , which has the form\n\ud835\udc58RBF\u00b9\ud835\udc65,\ud835\udc650\u00ba=Cov\u00b9\ud835\udc53\u00b9\ud835\udc65\u00ba, \ud835\udc53\u00b9\ud835\udc650\u00ba\u00ba=\ud835\udc4e2exp\u0012\n\u00001\n2\u21132jj\ud835\udc65\u0000\ud835\udc650jj2\u0013\n(18.1.1)\nThehyperparameters of this kernel are interpretable. The amplitude parameter\ud835\udc4econtrols\nthe vertical scale over which the function is varying, and the length-scale parameter\u2113con-\ntrols the rate of variation (the wiggliness) of the function. Larger \ud835\udc4emeans larger function\nvalues, and larger \u2113means more slowly varying functions. Let\u2019s see what happens to our\nsample prior and posterior functions as we vary \ud835\udc4eand\u2113.\nThelength-scale has a particularly pronounced effect on the predictions and uncertainty of\na GP. Atjj\ud835\udc65\u0000\ud835\udc650jj=\u2113, the covariance between a pair of function values is \ud835\udc4e2exp\u00b9\u00000.5\u00ba.\nAt larger distances than \u2113, the values of the function values becomes nearly uncorrelated.\nThis means that if we want to make a prediction at a point \ud835\udc65\u0003, then function values with\ninputs\ud835\udc65such thatjj\ud835\udc65\u0000\ud835\udc650jj>\u2113will not have a strong effect on our predictions.\nLet\u2019s see how changing the lengthscale affects sample prior and posterior functions, and", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7706b2ab-2b76-4918-82f2-27a4bec76622": {"__data__": {"id_": "7706b2ab-2b76-4918-82f2-27a4bec76622", "embedding": null, "metadata": {"page_label": "801", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "784be029-23c4-47e2-bdf8-5090491c901e", "node_type": "4", "metadata": {"page_label": "801", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c983f4aedcae9e1629a3887000f54a53f121225b7422504293394c14b77abc6b", "class_name": "RelatedNodeInfo"}}, "text": "801 Introduction to Gaussian Processes\ncrediblesets. Theabovefitsusealength-scaleof 2. Let\u2019snowconsider \u2113=0.1,0.5,2,5,10\n. Alength-scaleof 0.1isverysmallrelativetotherangeoftheinputdomainweareconsid-\nering, 25. For example, the values of the function at \ud835\udc65=5and\ud835\udc65=10will have essentially\nno correlation at such a length-scale. On the other hand, for a length-scale of 10, the func-\ntion values at these inputs will be highly correlated. Note that the vertical scale changes in\nthe following figures.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 497, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3895357c-aa77-4eca-a240-ebe16c6e2179": {"__data__": {"id_": "3895357c-aa77-4eca-a240-ebe16c6e2179", "embedding": null, "metadata": {"page_label": "802", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a8c1c69-39cb-4151-882f-cbb0291d5719", "node_type": "4", "metadata": {"page_label": "802", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f72cf0963f1031358ff916b191e75e5b5ce2f4f5f53e36b8928eea39b187e80f", "class_name": "RelatedNodeInfo"}}, "text": "802 Gaussian Processes", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 22, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25ad30e8-57d2-4820-bdc3-fed766c18f44": {"__data__": {"id_": "25ad30e8-57d2-4820-bdc3-fed766c18f44", "embedding": null, "metadata": {"page_label": "803", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64175e74-07a9-49a0-8e56-3aa026a55bb2", "node_type": "4", "metadata": {"page_label": "803", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d9e09df2d20495fb731160d0a2f896e437f0c1417eed068786f935c1302b67b3", "class_name": "RelatedNodeInfo"}}, "text": "803 Introduction to Gaussian Processes\nNotice as the length-scale increases the \u2018wiggliness\u2019 of the functions decrease, and our\nuncertainty decreases. If the length-scale is small, the uncertainty will quickly increase as\nwe move away from the data, as the datapoints become less informative about the function\nvalues.\nNow, let\u2019s vary the amplitude parameter, holding the length-scale fixed at 2. Note the ver-\ntical scale is held fixed for the prior samples, and varies for the posterior samples, so you\ncan clearly see both the increasing scale of the function, and the fits to the data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 589, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d75b9c67-0990-4fa8-8a7f-c6e95606572d": {"__data__": {"id_": "d75b9c67-0990-4fa8-8a7f-c6e95606572d", "embedding": null, "metadata": {"page_label": "804", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "beec1148-fd1f-46bb-a681-21bb55fea26c", "node_type": "4", "metadata": {"page_label": "804", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1af34d267d062b6e65a720e7313f830e1ef13180ff824754533277b4e4e30352", "class_name": "RelatedNodeInfo"}}, "text": "804 Gaussian Processes\nWeseetheamplitudeparameteraffectsthescaleofthefunction,butnottherateofvariation.\nAt this point, we also have the sense that the generalization performance of our procedure\nwilldependonhavingreasonablevaluesforthesehyperparameters. Valuesof \u2113=2and\ud835\udc4e=\n1appeared to provide reasonable fits, while some of the other values did not. Fortunately,\nthere is a robust and automatic way to specify these hyperparameters, using what is called\nthemarginallikelihood , which we will return to in the notebook on inference.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6bd2f278-aee8-4434-bd31-4fd92c885c72": {"__data__": {"id_": "6bd2f278-aee8-4434-bd31-4fd92c885c72", "embedding": null, "metadata": {"page_label": "805", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "48bae3a8-01e6-4887-abb1-ae54676b3cf6", "node_type": "4", "metadata": {"page_label": "805", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "90f9530408ae425d1c706dd24df2e9152c709c982ffb6421076da26d0579648d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb9456ab-509b-4178-b44b-7b0535a777f2", "node_type": "1", "metadata": {}, "hash": "27d25e2304ece64679e715cdc2c15a9a04145d93afb1403e5fbeedab512e82ed", "class_name": "RelatedNodeInfo"}}, "text": "805 Introduction to Gaussian Processes\nSo what is a GP, really? As we started, a GP simply says that any collection of function\nvalues\ud835\udc53\u00b9\ud835\udc651\u00ba,..., \ud835\udc53\u00b9\ud835\udc65\ud835\udc5b\u00ba, indexed by any collection of inputs \ud835\udc651,...,\ud835\udc65\ud835\udc5bhas a joint multi-\nvariate Gaussian distribution. The mean vector \ud835\udf07of this distribution is given by a mean\nfunction, which is typically taken to be a constant or zero. The covariance matrix of this\ndistribution is given by the kernelevaluated at all pairs of the inputs \ud835\udc65.\n266666664\ud835\udc53\u00b9\ud835\udc65\u00ba\n\ud835\udc53\u00b9\ud835\udc651\u00ba\n...\n\ud835\udc53\u00b9\ud835\udc65\ud835\udc5b\u00ba377777775\u0018N\u00a9\u00ad\u00ad\u00ad\u00ad\u00ad\n\u00ab\ud835\udf07,266666664\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65\u00ba\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1\u00ba... \ud835\udc58\u00b9\ud835\udc65,\ud835\udc65\ud835\udc5b\u00ba\n\ud835\udc58\u00b9\ud835\udc651,\ud835\udc65\u00ba\ud835\udc58\u00b9\ud835\udc651,\ud835\udc651\u00ba... \ud835\udc58\u00b9\ud835\udc651,\ud835\udc65\ud835\udc5b\u00ba\n............\n\ud835\udc58\u00b9\ud835\udc65\ud835\udc5b,\ud835\udc65\u00ba\ud835\udc58\u00b9\ud835\udc65\ud835\udc5b,\ud835\udc651\u00ba... \ud835\udc58\u00b9\ud835\udc65\ud835\udc5b,\ud835\udc65\ud835\udc5b\u00ba377777775\u00aa\u00ae\u00ae\u00ae\u00ae\u00ae\n\u00ac(18.1.2)\nEquation (18.1.2 )specifiesaGPprior. Wecancomputetheconditionaldistributionof \ud835\udc53\u00b9\ud835\udc65\u00ba\nfor any\ud835\udc65given\ud835\udc53\u00b9\ud835\udc651\u00ba,..., \ud835\udc53\u00b9\ud835\udc65\ud835\udc5b\u00ba, the function values we have observed. This conditional\ndistribution is called the posterior , and it is what we use to make predictions.\nIn particular,\n\ud835\udc53\u00b9\ud835\udc65\u00baj\ud835\udc53\u00b9\ud835\udc651\u00ba,..., \ud835\udc53\u00b9\ud835\udc65\ud835\udc5b\u00ba\u0018N\u00b9\ud835\udc5a,\ud835\udc602\u00ba (18.1.3)\nwhere\n\ud835\udc5a=\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1:\ud835\udc5b\u00ba\ud835\udc58\u00b9\ud835\udc651:\ud835\udc5b,\ud835\udc651:\ud835\udc5b\u00ba\u00001\ud835\udc53\u00b9\ud835\udc651:\ud835\udc5b\u00ba (18.1.4)\n\ud835\udc602=\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65\u00ba\u0000\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1:\ud835\udc5b\u00ba\ud835\udc58\u00b9\ud835\udc651:\ud835\udc5b,\ud835\udc651:\ud835\udc5b\u00ba\u00001\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1:\ud835\udc5b\u00ba (18.1.5)\nwhere\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1:\ud835\udc5b\u00bais a 1\u0002\ud835\udc5bvector formed by evaluating \ud835\udc58\u00b9\ud835\udc65,\ud835\udc65\ud835\udc56\u00bafor\ud835\udc56=1,...,\ud835\udc5band\n\ud835\udc58\u00b9\ud835\udc651:\ud835\udc5b,\ud835\udc651:\ud835\udc5b\u00bais an\ud835\udc5b\u0002\ud835\udc5bmatrix formed by evaluating \ud835\udc58\u00b9\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57\u00bafor\ud835\udc56,\ud835\udc57=1,...,\ud835\udc5b.\ud835\udc5ais\nwhat we can use as a point predictor for any \ud835\udc65, and\ud835\udc602is what we use for uncertainty: if we\nwant to create an interval with a 95% probability that \ud835\udc53\u00b9\ud835\udc65\u00bais in the interval, we would use\n\ud835\udc5a\u00062\ud835\udc60. The predictivemeans and uncertainties forall the above figures werecreated using\ntheseequations. Theobserveddatapointsweregivenby \ud835\udc53\u00b9\ud835\udc651\u00ba,..., \ud835\udc53\u00b9\ud835\udc65\ud835\udc5b\u00baandchoseafine\ngrained set of \ud835\udc65points to make predictions.\nLet\u2019s suppose we observe a single datapoint, \ud835\udc53\u00b9\ud835\udc651\u00ba, and we want to determine the value\nof\ud835\udc53\u00b9\ud835\udc65\u00baat some\ud835\udc65.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb9456ab-509b-4178-b44b-7b0535a777f2": {"__data__": {"id_": "fb9456ab-509b-4178-b44b-7b0535a777f2", "embedding": null, "metadata": {"page_label": "805", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "48bae3a8-01e6-4887-abb1-ae54676b3cf6", "node_type": "4", "metadata": {"page_label": "805", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "90f9530408ae425d1c706dd24df2e9152c709c982ffb6421076da26d0579648d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6bd2f278-aee8-4434-bd31-4fd92c885c72", "node_type": "1", "metadata": {"page_label": "805", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2154c8b0c1e956971c62cff82fc529295cd44a59b10455051213f73ad9d72446", "class_name": "RelatedNodeInfo"}}, "text": "The predictivemeans and uncertainties forall the above figures werecreated using\ntheseequations. Theobserveddatapointsweregivenby \ud835\udc53\u00b9\ud835\udc651\u00ba,..., \ud835\udc53\u00b9\ud835\udc65\ud835\udc5b\u00baandchoseafine\ngrained set of \ud835\udc65points to make predictions.\nLet\u2019s suppose we observe a single datapoint, \ud835\udc53\u00b9\ud835\udc651\u00ba, and we want to determine the value\nof\ud835\udc53\u00b9\ud835\udc65\u00baat some\ud835\udc65. Because\ud835\udc53\u00b9\ud835\udc65\u00bais described by a Gaussian process, we know the joint\ndistribution over\u00b9\ud835\udc53\u00b9\ud835\udc65\u00ba, \ud835\udc53\u00b9\ud835\udc651\u00ba\u00bais Gaussian:\n\u0014\ud835\udc53\u00b9\ud835\udc65\u00ba\n\ud835\udc53\u00b9\ud835\udc651\u00ba\u0015\n\u0018N\u0012\n\ud835\udf07,\u0014\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65\u00ba\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1\u00ba\n\ud835\udc58\u00b9\ud835\udc651,\ud835\udc65\u00ba\ud835\udc58\u00b9\ud835\udc651,\ud835\udc651\u00ba\u0015\u0013\n(18.1.6)\nThe off-diagonal expression \ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1\u00ba=\ud835\udc58\u00b9\ud835\udc651,\ud835\udc65\u00batells us how correlated the function values\nwill be \u2014 how strongly determined \ud835\udc53\u00b9\ud835\udc65\u00bawill be from \ud835\udc53\u00b9\ud835\udc651\u00ba. We have seen already that if\nwe use a large length-scale, relative to the distance between \ud835\udc65and\ud835\udc651,jj\ud835\udc65\u0000\ud835\udc651jj, then the\nfunctionvalueswillbehighlycorrelated. Wecanvisualizetheprocessofdetermining \ud835\udc53\u00b9\ud835\udc65\u00ba\nfrom\ud835\udc53\u00b9\ud835\udc651\u00baboth in the space of functions, and in the joint distribution over \ud835\udc53\u00b9\ud835\udc651\u00ba, \ud835\udc53\u00b9\ud835\udc65\u00ba.\nLet\u2019s initially consider an \ud835\udc65such that\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1\u00ba=0.9, and\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65\u00ba=1, meaning that the\nvalueof\ud835\udc53\u00b9\ud835\udc65\u00baismoderatelycorrelatedwiththevalueof \ud835\udc53\u00b9\ud835\udc651\u00ba. Inthejointdistribution,the\ncontours of constant probability will be relatively narrow ellipses.\nSuppose we observe \ud835\udc53\u00b9\ud835\udc651\u00ba=1.2. To condition on this value of \ud835\udc53\u00b9\ud835\udc651\u00ba, we can draw a", "mimetype": "text/plain", "start_char_idx": 1395, "end_char_idx": 2618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8008c2b-d093-4d35-99a6-58770f39a0aa": {"__data__": {"id_": "c8008c2b-d093-4d35-99a6-58770f39a0aa", "embedding": null, "metadata": {"page_label": "806", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "246062dc-ef95-400b-abac-45eb0bf6b60d", "node_type": "4", "metadata": {"page_label": "806", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2e06976ab4c02a536b22d8009c651b13b7807aa60d5a363c59abe84c08504d2f", "class_name": "RelatedNodeInfo"}}, "text": "806 Gaussian Processes\nhorizontal line at 1.2on our plot of the density, and see that the value of \ud835\udc53\u00b9\ud835\udc65\u00bais mostly\nconstrained to\u00bb0.64,1.52\u00bc. We have also drawn this plot in function space, showing the\nobservedpoint \ud835\udc53\u00b9\ud835\udc651\u00bainorange,and1standarddeviationoftheGaussianprocesspredictive\ndistribution for \ud835\udc53\u00b9\ud835\udc65\u00bain blue, about the mean value of 1.08.\nNow suppose we have a stronger correlation, \ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1\u00ba=0.95. Now the ellipses have nar-\nrowed further, and the value of \ud835\udc53\u00b9\ud835\udc65\u00bais even more strongly determined by \ud835\udc53\u00b9\ud835\udc651\u00ba. Draw-\ning a horizontal line at 1.2, we see the contours for \ud835\udc53\u00b9\ud835\udc65\u00basupport values mostly within\n\u00bb0.83,1.45\u00bc. Again, we also show the plot in function space, with one standard deviation\nabout the mean predictive value of 1.14.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9088e13-8f94-4308-adb5-df414805c66f": {"__data__": {"id_": "a9088e13-8f94-4308-adb5-df414805c66f", "embedding": null, "metadata": {"page_label": "807", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a19c1116-e34d-4ae1-9ef6-d954e12bc1b3", "node_type": "4", "metadata": {"page_label": "807", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "023971e0d5523038d75684dc09546b33ad9e43ba919161226f4dfd2b2e0589ac", "class_name": "RelatedNodeInfo"}}, "text": "807 Introduction to Gaussian Processes\nWe see that the posterior mean predictor of our Gaussian process is closer to 1.2, be-\ncause there is now a stronger correlation. We also see that our uncertainty (the error bars)\nhave somewhat decreased. Despite the strong correlation between these function values,\nour uncertainty is still righly quite large, because we have only observed a single data\npoint!\nThis procedure can give us a posterior on \ud835\udc53\u00b9\ud835\udc65\u00bafor any\ud835\udc65, for any number of points we have\nobserved. Suppose we observe \ud835\udc53\u00b9\ud835\udc651\u00ba, \ud835\udc53\u00b9\ud835\udc652\u00ba. We now visualize the posterior for \ud835\udc53\u00b9\ud835\udc65\u00baat a\nparticular\ud835\udc65=\ud835\udc650in function space. The exact distribution for \ud835\udc53\u00b9\ud835\udc65\u00bais given by the above\nequations.\ud835\udc53\u00b9\ud835\udc65\u00bais Gaussian distributed, with mean\n\ud835\udc5a=\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1:3\u00ba\ud835\udc58\u00b9\ud835\udc651:3,\ud835\udc651:3\u00ba\u00001\ud835\udc53\u00b9\ud835\udc651:3\u00ba (18.1.7)\nand variance\n\ud835\udc602=\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65\u00ba\u0000\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1:3\u00ba\ud835\udc58\u00b9\ud835\udc651:3,\ud835\udc651:3\u00ba\u00001\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1:3\u00ba (18.1.8)\nInthisintroductorynotebook,wehavebeenconsidering noisefree observations. Aswewill\nsee, it is easy to include observation noise. If we assume that the data are generated from a\nlatent noise free function \ud835\udc53\u00b9\ud835\udc65\u00baplus iid Gaussian noise \ud835\udf16\u00b9\ud835\udc65\u00ba\u0018N\u00b9 0,\ud835\udf0e2\u00bawith variance \ud835\udf0e2,\nthenourcovariancefunctionsimplybecomes \ud835\udc58\u00b9\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57\u00ba!\ud835\udc58\u00b9\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57\u00ba\u00b8\ud835\udeff\ud835\udc56\ud835\udc57\ud835\udf0e2, where\ud835\udeff\ud835\udc56\ud835\udc57=1\nif\ud835\udc56=\ud835\udc57and0otherwise.\nWe have already started getting some intuition about how we can use a Gaussian process\nto specify a prior and posterior over solutions, and how the kernel function affects the\nproperties of these solutions. In the following notebooks, we will precisely show how to\nspecify a Gaussian process prior, introduce and derive various kernel functions, and then\ngo through the mechanics of how to automatically learn kernel hyperparameters, and form\na Gaussian process posterior to make predictions. While it takes time and practice to get\nused to concepts such as a \u201cdistributions over functions\u201d, the actual mechanics of finding\nthe GP predictive equations is actually quite simple \u2014 making it easy to get practice to\nform an intuitive understanding of these concepts.\n18.1.1Summary\nIn typical machine learning, we specify a function with some free parameters (such as\na neural network and its weights), and we focus on estimating those parameters, which", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fdec6584-db17-495d-8060-e4a074ac7a32": {"__data__": {"id_": "fdec6584-db17-495d-8060-e4a074ac7a32", "embedding": null, "metadata": {"page_label": "808", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6ce8bdbb-b31c-405b-a012-6c4c5db48cad", "node_type": "4", "metadata": {"page_label": "808", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "262cd9c694fbc8f725db7b8f91591c0d34bab6cdb268e251d07de8a1b4416e0e", "class_name": "RelatedNodeInfo"}}, "text": "808 Gaussian Processes\nmay not be interpretable. With a Gaussian process, we instead reason about distributions\nover functions directly, which enables us to reason about the high-level properties of the\nsolutions. These properties are controlled by a covariance function (kernel), which often\nhasafewhighlyinterpretablehyperparameters. Thesehyperparametersincludethe length-\nscale,whichcontrolshowrapidly(howwiggily)thefunctionsare. Anotherhyperparameter\nis the amplitude, which controls the vertical scale over which our functions are varying.\nRepresentingmanydifferentfunctionsthatcanfitthedata,andcombiningthemalltogether\ninto a predictive distribution, is a distinctive feature of Bayesian methods. Because there\nis a greater amount of variability between possible solutions far away from the data, our\nuncertainty intuitively grows as we move from the data.\nA Gaussian process represents a distribution over functions by specifying a multivariate\nnormal (Gaussian) distribution over all possible function values. It is possible to easily\nmanipulate Gaussian distributions to find the distribution of one function value based on\nthe values of any set of other values. In other words, if we observe a set of points, then we\ncan condition on these points and infer a distribution over what the value of the function\nmight look like at any other input. How we model the correlations between these points is\ndetermined by the covariance function and is what defines the generalization properties of\nthe Gaussian process. While it takes time to get used to Gaussian processes, they are easy\nto work with, have many applications, and help us understand and develop other model\nclasses, like neural networks.\n18.1.2Exercises\n1.What is the difference between epistemic uncertainty versus observation uncertainty?\n2.Besides rate of variation and amplitude, what other properties of functions might we\nwant to consider, and what would be real-world examples of functions that have those\nproperties?\n3.The RBF covariance function we considered says that covariances (and correlations)\nbetween observations decrease with their distance in the input space (times, spatial lo-\ncations, etc.). Is this a reasonable assumption? Why or why not?\n4.Is a sum of two Gaussian variables Gaussian? Is a product of two Gaussian variables\nGaussian? If (a,b) have a joint Gaussian distribution, is a|b (a given b) Gaussian? Is a\nGaussian?\n5.Repeat the exercise where we observe a data point at \ud835\udc53\u00b9\ud835\udc651\u00ba=1.2, but now suppose we\nadditionally observe \ud835\udc53\u00b9\ud835\udc652\u00ba=1.4. Let\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 1\u00ba=0.9, and\ud835\udc58\u00b9\ud835\udc65,\ud835\udc65 2\u00ba=0.8. Will we be\nmore or less certain about the value of \ud835\udc53\u00b9\ud835\udc65\u00ba, than when we had only observed \ud835\udc53\u00b9\ud835\udc651\u00ba?\nWhat is the mean and 95% credible set for our value of \ud835\udc53\u00b9\ud835\udc65\u00banow?\n6.Do you think increasing our estimate of observation noise would increase or decrease\nour estimate of the length-scale of the ground truth function?\n7.As we move away from the data, suppose the uncertainty in our predictive distribution\nincreases to a point, then stops increasing. Why might that happen?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3032, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6985f50c-b4b0-477a-aabc-18ef484e5059": {"__data__": {"id_": "6985f50c-b4b0-477a-aabc-18ef484e5059", "embedding": null, "metadata": {"page_label": "809", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e5eab7f-1012-4e31-a6c0-4f897ce5eee2", "node_type": "4", "metadata": {"page_label": "809", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2f49e50546110dbcfe1ddfe0de44f805927e9b022e28926af5024da4b715984e", "class_name": "RelatedNodeInfo"}}, "text": "809 Gaussian Process Priors\n259Discussions259.\n18.2GaussianProcess Priors\nUnderstandingGaussianprocesses(GPs)isimportantforreasoningaboutmodelconstruc-\ntionandgeneralization,andforachievingstate-of-the-artperformanceinavarietyofappli-\ncations,includingactivelearning,andhyperparametertuningindeeplearning. GPsareev-\nerywhere, anditisinourintereststoknowwhattheyareandhowwecanusethem.\nIn this section, we introduce Gaussian process priorsover functions. In the next notebook,\nwe show how to use these priors to do posterior inference and make predictions. The\nnext section can be viewed as \u201cGPs in a nutshell\u201d, quickly giving what you need to apply\nGaussian processes in practice.\nimport numpy asnp\nfrom scipy .spatial import distance_matrix\nfrom d2l import torch asd2l\nd2l.set_figsize()\n18.2.1Definition\nA Gaussian process is defined as a collection of random variables, any finite number of\nwhich have a joint Gaussian distribution . If a function \ud835\udc53\u00b9\ud835\udc65\u00bais a Gaussian process, with\nmeanfunction \ud835\udc5a\u00b9\ud835\udc65\u00baandcovariancefunction orkernel\ud835\udc58\u00b9\ud835\udc65,\ud835\udc650\u00ba,\ud835\udc53\u00b9\ud835\udc65\u00ba\u0018GP\u00b9\ud835\udc5a,\ud835\udc58\u00ba,thenany\ncollection of function values queried at any collection of input points \ud835\udc65(times, spatial lo-\ncations, imagepixels, etc.), hasajointmultivariateGaussiandistributionwithmeanvector\n\ud835\udf07and covariance matrix \ud835\udc3e:\ud835\udc53\u00b9\ud835\udc651\u00ba,..., \ud835\udc53\u00b9\ud835\udc65\ud835\udc5b\u00ba\u0018N\u00b9\ud835\udf07,\ud835\udc3e\u00ba, where\ud835\udf07\ud835\udc56=\ud835\udc38\u00bb\ud835\udc53\u00b9\ud835\udc65\ud835\udc56\u00ba\u00bc=\ud835\udc5a\u00b9\ud835\udc65\ud835\udc56\u00ba\nand\ud835\udc3e\ud835\udc56\ud835\udc57=Cov\u00b9\ud835\udc53\u00b9\ud835\udc65\ud835\udc56\u00ba, \ud835\udc53\u00b9\ud835\udc65\ud835\udc57\u00ba\u00ba=\ud835\udc58\u00b9\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57\u00ba.\nThis definition may seem abstract and inaccessible, but Gaussian processes are in fact very\nsimple objects. Any function\n\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc64>\ud835\udf19\u00b9\ud835\udc65\u00ba=h\ud835\udc64,\ud835\udf19\u00b9\ud835\udc65\u00bai, (18.2.1)\nwith\ud835\udc64drawn from a Gaussian (normal) distribution, and \ud835\udf19being any vector of basis func-\ntions, for example \ud835\udf19\u00b9\ud835\udc65\u00ba=\u00b91,\ud835\udc65,\ud835\udc652,...,\ud835\udc65\ud835\udc51\u00ba>, is a Gaussian process. Moreover, any Gaus-\nsian process f(x) can be expressed in the form of equation (18.2.1 ). Let\u2019s consider a few\nconcrete examples, to begin getting acquainted with Gaussian processes, after which we\ncan appreciate how simple and useful they really are.\n18.2.2ASimpleGaussian Process", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e362f91a-0882-40d9-8cd8-ece14fed4064": {"__data__": {"id_": "e362f91a-0882-40d9-8cd8-ece14fed4064", "embedding": null, "metadata": {"page_label": "810", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f5cbeb6-935c-4c7e-a439-edeef28d34c3", "node_type": "4", "metadata": {"page_label": "810", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a054df4d136d7842a9fde534c69d80dd830c0cb80157cd76550443894a6898ee", "class_name": "RelatedNodeInfo"}}, "text": "810 Gaussian Processes\nSuppose\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc640\u00b8\ud835\udc641\ud835\udc65, and\ud835\udc640,\ud835\udc641\u0018N\u00b9 0,1\u00ba, with\ud835\udc640,\ud835\udc641,\ud835\udc65all in one dimension.\nWe can equivalently write this function as the inner product \ud835\udc53\u00b9\ud835\udc65\u00ba=\u00b9\ud835\udc640,\ud835\udc641\u00ba\u00b91,\ud835\udc65\u00ba>. In\n(18.2.1 )above,\ud835\udc64=\u00b9\ud835\udc640,\ud835\udc641\u00ba>and\ud835\udf19\u00b9\ud835\udc65\u00ba=\u00b91,\ud835\udc65\u00ba>.\nFor any\ud835\udc65,\ud835\udc53\u00b9\ud835\udc65\u00bais a sum of two Gaussian random variables. Since Gaussians are closed\nunder addition, \ud835\udc53\u00b9\ud835\udc65\u00bais also a Gaussian random variable for any \ud835\udc65. In fact, we can compute\nfor any particular \ud835\udc65that\ud835\udc53\u00b9\ud835\udc65\u00baisN\u00b90,1\u00b8\ud835\udc652\u00ba. Similarly, the joint distribution for any col-\nlection of function values, \u00b9\ud835\udc53\u00b9\ud835\udc651\u00ba,..., \ud835\udc53\u00b9\ud835\udc65\ud835\udc5b\u00ba\u00ba, for any collection of inputs \ud835\udc651,...,\ud835\udc65\ud835\udc5b, is a\nmultivariate Gaussian distribution. Therefore \ud835\udc53\u00b9\ud835\udc65\u00bais a Gaussian process.\nIn short,\ud835\udc53\u00b9\ud835\udc65\u00bais arandom function , or adistribution over functions . We can gain some\ninsights into this distribution by repeatedly sampling values for \ud835\udc640,\ud835\udc641, and visualizing the\ncorresponding functions \ud835\udc53\u00b9\ud835\udc65\u00ba, which are straight lines with slopes and different intercepts,\nas follows:\ndef lin_func (x, n_sample):\npreds =np.zeros((n_sample, x .shape[ 0]))\nfor iiinrange (n_sample):\nw=np.random .normal( 0,1,2)\ny=w[0]+w[1]*x\npreds[ii, :] =y\nreturn preds\nx_points =np.linspace( -5,5,50)\nouts =lin_func(x_points, 10)\nlw_bd =-2*np.sqrt(( 1+x_points **2))\nup_bd =2*np.sqrt(( 1+x_points **2))\nd2l.plt.fill_between(x_points, lw_bd, up_bd, alpha =0.25 )\nd2l.plt.plot(x_points, np .zeros( len(x_points)), linewidth =4, color ='black ')\nd2l.plt.plot(x_points, outs .T)\nd2l.plt.xlabel( \"x\", fontsize =20)\nd2l.plt.ylabel( \"f(x) \", fontsize =20)\nd2l.plt.show()\nIf\ud835\udc640and\ud835\udc641are instead drawn from N\u00b90,\ud835\udefc2\u00ba, how do you imagine varying \ud835\udefcaffects the\ndistribution over functions?\n18.2.3FromWeightSpace to Function Space", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1641, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfd71b81-5f9d-48cf-86f1-99a2b519efb8": {"__data__": {"id_": "cfd71b81-5f9d-48cf-86f1-99a2b519efb8", "embedding": null, "metadata": {"page_label": "811", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ee5bb24-c120-43a3-9528-a452917cf703", "node_type": "4", "metadata": {"page_label": "811", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cc934c74379e0ff8219cd6f924e7f972585671f6209f735eb3aa5a1b5b769daa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "66b9e61c-d156-4042-824b-83324d37f50e", "node_type": "1", "metadata": {}, "hash": "bb684a66c0e60f91cb25b5b3660cfc0bcd70bc8c6227e5b05a386d185f5e6687", "class_name": "RelatedNodeInfo"}}, "text": "811 Gaussian Process Priors\nIn the plot above, we saw how a distribution over parameters in a model induces a distri-\nbution over functions. While we often have ideas about the functions we want to model \u2014\nwhether they\u2019re smooth, periodic, quickly varying, etc. \u2014 it is relatively tedious to reason\nabout the parameters, which are largely uninterpretable. Fortunately, Gaussian processes\nprovide an easy mechanism to reason directlyabout functions. Since a Gaussian distribu-\ntionisentirelydefinedbyitsfirsttwomoments,itsmeanandcovariancematrix,aGaussian\nprocess by extension is defined by its mean function and covariance function.\nIn the above example, the mean function\n\ud835\udc5a\u00b9\ud835\udc65\u00ba=\ud835\udc38\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba\u00bc=\ud835\udc38\u00bb\ud835\udc640\u00b8\ud835\udc641\ud835\udc65\u00bc=\ud835\udc38\u00bb\ud835\udc640\u00bc\u00b8\ud835\udc38\u00bb\ud835\udc641\u00bc\ud835\udc65=0\u00b80=0. (18.2.2)\nSimilarly, the covariance function is\n\ud835\udc58\u00b9\ud835\udc65,\ud835\udc650\u00ba=Cov\u00b9\ud835\udc53\u00b9\ud835\udc65\u00ba, \ud835\udc53\u00b9\ud835\udc650\u00ba\u00ba=\ud835\udc38\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc53\u00b9\ud835\udc650\u00ba\u00bc\u0000\ud835\udc38\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba\u00bc\ud835\udc38\u00bb\ud835\udc53\u00b9\ud835\udc650\u00ba\u00bc=\ud835\udc38\u00bb\ud835\udc642\n0\u00b8\ud835\udc640\ud835\udc641\ud835\udc650\u00b8\ud835\udc641\ud835\udc640\ud835\udc65\u00b8\ud835\udc642\n1\ud835\udc65\ud835\udc650\u00bc=1\u00b8\ud835\udc65\ud835\udc650.\n(18.2.3)\nOur distribution over functions can now be directly specified and sampled from, without\nneeding to sample from the distribution over parameters. For example, to draw from \ud835\udc53\u00b9\ud835\udc65\u00ba,\nwecansimplyformourmultivariateGaussiandistributionassociatedwithanycollectionof\n\ud835\udc65wewanttoquery,andsamplefromitdirectly. Wewillbegintoseejusthowadvantageous\nthis formulation will be.\nFirst, we note that essentially the same derivation for the simple straight line model above\ncan be applied to find the mean and covariance function for anymodel of the form \ud835\udc53\u00b9\ud835\udc65\u00ba=\n\ud835\udc64>\ud835\udf19\u00b9\ud835\udc65\u00ba, with\ud835\udc64\u0018N\u00b9\ud835\udc62,\ud835\udc46\u00ba. In this case, the mean function \ud835\udc5a\u00b9\ud835\udc65\u00ba=\ud835\udc62>\ud835\udf19\u00b9\ud835\udc65\u00ba, and the\ncovariancefunction \ud835\udc58\u00b9\ud835\udc65,\ud835\udc650\u00ba=\ud835\udf19\u00b9\ud835\udc65\u00ba>\ud835\udc46\ud835\udf19\u00b9\ud835\udc650\u00ba. Since\ud835\udf19\u00b9\ud835\udc65\u00bacanrepresentavectorofanynon-\nlinear basis functions, we are considering a very general model class, including models\nwith an even an infinitenumber of parameters.\n18.2.4TheRadial Basis Function (RBF) Kernel\nTheradial basis function (RBF) kernel is the most popular covariance function for Gaus-\nsian processes, and kernel machines in general. This kernel has the form \ud835\udc58RBF\u00b9\ud835\udc65,\ud835\udc650\u00ba=\n\ud835\udc4e2exp\u0010\n\u00001\n2\u21132jj\ud835\udc65\u0000\ud835\udc650jj2\u0011\n, where\ud835\udc4eis an amplitude parameter, and \u2113is alengthscale hyper-\nparameter.\nLet\u2019s derive this kernel starting from weight space. Consider the function\n\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc3d\u00d5\n\ud835\udc56=1\ud835\udc64\ud835\udc56\ud835\udf19\ud835\udc56\u00b9\ud835\udc65\u00ba,\ud835\udc64\ud835\udc56\u0018N\u0012\n0,\ud835\udf0e2\n\ud835\udc3d\u0013\n,\ud835\udf19\ud835\udc56\u00b9\ud835\udc65\u00ba=exp\u0012\n\u0000\u00b9\ud835\udc65\u0000\ud835\udc50\ud835\udc56\u00ba2\n2\u21132\u0013\n. (18.2.4)\n\ud835\udc53\u00b9\ud835\udc65\u00bais a sum of radial basis functions, with width \u2113, centred at the points \ud835\udc50\ud835\udc56, as shown in\nthe following figure.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66b9e61c-d156-4042-824b-83324d37f50e": {"__data__": {"id_": "66b9e61c-d156-4042-824b-83324d37f50e", "embedding": null, "metadata": {"page_label": "811", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ee5bb24-c120-43a3-9528-a452917cf703", "node_type": "4", "metadata": {"page_label": "811", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cc934c74379e0ff8219cd6f924e7f972585671f6209f735eb3aa5a1b5b769daa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cfd71b81-5f9d-48cf-86f1-99a2b519efb8", "node_type": "1", "metadata": {"page_label": "811", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fe1e4e43a8682f5ee6f6830a2f0cb035bec50ad8c4fc76bc97fb82e27bc3c879", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s derive this kernel starting from weight space. Consider the function\n\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc3d\u00d5\n\ud835\udc56=1\ud835\udc64\ud835\udc56\ud835\udf19\ud835\udc56\u00b9\ud835\udc65\u00ba,\ud835\udc64\ud835\udc56\u0018N\u0012\n0,\ud835\udf0e2\n\ud835\udc3d\u0013\n,\ud835\udf19\ud835\udc56\u00b9\ud835\udc65\u00ba=exp\u0012\n\u0000\u00b9\ud835\udc65\u0000\ud835\udc50\ud835\udc56\u00ba2\n2\u21132\u0013\n. (18.2.4)\n\ud835\udc53\u00b9\ud835\udc65\u00bais a sum of radial basis functions, with width \u2113, centred at the points \ud835\udc50\ud835\udc56, as shown in\nthe following figure.\nWe can recognize \ud835\udc53\u00b9\ud835\udc65\u00baas having the form \ud835\udc64>\ud835\udf19\u00b9\ud835\udc65\u00ba, where\ud835\udc64=\u00b9\ud835\udc641,...,\ud835\udc64\ud835\udc3d\u00ba>and\ud835\udf19\u00b9\ud835\udc65\u00ba\nis a vector containing each of the radial basis functions. The covariance function of this", "mimetype": "text/plain", "start_char_idx": 2003, "end_char_idx": 2427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e5966ae-30b1-4bb5-8e19-3413171d49e1": {"__data__": {"id_": "1e5966ae-30b1-4bb5-8e19-3413171d49e1", "embedding": null, "metadata": {"page_label": "812", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51605275-0f9b-4bc1-b9af-171f8522911f", "node_type": "4", "metadata": {"page_label": "812", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5049e0e8ea1e113c7bdeea57bd329bb915f39d86f68906af7bc357b8de5f9863", "class_name": "RelatedNodeInfo"}}, "text": "812 Gaussian Processes\nGaussian process is then\n\ud835\udc58\u00b9\ud835\udc65,\ud835\udc650\u00ba=\ud835\udf0e2\n\ud835\udc3d\ud835\udc3d\u00d5\n\ud835\udc56=1\ud835\udf19\ud835\udc56\u00b9\ud835\udc65\u00ba\ud835\udf19\ud835\udc56\u00b9\ud835\udc650\u00ba. (18.2.5)\nNowlet\u2019sconsiderwhathappensaswetakethenumberofparameters(andbasisfunctions)\nto infinity. Let \ud835\udc50\ud835\udc3d=log\ud835\udc3d,\ud835\udc501=\u0000log\ud835\udc3d, and\ud835\udc50\ud835\udc56\u00b81\u0000\ud835\udc50\ud835\udc56= \u0394\ud835\udc50=2log\ud835\udc3d\n\ud835\udc3d, and\ud835\udc3d!1. The\ncovariance function becomes the Riemann sum:\n\ud835\udc58\u00b9\ud835\udc65,\ud835\udc650\u00ba=lim\n\ud835\udc3d!1\ud835\udf0e2\n\ud835\udc3d\ud835\udc3d\u00d5\n\ud835\udc56=1\ud835\udf19\ud835\udc56\u00b9\ud835\udc65\u00ba\ud835\udf19\ud835\udc56\u00b9\ud835\udc650\u00ba=\u00b9\ud835\udc501\n\ud835\udc500\ud835\udf19\ud835\udc50\u00b9\ud835\udc65\u00ba\ud835\udf19\ud835\udc50\u00b9\ud835\udc650\u00ba\ud835\udc51\ud835\udc50. (18.2.6)\nBy setting\ud835\udc500=\u00001and\ud835\udc501=1, we spread the infinitely many basis functions across the\nwhole real line, each a distance \u0394\ud835\udc50!0apart:\n\ud835\udc58\u00b9\ud835\udc65,\ud835\udc650\u00ba=\u00b91\n\u00001exp\u00b9\u0000\u00b9\ud835\udc65\u0000\ud835\udc50\u00ba2\n2\u21132\u00baexp\u00b9\u0000\u00b9\ud835\udc650\u0000\ud835\udc50\u00ba2\n2\u21132\u00ba\ud835\udc51\ud835\udc50=p\ud835\udf0b\u2113\ud835\udf0e2exp\u00b9\u0000\u00b9\ud835\udc65\u0000\ud835\udc650\u00ba2\n2\u00b9p\n2\u2113\u00ba2\u00ba/\ud835\udc58RBF\u00b9\ud835\udc65,\ud835\udc650\u00ba.\n(18.2.7)\nItisworthtakingamomenttoabsorbwhatwehavedonehere. Bymovingintothefunction\nspace representation, we have derived how to represent a model with an infinitenumber of\nparameters, using a finite amount of computation. A Gaussian process with an RBF kernel\nis auniversal approximator , capable of representing any continuous function to arbitrary\nprecision. We can intuitively see why from the above derivation. We can collapse each\nradial basis function to a point mass taking \u2113!0, and give each point mass any height we\nwish.\nSo a Gaussian process with an RBF kernel is a model with an infinite number of param-\neters and much more flexibility than any finite neural network. Perhaps all the fuss about\noverparametrized neural networks is misplaced. As we will see, GPs with RBF kernels\ndo not overfit, and in fact provide especially compelling generalization performance on\nsmall datasets. Moreover, the examples in ( Zhanget al., 2021), such as the ability to fit\nimages with random labels perfectly, but still generalize well on structured problems, (can\nbe perfectly reproduced using Gaussian processes) ( Wilson and Izmailov, 2020 ). Neural\nnetworks are not as distinct as we make them out to be.\nWe can build further intuition about Gaussian processes with RBF kernels, and hyperpa-\nrameters such as length-scale , by sampling directly from the distribution over functions.\nAs before, this involves a simple procedure:\n1.Choose the input \ud835\udc65points we want to query the GP: \ud835\udc651,...,\ud835\udc65\ud835\udc5b.\n2.Evaluate\ud835\udc5a\u00b9\ud835\udc65\ud835\udc56\u00ba,\ud835\udc56=1,...,\ud835\udc5b, and\ud835\udc58\u00b9\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57\u00bafor\ud835\udc56,\ud835\udc57=1,...,\ud835\udc5bto respectively form the\nmean vector and covariance matrix \ud835\udf07and\ud835\udc3e, where\u00b9\ud835\udc53\u00b9\ud835\udc651\u00ba,..., \ud835\udc53\u00b9\ud835\udc65\ud835\udc5b\u00ba\u00ba\u0018N\u00b9\ud835\udf07,\ud835\udc3e\u00ba.\n3.Sample from this multivariate Gaussian distribution to obtain the sample function val-\nues.\n4.Sample more times to visualize more sample functions queried at those points.\nWe illustrate this process in the figure below.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69813ee1-f20c-4843-8c2a-def45907f5e9": {"__data__": {"id_": "69813ee1-f20c-4843-8c2a-def45907f5e9", "embedding": null, "metadata": {"page_label": "813", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "124870af-5fb8-4ebe-b88c-db7bbd87dab2", "node_type": "4", "metadata": {"page_label": "813", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f2e38b800582279b18e4417241ba91be6dbde4aeed2b288815db1c16c6baf99b", "class_name": "RelatedNodeInfo"}}, "text": "813 Gaussian Process Priors\ndef rbfkernel (x1, x2, ls =4.): #@save\ndist =distance_matrix(np .expand_dims(x1, 1), np .expand_dims(x2, 1))\nreturn np.exp( -(1./ls/2)*(dist **2))\nx_points =np.linspace( 0,5,50)\nmeanvec =np.zeros( len(x_points))\ncovmat =rbfkernel(x_points,x_points, 1)\nprior_samples =np.random .multivariate_normal(meanvec, covmat, size =5);\nd2l.plt.plot(x_points, prior_samples .T, alpha =0.5)\nd2l.plt.show()\n18.2.5TheNeuralNetworkKernel\nResearch on Gaussian processes in machine learning was triggered by research on neu-\nral networks. Radford Neal was pursuing ever larger Bayesian neural networks, ultimately\nshowing in 1994 (later published in 1996, as it was one of the most infamous NeurIPS\nrejections) that such networks with an infinite number of hidden units become Gaussian\nprocesses with particular kernel functions ( Neal, 1996 ). Interest in this derivation has re-\nsurfaced, with ideas like the neural tangent kernel being used to investigate the generaliza-\ntion properties of neural networks ( Matthews et al., 2018) (Novaket al., 2018). We can\nderive the neural network kernel as follows.\nConsider a neural network function \ud835\udc53\u00b9\ud835\udc65\u00bawith one hidden layer:\n\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc4f\u00b8\ud835\udc3d\u00d5\n\ud835\udc56=1\ud835\udc63\ud835\udc56\u210e\u00b9\ud835\udc65;\ud835\udc62\ud835\udc56\u00ba. (18.2.8)\n\ud835\udc4fis a bias,\ud835\udc63\ud835\udc56are the hidden to output weights, \u210eis any bounded hidden unit transfer\nfunction,\ud835\udc62\ud835\udc56are the input to hidden weights, and \ud835\udc3dis the number of hidden units. Let \ud835\udc4fand\n\ud835\udc63\ud835\udc56be independent with zero mean and variances \ud835\udf0e2\n\ud835\udc4fand\ud835\udf0e2\n\ud835\udc63\u009d\ud835\udc3d, respectively, and let the \ud835\udc62\ud835\udc56\nhaveindependentidenticaldistributions. Wecanthenusethecentrallimittheoremtoshow\nthat any collection of function values \ud835\udc53\u00b9\ud835\udc651\u00ba,..., \ud835\udc53\u00b9\ud835\udc65\ud835\udc5b\u00bahas a joint multivariate Gaussian\ndistribution.\nThe mean and covariance function of the corresponding Gaussian process are:\n\ud835\udc5a\u00b9\ud835\udc65\u00ba=\ud835\udc38\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba\u00bc=0 (18.2.9)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1758, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd71d456-d9c7-4392-87e7-490b559d965b": {"__data__": {"id_": "fd71d456-d9c7-4392-87e7-490b559d965b", "embedding": null, "metadata": {"page_label": "814", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "359bcd3f-673d-4481-9332-631ed382ddae", "node_type": "4", "metadata": {"page_label": "814", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "335c9847007a529108981a80387d11597a9ac2768aff29ee82bc630c44def549", "class_name": "RelatedNodeInfo"}}, "text": "814 Gaussian Processes\n\ud835\udc58\u00b9\ud835\udc65,\ud835\udc650\u00ba=cov\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba, \ud835\udc53\u00b9\ud835\udc650\u00ba\u00bc=\ud835\udc38\u00bb\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc53\u00b9\ud835\udc650\u00ba\u00bc=\ud835\udf0e2\n\ud835\udc4f\u00b81\n\ud835\udc3d\ud835\udc3d\u00d5\n\ud835\udc56=1\ud835\udf0e2\n\ud835\udc63\ud835\udc38\u00bb\u210e\ud835\udc56\u00b9\ud835\udc65;\ud835\udc62\ud835\udc56\u00ba\u210e\ud835\udc56\u00b9\ud835\udc650;\ud835\udc62\ud835\udc56\u00ba\u00bc\n(18.2.10)\nIn some cases, we can essentially evaluate this covariance function in closed form. Let\n\u210e\u00b9\ud835\udc65;\ud835\udc62\u00ba=erf\u00b9\ud835\udc620\u00b8\u00cd\ud835\udc43\n\ud835\udc57=1\ud835\udc62\ud835\udc57\ud835\udc65\ud835\udc57\u00ba, where erf\u00b9\ud835\udc67\u00ba=2p\ud835\udf0b\u00af\ud835\udc67\n0\ud835\udc52\u0000\ud835\udc612\ud835\udc51\ud835\udc61, and\ud835\udc62\u0018N\u00b9 0,\u03a3\u00ba. Then\n\ud835\udc58\u00b9\ud835\udc65,\ud835\udc650\u00ba=2\n\ud835\udf0bsin\u00b92 \u02dc\ud835\udc65>\u03a3\u02dc\ud835\udc650p\n\u00b91\u00b82 \u02dc\ud835\udc65>\u03a3\u02dc\ud835\udc65\u00ba\u00b91\u00b82 \u02dc\ud835\udc650>\u03a3\u02dc\ud835\udc650\u00ba\u00ba.\nThe RBF kernel is stationary , meaning that it is translation invariant , and therefore can\nbe written as a function of \ud835\udf0f=\ud835\udc65\u0000\ud835\udc650. Intuitively, stationarity means that the high-level\nproperties of the function, such as rate of variation, do not change as we move in input\nspace. The neural network kernel, however, is non-stationary . Below, we show sample\nfunctions from a Gaussian process with this kernel. We can see that the function looks\nqualitatively different near the origin.\n18.2.6Summary\nThe first step in performing Bayesian inference involves specifying a prior. Gaussian pro-\ncesses can be used to specify a whole prior over functions. Starting from a traditional\n\u201cweightspace\u201dviewofmodelling,wecaninduceaprioroverfunctionsbystartingwiththe\nfunctional form of a model, and introducing a distribution over its parameters. We can al-\nternativelyspecifyapriordistributiondirectlyinfunctionspace,withpropertiescontrolled\nby a kernel. The function-space approach has many advantages. We can build models that\nactually correspond to an infinite number of parameters, but use a finite amount of com-\nputation! Moreover, while these models have a great amount of flexibility, they also make\nstrong assumptions about what types of functions are a priori likely, leading to relatively\ngood generalization on small datasets.\nTheassumptionsofmodelsinfunctionspaceareintuitivelycontrolledbykernels,whichof-\nten encode higher level properties of functions, such as smoothness and periodicity. Many\nkernels are stationary, meaning that they are translation invariant. Functions drawn from\na Gaussian process with a stationary kernel have roughly the same high-level properties\n(such as rate of variation) regardless of where we look in the input space.\nGaussianprocessesarearelativelygeneralmodelclass,containingmanyexamplesofmod-\nels we are already familiar with, including polynomials, Fourier series, and so on, as long\nas we have a Gaussian prior over the parameters. They also include neural networks with\nan infinite number of parameters, even without Gaussian distributions over the parameters.\nThis connection, discovered by Radford Neal, triggered machine learning researchers to\nmove away from neural networks, and towards Gaussian processes.\n18.2.7Exercises\n1.DrawsamplepriorfunctionsfromaGPwithanOrnstein-Uhlenbeck(OU)kernel, \ud835\udc58OU\u00b9\ud835\udc65,\ud835\udc650\u00ba=\nexp\u0010\n\u00001\n2\u2113jj\ud835\udc65\u0000\ud835\udc650j\u0011\n. If you fix the lengthscale \u2113to be the same, how do these functions\nlook different than sample functions from a GP with an RBF kernel?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "179b325d-5bab-459e-a0fa-d4515753018c": {"__data__": {"id_": "179b325d-5bab-459e-a0fa-d4515753018c", "embedding": null, "metadata": {"page_label": "815", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6644cc7b-7f6c-49f8-a567-a9032fc20b74", "node_type": "4", "metadata": {"page_label": "815", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "790a47b2afcf596daf2d0b3d4142fa8d50bc9406effee7ffa5cfbbd7ce2a83a5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55892b6f-3b03-4d86-aa18-979028e09910", "node_type": "1", "metadata": {}, "hash": "93eb284c75fa115f9465bc51b81c286dc3052230397620937575df239cad9bea", "class_name": "RelatedNodeInfo"}}, "text": "815 Gaussian Process Inference\n260\n2612.How does changing the amplitude\ud835\udc4e2of the RBF kernel affect the distribution over\nfunctions?\n3.Supposeweform \ud835\udc62\u00b9\ud835\udc65\u00ba=\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b82\ud835\udc54\u00b9\ud835\udc65\u00ba,where\ud835\udc53\u00b9\ud835\udc65\u00ba\u0018GP\u00b9\ud835\udc5a1,\ud835\udc581\u00baand\ud835\udc54\u00b9\ud835\udc65\u00ba\u0018GP\u00b9\ud835\udc5a2,\ud835\udc582\u00ba.\nIs\ud835\udc62\u00b9\ud835\udc65\u00baa Gaussian process, and if so, what is its mean and covariance function?\n4.Suppose we form \ud835\udc54\u00b9\ud835\udc65\u00ba=\ud835\udc4e\u00b9\ud835\udc65\u00ba\ud835\udc53\u00b9\ud835\udc65\u00ba, where\ud835\udc53\u00b9\ud835\udc65\u00ba\u0018GP\u00b9 0,\ud835\udc58\u00baand\ud835\udc4e\u00b9\ud835\udc65\u00ba=\ud835\udc652. Is\ud835\udc54\u00b9\ud835\udc65\u00ba\na Gaussian process, and if so, what is its mean and covariance function? What is the\neffect of\ud835\udc4e\u00b9\ud835\udc65\u00ba? What do sample functions drawn from \ud835\udc54\u00b9\ud835\udc65\u00balook like?\n5.Supposeweform \ud835\udc62\u00b9\ud835\udc65\u00ba=\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc54\u00b9\ud835\udc65\u00ba,where\ud835\udc53\u00b9\ud835\udc65\u00ba\u0018GP\u00b9\ud835\udc5a1,\ud835\udc581\u00baand\ud835\udc54\u00b9\ud835\udc65\u00ba\u0018GP\u00b9\ud835\udc5a2,\ud835\udc582\u00ba.\nIs\ud835\udc62\u00b9\ud835\udc65\u00baa Gaussian process, and if so, what is its mean and covariance function?\nDiscussions260.\n18.3Gaussian ProcessInference\nInthissection,wewillshowhowtoperformposteriorinferenceandmakepredictionsusing\ntheGPpriorsweintroducedinthelastsection. Wewillstartwithregression,wherewecan\nperform inference in closed form . This is a \u201cGPs in a nutshell\u201d section to quickly get up\nandrunningwithGaussianprocessesinpractice. We\u2019llstartcodingallthebasicoperations\nfromscratch,andthenintroduce GPyTorch261,whichwillmakeworkingwithstate-of-the-\nart Gaussian processes and integration with deep neural networks much more convenient.\nWe will consider these more advanced topics in depth in the next section. In that section,\nwe will also consider settings where approximate inference is required \u2014 classification,\npoint processes, or any non-Gaussian likelihoods.\n18.3.1PosteriorInferencefor Regression\nAnobservation model relates the function we want to learn, \ud835\udc53\u00b9\ud835\udc65\u00ba, to our observations\n\ud835\udc66\u00b9\ud835\udc65\u00ba, both indexed by some input \ud835\udc65. In classification, \ud835\udc65could be the pixels of an image,\nand\ud835\udc66couldbe the associated classlabel. In regression, \ud835\udc66typicallyrepresents a continuous\noutput, such as a land surface temperature, a sea-level, a \ud835\udc36\ud835\udc42 2concentration, etc.\nIn regression, we often assume the outputs are given by a latent noise-free function \ud835\udc53\u00b9\ud835\udc65\u00ba\nplus i.i.d. Gaussian noise \ud835\udf16\u00b9\ud835\udc65\u00ba:\n\ud835\udc66\u00b9\ud835\udc65\u00ba=\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\u00b9\ud835\udc65\u00ba, (18.3.1)\nwith\ud835\udf16\u00b9\ud835\udc65\u00ba\u0018N\u00b9 0,\ud835\udf0e2\u00ba. Lety=\ud835\udc66\u00b9\ud835\udc4b\u00ba=\u00b9\ud835\udc66\u00b9\ud835\udc651\u00ba,...,\ud835\udc66\u00b9\ud835\udc65\ud835\udc5b\u00ba\u00ba>be a vector of our training\nobservations, and f=\u00b9\ud835\udc53\u00b9\ud835\udc651\u00ba,..., \ud835\udc53\u00b9\ud835\udc65\ud835\udc5b\u00ba\u00ba>be a vector of the latent noise-free function\nvalues, queried at the training inputs \ud835\udc4b=\ud835\udc651,...,\ud835\udc65\ud835\udc5b.\nWe will assume \ud835\udc53\u00b9\ud835\udc65\u00ba\u0018GP\u00b9\ud835\udc5a,\ud835\udc58\u00ba, which means that any collection of function values f\nhasajointmultivariateGaussiandistribution,withmeanvector \ud835\udf07\ud835\udc56=\ud835\udc5a\u00b9\ud835\udc65\ud835\udc56\u00baandcovariance\nmatrix\ud835\udc3e\ud835\udc56\ud835\udc57=\ud835\udc58\u00b9\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57\u00ba.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55892b6f-3b03-4d86-aa18-979028e09910": {"__data__": {"id_": "55892b6f-3b03-4d86-aa18-979028e09910", "embedding": null, "metadata": {"page_label": "815", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6644cc7b-7f6c-49f8-a567-a9032fc20b74", "node_type": "4", "metadata": {"page_label": "815", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "790a47b2afcf596daf2d0b3d4142fa8d50bc9406effee7ffa5cfbbd7ce2a83a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "179b325d-5bab-459e-a0fa-d4515753018c", "node_type": "1", "metadata": {"page_label": "815", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1e50e048ded0a3b748054b71f88ddeccad5f487f94f9e8892d3a4183a9458197", "class_name": "RelatedNodeInfo"}}, "text": "We will assume \ud835\udc53\u00b9\ud835\udc65\u00ba\u0018GP\u00b9\ud835\udc5a,\ud835\udc58\u00ba, which means that any collection of function values f\nhasajointmultivariateGaussiandistribution,withmeanvector \ud835\udf07\ud835\udc56=\ud835\udc5a\u00b9\ud835\udc65\ud835\udc56\u00baandcovariance\nmatrix\ud835\udc3e\ud835\udc56\ud835\udc57=\ud835\udc58\u00b9\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57\u00ba. The RBF kernel \ud835\udc58\u00b9\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57\u00ba=\ud835\udc4e2exp\u0010\n\u00001\n2\u21132jj\ud835\udc65\ud835\udc56\u0000\ud835\udc65\ud835\udc57jj2\u0011\nwould be a", "mimetype": "text/plain", "start_char_idx": 2162, "end_char_idx": 2402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3abd310c-0878-4a8f-896a-7da87c49c6e5": {"__data__": {"id_": "3abd310c-0878-4a8f-896a-7da87c49c6e5", "embedding": null, "metadata": {"page_label": "816", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1c8a088f-907f-4cd3-a9da-7279e4125309", "node_type": "4", "metadata": {"page_label": "816", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "836568ac48996398018d7abc3e1ce07d8b4074fa88241c666e7f85b4b8067bf6", "class_name": "RelatedNodeInfo"}}, "text": "816 Gaussian Processes\nstandardchoiceofcovariancefunction. Fornotationalsimplicity,wewillassumethemean\nfunction\ud835\udc5a\u00b9\ud835\udc65\u00ba=0; our derivations can easily be generalized later on.\nSuppose we want to make predictions at a set of inputs\n\ud835\udc4b\u0003=\ud835\udc65\u00031,\ud835\udc65\u00032,...,\ud835\udc65\u0003\ud835\udc5a. (18.3.2)\nThen we want to find \ud835\udc652and\ud835\udc5d\u00b9f\u0003jy,\ud835\udc4b\u00ba. In the regression setting, we can conveniently\nfind this distribution by using Gaussian identities, after finding the joint distribution over\nf\u0003=\ud835\udc53\u00b9\ud835\udc4b\u0003\u00baandy.\nIf we evaluate equation (18.3.1 )at the training inputs \ud835\udc4b, we have y=f\u00b8\ufb04. By the\ndefinition of a Gaussian process (see last section), f\u0018N\u00b9 0,\ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u00ba\u00bawhere\ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u00bais\nan\ud835\udc5b\u0002\ud835\udc5bmatrix formed by evaluating our covariance function (aka kernel) at all possible\npairs of inputs \ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc572\ud835\udc4b.\ufb04is simply a vector comprised of iid samples from N\u00b90,\ud835\udf0e2\u00ba\nand thus has distribution N\u00b90,\ud835\udf0e2\ud835\udc3c\u00ba.yis therefore a sum of two independent multivariate\nGaussian variables, and thus has distribution N\u00b90,\ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u00ba\u00b8\ud835\udf0e2\ud835\udc3c\u00ba. One can also show\nthat cov\u00b9f\u0003,y\u00ba=cov\u00b9y,f\u0003\u00ba>=\ud835\udc3e\u00b9\ud835\udc4b\u0003,\ud835\udc4b\u00bawhere\ud835\udc3e\u00b9\ud835\udc4b\u0003,\ud835\udc4b\u00bais an\ud835\udc5a\u0002\ud835\udc5bmatrix formed by\nevaluating the kernel at all pairs of test and training inputs.\n\u0014y\nf\u0003\u0015\n\u0018N\u0012\n0,A=\u0014\ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u00ba\u00b8\ud835\udf0e2\ud835\udc3c \ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u0003\u00ba\n\ud835\udc3e\u00b9\ud835\udc4b\u0003,\ud835\udc4b\u00ba\ud835\udc3e\u00b9\ud835\udc4b\u0003,\ud835\udc4b\u0003\u00ba\u0015\u0013\n(18.3.3)\nWe can then use standard Gaussian identities to find the conditional distribution from the\njoint distribution (see, e.g., Bishop Chapter 2), f\u0003jy,\ud835\udc4b,\ud835\udc4b\u0003\u0018 N\u00b9\ud835\udc5a\u0003,\ud835\udc46\u0003\u00ba, where\ud835\udc5a\u0003=\n\ud835\udc3e\u00b9\ud835\udc4b\u0003,\ud835\udc4b\u00ba\u00bb\ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u00ba\u00b8\ud835\udf0e2\ud835\udc3c\u00bc\u00001y,and\ud835\udc46=\ud835\udc3e\u00b9\ud835\udc4b\u0003,\ud835\udc4b\u0003\u00ba\u0000\ud835\udc3e\u00b9\ud835\udc4b\u0003,\ud835\udc4b\u00ba\u00bb\ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u00ba\u00b8\ud835\udf0e2\ud835\udc3c\u00bc\u00001\ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u0003\u00ba.\nTypically, we do not need to make use of the full predictive covariance matrix \ud835\udc46, and in-\nstead use the diagonal of \ud835\udc46for uncertainty about each prediction. Often for this reason we\nwrite the predictive distribution for a single test point \ud835\udc65\u0003, rather than a collection of test\npoints.\nThekernelmatrixhasparameters \ud835\udf03thatwealsowishtoestimate, suchtheamplitude \ud835\udc4eand\nlengthscale\u2113of the RBF kernel above. For these purposes we use the marginallikelihood ,\n\ud835\udc5d\u00b9yj\ud835\udf03,\ud835\udc4b\u00ba, which we already derived in working out the marginal distributions to find the\njointdistributionover y,f\u0003. Aswewillsee,themarginallikelihoodcompartmentalizesinto\nmodelfitandmodelcomplexityterms,andautomaticallyencodesanotionofOccam\u2019srazor\nfor learning hyperparameters. For a full discussion, see MacKay Ch. 28 ( MacKay, 2003 ),\nand Rasmussen and Williams Ch. 5 ( Rasmussen and Williams, 2006 ).\nimport math\nimport os\nimport gpytorch\nimport matplotlib .pyplot asplt\nimport numpy asnp\nimport torch\nfrom scipy import optimize\nfrom scipy .spatial import distance_matrix\nfrom d2l import torch asd2l\nd2l.set_figsize()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c96a450-ef7d-4096-b67f-5bd8e80edc95": {"__data__": {"id_": "5c96a450-ef7d-4096-b67f-5bd8e80edc95", "embedding": null, "metadata": {"page_label": "817", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a8962609-5222-4c18-b050-d24755f4c61f", "node_type": "4", "metadata": {"page_label": "817", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "677326068a03a9285af8351ce72440728615800b6ad4e0246533d36d96d58492", "class_name": "RelatedNodeInfo"}}, "text": "817 Gaussian Process Inference\n18.3.2EquationsforMaking Predictionsand Learning Kernel\nHyperparametersin GP Regression\nWelistheretheequationsyouwilluseforlearninghyperparametersandmakingpredictions\nin Gaussian process regression. Again, we assume a vector of regression targets y, indexed\nbyinputs\ud835\udc4b=f\ud835\udc651,...,\ud835\udc65\ud835\udc5bg,andwewishtomakeapredictionatatestinput \ud835\udc65\u0003. Weassume\ni.i.d. additive zero-mean Gaussian noise with variance \ud835\udf0e2. We use a Gaussian process\nprior\ud835\udc53\u00b9\ud835\udc65\u00ba\u0018GP\u00b9\ud835\udc5a,\ud835\udc58\u00baforthelatentnoise-freefunction,withmeanfunction \ud835\udc5aandkernel\nfunction\ud835\udc58. The kernel itself has parameters \ud835\udf03that we want to learn. For example, if we\nuse an RBF kernel, \ud835\udc58\u00b9\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57\u00ba=\ud835\udc4e2exp\u0010\n\u00001\n2\u21132jj\ud835\udc65\u0000\ud835\udc650jj2\u0011\n, we want to learn \ud835\udf03=f\ud835\udc4e2,\u21132g. Let\n\ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u00barepresent an \ud835\udc5b\u0002\ud835\udc5bmatrix corresponding to evaluating the kernel for all possible\npairs of\ud835\udc5btraining inputs. Let \ud835\udc3e\u00b9\ud835\udc65\u0003,\ud835\udc4b\u00barepresent a 1\u0002\ud835\udc5bvector formed by evaluating\n\ud835\udc58\u00b9\ud835\udc65\u0003,\ud835\udc65\ud835\udc56\u00ba,\ud835\udc56=1,...,\ud835\udc5b. Let\ud835\udf07be a mean vector formed by evaluating the mean function\n\ud835\udc5a\u00b9\ud835\udc65\u00baat every training points \ud835\udc65.\nTypically in working with Gaussian processes, we follow a two-step procedure. 1. Learn\nkernel hyperparameters \u02c6\ud835\udf03by maximizing the marginal likelihood with respect to these hy-\nperparameters. 2. Use the predictive mean as a point predictor, and 2 times the predictive\nstandard deviation to form a 95% credible set, conditioning on these learned hyperparam-\neters \u02c6\ud835\udf03.\nThe log marginal likelihood is simply a log Gaussian density, which has the form:\nlog\ud835\udc5d\u00b9yj\ud835\udf03,\ud835\udc4b\u00ba=\u00001\n2y>\u00bb\ud835\udc3e\ud835\udf03\u00b9\ud835\udc4b,\ud835\udc4b\u00ba\u00b8\ud835\udf0e2\ud835\udc3c\u00bc\u00001y\u00001\n2logj\ud835\udc3e\ud835\udf03\u00b9\ud835\udc4b,\ud835\udc4b\u00baj\u00b8\ud835\udc50 (18.3.4)\nThe predictive distribution has the form:\n\ud835\udc5d\u00b9\ud835\udc66\u0003j\ud835\udc65\u0003,y,\ud835\udf03\u00ba=N\u00b9\ud835\udc4e\u0003,\ud835\udc63\u0003\u00ba (18.3.5)\n\ud835\udc4e\u0003=\ud835\udc58\ud835\udf03\u00b9\ud835\udc65\u0003,\ud835\udc4b\u00ba\u00bb\ud835\udc3e\ud835\udf03\u00b9\ud835\udc4b,\ud835\udc4b\u00ba\u00b8\ud835\udf0e2\ud835\udc3c\u00bc\u00001\u00b9y\u0000\ud835\udf07\u00ba\u00b8\ud835\udf07 (18.3.6)\n\ud835\udc63\u0003=\ud835\udc58\ud835\udf03\u00b9\ud835\udc65\u0003,\ud835\udc65\u0003\u00ba\u0000\ud835\udc3e\ud835\udf03\u00b9\ud835\udc65\u0003,\ud835\udc4b\u00ba\u00bb\ud835\udc3e\ud835\udf03\u00b9\ud835\udc4b,\ud835\udc4b\u00ba\u00b8\ud835\udf0e2\ud835\udc3c\u00bc\u00001\ud835\udc58\ud835\udf03\u00b9\ud835\udc4b,\ud835\udc65\u0003\u00ba (18.3.7)\n18.3.3InterpretingEquations forLearning and Predictions\nThere are some key points to note about the predictive distributions for Gaussian pro-\ncesses:\n\u000fDespitetheflexibilityofthemodelclass,itispossibletodo exactBayesianinferencefor\nGP regression in closed form . Aside from learning the kernel hyperparameters, there\nis notraining. We can write down exactly what equations we want to use to make\npredictions. Gaussian processes are relatively exceptional in this respect, and it has\ngreatly contributed to their convenience, versatility, and continued popularity.\n\u000fThe predictive mean \ud835\udc4e\u0003is a linear combination of the training targets y, weighted by the\nkernel\ud835\udc58\ud835\udf03\u00b9\ud835\udc65\u0003,\ud835\udc4b\u00ba\u00bb\ud835\udc3e\ud835\udf03\u00b9\ud835\udc4b,\ud835\udc4b\u00ba\u00b8\ud835\udf0e2\ud835\udc3c\u00bc\u00001. Aswewillsee,thekernel(anditshyperparam-\neters) thus plays a crucial role in the generalization properties of the model.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2458, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "163ba4dd-0169-4ef2-bfb8-c3a318b62152": {"__data__": {"id_": "163ba4dd-0169-4ef2-bfb8-c3a318b62152", "embedding": null, "metadata": {"page_label": "818", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd8f1cba-b5d9-4b89-8fc0-bc7fbff84e63", "node_type": "4", "metadata": {"page_label": "818", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ea92a647f56c4ae81e5beea2bf07b07f837a4afa4174d222bca35f3653ee443e", "class_name": "RelatedNodeInfo"}}, "text": "818 Gaussian Processes\n\u000fThe predictivemean explicitlydepends on the targetvalues ybut the predictivevariance\ndoes not. The predictive uncertainty instead grows as the test input \ud835\udc65\u0003moves away\nfrom the target locations \ud835\udc4b, as governed by the kernel function. However, uncertainty\nwillimplicitlydependonthevaluesofthetargets ythroughthekernelhyperparameters\n\ud835\udf03, which are learned from the data.\n\u000fThe marginal likelihood compartmentalizes into model fit and model complexity (log\ndeterminant) terms. The marginal likelihood tends to select for hyperparameters that\nprovide the simplest fits that are still consistent with the data.\n\u000fThe key computational bottlenecks come from solving a linear system and computing\na log determinant over an \ud835\udc5b\u0002\ud835\udc5bsymmetric positive definite matrix \ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u00bafor\ud835\udc5b\ntraining points. Naively, these operations each incur O\u00b9\ud835\udc5b3\u00bacomputations, as well as\nO\u00b9\ud835\udc5b2\u00bastorage for each entry of the kernel (covariance) matrix, often starting with a\nCholeskydecomposition. Historically,thesebottleneckshavelimitedGPstoproblems\nwith fewer than about 10,000 training points, and have given GPs a reputation for\n\u201cbeing slow\u201d that has been inaccurate now for almost a decade. In advanced topics,\nwe will discuss how GPs can be scaled to problems with millions of points.\n\u000fFor popular choices of kernel functions, \ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u00bais often close to singular, which can\ncause numerical issues when performing Cholesky decompositions or other opera-\ntionsintendedtosolvelinearsystems. Fortunately,inregressionweareoftenworking\nwith\ud835\udc3e\ud835\udf03\u00b9\ud835\udc4b,\ud835\udc4b\u00ba\u00b8\ud835\udf0e2\ud835\udc3c, such that the noise variance \ud835\udf0e2gets added to the diagonal of\n\ud835\udc3e\u00b9\ud835\udc4b,\ud835\udc4b\u00ba, significantly improving its conditioning. If the noise variance is small, or\nwe are doing noise free regression, it is common practice to add a small amount of\n\u201cjitter\u201d to the diagonal, on the order of 10\u00006, to improve conditioning.\n18.3.4WorkedExamplefromScratch\nLet\u2019s create some regression data, and then fit the data with a GP, implementing every step\nfrom scratch. We\u2019ll sample data from\n\ud835\udc66\u00b9\ud835\udc65\u00ba=sin\u00b9\ud835\udc65\u00ba\u00b81\n2sin\u00b94\ud835\udc65\u00ba\u00b8\ud835\udf16, (18.3.8)\nwith\ud835\udf16\u0018N\u00b9 0,\ud835\udf0e2\u00ba. The noise free function we wish to find is \ud835\udc53\u00b9\ud835\udc65\u00ba=sin\u00b9\ud835\udc65\u00ba\u00b81\n2sin\u00b94\ud835\udc65\u00ba.\nWe\u2019ll start by using a noise standard deviation \ud835\udf0e=0.25.\ndef data_maker1 (x, sig):\nreturn np.sin(x) +0.5 *np.sin( 4*x)+np.random .randn(x .shape[ 0])*sig\nsig =0.25\ntrain_x, test_x =np.linspace( 0,5,50), np .linspace( 0,5,500)\ntrain_y, test_y =data_maker1(train_x, sig =sig), data_maker1(test_x, sig =0.)\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y)\nd2l.plt.xlabel( \"x\", fontsize =20)\nd2l.plt.ylabel( \"Observations y \", fontsize =20)\nd2l.plt.show()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "10c5b82a-6940-40fa-9423-30f346f62f9a": {"__data__": {"id_": "10c5b82a-6940-40fa-9423-30f346f62f9a", "embedding": null, "metadata": {"page_label": "819", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a078d418-b645-42b0-bc41-d0b160fbd004", "node_type": "4", "metadata": {"page_label": "819", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b46212d9d5d0610ad87cb536f4b104a0024c6b1ed2510374aea6f0b537adc9f8", "class_name": "RelatedNodeInfo"}}, "text": "819 Gaussian Process Inference\nHere we see the noisy observations as circles, and the noise-free function in blue that we\nwish to find.\nNow, let\u2019s specify a GP prior over the latent noise-free function, \ud835\udc53\u00b9\ud835\udc65\u00ba\u0018GP\u00b9\ud835\udc5a,\ud835\udc58\u00ba. We\u2019ll\nuse a mean function \ud835\udc5a\u00b9\ud835\udc65\u00ba=0, and an RBF covariance function (kernel)\n\ud835\udc58\u00b9\ud835\udc65\ud835\udc56,\ud835\udc65\ud835\udc57\u00ba=\ud835\udc4e2exp\u0012\n\u00001\n2\u21132jj\ud835\udc65\u0000\ud835\udc650jj2\u0013\n. (18.3.9)\nmean =np.zeros(test_x .shape[ 0])\ncov =d2l.rbfkernel(test_x, test_x, ls =0.2)\nWehavestartedwithalength-scaleof0.2. Beforewefitthedata,itisimportanttoconsider\nwhether we have specified a reasonable prior. Let\u2019s visualize some sample functions from\nthis prior, as well as the 95% credible set (we believe there\u2019s a 95% chance that the true\nfunction is within this region).\nprior_samples =np.random .multivariate_normal(mean =mean, cov =cov, size =5)\nd2l.plt.plot(test_x, prior_samples .T, color ='black ', alpha =0.5)\nd2l.plt.plot(test_x, mean, linewidth =2.)\nd2l.plt.fill_between(test_x, mean -2*np.diag(cov), mean +2*np.diag(cov),\nalpha =0.25 )\nd2l.plt.show()\nDo these samples look reasonable? Are the high-level properties of the functions aligned\nwith the type of data we are trying to model?\nNowlet\u2019sformthemeanandvarianceoftheposteriorpredictivedistributionatanyarbitrary", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f23a7dac-1f1c-49df-91ba-bafb743f5333": {"__data__": {"id_": "f23a7dac-1f1c-49df-91ba-bafb743f5333", "embedding": null, "metadata": {"page_label": "820", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "604fc341-6e0c-4e9e-9b25-38c0ca29f3d9", "node_type": "4", "metadata": {"page_label": "820", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a27ea3375fbf379086724d4442f4f7628e3307e73075f0d4a9738bcf9cc89550", "class_name": "RelatedNodeInfo"}}, "text": "820 Gaussian Processes\ntest point\ud835\udc65\u0003.\n\u00af\ud835\udc53\u0003=\ud835\udc3e\u00b9\ud835\udc65,\ud835\udc65\u0003\u00ba\ud835\udc47\u00b9\ud835\udc3e\u00b9\ud835\udc65,\ud835\udc65\u00ba\u00b8\ud835\udf0e2\ud835\udc3c\u00ba\u00001\ud835\udc66 (18.3.10)\n\ud835\udc49\u00b9\ud835\udc53\u0003\u00ba=\ud835\udc3e\u00b9\ud835\udc65\u0003,\ud835\udc65\u0003\u00ba\u0000\ud835\udc3e\u00b9\ud835\udc65,\ud835\udc65\u0003\u00ba\ud835\udc47\u00b9\ud835\udc3e\u00b9\ud835\udc65,\ud835\udc65\u00ba\u00b8\ud835\udf0e2\ud835\udc3c\u00ba\u00001\ud835\udc3e\u00b9\ud835\udc65,\ud835\udc65\u0003\u00ba (18.3.11)\nBefore we make predictions, we should learn our kernel hyperparameters \ud835\udf03and noise vari-\nance\ud835\udf0e2. Let\u2019s initialize our length-scale at 0.75, as our prior functions looked too quickly\nvarying compared to the data we are fitting. We\u2019ll also guess a noise standard deviation \ud835\udf0e\nof 0.75.\nIn order to learn these parameters, we will maximize the marginal likelihood with respect\nto these parameters.\nlog\ud835\udc5d\u00b9\ud835\udc66j\ud835\udc4b\u00ba=log\u00b9\n\ud835\udc5d\u00b9\ud835\udc66j\ud835\udc53,\ud835\udc4b\u00ba\ud835\udc5d\u00b9\ud835\udc53j\ud835\udc4b\u00ba\ud835\udc51\ud835\udc53 (18.3.12)\nlog\ud835\udc5d\u00b9\ud835\udc66j\ud835\udc4b\u00ba=\u00001\n2\ud835\udc66\ud835\udc47\u00b9\ud835\udc3e\u00b9\ud835\udc65,\ud835\udc65\u00ba\u00b8\ud835\udf0e2\ud835\udc3c\u00ba\u00001\ud835\udc66\u00001\n2logj\ud835\udc3e\u00b9\ud835\udc65,\ud835\udc65\u00ba\u00b8\ud835\udf0e2\ud835\udc3cj\u0000\ud835\udc5b\n2log 2\ud835\udf0b(18.3.13)\nPerhaps our prior functions were too quickly varying. Let\u2019s guess a length-scale of 0.4.\nWe\u2019ll also guess a noise standard deviation of 0.75. These are simply hyperparameter ini-\ntializations \u2014 we will learn these parameters from the marginal likelihood.\nell_est =0.4\npost_sig_est =0.5\ndef neg_MLL (pars):\nK=d2l.rbfkernel(train_x, train_x, ls =pars[ 0])\nkernel_term =-0.5 *train_y @\\\nnp.linalg .inv(K +pars[ 1]**2*np.eye(train_x .shape[ 0])) @train_y\nlogdet =-0.5 *np.log(np .linalg .det(K +pars[ 1]**2*\\\nnp.eye(train_x .shape[ 0])))\nconst =-train_x .shape[ 0]/2.*np.log( 2*np.pi)\nreturn -(kernel_term +logdet +const)\nlearned_hypers =optimize .minimize(neg_MLL, x0 =np.array([ell_est,post_sig_\n\u21a9!est]),\nbounds =((0.01 ,10.), ( 0.01 ,10.)))\nell =learned_hypers .x[0]\npost_sig_est =learned_hypers .x[1]\nIn this instance, we learn a length-scale of 0.299, and a noise standard deviation of 0.24.\nNote that the learned noise is extremely close to the true noise, which helps indicate that\nour GP is a very well-specified to this problem.\nIn general, it is crucial to put careful thought into selecting the kernel and initializing the\nhyperparameters. While marginal likelihood optimization can be relatively robust to ini-\ntialization, it is not immune to poor initializations. Try running the above script with a\nvariety of initializations and see what results you find.\nNow, let\u2019s make predictions with these learned hypers.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5df50c6e-d10f-43cb-8791-ba26e34c1b23": {"__data__": {"id_": "5df50c6e-d10f-43cb-8791-ba26e34c1b23", "embedding": null, "metadata": {"page_label": "821", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2ec51096-01d3-4ed2-88a1-83e80259a523", "node_type": "4", "metadata": {"page_label": "821", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2e3d58e1c7ba7bd392d53428d4003fd75e27c5d4ad6a2e77b8b319a84c370403", "class_name": "RelatedNodeInfo"}}, "text": "821 Gaussian Process Inference\nK_x_xstar =d2l.rbfkernel(train_x, test_x, ls =ell)\nK_x_x =d2l.rbfkernel(train_x, train_x, ls =ell)\nK_xstar_xstar =d2l.rbfkernel(test_x, test_x, ls =ell)\npost_mean =K_x_xstar .T@np.linalg .inv((K_x_x +\\\npost_sig_est **2*np.eye(train_x .shape[ 0]))) @train_y\npost_cov =K_xstar_xstar -K_x_xstar .T@np.linalg .inv((K_x_x +\\\npost_sig_est **2*np.eye(train_x .shape[ 0]))) @K_x_xstar\nlw_bd =post_mean -2*np.sqrt(np .diag(post_cov))\nup_bd =post_mean +2*np.sqrt(np .diag(post_cov))\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y, linewidth =2.)\nd2l.plt.plot(test_x, post_mean, linewidth =2.)\nd2l.plt.fill_between(test_x, lw_bd, up_bd, alpha =0.25 )\nd2l.plt.legend([ 'Observed Data ','True Function ','Predictive Mean ','95%Set\u2423\n\u21a9!on True Func '])\nd2l.plt.show()\nWe see the posterior mean in orange almost perfectly matches the true noise free function!\nNote that the 95% credible set we are showing is for the latent noise free (true) function,\nandnotthedatapoints. Weseethatthiscrediblesetentirelycontainsthetruefunction,and\ndoes not seem overly wide or narrow. We would not want nor expect it to contain the data\npoints. If we wish to have a credible set for the observations, we should compute\nlw_bd_observed =post_mean -2*np.sqrt(np .diag(post_cov) +post_sig_est **2)\nup_bd_observed =post_mean +2*np.sqrt(np .diag(post_cov) +post_sig_est **2)\nThere are two sources of uncertainty, epistemic uncertainty, representing reducible uncer-\ntainty, and aleatoric orirreducible uncertainty. The epistemic uncertainty here represents\nuncertainty about the true values of the noise free function. This uncertainty should grow\nas we move away from the data points, as away from the data there are a greater variety of\nfunction values consistent with our data. As we observe more and more data, our beliefs\nabout the true function become more confident, and the epistemic uncertainty disappears.\nThealeatoric uncertainty in this instance is the observation noise, since the data are given\nto us with this noise, and it cannot be reduced.\nTheepistemic uncertaintyinthedataiscapturedbyvarianceofthelatentnoisefreefunction\nnp.diag(post_cov). The aleatoric uncertaintyiscapturedbythenoisevariancepost_sig_est**2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a410acb2-e91f-4ba1-aae6-36dca069cfd7": {"__data__": {"id_": "a410acb2-e91f-4ba1-aae6-36dca069cfd7", "embedding": null, "metadata": {"page_label": "822", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a41b72f-eb21-4a70-9f48-fa7efccc8866", "node_type": "4", "metadata": {"page_label": "822", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e7e0eba99c4371e2ceab26dbb491056abae938e29fbfd348404466aaf1c3d001", "class_name": "RelatedNodeInfo"}}, "text": "822 Gaussian Processes\nUnfortunately, people are often careless about how they represent uncertainty, with many\npapers showing error bars that are completely undefined, no clear sense of whether we are\nvisualizing epistemic or aleatoric uncertainty or both, and confusing noise variances with\nnoise standard deviations, standard deviations with standard errors, confidence intervals\nwith credible sets, and so on. Without being precise about what the uncertainty represents,\nit is essentially meaningless.\nIn the spirit of playing close attention to what our uncertainty represents, it is crucial to\nnote that we are taking twotimes thesquareroot of our variance estimate for the noise free\nfunction. Since our predictive distribution is Gaussian, this quantity enables us to form a\n95% credible set, representing our beliefs about the interval which is 95% likely to contain\nthe ground truth function. The noise variance is living on a completely different scale, and\nis much less interpretable.\nFinally, let\u2019s take a look at 20 posterior samples. These samples tell us what types of\nfunctions we believe might fit our data, a posteriori.\npost_samples =np.random .multivariate_normal(post_mean, post_cov, size =20)\nd2l.plt.scatter(train_x, train_y)\nd2l.plt.plot(test_x, test_y, linewidth =2.)\nd2l.plt.plot(test_x, post_mean, linewidth =2.)\nd2l.plt.plot(test_x, post_samples .T, color ='gray ', alpha =0.25 )\nd2l.plt.fill_between(test_x, lw_bd, up_bd, alpha =0.25 )\nplt.legend([ 'Observed Data ','True Function ','Predictive Mean ','Posterior \u2423\n\u21a9!Samples '])\nd2l.plt.show()\nIn basic regression applications, it is most common to use the posterior predictive mean\nand standard deviation as a point predictor and metric for uncertainty, respectively. In\nmore advanced applications, such as Bayesian optimization with Monte Carlo acquisition\nfunctions, or Gaussian processes for model-based RL, it often necessary to take posterior\nsamples. However, even if not strictly required in the basic applications, these samples\ngive us more intuition about the fit we have for the data, and are often useful to include in\nvisualizations.\n18.3.5Making LifeEasy with GPyTorch\nAs we have seen, it is actually pretty easy to implement basic Gaussian process regres-\nsion entirely from scratch. However, as soon as we want to explore a variety of kernel", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "74605981-9cf7-4e6a-9992-0b05073d65d7": {"__data__": {"id_": "74605981-9cf7-4e6a-9992-0b05073d65d7", "embedding": null, "metadata": {"page_label": "823", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ffe1006-dd87-4936-a453-7e81e59b4232", "node_type": "4", "metadata": {"page_label": "823", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7d441da4380b630caa32d3ef94189e5ef3ed325f8e5b18c8c67ca130c5396cfa", "class_name": "RelatedNodeInfo"}}, "text": "823 Gaussian Process Inference\n262\n263choices, considerapproximateinference(whichisneededevenforclassification), combine\nGPs with neural networks, or even have a dataset larger than about 10,000 points, then an\nimplementationfromscratchbecomesunwieldyandcumbersome. Someofthemosteffec-\ntive methods for scalable GP inference, such as SKI (also known as KISS-GP), can require\nhundredsoflinesofcodeimplementingadvancednumericallinearalgebraroutines.\nIn these cases, the GPyTorch library will make our lives a lot easier. We\u2019ll be discussing\nGPyTorchmoreinfuturenotebooksonGaussianprocessnumerics,andadvancedmethods.\nThe GPyTorch library contains many examples262. To get a feel for the package, we will\nwalkthroughthe simpleregressionexample263,showinghowitcanbeadaptedtoreproduce\nouraboveresultsusingGPyTorch. Thismayseemlikealotofcodetosimplyreproducethe\nbasicregressionabove,andinasense,itis. Butwecanimmediatelyuseavarietyofkernels,\nscalable inference techniques, and approximate inference, by only changing a few lines of\ncode from below, instead of writing potentially thousands of lines of new code.\n# First let's convert our data into tensors for use with PyTorch\ntrain_x =torch .tensor(train_x)\ntrain_y =torch .tensor(train_y)\ntest_y =torch .tensor(test_y)\n# We are using exact GP inference with a zero mean and RBF kernel\nclass ExactGPModel (gpytorch .models .ExactGP):\ndef __init__ (self , train_x, train_y, likelihood):\nsuper (ExactGPModel, self ).__init__ (train_x, train_y, likelihood)\nself .mean_module =gpytorch .means .ZeroMean()\nself .covar_module =gpytorch .kernels .ScaleKernel(\ngpytorch .kernels .RBFKernel())\ndef forward (self , x):\nmean_x =self .mean_module(x)\ncovar_x =self .covar_module(x)\nreturn gpytorch .distributions .MultivariateNormal(mean_x, covar_x)\nThis code block puts the data in the right format for GPyTorch, and specifies that we are\nusing exact inference, as well the mean function (zero) and kernel function (RBF) that\nwe want to use. We can use any other kernel very easily, by calling, for instance, gpy-\ntorch.kernels.matern_kernel(), or gpyotrch.kernels.spectral_mixture_kernel(). So far, we\nhave only discussed exact inference, where it is possible to infer a predictive distribution\nwithout making any approximations. For Gaussian processes, we can only perform exact\ninferencewhenwehaveaGaussianlikelihood; morespecifically, whenweassumethatour\nobservationsaregeneratedasanoise-freefunctionrepresentedbyaGaussianprocess,plus\nGaussiannoise. Infuturenotebooks, wewillconsiderothersettings, suchasclassification,\nwhere we cannot make these assumptions.\n# Initialize Gaussian likelihood\nlikelihood =gpytorch .likelihoods .GaussianLikelihood()\nmodel =ExactGPModel(train_x, train_y, likelihood)\ntraining_iter =50\n# Find optimal model hyperparameters\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d67530fe-05bb-492c-8137-8998b5ebbca6": {"__data__": {"id_": "d67530fe-05bb-492c-8137-8998b5ebbca6", "embedding": null, "metadata": {"page_label": "824", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1b0a366f-eac6-4b24-bd1e-f195267bc162", "node_type": "4", "metadata": {"page_label": "824", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1659d14d05ab700db6d8d3257bb0a9c3a0453d200be99e2d8c6ae3d065126185", "class_name": "RelatedNodeInfo"}}, "text": "824 Gaussian Processes\n(continued from previous page)\nmodel .train()\nlikelihood .train()\n# Use the adam optimizer, includes GaussianLikelihood parameters\noptimizer =torch .optim .Adam(model .parameters(), lr =0.1)\n# Set our loss as the negative log GP marginal likelihood\nmll =gpytorch .mlls .ExactMarginalLogLikelihood(likelihood, model)\nHere, we explicitly specify the likelihood we want to use (Gaussian), the objective we will\nuse for training kernel hyperparameters (here, the marginal likelihood), and the procedure\nwe we want to use for optimizing that objective (in this case, Adam). We note that while\nwe are using Adam, which is a \u201cstochastic\u201d optimizer, in this case, it is full-batch Adam.\nBecause the marginal likelihood does not factorize over data instances, we cannot use an\noptimizer over \u201cmini-batches\u201d of data and be guaranteed convergence. Other optimizers,\nsuchasL-BFGS,arealsosupportedbyGPyTorch. Unlikeinstandarddeeplearning, doing\na good job of optimizing the marginal likelihood corresponds strongly with good general-\nization, which often inclines us towards powerful optimizers like L-BFGS, assuming they\nare not prohibitively expensive.\nfor iinrange (training_iter):\n# Zero gradients from previous iteration\noptimizer .zero_grad()\n# Output from model\noutput =model(train_x)\n# Calc loss and backprop gradients\nloss =-mll(output, train_y)\nloss .backward()\nifi%10==0:\nprint (f'Iter {i+1:d}/{training_iter :d}- Loss: {loss .item() :.3f}'\nf'squared lengthscale: '\nf'{model .covar_module .base_kernel .lengthscale .item() :.3f}'\nf'noise variance: {model .likelihood .noise .item() :.3f}')\noptimizer .step()\nIter 1/50-Loss: 1.000 squared lengthscale: 0.693 noise variance: 0.693\nIter 11/50-Loss: 0.711 squared lengthscale: 0.490 noise variance: 0.312\nIter 21/50-Loss: 0.451 squared lengthscale: 0.506 noise variance: 0.127\nIter 31/50-Loss: 0.330 squared lengthscale: 0.485 noise variance: 0.055\nIter 41/50-Loss: 0.344 squared lengthscale: 0.472 noise variance: 0.038\nHere we actually run the optimization procedure, outputting the values of the loss every 10\niterations.\n# Get into evaluation (predictive posterior) mode\ntest_x =torch .tensor(test_x)\nmodel .eval()\nlikelihood .eval()\nobserved_pred =likelihood(model(test_x))\nThe above codeblock enables us to make predictions on our test inputs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c307ee79-5254-4d9f-af82-9b67c0de71e1": {"__data__": {"id_": "c307ee79-5254-4d9f-af82-9b67c0de71e1", "embedding": null, "metadata": {"page_label": "825", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32d20442-5c93-413d-adba-c9221154560f", "node_type": "4", "metadata": {"page_label": "825", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "162d039a2c46ca4e621c8f8dafe25d9d6446e1ccfb20550161c23f6f4837fb95", "class_name": "RelatedNodeInfo"}}, "text": "825 Gaussian Process Inference\nwith torch .no_grad():\n# Initialize plot\nf, ax =d2l.plt.subplots( 1,1, figsize =(4,3))\n# Get upper and lower bounds for 95\\% credible set (in this case, in\n# observation space)\nlower, upper =observed_pred .confidence_region()\nax.scatter(train_x .numpy(), train_y .numpy())\nax.plot(test_x .numpy(), test_y .numpy(), linewidth =2.)\nax.plot(test_x .numpy(), observed_pred .mean .numpy(), linewidth =2.)\nax.fill_between(test_x .numpy(), lower .numpy(), upper .numpy(), alpha =0.25 )\nax.set_ylim([ -1.5,1.5])\nax.legend([ 'True Function ','Predictive Mean ','Observed Data ',\n'95%Credible Set '])\nFinally, we plot the fit.\nWe see the fits are virtually identical. A few things to note: GPyTorch is working with\nsquared length-scales and observation noise. For example, our learned noise standard de-\nviation in the for scratch code is about 0.283. The noise variance found by GPyTorch is\n0.81\u00190.2832. In the GPyTorch plot, we also show the credible set in the observation\nspacerather than the latent function space, to demonstrate that they indeed cover the ob-\nserved datapoints.\n18.3.6Summary\nWe can combine a Gaussian process prior with data to form a posterior, which we use to\nmake predictions. We can also form a marginal likelihood, which is useful for automatic\nlearningofkernelhyperparameters,whichcontrolpropertiessuchastherateofvariationof\ntheGaussianprocess. Themechanicsofformingtheposteriorandlearningkernelhyperpa-\nrameters for regression are simple, involving about a dozen lines of code. This notebook is\nagoodreferenceforanyreaderwantingtoquicklyget\u201cupandrunning\u201dwithGaussianpro-\ncesses. We also introduced the GPyTorch library. Although the GPyTorch code for basic\nregression is relatively long, it can be trivially modified for other kernel functions, or more\nadvanced functionality we will discuss in future notebooks, such as scalable inference, or\nnon-Gaussian likelihoods for classification.\n18.3.7Exercises", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1956, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f116b160-1b0b-4177-b659-0c0790d1580e": {"__data__": {"id_": "f116b160-1b0b-4177-b659-0c0790d1580e", "embedding": null, "metadata": {"page_label": "826", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "39da3daf-dac6-4def-bedb-9bcecdf5e61c", "node_type": "4", "metadata": {"page_label": "826", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d857ce277f451ab26ccd8188b383af9f93e93107d111ec4b43451f39abff20cd", "class_name": "RelatedNodeInfo"}}, "text": "826 Gaussian Processes\n1.We have emphasized the importance of learning kernel hyperparameters, and the effect\nof hyperparameters and kernels on the generalization properties of Gaussian processes.\nTryskippingthestepwherewelearnhypers,andinsteadguessavarietyoflength-scales\nand noise variances, and check their effect on predictions. What happens when you use\na large length-scale? A small length-scale? A large noise variance? A small noise\nvariance?\n2.We have said that the marginal likelihood is not a convex objective, but that hyperpa-\nrameterslikelength-scaleandnoisevariancecanbereliablyestimatedinGPregression.\nThisisgenerallytrue\u2014infact,themarginallikelihoodis muchbetteratlearninglength-\nscale hyperparameters than conventional approaches in spatial statistics, which involve\nfittingempiricalautocorrelationfunctions(\u201ccovariograms\u201d). Arguably,thebiggestcon-\ntributionfrommachinelearningtoGaussianprocessresearch,atleastbeforerecentwork\nonscalableinference,wastheintroductionofthemarginallkelihoodforhyperparameter\nlearning.\nHowever , different pairings of even these parameters provide interpretably different plau-\nsible explanations for many datasets, leading to local optima in our objective. If we use a\nlarge length-scale, then we assume the true underlying function is slowly varying. If the\nobserved data arevarying significantly, then the only we can plausibly have a large length-\nscaleiswithalargenoise-variance. Ifweuseasmalllength-scale,ontheotherhand,ourfit\nwill be very sensitive to the variations in the data, leaving little room to explain variations\nwith noise (aleatoric uncertainty).\nTry seeing if you can find these local optima: initialize with very large length-scale with\nlarge noise, and small length-scales with small noise. Do you converge to different solu-\ntions?\n3.We have said that a fundamental advantage of Bayesian methods is in naturally repre-\nsentingepistemic uncertainty. In the above example, we cannot fully see the effects of\nepistemic uncertainty. Try instead to predict with test_x = np.linspace(0, 10,\n1000). What happens to the 95% credible set as your predictions move beyond the\ndata? Does it cover the true function in that interval? What happens if you only visual-\nize aleatoric uncertainty in that region?\n4.Try running the above example, but instead with 10,000, 20,000 and 40,000 training\npoints,andmeasuretheruntimes. Howdoesthetrainingtimescale? Alternatively,how\ndo the runtimes scale with the number of test points? Is it different for the predictive\nmean and the predictive variance? Answer this question both by theoretically working\nout the training and testing time complexities, and by running the code above with a\ndifferent number of points.\n5.Try running the GPyTorch example with different covariance functions, such as the\nMatern kernel. How do the results change? How about the spectral mixture kernel,\nfound in the GPyTorch library? Are some easier to train the marginal likelihood than\nothers? Are some more valuable for long-range versus short-range predictions?\n6.In our GPyTorch example, we plotted the predictive distribution including observation", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49190b57-77dc-4635-89f9-ffa1a07ed402": {"__data__": {"id_": "49190b57-77dc-4635-89f9-ffa1a07ed402", "embedding": null, "metadata": {"page_label": "827", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a36e428a-710a-47dc-8317-81cf415ab194", "node_type": "4", "metadata": {"page_label": "827", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "90ac6af1921a519cccbfdb9dde1bc37dac04bc63c9581b0fc9a54a5dc1d0f652", "class_name": "RelatedNodeInfo"}}, "text": "827 Gaussian Process Inference\n264noise, while in our \u201cfrom scratch\u201d example, we only included epistemic uncertainty.\nRe-do the GPyTorch example, but this time only plotting epistemic uncertainty, and\ncomparetothefrom-scratchresults. Dothepredictivedistributionsnowlookthesame?\n(They should.)\nDiscussions264.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 308, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ea66508-37cc-4c0a-a121-3213ec875673": {"__data__": {"id_": "8ea66508-37cc-4c0a-a121-3213ec875673", "embedding": null, "metadata": {"page_label": "828", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b1235b0-ce4f-4239-80ed-bf49f662714e", "node_type": "4", "metadata": {"page_label": "828", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c325891e0c4dcc545a296cd478a07502701b1d393cf2ad1546ccc513cf412395", "class_name": "RelatedNodeInfo"}}, "text": "19 Hyperparameter Optimization\nAaron Klein (Amazon),Matthias Seeger (Amazon), andCedric Archambeau (Ama-\nzon)\nThe performance of every machine learning model depends on its hyperparameters. They\ncontrolthelearningalgorithmorthestructureoftheunderlyingstatisticalmodel. However,\nthere is no general way to choose hyperparameters in practice. Instead, hyperparameters\nare often set in a trial-and-error manner or sometimes left to their default values by practi-\ntioners, leading to suboptimal generalization.\nHyperparameter optimization provides a systematic approach to this problem, by casting\nit as an optimization problem: a good set of hyperparameters should (at least) minimize a\nvalidation error. Compared to most other optimization problems arising in machine learn-\ning,hyperparameteroptimizationisanestedone,whereeachiterationrequirestrainingand\nvalidating a machine learning model.\nIn this chapter, we will first introduce the basics of hyperparameter optimization. We will\nalsopresentsomerecentadvancementsthatimprovetheoverallefficiencyofhyperparame-\nter optimization by exploiting cheap-to-evaluate proxies of the original objective function.\nAt the end of this chapter, you should be able to apply state-of-the-art hyperparameter\noptimization techniques to optimize the hyperparameter of your own machine learning al-\ngorithm.\n19.1What Is HyperparameterOptimization?\nAs we have seen in the previous chapters, deep neural networks come with a large number\nofparametersorweightsthatarelearnedduringtraining. Ontopofthese,everyneuralnet-\nwork has additional hyperparameters that need to be configured by the user. For example,\nto ensure that stochastic gradient descent converges to a local optimum of the training loss\n(seeChapter 12 ), we have to adjust the learning rate and batch size. To avoid overfitting on\ntrainingdatasets,wemighthavetosetregularizationparameters,suchasweightdecay(see\nSection 3.7 ) or dropout (see Section 5.6 ). We can define the capacity and inductive bias of\nthe model by setting the number of layers and number of units or filters per layer (i.e., the\neffective number of weights).\n828", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2126, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c23c5bdf-a5b7-4681-8876-78abeb44ee50": {"__data__": {"id_": "c23c5bdf-a5b7-4681-8876-78abeb44ee50", "embedding": null, "metadata": {"page_label": "829", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2cf657d8-c137-40af-b472-0306010a0b08", "node_type": "4", "metadata": {"page_label": "829", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d0b93cdc97ad78f5dcb0b529721cffa325bd0fec1ce7c113b40e1fd85f904683", "class_name": "RelatedNodeInfo"}}, "text": "829 What Is Hyperparameter Optimization?\nUnfortunately, we cannot simply adjust these hyperparameters by minimizing the training\nloss, because this would lead to overfitting on the training data. For example, setting reg-\nularization parameters, such as dropout or weight decay to zero leads to a small training\nloss, but might hurt the generalization performance.\nSet Hyperparameters\nTrain\nEvaluate DeployLoop until validation\nperformance is maximised\ntFig. 19.1.1 Typical work\ufb02ow in machine learning that consists of training the model multiple times\nwith different hyperparameters.\nWithouta different form of automation, hyperparameters haveto be setmanuallyin a trial-\nand-errorfashion,inwhatamountstoatime-consuminganddifficultpartofmachinelearn-\ning workflows. For example, consider training a ResNet (see Section 8.6 ) on CIFAR-10,\nwhich requires more than 2 hours on an Amazon Elastic Cloud Compute (EC2) g4dn.\nxlarge instance. Even just trying ten hyperparameter configurations in sequence, this\nwould already take us roughly one day. To make matters worse, hyperparameters are usu-\nallynotdirectlytransferableacrossarchitecturesanddatasets( Bardenet etal.,2013,Feurer\net al., 2022,Wistubaet al., 2018), and need to be re-optimized for every new task. Also,\nfor most hyperparameters, there are no rule-of-thumbs, and expert knowledge is required\nto find sensible values.\nHyperparameter optimization (HPO) algorithms are designed to tackle this problem in a\nprincipled and automated fashion ( Feurer and Hutter, 2018 ), by framing it as a global op-\ntimization problem. The default objective is the error on a hold-out validation dataset, but\ncould in principle be any other business metric. It can be combined with or constrained by\nsecondary objectives, such as training time, inference time, or model complexity.\nRecently, hyperparameter optimization has been extended to neural architecture search\n(NAS)(Elskenet al., 2018,Wistubaet al., 2019), where the goal is to find entirely new\nneural network architectures. Compared to classical HPO, NAS is even more expensive in\nterms of computation and requires additional efforts to remain feasible in practice. Both,\nHPOandNAScanbeconsideredassub-fieldsofAutoML( Hutteretal.,2019),whichaims\nto automate the entire ML pipeline.\nIn this section we will introduce HPO and show how we can automatically find the best\nhyperparameters of the logistic regression example introduced in Section 4.5 .\n19.1.1TheOptimizationProblem\nWe will start with a simple toy problem: searching for the learning rate of the multi-class\nlogisticregressionmodel SoftmaxRegression fromSection4.5 tominimizethevalidation", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4878d4d0-9dcf-4d59-a113-ce59de5a21a4": {"__data__": {"id_": "4878d4d0-9dcf-4d59-a113-ce59de5a21a4", "embedding": null, "metadata": {"page_label": "830", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5e7b054-13fb-4818-8455-0aa0a204501b", "node_type": "4", "metadata": {"page_label": "830", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "95f60a16765942456aecdd540a71e3a3be1ca99a46c9a1cf999d3dfeae207fc9", "class_name": "RelatedNodeInfo"}}, "text": "830 Hyperparameter Optimization\nerror on the Fashion MNIST dataset. While other hyperparameters like batch size or num-\nber of epochs are also worth tuning, we focus on learning rate alone for simplicity.\nimport numpy asnp\nimport torch\nfrom scipy import stats\nfrom torch import nn\nfrom d2l import torch asd2l\nBefore we can run HPO, we first need to define two ingredients: the objective function and\nthe configuration space.\nTheObjectiveFunction\nThe performance of a learning algorithm can be seen as a function \ud835\udc53:X! Rthat maps\nfrom the hyperparameter space x2Xto the validation loss. For every evaluation of \ud835\udc53\u00b9x\u00ba,\nwe have to train and validate our machine learning model, which can be time and compute\nintensive in the case of deep neural networks trained on large datasets. Given our criterion\n\ud835\udc53\u00b9x\u00baour goal is to find x\u26052argminx2X\ud835\udc53\u00b9x\u00ba.\nThereisnosimplewaytocomputegradientsof \ud835\udc53withrespectto x,becauseitwouldrequire\nto propagate the gradient through the entire training process. While there is recent work\n(Franceschi etal.,2017,Maclaurin etal.,2015)todriveHPObyapproximate\u201chypergradi-\nents\u201d, none of the existing approaches are competitive with the state-of-the-art yet, and we\nwillnotdiscussthemhere. Furthermore,thecomputationalburdenofevaluating \ud835\udc53requires\nHPO algorithms to approach the global optimum with as few samples as possible.\nThe training of neural networks is stochastic (e.g., weights are randomly initialized, mini-\nbatchesarerandomlysampled),sothatourobservationswillbenoisy: \ud835\udc66\u0018\ud835\udc53\u00b9x\u00ba\u00b8\ud835\udf16,where\nwe usually assume that the \ud835\udf16\u0018\ud835\udc41\u00b90,\ud835\udf0e\u00baobservation noise is Gaussian distributed.\nFaced with all these challenges, we usually try to identify a small set of well performing\nhyperparameter configurations quickly, instead of hitting the global optima exactly. How-\never, due to large computational demands of most neural networks models, even this can\ntake days or weeks of compute. We will explore in Section 19.4 how we can speed-up the\noptimization process by either distributing the search or using cheaper-to-evaluate approx-\nimations of the objective function.\nWe begin with a method for computing the validation error of a model.\nclass HPOTrainer (d2l .Trainer): #@save\ndef validation_error (self ):\nself .model .eval()\naccuracy =0\nval_batch_idx =0\nfor batch inself .val_dataloader:\nwith torch .no_grad():\nx, y =self .prepare_batch(batch)\ny_hat =self .model(x)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2394, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "986cce1c-46ed-4ef7-b82f-de28706cb40a": {"__data__": {"id_": "986cce1c-46ed-4ef7-b82f-de28706cb40a", "embedding": null, "metadata": {"page_label": "831", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf1d45f7-7a98-4cfd-9fc5-cc9c550f4904", "node_type": "4", "metadata": {"page_label": "831", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cbaf3f9b5de783c6e807a0f25ec453afe1836fceb161552a58da524205291880", "class_name": "RelatedNodeInfo"}}, "text": "831 What Is Hyperparameter Optimization?\n(continued from previous page)\naccuracy +=self .model .accuracy(y_hat, y)\nval_batch_idx +=1\nreturn 1-accuracy /val_batch_idx\nWe optimize validation error with respect to the hyperparameter configuration config,\nconsistingofthe learning_rate . Foreachevaluation,wetrainourmodelfor max_epochs\nepochs, then compute and return its validation error:\ndef hpo_objective_softmax_classification (config, max_epochs =8):\nlearning_rate =config[ \"learning_rate \"]\ntrainer =d2l.HPOTrainer(max_epochs =max_epochs)\ndata =d2l.FashionMNIST(batch_size =16)\nmodel =d2l.SoftmaxRegression(num_outputs =10, lr =learning_rate)\ntrainer .fit(model =model, data =data)\nreturn trainer .validation_error() .detach() .numpy()\nTheConfiguration Space\nAlong with the objective function \ud835\udc53\u00b9x\u00ba, we also need to define the feasible set x2Xto\noptimize over, known as configuration space orsearch space . For our logistic regression\nexample, we will use:\nconfig_space ={\"learning_rate \": stats .loguniform( 1e-4 ,1)}\nHere we use the use the loguniform object from SciPy, which represents a uniform distri-\nbution between -4 and -1 in the logarithmic space. This object allows us to sample random\nvariables from this distribution.\nEachhyperparameterhasadatatype,suchas floatforlearning_rate ,aswellasaclosed\nbounded range (i.e., lower and upper bounds). We usually assign a prior distribution (e.g,\nuniformorlog-uniform)toeachhyperparametertosamplefrom. Somepositiveparameters,\nsuch as learning_rate , are best represented on a logarithmic scale as optimal values can\ndiffer by several orders of magnitude, while others, such as momentum, come with linear\nscale.\nBelow we show a simple example of a configuration space consisting of typical hyperpa-\nrameters of a multi-layer perceptron including their type and standard ranges.\n: Example configuration space of multi-layer perceptron\nTable 19.1.1: label: tab_example_configspace", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1931, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77c0bd3d-6f57-465f-9270-bc7d1f1e00c0": {"__data__": {"id_": "77c0bd3d-6f57-465f-9270-bc7d1f1e00c0", "embedding": null, "metadata": {"page_label": "832", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "94c21bec-196b-40a1-892d-f12d241167ea", "node_type": "4", "metadata": {"page_label": "832", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0ffa6964b4bd20fa9b38472a2c9a0bde8b18f59d963f6ca98bb785207689c229", "class_name": "RelatedNodeInfo"}}, "text": "832 Hyperparameter Optimization\nName Type Hyperparameter\nRangeslog-scale\nlearning rate float :math:\u2018 [10^{-\n6},10^{-1}]\u2018yes\nbatch size integer \u00bb8,256\u00bc yes\nmomentum float \u00bb0,0.99\u00bc no\nactivation function categorical :mat\nh:{textrm{tanh}\n, textrm{relu}}\u000f\nnumber of units integer \u00bb32,1024\u00bc yes\nnumber of layers integer \u00bb1,6\u00bc no\nIn general, the structure of the configuration space Xcan be complex and it can be quite\ndifferent from R\ud835\udc51. In practice, some hyperparameters may depend on the value of others.\nFor example, assume we try to tune the number of layers for a multi-layer perceptron, and\nfor each layer the number of units. The number of units of the \ud835\udc59-th layer is relevant only if\nthe network has at least \ud835\udc59\u00b81layers. These advanced HPO problems are beyond the scope\nof this chapter. We refer the interested reader to ( Baptista and Poloczek, 2018 ,Hutteret\nal., 2011,Jenattonetal., 2017).\nThe configuration space plays an important role for hyperparameter optimization, since\nno algorithms can find something that is not included in the configuration space. On the\nother hand, if the ranges are too large, the computation budget to find well performing\nconfigurations might become infeasible.\n19.1.2RandomSearch\nRandom search is the first hyperparameter optimization algorithm we will consider. The\nmain idea of random search is to independently sample from the configuration space until\na predefined budget (e.g maximum number of iterations) is exhausted, and to return the\nbestobservedconfiguration. Allevaluationscanbeexecutedindependentlyinparallel(see\nSection 19.3 ), but here we use a sequential loop for simplicity.\nerrors, values =[], []\nnum_iterations =5\nfor iinrange (num_iterations):\nlearning_rate =config_space[ \"learning_rate \"].rvs()\nprint (f\"Trial {i}: learning_rate = {learning_rate }\")\ny=hpo_objective_softmax_classification({ \"learning_rate \": learning_rate})\nprint (f\" validation_error = {y}\")\nvalues .append(learning_rate)\nerrors .append(y)\nvalidation_error =0.17070001363754272\nThe best learning rate is then simply the one with the lowest validation error.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2082, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b721d2cd-c81b-4c99-9699-fa5d6786041a": {"__data__": {"id_": "b721d2cd-c81b-4c99-9699-fa5d6786041a", "embedding": null, "metadata": {"page_label": "833", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31fac131-303b-483f-a23b-4479b48169d4", "node_type": "4", "metadata": {"page_label": "833", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e271de6ec6a62be1516ffe29d5f148a2fa5d5875d12fe2938f6f80685d36600a", "class_name": "RelatedNodeInfo"}}, "text": "833 19.1 What Is Hyperparameter Optimization?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 45, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb8fe440-ee71-46ef-ae48-7b27f9777a28": {"__data__": {"id_": "cb8fe440-ee71-46ef-ae48-7b27f9777a28", "embedding": null, "metadata": {"page_label": "834", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a6b0afe5-5335-45b7-870b-94c517864f57", "node_type": "4", "metadata": {"page_label": "834", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f9b9fa111f2d068da69ee1b84d7c2bc38ec3f503bde5a3daa2c9b92d6d0dd122", "class_name": "RelatedNodeInfo"}}, "text": "834 Hyperparameter Optimization\nbest_idx =np.argmin(errors)\nprint (f\"optimal learning rate = {values[best_idx] }\")\noptimal learning rate =0.09844872561810249\nDuetoitssimplicityandgenerality, randomsearchisoneofthemostfrequentlyusedHPO\nalgorithms. It does not require any sophisticated implementation and can be applied to\nany configuration space as long as we can define some probability distribution for each\nhyperparameter.\nUnfortunately random search also comes with a few shortcomings. First, it does not adapt\nthe sampling distribution based on the previous observations it collected so far. Hence,\nit is equally likely to sample a poorly performing configuration than a better performing\nconfiguration. Second, the same amount of resources are spent for all configurations, even\nthoughsomemayshowpoorinitialperformanceandarelesslikelytooutperformpreviously\nseen configurations.\nIn the next sections we will look at more sample efficient hyperparameter optimization\nalgorithms that overcome the shortcomings of random search by using a model to guide\nthe search. We will also look at algorithms that automatically stop the evaluation process\nof poorly performing configurations to speed up the optimization process.\n19.1.3Summary\nIn this section we introduced hyperparameter optimization (HPO) and how we can phrase\nit as a global optimization by defining a configuration space and an objective function.\nWe also implemented our first HPO algorithm, random search, and applied it on a simple\nsoftmax classification problem.\nWhilerandomsearchisverysimple, itisthebetteralternativetogridsearch, whichsimply\nevaluates a fixed set of hyperparameters. Random search somewhat mitigates the curse\nof dimensionality ( Bellman, 1966 ), and can be far more efficient than grid search if the\ncriterion most strongly depends on a small subset of the hyperparameters.\n19.1.4Exercises", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5db0efcf-c8de-4053-8828-f4a23c6430a2": {"__data__": {"id_": "5db0efcf-c8de-4053-8828-f4a23c6430a2", "embedding": null, "metadata": {"page_label": "835", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e1d98cd-dda5-4e38-912e-9dfa1afbe47a", "node_type": "4", "metadata": {"page_label": "835", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0396785a104ac1dc31d853ac03e0a8970d0169e64c74a74e8ca2f842ebb30c7b", "class_name": "RelatedNodeInfo"}}, "text": "835 What Is Hyperparameter Optimization?\n2651.In this chapter, we optimize the validation error of a model after training on a disjoint\ntraining set. For simplicity, our code uses Trainer.val_dataloader , which maps to a\nloader around FashionMNIST.val .\n1.Convince yourself (by looking at the code) that this means we use the original Fash-\nionMNISTtrainingset(60000examples)fortraining,andtheoriginal testset(10000\nexamples) for validation.\n2.Whycouldthispracticebeproblematic? Hint: Re-read Section3.6 ,especiallyabout\nmodel selection .\n3.What should we have done instead?\n2.Westatedabovethathyperparameteroptimizationbygradientdescentisveryhardtodo.\nConsiderasmallproblem,suchastrainingatwo-layerperceptronontheFashionMNIST\ndataset ( Section 5.2 ) with a batch size of 256. We would like to tune the learning rate\nof SGD in order to minimize a validation metric after one epoch of training.\n1.Why cannot we use validation errorfor this purpose? What metric on the validation\nset would you use?\n2.Sketch (roughly) the computational graph of the validation metric after training for\noneepoch. Youmayassumethatinitialweightsandhyperparameters(suchaslearn-\ning rate) are input nodes to this graph. Hint: Re-read about computational graphs in\nSection 5.3 .\n3.Givearoughestimateofthenumberoffloatingpointvaluesyouneedtostoreduring\na forward pass on this graph. Hint: FashionMNIST has 60000 cases. Assume the\nrequired memory is dominated by the activations after each layer, and look up the\nlayer widths in Section 5.2 .\n4.Apart from the sheer amount of compute and storage required, what other issues\nwould gradient-based hyperparameter optimization run into? Hint: Re-read about\nvanishing and exploding gradients in Section 5.4 .\n5.Advanced : Read (Maclaurin et al., 2015) for an elegant (yet still somewhat unprac-\ntical) approach to gradient-based HPO.\n3.Grid search is another HPO baseline, where we define an equi-spaced grid for each hy-\nperparameter,theniterateoverthe(combinatorial)Cartesianproductinordertosuggest\nconfigurations.\n1.We stated above that random search can be much more efficient than grid search for\nHPOonasizablenumberofhyperparameters, ifthecriterionmoststronglydepends\non a small subset of the hyperparameters. Why is this? Hint: Read ( Bergstraet al.,\n2011).\nDiscussions265.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc32a593-37a6-4b89-a987-aa526e29baa5": {"__data__": {"id_": "fc32a593-37a6-4b89-a987-aa526e29baa5", "embedding": null, "metadata": {"page_label": "836", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65db84e1-f92a-45fb-bc28-27bd55d563cd", "node_type": "4", "metadata": {"page_label": "836", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "41f68b14086997ae049d62414477b3d9a8fd942775941436586de4b4893603a7", "class_name": "RelatedNodeInfo"}}, "text": "836 Hyperparameter Optimization\n19.2HyperparameterOptimizationAPI\nBefore we dive into the methodology, we will first discuss a basic code structure that al-\nlows us to efficiently implement various HPO algorithms. In general, all HPO algorithms\nconsideredhereneedtoimplementtwodecisionmakingprimitives, searching andschedul-\ning. First, they need to sample new hyperparameter configurations, which often involves\nsome kind of search over the configuration space. Second, for each configuration, an HPO\nalgorithm needs to schedule its evaluation and decide how many resources to allocate for\nit. Once we start to evaluate a configuration, we will refer to it as a trial. We map these\ndecisionstotwoclasses, HPOSearcher andHPOScheduler . Ontopofthat,wealsoprovide\naHPOTuner class that executes the optimization process.\nThis concept of scheduler and searcher is also implemented in popular HPO libraries, such\nas Syne Tune ( Salinaset al., 2022), Ray Tune ( Liawet al., 2018) or Optuna ( Akibaet al.,\n2019).\nimport time\nfrom scipy import stats\nfrom d2l import torch asd2l\n19.2.1Searcher\nBelow we define a base class for searchers, which provides a new candidate configuration\nthrough the sample_configuration function. A simple way to implement this function\nwould be to sample configurations uniformly at random, as we did for random search in\nSection 19.1 . More sophisticated algorithms, such as Bayesian optimization, will make\nthese decisions based on the performance of previous trials. As a result, these algorithms\nare able to sample more promising candidates over time. We add the updatefunction in\norder to update the history of previous trials, which can then be exploited to improve our\nsampling distribution.\nclass HPOSearcher (d2l .HyperParameters): #@save\ndef sample_configuration ()->dict :\nraise NotImplementedError\ndef update (self , config: dict , error: float , additional_info =None ):\npass\nThe following code shows how to implement our random search optimizer from the pre-\nvious section in this API. As a slight extension, we allow the user to prescribe the first\nconfiguration to be evaluated via initial_config , while subsequent ones are drawn at\nrandom.\nclass RandomSearcher (HPOSearcher): #@save\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd7138b1-f320-42a8-8554-925c09e16009": {"__data__": {"id_": "dd7138b1-f320-42a8-8554-925c09e16009", "embedding": null, "metadata": {"page_label": "837", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7338dfe4-7ca9-4526-bcc8-bfb199e92eb5", "node_type": "4", "metadata": {"page_label": "837", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c00312cd908dc1b1f27f116a2741038dbe212116c205ade8cf8725c8710caeaf", "class_name": "RelatedNodeInfo"}}, "text": "837 Hyperparameter Optimization API\n(continued from previous page)\ndef __init__ (self , config_space: dict , initial_config =None ):\nself .save_hyperparameters()\ndef sample_configuration (self )->dict :\nifself .initial_config isnot None :\nresult =self .initial_config\nself .initial_config =None\nelse :\nresult ={\nname: domain .rvs()\nfor name, domain inself .config_space .items()\n}\nreturn result\n19.2.2Scheduler\nBeyond sampling configurations for new trials, we also need to decide when and for how\nlong to run a trial. In practice, all these decisions are done by the HPOScheduler , which\ndelegates the choice of new configurations to a HPOSearcher . The suggest method is\ncalled whenever some resource for training becomes available. Apart from invoking sam-\nple_configuration of a searcher, it may also decide upon parameters like max_epochs\n(i.e.,howlongtotrainthemodelfor). The updatemethodiscalledwheneveratrialreturns\na new observation.\nclass HPOScheduler (d2l .HyperParameters): #@save\ndef suggest (self )->dict :\nraise NotImplementedError\ndef update (self , config: dict , error: float , info =None ):\nraise NotImplementedError\nTo implement random search, but also other HPO algorithms, we only need a basic sched-\nulerthatschedulesanewconfigurationeverytimenewresourcesbecomeavailable.\nclass BasicScheduler (HPOScheduler): #@save\ndef __init__ (self , searcher: HPOSearcher):\nself .save_hyperparameters()\ndef suggest (self )->dict :\nreturn self .searcher .sample_configuration()\ndef update (self , config: dict , error: float , info =None ):\nself .searcher .update(config, error, additional_info =info)\n19.2.3Tuner\nFinally,weneedacomponentthatrunsthescheduler/searcheranddoessomebook-keeping\noftheresults. ThefollowingcodeimplementsasequentialexecutionoftheHPOtrialsthat", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1cd795f7-d7aa-42a9-b4b8-41bb723b1f91": {"__data__": {"id_": "1cd795f7-d7aa-42a9-b4b8-41bb723b1f91", "embedding": null, "metadata": {"page_label": "838", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f2e7113-15ed-4db4-981e-09bbc754cd89", "node_type": "4", "metadata": {"page_label": "838", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c28787a99e7946e63c4834d2458b717a88f5ecb06d47b5c8281f8436546aea8f", "class_name": "RelatedNodeInfo"}}, "text": "838 Hyperparameter Optimization\nevaluates one training job after the next and will serve as a basic example. We will later\nuseSyne Tune for more scalable distributed HPO cases.\nclass HPOTuner (d2l .HyperParameters): #@save\ndef __init__ (self , scheduler: HPOScheduler, objective: callable ):\nself .save_hyperparameters()\n# Bookeeping results for plotting\nself .incumbent =None\nself .incumbent_error =None\nself .incumbent_trajectory =[]\nself .cumulative_runtime =[]\nself .current_runtime =0\nself .records =[]\ndef run(self , number_of_trials):\nfor iinrange (number_of_trials):\nstart_time =time .time()\nconfig =self .scheduler .suggest()\nprint (f\"Trial {i}: config = {config }\")\nerror =self .objective( **config)\nerror =float (error .cpu() .detach() .numpy())\nself .scheduler .update(config, error)\nruntime =time .time() -start_time\nself .bookkeeping(config, error, runtime)\nprint (f\" error = {error }, runtime = {runtime }\")\n19.2.4Bookkeepingthe Performanceof HPO Algorithms\nWith any HPO algorithm, we are mostly interested in the best performing configuration\n(calledincumbent ) and its validation error after a given wall-clock time. This is why we\ntrack runtime per iteration, which includes both the time to run an evaluation (call of\nobjective ) and the time to make a decision (call of scheduler.suggest ). In the se-\nquel, wewill plot cumulative_runtime against incumbent_trajectory in order to visu-\nalize theany-timeperformance of the HPO algorithm defined in terms of scheduler (and\nsearcher ). This allows us to quantify not only how well the configuration found by an\noptimizer works, but also how quickly an optimizer is able to find it.\n@d2l .add_to_class(HPOTuner) #@save\ndef bookkeeping (self , config: dict , error: float , runtime: float ):\nself .records .append({ \"config \": config, \"error \": error, \"runtime \": runtime})\n# Check if the last hyperparameter configuration performs better\n# than the incumbent\nifself .incumbent isNone orself .incumbent_error >error:\nself .incumbent =config\nself .incumbent_error =error\n# Add current best observed performance to the optimization trajectory\nself .incumbent_trajectory .append( self .incumbent_error)\n# Update runtime\nself .current_runtime +=runtime\nself .cumulative_runtime .append( self .current_runtime)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2270, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ee354e4-9257-447b-ae48-666606e9a5af": {"__data__": {"id_": "8ee354e4-9257-447b-ae48-666606e9a5af", "embedding": null, "metadata": {"page_label": "839", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f1ac8b69-9df5-4e79-9a21-4dae840f7a17", "node_type": "4", "metadata": {"page_label": "839", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9d100472900dd5d09f132c2a593d53183ca493e1427c27d79079c902a4910508", "class_name": "RelatedNodeInfo"}}, "text": "839 Hyperparameter Optimization API\n19.2.5Example: Optimizingthe Hyperparametersof a Convolutional\nNeuralNetwork\nWe now use our new implementation of random search to optimize the batch size and\nlearning rate of the LeNetconvolutional neural network from Section 7.6 . We being by\ndefining the objective function, which will once more be validation error.\ndef hpo_objective_lenet (learning_rate, batch_size, max_epochs =10): #@save\nmodel =d2l.LeNet(lr =learning_rate, num_classes =10)\ntrainer =d2l.HPOTrainer(max_epochs =max_epochs, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =batch_size)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\ntrainer .fit(model =model, data =data)\nvalidation_error =trainer .validation_error()\nreturn validation_error\nWe also need to define the configuration space. Moreover, the first configuration to be\nevaluated is the default setting used in Section 7.6 .\nconfig_space ={\n\"learning_rate \": stats .loguniform( 1e-2 ,1),\n\"batch_size \": stats .randint( 32,256),\n}\ninitial_config ={\n\"learning_rate \":0.1,\n\"batch_size \":128,\n}\nNow we can start our random search:\nsearcher =RandomSearcher(config_space, initial_config =initial_config)\nscheduler =BasicScheduler(searcher =searcher)\ntuner =HPOTuner(scheduler =scheduler, objective =hpo_objective_lenet)\ntuner .run(number_of_trials =5)\nerror =0.9000097513198853 , runtime =62.85189199447632\nBelowweplottheoptimizationtrajectoryoftheincumbenttogettheany-timeperformance\nof random search:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59fcccaa-b9bf-4754-b54d-4690f97751be": {"__data__": {"id_": "59fcccaa-b9bf-4754-b54d-4690f97751be", "embedding": null, "metadata": {"page_label": "840", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "110cc1fe-ac50-4d5c-bc9e-3e94486485a4", "node_type": "4", "metadata": {"page_label": "840", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "327a43e294d1fa7c4b1d3d0fb5209ff7c1da940a7f8830a4a1a216c4f80d4f1e", "class_name": "RelatedNodeInfo"}}, "text": "840 Hyperparameter Optimization", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 31, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5114c0c-243d-46d2-9027-f3de53cbcf8d": {"__data__": {"id_": "b5114c0c-243d-46d2-9027-f3de53cbcf8d", "embedding": null, "metadata": {"page_label": "841", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed270164-359f-487f-a02d-56f657461b19", "node_type": "4", "metadata": {"page_label": "841", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8e56841da3154572160321723ea03ee29cec5187de17e2f0c4155bb7899f17f2", "class_name": "RelatedNodeInfo"}}, "text": "841 Hyperparameter Optimization API\nboard =d2l.ProgressBoard(xlabel =\"time \", ylabel =\"error \")\nfor time_stamp, error inzip(\ntuner .cumulative_runtime, tuner .incumbent_trajectory\n):\nboard .draw(time_stamp, error, \"random search \", every_n =1)\n19.2.6ComparingHPO Algorithms\nJust as with training algorithms or model architectures, it is important to understand how\nto best compare different HPO algorithms. Each HPO run depends on two major sources\nof randomness: the random effects of the training process, such as random weight initial-\nization or mini-batch ordering, and the intrinsic randomness of the HPO algorithm itself,\nsuch as the random sampling of random search. Hence, when comparing different algo-\nrithms, it is crucial to run each experiment several times and report statistics, such as mean\nor median, across a population of multiple repetitions of an algorithm based on different\nseeds of the random number generator.\nTo illustrate this, we compare random search (see Section 19.1.2 ) and Bayesian optimiza-\ntion (Snoeket al., 2012) on tuning the hyperparameters of a feed-forward neural network.\nEach algorithm was evaluated 50times with a different random seed. The solid line indi-\ncates the average performance of the incumbent across these 50repetitions and the dashed\nline the standard deviation. We can see that random search and Bayesian optimization per-\nform roughly the same up to ~1000 seconds, but Bayesian optimization can make use of\nthe past observation to identify better configurations and thus quickly outperforms random\nsearch afterwards.\ntFig. 19.2.1 Example any-time performance plot to compare two algorithms A and B.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "769fced0-12b8-43cd-97e8-fa27bc48a22e": {"__data__": {"id_": "769fced0-12b8-43cd-97e8-fa27bc48a22e", "embedding": null, "metadata": {"page_label": "842", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56c0033d-29cc-40fe-9cbb-e7b585840e06", "node_type": "4", "metadata": {"page_label": "842", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "12f94990c481c5ec8cf2e8bc996f3d3f17e908b707e92463dd813cc3a34c2b77", "class_name": "RelatedNodeInfo"}}, "text": "842 Hyperparameter Optimization\n26619.2.7Summary\nThis section laid out a simple, yet flexible interface to implement various HPO algorithms\nthat we will look at in this chapter. Similar interfaces can be found in popular open-source\nHPO frameworks. We also looked at how we can compare HPO algorithms, and potential\npitfall one needs to be aware.\n19.2.8Exercises\n1.The goal of this exercise is to implement the objective function for a slightly more chal-\nlengingHPOproblem,andtorunmorerealisticexperiments. Wewillusethetwohidden\nlayer MLP DropoutMLP implemented in Section 5.6 .\n1.Code up the objective function, which should depend on all hyperparameters of the\nmodeland batch_size . Use max_epochs=50 . GPUsdonothelphere,so num_gpus=0 .\nHint: Modify hpo_objective_lenet .\n2.Chooseasensiblesearchspace,where num_hiddens_1 ,num_hiddens_2 areintegers\nin\u00bb8,1024\u00bc,anddropoutvaluesliein \u00bb0,0.95\u00bc,while batch_size liesin\u00bb16,384\u00bc.\nProvide code for config_space , using sensible distributions from scipy.stats .\n3.Run random search on this example with number_of_trials=20 and plot the re-\nsults. Make sure to first evaluate the default configuration of Section 5.6 , which\nisinitial_config = {'num_hiddens_1': 256, 'num_hiddens_2': 256,\n'dropout_1': 0.5, 'dropout_2': 0.5, 'lr': 0.1, 'batch_size': 256} .\n2.In this exercise, you will implement a new searcher (subclass of HPOSearcher ) which\nmakesdecisionsbasedonpastdata. Itdependsonparameters probab_local ,num_init_random .\nItssample_configuration method works as follows. For the first num_init_random\ncalls,dothesameas RandomSearcher.sample_configuration . Otherwise,withprob-\nability 1 - probab_local ,dothesameas RandomSearcher.sample_configuration .\nOtherwise, pick the configuration which attained the smallest validation error so far,\nselect one of its hyperparameters at random, and sample its value randomly like in\nRandomSearcher.sample_configuration , but leave all other values the same. Re-\nturn this configuration, which is identical to the best configuration so far, except in this\none hyperparameter.\n1.Code up this new LocalSearcher . Hint: Your searcher requires config_space as\nargument at construction. Feel free to use a member of type RandomSearcher . You\nwill also have to implement the updatemethod.\n2.Re-run the experiment from the previous exercise, but using your new searcher in-\nstead of RandomSearcher . Experiment with different values for probab_local ,\nnum_init_random . However, note that a proper comparison between different HPO\nmethods requires repeating experiments several times, and ideally considering a\nnumber of benchmark tasks.\nDiscussions266.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3e5dc8f2-7def-4380-9433-ac7e1c908334": {"__data__": {"id_": "3e5dc8f2-7def-4380-9433-ac7e1c908334", "embedding": null, "metadata": {"page_label": "843", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3725b24f-89e2-4b7c-ad3b-1bfffdb29df5", "node_type": "4", "metadata": {"page_label": "843", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8feb4960f1a83c12b0692dce4888c7dbda516ec375f21ab61a075a21c6c964ee", "class_name": "RelatedNodeInfo"}}, "text": "843 Asynchronous Random Search\n19.3AsynchronousRandom Search\nAswehaveseenintheprevious Section19.2 , wemighthavetowaithoursorevendaysbe-\nfore random search returns a good hyperparameter configuration, because of the expensive\nevaluation of hyperparameter configurations. In practice, we have often access to a pool of\nresources such as multiple GPUs on the same machine or multiple machines with a single\nGPU. This begs the question: Howdo wee\ufb00iciently distributerandomsearch?\nIngeneral,wedistinguishbetweensynchronousandasynchronousparallelhyperparameter\noptimization (see Fig. 19.3.1 ). In the synchronous setting, we wait for all concurrently\nrunning trials to finish, before we start the next batch. Consider configuration spaces that\ncontain hyperparameters such as the number of filters or number of layers of a deep neural\nnetwork. Hyperparameter configurations that contain a larger number of layers of filters\nwillnaturallytakemoretimetofinish,andallothertrialsinthesamebatchwillhavetowait\natsynchronisationpoints(greyareain Fig.19.3.1 )beforewecancontinuetheoptimization\nprocess.\nIn the asynchronous setting we immediately schedule a new trial as soon as resources be-\ncome available. This will optimally exploit our resources, since we can avoid any synchro-\nnisation overhead. For random search, each new hyperparameter configuration is chosen\nindependentlyofallothers,andinparticularwithoutexploitingobservationsfromanyprior\nevaluation. This means we can trivially parallelize random search asynchronously. This is\nnot straight-forward with more sophisticated methods that make decision based on previ-\nous observations (see Section 19.5 ). While we need access to more resources than in the\nsequential setting, asynchronousrandom searchexhibits a linear speed-up, in that a certain\nperformance is reached \ud835\udc3etimes faster if \ud835\udc3etrials can be run in parallel.\nSequential Trial-0 Trial-1 Trial-2 Trial-3 Trial-4\nSynchronous\nAsynchronousTrial-0 Trial-2\nTrial-3\nTrial-0\nTimeTrial-5\nTrial-1\nTrial-1Trial-4\nTrial-5\nTrial-3\nTrial-2Trial-4\nTrial-5\ntFig. 19.3.1 Distributing the hyperparameter optimization process either synchronously or\nasynchronously. Compared to the sequential setting, we can reduce the overall wall-clock\ntime while keep the total compute constant. Synchronous scheduling might lead to idling\nworkers in the case of stragglers.\nIn this notebook, we will look at asynchronous random search that, where trials are exe-\ncuted in multiple python processes on the same machine. Distributed job scheduling and\nexecution is difficult to implement from scratch. We will use Syne Tune (Salinaset al.,\n2022), which provides us with a simple interface for asynchronous HPO. Syne Tune is de-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63dca23a-ab70-4bd6-a36e-f3a0be20a20b": {"__data__": {"id_": "63dca23a-ab70-4bd6-a36e-f3a0be20a20b", "embedding": null, "metadata": {"page_label": "844", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e927557-037e-4583-9ac7-66db4dc513ba", "node_type": "4", "metadata": {"page_label": "844", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0c100aa52e76b7f6655eef00a92310afddd0503f1eb5f9672289a8bd3143f2ec", "class_name": "RelatedNodeInfo"}}, "text": "844 Hyperparameter Optimization\nsigned to be run with different execution back-ends, and the interested reader is invited to\nstudy its simple APIs in order to learn more about distributed HPO.\nimport logging\nfrom d2l import torch asd2l\nlogging .basicConfig(level =logging .INFO)\nfrom syne_tune import StoppingCriterion, Tuner\nfrom syne_tune .backend .python_backend import PythonBackend\nfrom syne_tune .config_space import loguniform, randint\nfrom syne_tune .experiments import load_experiment\nfrom syne_tune .optimizer .baselines import RandomSearch\nINFO:root:SageMakerBackend isnot imported since dependencies are missing .You\u2423\n\u21a9!can install them with\npip install 'syne-tune[extra] '\nAWS dependencies are not imported since dependencies are missing .You can \u2423\n\u21a9!install them with\npip install 'syne-tune[aws] '\nor(for everything)\npip install 'syne-tune[extra] '\nAWS dependencies are not imported since dependencies are missing .You can \u2423\n\u21a9!install them with\npip install 'syne-tune[aws] '\nor(for everything)\npip install 'syne-tune[extra] '\nINFO:root:Ray Tune schedulers and searchers are not imported since \u2423\n\u21a9!dependencies are missing .You can install them with\npip install 'syne-tune[raytune] '\nor(for everything)\npip install 'syne-tune[extra] '\n19.3.1ObjectiveFunction\nFirst, we have to define a new objective function such that it now returns the performance\nback to Syne Tune via the reportcallback.\ndef hpo_objective_lenet_synetune (learning_rate, batch_size, max_epochs):\nfrom syne_tune import Reporter\nfrom d2l import torch asd2l\nmodel =d2l.LeNet(lr =learning_rate, num_classes =10)\ntrainer =d2l.HPOTrainer(max_epochs =1, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =batch_size)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\nreport =Reporter()\nfor epoch inrange (1, max_epochs +1):\nifepoch ==1:\n# Initialize the state of Trainer\ntrainer .fit(model =model, data =data)\nelse :\ntrainer .fit_epoch()\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7eb47cb7-6ae8-4c4a-b27a-2e9e8dec25d6": {"__data__": {"id_": "7eb47cb7-6ae8-4c4a-b27a-2e9e8dec25d6", "embedding": null, "metadata": {"page_label": "845", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "530608eb-cdb1-4fce-872b-6a58f7bb0466", "node_type": "4", "metadata": {"page_label": "845", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6cf5b78215f8768e845182c8a20e6089498fb6f1d203975fed481ab34bbbea12", "class_name": "RelatedNodeInfo"}}, "text": "845 Asynchronous Random Search\n(continued from previous page)\nvalidation_error =trainer .validation_error() .cpu() .detach() .numpy()\nreport(epoch =epoch, validation_error =float (validation_error))\nNote that the PythonBackend of Syne Tune requires dependencies to be imported inside\nthe function definition.\n19.3.2AsynchronousScheduler\nFirst, we define the number of workers that evaluate trials concurrently. We also need to\nspecify how long we want to run random search, by defining an upper limit on the total\nwall-clock time.\nn_workers =2# Needs to be <= the number of available GPUs\nmax_wallclock_time =12*60 # 12 minutes\nNext, we state which metric we want to optimize and whether we want to minimize or\nmaximize this metric. Namely, metricneeds to correspond to the argument name passed\nto the reportcallback.\nmode =\"min\"\nmetric =\"validation_error \"\nWe use the configuration space from our previous example. In Syne Tune, this dictionary\ncan also be used to pass constant attributes to the training script. We make use of this\nfeature in order to pass max_epochs . Moreover, we specify the first configuration to be\nevaluated in initial_config .\nconfig_space ={\n\"learning_rate \": loguniform( 1e-2 ,1),\n\"batch_size \": randint( 32,256),\n\"max_epochs \":10,\n}\ninitial_config ={\n\"learning_rate \":0.1,\n\"batch_size \":128,\n}\nNext, we need to specify the back-end for job executions. Here we just consider the distri-\nbution on a local machine where parallel jobs are executed as sub-processes. However, for\nlarge scale HPO, we could run this also on a cluster or cloud environment, where each trial\nconsumes a full instance.\ntrial_backend =PythonBackend(\ntune_function =hpo_objective_lenet_synetune,\nconfig_space =config_space,\n)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1728, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aaf6458c-af1c-4651-bc36-4c9e4d5ed576": {"__data__": {"id_": "aaf6458c-af1c-4651-bc36-4c9e4d5ed576", "embedding": null, "metadata": {"page_label": "846", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d8a0107-2c78-4811-b35a-71a77dd713ef", "node_type": "4", "metadata": {"page_label": "846", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "231c1f1bd61813882c1df451bffefb2e792545b6df524658f5f67bdc677cbddb", "class_name": "RelatedNodeInfo"}}, "text": "846 Hyperparameter Optimization\nWe can now create the scheduler for asynchronous random search, which is similar in be-\nhaviour to our BasicScheduler fromSection 19.2 .\nscheduler =RandomSearch(\nconfig_space,\nmetric =metric,\nmode =mode,\npoints_to_evaluate =[initial_config],\n)\nINFO:syne_tune .optimizer .schedulers .fifo:max_resource_level =10,asinferred \u2423\n\u21a9!from config_space\nINFO:syne_tune .optimizer .schedulers .fifo:Master random_seed =2737092907\nSyne Tune also features a Tuner, where the main experiment loop and bookkeeping is\ncentralized, and interactions between scheduler and back-end are mediated.\nstop_criterion =StoppingCriterion(max_wallclock_time =max_wallclock_time)\ntuner =Tuner(\ntrial_backend =trial_backend,\nscheduler =scheduler,\nstop_criterion =stop_criterion,\nn_workers =n_workers,\nprint_update_interval =int(max_wallclock_time *0.6),\n)\nLetusrunourdistributedHPOexperiment. Accordingtoourstoppingcriterion,itwillrun\nfor about 12 minutes.\ntuner .run()\nINFO:syne_tune .tuner:results of trials will be saved on /home /ci/syne -tune /\n\u21a9!python -entrypoint -2023 -08-18-19-45-39-958\nINFO:root:Detected 4GPUs\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.1 --batch_size 128 --max_epochs 10--tune_function_root \u2423\n\u21a9!/home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/tune_function --\n\u21a9!tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_checkpoint_dir /\n\u21a9!home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/0/checkpoints\nINFO:syne_tune .tuner:(trial 0)-scheduled config { 'learning_rate ':0.1,\n\u21a9!'batch_size ':128,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.1702844732454753 --batch_size 114 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!1/checkpoints\nINFO:syne_tune .tuner:(trial 1)-scheduled config { 'learning_rate ':0.\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8881196-1913-4fcd-bf15-3346ec1f87fd": {"__data__": {"id_": "c8881196-1913-4fcd-bf15-3346ec1f87fd", "embedding": null, "metadata": {"page_label": "847", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d3bb5f7-7610-443f-af36-1cc6c11d6b5f", "node_type": "4", "metadata": {"page_label": "847", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "94a743a709d330854cab2130d3f457edc73c50ae2d555e2a403feb6d24329f28", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2bca6cd1-b02e-4386-8607-9b0734bb8362", "node_type": "1", "metadata": {}, "hash": "a40d4f6058d027d0352ef40c815105a3beea80ca02b7e8b316c37c17f37a5d0f", "class_name": "RelatedNodeInfo"}}, "text": "847 Asynchronous Random Search\n(continued from previous page)\n\u21a9!1702844732454753 ,'batch_size ':114,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 0completed .\nINFO:syne_tune .tuner:Trial trial_id 1completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.34019846567238493 --batch_size 221 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!2/checkpoints\nINFO:syne_tune .tuner:(trial 2)-scheduled config { 'learning_rate ':0.\n\u21a9!34019846567238493 ,'batch_size ':221,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.014628124155727769 --batch_size 88--max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!3/checkpoints\nINFO:syne_tune .tuner:(trial 3)-scheduled config { 'learning_rate ':0.\n\u21a9!014628124155727769 ,'batch_size ':88,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 2completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.1114831485450576 --batch_size 142 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!4/checkpoints\nINFO:syne_tune .tuner:(trial 4)-scheduled config { 'learning_rate ':0.\n\u21a9!1114831485450576 ,'batch_size ':142,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 3completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.014076038679980779 --batch_size 223 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!5/checkpoints\nINFO:syne_tune .tuner:(trial 5)-scheduled config { 'learning_rate ':0.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2bca6cd1-b02e-4386-8607-9b0734bb8362": {"__data__": {"id_": "2bca6cd1-b02e-4386-8607-9b0734bb8362", "embedding": null, "metadata": {"page_label": "847", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7d3bb5f7-7610-443f-af36-1cc6c11d6b5f", "node_type": "4", "metadata": {"page_label": "847", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "94a743a709d330854cab2130d3f457edc73c50ae2d555e2a403feb6d24329f28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8881196-1913-4fcd-bf15-3346ec1f87fd", "node_type": "1", "metadata": {"page_label": "847", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6c4a2bd5404302b75f7294af94b7b4ac4ca764bf3846a4b34d8d664538a67da0", "class_name": "RelatedNodeInfo"}}, "text": "\u21a9!py--learning_rate 0.014076038679980779 --batch_size 223 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!5/checkpoints\nINFO:syne_tune .tuner:(trial 5)-scheduled config { 'learning_rate ':0.\n\u21a9!014076038679980779 ,'batch_size ':223,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 4completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.02558173674804846 --batch_size 62--max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!6/checkpoints\nINFO:syne_tune .tuner:(trial 6)-scheduled config { 'learning_rate ':0.\n\u21a9!02558173674804846 ,'batch_size ':62,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 5completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.026035979388614055 --batch_size 139 --max_epochs 10--\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 2390, "end_char_idx": 3861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7101c4a3-15af-479e-91b7-734c641d02de": {"__data__": {"id_": "7101c4a3-15af-479e-91b7-734c641d02de", "embedding": null, "metadata": {"page_label": "848", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "779ba5bd-e08f-4714-b0c5-43aa1106be38", "node_type": "4", "metadata": {"page_label": "848", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0dd96e195ab1143d5004d029a5fb1d7ccd01d9d0bae57e6c41173a6bdd50d07c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8a58d56-c22e-44c0-b3aa-1c17950e20c2", "node_type": "1", "metadata": {}, "hash": "6819347b6f932364bcd00b1f965f1b35c621415dff25fa725e02fbb3ed480dd2", "class_name": "RelatedNodeInfo"}}, "text": "848 Hyperparameter Optimization\n(continued from previous page)\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!7/checkpoints\nINFO:syne_tune .tuner:(trial 7)-scheduled config { 'learning_rate ':0.\n\u21a9!026035979388614055 ,'batch_size ':139,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 6completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.24202494130424274 --batch_size 231 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!8/checkpoints\nINFO:syne_tune .tuner:(trial 8)-scheduled config { 'learning_rate ':0.\n\u21a9!24202494130424274 ,'batch_size ':231,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 7completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.10483132064775551 --batch_size 145 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!9/checkpoints\nINFO:syne_tune .tuner:(trial 9)-scheduled config { 'learning_rate ':0.\n\u21a9!10483132064775551 ,'batch_size ':145,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 8completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.017898854850751864 --batch_size 51--max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!10/checkpoints\nINFO:syne_tune .tuner:(trial 10)-scheduled config { 'learning_rate ':0.\n\u21a9!017898854850751864 ,'batch_size ':51,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 9completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8a58d56-c22e-44c0-b3aa-1c17950e20c2": {"__data__": {"id_": "f8a58d56-c22e-44c0-b3aa-1c17950e20c2", "embedding": null, "metadata": {"page_label": "848", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "779ba5bd-e08f-4714-b0c5-43aa1106be38", "node_type": "4", "metadata": {"page_label": "848", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0dd96e195ab1143d5004d029a5fb1d7ccd01d9d0bae57e6c41173a6bdd50d07c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7101c4a3-15af-479e-91b7-734c641d02de", "node_type": "1", "metadata": {"page_label": "848", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8ee78a640d3e26e444667f761b050c7455070e905497d1fad598cf10c12d2cdf", "class_name": "RelatedNodeInfo"}}, "text": "\u21a9!017898854850751864 ,'batch_size ':51,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 9completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.9645419978270817 --batch_size 200 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!11/checkpoints\nINFO:syne_tune .tuner:(trial 11)-scheduled config { 'learning_rate ':0.\n\u21a9!9645419978270817 ,'batch_size ':200,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 11completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.10559888854748693 --batch_size 40--max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!12/checkpoints\nINFO:syne_tune .tuner:(trial 12)-scheduled config { 'learning_rate ':0.\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 2460, "end_char_idx": 3857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b96aff72-42bd-4642-967d-fbcaa8044a6e": {"__data__": {"id_": "b96aff72-42bd-4642-967d-fbcaa8044a6e", "embedding": null, "metadata": {"page_label": "849", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e17272c6-7085-4124-be4d-f078d58a8ef6", "node_type": "4", "metadata": {"page_label": "849", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c5ade7a1963842cb82360f8db38d30d787e48a9d17802174b4b3cb69cd50b30b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a1e3edb6-4574-4892-ba50-d1f4dd8ad966", "node_type": "1", "metadata": {}, "hash": "32bc8f15e046edbe9d68c0ef101d8a440b4210760f6b7699538ceaa22820017b", "class_name": "RelatedNodeInfo"}}, "text": "849 Asynchronous Random Search\n(continued from previous page)\n\u21a9!10559888854748693 ,'batch_size ':40,'max_epochs ':10}\nINFO:syne_tune .tuner:tuning status (last metric isreported)\ntrial_id status iter learning_rate batch_size max_epochs epoch \u2423\n\u21a9!validation_error worker -time\n0Completed 10 0.100000 128 10 10.0 \u2423\n\u21a9! 0.277195 64.928907\n1Completed 10 0.170284 114 10 10.0 \u2423\n\u21a9! 0.286225 65.434195\n2Completed 10 0.340198 221 10 10.0 \u2423\n\u21a9! 0.218990 59.729758\n3Completed 10 0.014628 88 10 10.0 \u2423\n\u21a9! 0.899920 81.001636\n4Completed 10 0.111483 142 10 10.0 \u2423\n\u21a9! 0.268684 64.427400\n5Completed 10 0.014076 223 10 10.0 \u2423\n\u21a9! 0.899922 61.264475\n6Completed 10 0.025582 62 10 10.0 \u2423\n\u21a9! 0.399520 75.966186\n7Completed 10 0.026036 139 10 10.0 \u2423\n\u21a9! 0.899988 62.261541\n8Completed 10 0.242025 231 10 10.0 \u2423\n\u21a9! 0.257636 58.186485\n9Completed 10 0.104831 145 10 10.0 \u2423\n\u21a9! 0.273898 59.771699\n10InProgress 8 0.017899 51 10 8.0 \u2423\n\u21a9! 0.496118 66.999746\n11 Completed 10 0.964542 200 10 10.0 \u2423\n\u21a9! 0.181600 59.159662\n12InProgress 0 0.105599 40 10 - \u2423\n\u21a9! - -\n2trials running, 11finished ( 11until the end), 436.60 s wallclock -time\nINFO:syne_tune .tuner:Trial trial_id 10completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.5846051207380589 --batch_size 35--max_epochs 10--tune_\n\u21a9!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!13/checkpoints\nINFO:syne_tune .tuner:(trial 13)-scheduled config { 'learning_rate ':0.\n\u21a9!5846051207380589 ,'batch_size ':35,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 12completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a1e3edb6-4574-4892-ba50-d1f4dd8ad966": {"__data__": {"id_": "a1e3edb6-4574-4892-ba50-d1f4dd8ad966", "embedding": null, "metadata": {"page_label": "849", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e17272c6-7085-4124-be4d-f078d58a8ef6", "node_type": "4", "metadata": {"page_label": "849", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c5ade7a1963842cb82360f8db38d30d787e48a9d17802174b4b3cb69cd50b30b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b96aff72-42bd-4642-967d-fbcaa8044a6e", "node_type": "1", "metadata": {"page_label": "849", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3048eac1b5f814f23d4f6e236ee278c613af1247926843956d2d4cdceef60eb7", "class_name": "RelatedNodeInfo"}}, "text": "\u21a9!5846051207380589 ,'batch_size ':35,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 12completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.2468891379769198 --batch_size 146 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!14/checkpoints\nINFO:syne_tune .tuner:(trial 14)-scheduled config { 'learning_rate ':0.\n\u21a9!2468891379769198 ,'batch_size ':146,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 13completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 1726, "end_char_idx": 2710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0ff30db-152e-43a7-a7c2-af2889767a56": {"__data__": {"id_": "a0ff30db-152e-43a7-a7c2-af2889767a56", "embedding": null, "metadata": {"page_label": "850", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "531f88b7-b6a2-4ccc-ac79-064f19d29c1b", "node_type": "4", "metadata": {"page_label": "850", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a08031dcd156774919bd237047db56262daf26052c7d6834b1044550e47afaf7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d8c0a719-fdfd-4e5a-98bd-ee52a8a2d2f0", "node_type": "1", "metadata": {}, "hash": "8052f05cf78c528bc8beb7bec99d54a5e039d8b8f192024b4efdc56572370d3d", "class_name": "RelatedNodeInfo"}}, "text": "850 Hyperparameter Optimization\n(continued from previous page)\n\u21a9!py--learning_rate 0.12956867470224812 --batch_size 218 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!15/checkpoints\nINFO:syne_tune .tuner:(trial 15)-scheduled config { 'learning_rate ':0.\n\u21a9!12956867470224812 ,'batch_size ':218,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 14completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.24900745354561854 --batch_size 103 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!16/checkpoints\nINFO:syne_tune .tuner:(trial 16)-scheduled config { 'learning_rate ':0.\n\u21a9!24900745354561854 ,'batch_size ':103,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 15completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.03903577426988046 --batch_size 80--max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!17/checkpoints\nINFO:syne_tune .tuner:(trial 17)-scheduled config { 'learning_rate ':0.\n\u21a9!03903577426988046 ,'batch_size ':80,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 16completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.01846559300690354 --batch_size 183 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-\n\u21a9!958/tune_function --tune_function_hash 4d7d5b85e4537ad0c5d0a202623dcec5 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958/\n\u21a9!18/checkpoints\nINFO:syne_tune .tuner:(trial 18)-scheduled config { 'learning_rate ':0.\n\u21a9!01846559300690354 ,'batch_size ':183,'max_epochs ':10}\nINFO:syne_tune .stopping_criterion:reaching max wallclock time ( 720), stopping \u2423\n\u21a9!there .\nINFO:syne_tune .tuner:Stopping trials that may still be running .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8c0a719-fdfd-4e5a-98bd-ee52a8a2d2f0": {"__data__": {"id_": "d8c0a719-fdfd-4e5a-98bd-ee52a8a2d2f0", "embedding": null, "metadata": {"page_label": "850", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "531f88b7-b6a2-4ccc-ac79-064f19d29c1b", "node_type": "4", "metadata": {"page_label": "850", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a08031dcd156774919bd237047db56262daf26052c7d6834b1044550e47afaf7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a0ff30db-152e-43a7-a7c2-af2889767a56", "node_type": "1", "metadata": {"page_label": "850", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "15967da1416d73efb1db20d766c3c3b8cb2f0a0cb19f74578d367f8da4f36356", "class_name": "RelatedNodeInfo"}}, "text": "\u21a9!01846559300690354 ,'batch_size ':183,'max_epochs ':10}\nINFO:syne_tune .stopping_criterion:reaching max wallclock time ( 720), stopping \u2423\n\u21a9!there .\nINFO:syne_tune .tuner:Stopping trials that may still be running .\nINFO:syne_tune .tuner:Tuning finished, results of trials can be found on /home /\n\u21a9!ci/syne -tune /python -entrypoint -2023 -08-18-19-45-39-958\n--------------------\nResource summary (last result isreported):\ntrial_id status iter learning_rate batch_size max_epochs epoch \u2423\n\u21a9!validation_error worker -time\n0Completed 10 0.100000 128 10 10 \u2423\n\u21a9! 0.277195 64.928907\n1Completed 10 0.170284 114 10 10 \u2423\n\u21a9! 0.286225 65.434195\n2Completed 10 0.340198 221 10 10 \u2423\n\u21a9! 0.218990 59.729758\n3Completed 10 0.014628 88 10 10 \u2423\n\u21a9! 0.899920 81.001636\n4Completed 10 0.111483 142 10 10 \u2423\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 2541, "end_char_idx": 3346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2659921-acd7-439e-8d69-f3badfd570a8": {"__data__": {"id_": "e2659921-acd7-439e-8d69-f3badfd570a8", "embedding": null, "metadata": {"page_label": "851", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1e037851-65ab-4083-b330-ba70f831a7ce", "node_type": "4", "metadata": {"page_label": "851", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "aed87672488def860e4338a7814f10882d980dd360f9e86d730d5903d66473bc", "class_name": "RelatedNodeInfo"}}, "text": "851 Asynchronous Random Search\n(continued from previous page)\n\u21a9! 0.268684 64.427400\n5Completed 10 0.014076 223 10 10 \u2423\n\u21a9! 0.899922 61.264475\n6Completed 10 0.025582 62 10 10 \u2423\n\u21a9! 0.399520 75.966186\n7Completed 10 0.026036 139 10 10 \u2423\n\u21a9! 0.899988 62.261541\n8Completed 10 0.242025 231 10 10 \u2423\n\u21a9! 0.257636 58.186485\n9Completed 10 0.104831 145 10 10 \u2423\n\u21a9! 0.273898 59.771699\n10 Completed 10 0.017899 51 10 10 \u2423\n\u21a9! 0.405545 83.778503\n11 Completed 10 0.964542 200 10 10 \u2423\n\u21a9! 0.181600 59.159662\n12 Completed 10 0.105599 40 10 10 \u2423\n\u21a9! 0.182500 94.734384\n13 Completed 10 0.584605 35 10 10 \u2423\n\u21a9! 0.153846 110.965637\n14 Completed 10 0.246889 146 10 10 \u2423\n\u21a9! 0.215050 65.142847\n15 Completed 10 0.129569 218 10 10 \u2423\n\u21a9! 0.313873 61.310455\n16 Completed 10 0.249007 103 10 10 \u2423\n\u21a9! 0.196101 72.519127\n17InProgress 9 0.039036 80 10 9 \u2423\n\u21a9! 0.369000 73.403000\n18InProgress 5 0.018466 183 10 5 \u2423\n\u21a9! 0.900263 34.714568\n2trials running, 17finished ( 17until the end), 722.84 s wallclock -time\nvalidation_error: best 0.14451533555984497 for trial -id13\n--------------------\nThe logs of all evaluated hyperparameter configurations are stored for further analysis. At\nany time during the tuning job, we can easily get the results obtained so far and plot the\nincumbent trajectory.\nd2l.set_figsize()\ntuning_experiment =load_experiment(tuner .name)\ntuning_experiment .plot()\nWARNING:matplotlib .legend:No artists with labels found to put inlegend .Note \u2423\n\u21a9!that artists whose label start with an underscore are ignored when legend() \u2423\n\u21a9!iscalled with no argument .\n19.3.3Visualizethe AsynchronousOptimization Process\nBelow we visualize how the learning curves of every trial (each color in the plot represents\na trial) evolve during the asynchronous optimization process. At any point in time, there\nare as many trials running concurrently as we have workers. Once a trial finishes, we", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1852, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "136d7519-4d1e-4301-9825-7527f2f80092": {"__data__": {"id_": "136d7519-4d1e-4301-9825-7527f2f80092", "embedding": null, "metadata": {"page_label": "852", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d288575-1054-4d06-a7e4-482c794c123b", "node_type": "4", "metadata": {"page_label": "852", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cf2116dda4a95a2a1b0d9b78610b6ca151b2eea32547832a14c97de8370c3ef9", "class_name": "RelatedNodeInfo"}}, "text": "852 Hyperparameter Optimization\nimmediately start the next trial, without waiting for the other trials to finish. Idle time\nof workers is reduced to a minimum with asynchronous scheduling.\nd2l.set_figsize([ 6,2.5])\nresults =tuning_experiment .results\nfor trial_id inresults .trial_id .unique():\ndf=results[results[ \"trial_id \"]==trial_id]\nd2l.plt.plot(\ndf[\"st_tuner_time \"],\ndf[\"validation_error \"],\nmarker =\"o\"\n)\nd2l.plt.xlabel( \"wall-clock time \")\nd2l.plt.ylabel( \"objective function \")\nText( 0,0.5,'objective function ')\n19.3.4Summary\nWecanreducethewaitingtimeforrandomsearchsubstantiallybydistributiontrialsacross\nparallel resources. In general, we distinguish between synchronous scheduling and asyn-\nchronous scheduling. Synchronous scheduling means that we sample a new batch of hy-\nperparameter configurations once the previous batch finished. If we have a stragglers -\ntrials that takes more time to finish than other trials - our workers need to wait at synchro-\nnization points. Asynchronous scheduling evaluates a new hyperparameter configurations", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a24270a9-b8ae-4b89-8799-30a053344137": {"__data__": {"id_": "a24270a9-b8ae-4b89-8799-30a053344137", "embedding": null, "metadata": {"page_label": "853", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f035cf80-3eec-4d97-8d95-78b2eebc897b", "node_type": "4", "metadata": {"page_label": "853", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "553109d3db360685d55eefd1b80f222200e4b653880f398da4de0455cfe721a6", "class_name": "RelatedNodeInfo"}}, "text": "853 Multi-Fidelity Hyperparameter Optimization\n267\n268\n269\n270\n271as soon as resources become available, and, hence, ensures that all workers are busy at\nany point in time. While random search is easy to distribute asynchronously and does not\nrequire any change of the actual algorithm, other methods require some additional modifi-\ncations.\n19.3.5Exercises\n1.Consider the DropoutMLP model implemented in Section 5.6 , and used in Exercise 1 of\nSection 19.2 .\n1.Implementanobjectivefunction hpo_objective_dropoutmlp_synetune tobeused\nwithSyneTune. Makesurethatyourfunctionreportsthevalidationerrorafterevery\nepoch.\n2.Using the setup of Exercise 1 in Section 19.2 , compare random search to Bayesian\noptimization. If you use SageMaker, feel free to use Syne Tune\u2019s benchmarking\nfacilities in order to run experiments in parallel. Hint: Bayesian optimization is\nprovided as syne_tune.optimizer.baselines.BayesianOptimization .\n3.For this exercise, you need to run on an instance with at least 4 CPU cores. For one\nofthemethodsusedabove(randomsearch,Bayesianoptimization),runexperiments\nwith n_workers=1 ,n_workers=2 ,n_workers=4 , and compare results (incumbent\ntrajectories). At least for random search, you should observe linear scaling with\nrespect to the number of workers. Hint: For robust results, you may have to average\nover several repetitions each.\n2.Advanced . The goal of this exercise is to implement a new scheduler in Syne Tune.\n1.Create a virtual environment containing both the d2lbook267andsyne-tune268\nsources.\n2.Implement the LocalSearcher from Exercise2 in Section 19.2 as a new searcher in\nSyne Tune. Hint: Read this tutorial269. Alternatively, you may follow this example\n270.\n3.Compare your new LocalSearcher with RandomSearch on the DropoutMLP bench-\nmark.\nDiscussions271.\n19.4Multi-Fidelity HyperparameterOptimization\nTraining neural networks can be expensive even on moderate size datasets. Depending\non the configuration space ( Section 19.1.1 ), hyperparameter optimization requires tens to\nhundreds of function evaluations to find a well-performing hyperparameter configuration.\nAswehaveseenin Section19.3 ,wecansignificantlyspeeduptheoverallwall-clocktimeof", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2186, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad462851-148b-42d2-a010-23caba5d4321": {"__data__": {"id_": "ad462851-148b-42d2-a010-23caba5d4321", "embedding": null, "metadata": {"page_label": "854", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e61ece3c-0d5c-4cd7-889c-24d278806d15", "node_type": "4", "metadata": {"page_label": "854", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1e86ca7af8078c84740c48bbe7151fc56ad59c8509fdc8a9363d0aebb54344ce", "class_name": "RelatedNodeInfo"}}, "text": "854 Hyperparameter Optimization\nHPObyexploitingparallelresources, butthis does not reducethe total amount ofcompute\nrequired.\nIn this section, we will show how the evaluation of hyperparameter configurations can be\nsped up. Methods such as random search allocate the same amount of resources (e.g.,\nnumber of epochs, training data points) to each hyperparameter evaluation. Fig. 19.4.1\ndepicts learning curves of a set of neural networks trained with different hyperparameter\nconfigurations. Afterafewepochswearealreadyabletovisuallydistinguishbetweenwell-\nperformingandsuboptimalconfigurations. However, thelearningcurvesarenoisy, andwe\nmightstillrequirethefullamountof100epochstoidentifythebestperformingone.\ntFig. 19.4.1 Learning curves of random hyperparameter con\ufb01gurations\nMulti-fidelityhyperparameteroptimizationallocatesmoreresourcestopromisingconfigu-\nrations and stop evaluations of poorly performing ones early. This speeds up the optimiza-\ntion process, since we can try a larger number of configurations for the same total amount\nof resources.\nMoreformally, weexpandourdefinitionin Section19.1.1 , suchthatourobjectivefunction\n\ud835\udc53\u00b9x,\ud835\udc5f\u00bagets an additional input \ud835\udc5f2\u00bb\ud835\udc5fmin,\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65\u00bc, specifying the amount of resources that\nwe are willing to spend for the evaluation of configuration x. We assume that the error\n\ud835\udc53\u00b9x,\ud835\udc5f\u00badecreases with \ud835\udc5f, whereas the computational cost \ud835\udc50\u00b9x,\ud835\udc5f\u00baincreases. Typically, \ud835\udc5f\nrepresents the number of epochs for training the neural network, but it could also be the\ntraining subset size or the number of cross-validation folds.\nfrom collections import defaultdict\nimport numpy asnp\nfrom scipy import stats\nfrom d2l import torch asd2l\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b909f588-a3de-45df-8762-3c1426e2a3ef": {"__data__": {"id_": "b909f588-a3de-45df-8762-3c1426e2a3ef", "embedding": null, "metadata": {"page_label": "855", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d3ac2a7-90f8-40ad-8b83-c33860f25f7f", "node_type": "4", "metadata": {"page_label": "855", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6e5b30337bc5758ec1aae13d8ae549b1af58f284504caaebf02a1bc07bd8f6ae", "class_name": "RelatedNodeInfo"}}, "text": "855 Multi-Fidelity Hyperparameter Optimization\n(continued from previous page)\nd2l.set_figsize()\n19.4.1SuccessiveHalving\nOne of the simplest ways to adapt random search to the multi-fidelity setting is successive\nhalving(JamiesonandTalwalkar, 2016 ,Karninetal., 2013). Thebasicideaistostartwith\n\ud835\udc41configurations,forexamplerandomlysampledfromtheconfigurationspace,andtotrain\neachofthemfor \ud835\udc5fminepochsonly. Wethendiscardafractionoftheworstperformingtrials\nand train the remaining ones for longer. Iterating this process, fewer trials run for longer,\nuntil at least one trial reaches \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65epochs.\nMore formally, consider a minimum budget \ud835\udc5fmin(for example 1 epoch), a maximum bud-\nget\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65, for example max_epochs in our previous example, and a halving constant \ud835\udf022\nf2,3,...g. For simplicity, assume that \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65=\ud835\udc5fmin\ud835\udf02\ud835\udc3e, with\ud835\udc3e2I. The number of initial\nconfigurationsisthen \ud835\udc41=\ud835\udf02\ud835\udc3e. Letusdefinethesetofrungs R=f\ud835\udc5fmin,\ud835\udc5fmin\ud835\udf02,\ud835\udc5f min\ud835\udf022,...,\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65g.\nOne round of successive halving proceeds as follows. We start with running \ud835\udc41trials un-\ntil the first rung \ud835\udc5fmin. Sorting the validation errors, we keep the top 1\u009d\ud835\udf02fraction (which\namounts to\ud835\udf02\ud835\udc3e\u00001configurations) and discard all the rest. The surviving trials are trained\nforthenextrung( \ud835\udc5fmin\ud835\udf02epochs),andtheprocessisrepeated. Ateachrung,a 1\u009d\ud835\udf02fractionof\ntrialssurvivesandtheirtrainingcontinueswitha \ud835\udf02timeslargerbudget. Withthisparticular\nchoice of\ud835\udc41, only a single trial will be trained to the full budget \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65. Once such a round\nof successive halving is done, we start the next one with a new set of initial configurations,\niterating until the total budget is spent.\ntFig. 19.4.2 Learning curves of random hyperparameter con\ufb01gurations.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1653, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7602450c-d56b-49ee-ab39-6c64047c491c": {"__data__": {"id_": "7602450c-d56b-49ee-ab39-6c64047c491c", "embedding": null, "metadata": {"page_label": "856", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6d65c86-b497-4c6f-8d4b-8f8e3d78fb52", "node_type": "4", "metadata": {"page_label": "856", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8e5ba93a8b77a59f6b6dcfaca4abd9e90deb06ddd9e692f05ac8e9862a368597", "class_name": "RelatedNodeInfo"}}, "text": "856 Hyperparameter Optimization\nWesubclassthe HPOScheduler baseclassfrom Section19.2 inordertoimplementsucces-\nsive halving, allowing for a generic HPOSearcher object to sample configurations (which,\nin our example below, will be a RandomSearcher ). Additionally, the user has to pass the\nminimum resource \ud835\udc5fmin, the maximum resource \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65and\ud835\udf02as input. Inside our scheduler,\nwemaintainaqueueofconfigurationsthatstillneedtobeevaluatedforthecurrentrung \ud835\udc5f\ud835\udc56.\nWe update the queue every time we jump to the next rung.\nclass SuccessiveHalvingScheduler (d2l .HPOScheduler): #@save\ndef __init__ (self , searcher, eta, r_min, r_max, prefact =1):\nself .save_hyperparameters()\n# Compute K, which is later used to determine the number of \u2423\n\u21a9!configurations\nself .K=int(np.log(r_max /r_min) /np.log(eta))\n# Define the rungs\nself .rung_levels =[r_min *eta **kfor kinrange (self .K+1)]\nifr_max not inself .rung_levels:\n# The final rung should be r_max\nself .rung_levels .append(r_max)\nself .K+=1\n# Bookkeeping\nself .observed_error_at_rungs =defaultdict( list )\nself .all_observed_error_at_rungs =defaultdict( list )\n# Our processing queue\nself .queue =[]\nIn the beginning our queue is empty, and we fill it with \ud835\udc5b=prefact\u0001\ud835\udf02\ud835\udc3econfigurations,\nwhich are first evaluated on the smallest rung \ud835\udc5fmin. Here, prefact allows us to reuse our\ncode in a different context. For the purpose of this section, we fix prefact =1. Every time\nresources become available and the HPOTuner object queries the suggest function, we\nreturn an element from the queue. Once we finish one round of successive halving, which\nmeansthatweevaluatedallsurvivingconfigurationsonthehighestresourcelevel \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65and\nour queue is empty, we start the entire process again with a new, randomly sampled set of\nconfigurations.\n@d2l .add_to_class(SuccessiveHalvingScheduler) #@save\ndef suggest (self ):\niflen(self .queue) ==0:\n# Start a new round of successive halving\n# Number of configurations for the first rung:\nn0=int(self .prefact *self .eta **self .K)\nfor _inrange (n0):\nconfig =self .searcher .sample_configuration()\nconfig[ \"max_epochs \"]=self .r_min # Set r = r_min\nself .queue .append(config)\n# Return an element from the queue\nreturn self .queue .pop()\nWhen we collected a new data point, we first update the searcher module. Afterwards we\ncheck if we already collect all data points on the current rung. If so, we sort all configura-\ntions and push the top1\n\ud835\udf02configurations into the queue.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1ade370-fc46-43d6-8bf3-257686b1f547": {"__data__": {"id_": "e1ade370-fc46-43d6-8bf3-257686b1f547", "embedding": null, "metadata": {"page_label": "857", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "588c75df-6df1-4243-bbcf-ec4da15cbaa2", "node_type": "4", "metadata": {"page_label": "857", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5d505e63134ef85023eb5e6d9c2a146fc4e16e9d0b9976f5d7e2eb9153c7ae07", "class_name": "RelatedNodeInfo"}}, "text": "857 Multi-Fidelity Hyperparameter Optimization\n@d2l .add_to_class(SuccessiveHalvingScheduler) #@save\ndef update (self , config: dict , error: float , info =None ):\nri=int(config[ \"max_epochs \"]) # Rung r_i\n# Update our searcher, e.g if we use Bayesian optimization later\nself .searcher .update(config, error, additional_info =info)\nself .all_observed_error_at_rungs[ri] .append((config, error))\nifri<self .r_max:\n# Bookkeeping\nself .observed_error_at_rungs[ri] .append((config, error))\n# Determine how many configurations should be evaluated on this rung\nki=self .K-self .rung_levels .index(ri)\nni=int(self .prefact *self .eta **ki)\n# If we observed all configuration on this rung r_i, we estimate the\n# top 1 / eta configuration, add them to queue and promote them for\n# the next rung r_{i+1}\niflen(self .observed_error_at_rungs[ri]) >=ni:\nkiplus1 =ki-1\nniplus1 =int(self .prefact *self .eta **kiplus1)\nbest_performing_configurations =self .get_top_n_configurations(\nrung_level =ri, n =niplus1\n)\nriplus1 =self .rung_levels[ self .K-kiplus1] # r_{i+1}\n# Queue may not be empty: insert new entries at the beginning\nself .queue =[\ndict (config, max_epochs =riplus1)\nfor config inbest_performing_configurations\n]+self .queue\nself .observed_error_at_rungs[ri] =[] # Reset\nConfigurationsaresortedbasedontheirobservedperformanceonthecurrentrung.\n@d2l .add_to_class(SuccessiveHalvingScheduler) #@save\ndef get_top_n_configurations (self , rung_level, n):\nrung =self .observed_error_at_rungs[rung_level]\nifnot rung:\nreturn []\nsorted_rung =sorted (rung, key =lambda x: x[ 1])\nreturn [x[0]for xinsorted_rung[:n]]\nLet us see how successive halving is doing on our neural network example. We will use\n\ud835\udc5fmin=2,\ud835\udf02=2,\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65=10, so that rung levels are 2,4,8,10.\nmin_number_of_epochs =2\nmax_number_of_epochs =10\neta =2\nnum_gpus =1\nconfig_space ={\n\"learning_rate \": stats .loguniform( 1e-2 ,1),\n\"batch_size \": stats .randint( 32,256),\n}\ninitial_config ={\n\"learning_rate \":0.1,\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "01ba77ba-fb95-46e8-be69-ff27d4090e4c": {"__data__": {"id_": "01ba77ba-fb95-46e8-be69-ff27d4090e4c", "embedding": null, "metadata": {"page_label": "858", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d151f6aa-271d-46c3-acdd-bd7172340711", "node_type": "4", "metadata": {"page_label": "858", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8b5a7be1386366fa4536206cb428f8d7a049add59c1b2d895de60ec5b53eb13f", "class_name": "RelatedNodeInfo"}}, "text": "858 Hyperparameter Optimization\n(continued from previous page)\n\"batch_size \":128,\n}\nWe just replace the scheduler with our new SuccessiveHalvingScheduler .\nsearcher =d2l.RandomSearcher(config_space, initial_config =initial_config)\nscheduler =SuccessiveHalvingScheduler(\nsearcher =searcher,\neta=eta,\nr_min =min_number_of_epochs,\nr_max =max_number_of_epochs,\n)\ntuner =d2l.HPOTuner(\nscheduler =scheduler,\nobjective =d2l.hpo_objective_lenet,\n)\ntuner .run(number_of_trials =30)\nerror =0.17762434482574463 , runtime =53.576584339141846\nWe can visualize the learning curves of all configurations that we evaluated. Most of the\nconfigurationsarestoppedearlyandonlythebetterperformingconfigurationssurviveuntil\n\ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65. Compare this to vanilla random search, which would allocate \ud835\udc5f\ud835\udc5a\ud835\udc4e\ud835\udc65to every config-\nuration.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 797, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3278088a-a925-4b84-b8a3-89a238104c5d": {"__data__": {"id_": "3278088a-a925-4b84-b8a3-89a238104c5d", "embedding": null, "metadata": {"page_label": "859", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d3aebd53-d2df-4f54-9f60-d8e8ca347b15", "node_type": "4", "metadata": {"page_label": "859", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d329265cbdd14bcb08486b1d9e739c5c28902a2aa7deab9def789db2449c0aed", "class_name": "RelatedNodeInfo"}}, "text": "859 19.4 Multi-Fidelity Hyperparameter Optimization", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 51, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87b1f151-8445-40ba-85a8-a09fdf56bf7d": {"__data__": {"id_": "87b1f151-8445-40ba-85a8-a09fdf56bf7d", "embedding": null, "metadata": {"page_label": "860", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4cd67c24-fbe5-41bc-a187-15fc5f828cfc", "node_type": "4", "metadata": {"page_label": "860", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d5930538dead730f274c224c5f0c56218a570c250ce744c97b9e6b747a7a840f", "class_name": "RelatedNodeInfo"}}, "text": "860 Hyperparameter Optimization", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 31, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c07c8bb8-5ab0-4946-bce3-e6bbc55b1ac1": {"__data__": {"id_": "c07c8bb8-5ab0-4946-bce3-e6bbc55b1ac1", "embedding": null, "metadata": {"page_label": "861", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "54e355fc-6c2d-4861-9d88-808af8e9725f", "node_type": "4", "metadata": {"page_label": "861", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c45ef97f7022374cef85cb566336718ae869405c4957aa0c672ee64006439976", "class_name": "RelatedNodeInfo"}}, "text": "861 19.4 Multi-Fidelity Hyperparameter Optimization", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 51, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c796b18f-18ac-4d2f-830a-c7324bcd2c55": {"__data__": {"id_": "c796b18f-18ac-4d2f-830a-c7324bcd2c55", "embedding": null, "metadata": {"page_label": "862", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0768b5a9-5cb4-48d4-88ec-ed9a34ed203a", "node_type": "4", "metadata": {"page_label": "862", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f599cd07b6b3434991201848e0f19ea09e23ce9bf4240a54417bf6af43dcacc7", "class_name": "RelatedNodeInfo"}}, "text": "862 Hyperparameter Optimization", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 31, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cca303b5-728b-463f-b652-e7be1a4db934": {"__data__": {"id_": "cca303b5-728b-463f-b652-e7be1a4db934", "embedding": null, "metadata": {"page_label": "863", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9790a890-7e97-49eb-922b-537b4dc0c023", "node_type": "4", "metadata": {"page_label": "863", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "24959526c60e6dbb6e6aca2e4a45acb9e68b619bcfa1f21df7b0b0060fae9999", "class_name": "RelatedNodeInfo"}}, "text": "863 19.4 Multi-Fidelity Hyperparameter Optimization", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 51, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a74a2e4-82c5-4a12-89a1-d18eac3cdde6": {"__data__": {"id_": "7a74a2e4-82c5-4a12-89a1-d18eac3cdde6", "embedding": null, "metadata": {"page_label": "864", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f9a0f2bf-83b4-4fcb-bff8-bd785e32047e", "node_type": "4", "metadata": {"page_label": "864", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "877c7eac6c6902ffd4397d0a9cdd33df34e07584e4c8d0a9bec9a96dd9fd8155", "class_name": "RelatedNodeInfo"}}, "text": "864 Hyperparameter Optimization", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 31, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ac9043a9-033c-4fa6-9595-30f20ca0016d": {"__data__": {"id_": "ac9043a9-033c-4fa6-9595-30f20ca0016d", "embedding": null, "metadata": {"page_label": "865", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31798e22-345d-431a-96cb-701d39cd51f1", "node_type": "4", "metadata": {"page_label": "865", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9d2296e7753775e9b4528335330f760cdb29210ba083ec161331257423f0d29d", "class_name": "RelatedNodeInfo"}}, "text": "865 19.4 Multi-Fidelity Hyperparameter Optimization", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 51, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed743a92-8157-4e05-885d-88fbbcdda53f": {"__data__": {"id_": "ed743a92-8157-4e05-885d-88fbbcdda53f", "embedding": null, "metadata": {"page_label": "866", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "698003fc-d2b5-49f0-a816-431af7f5337d", "node_type": "4", "metadata": {"page_label": "866", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8b0f4e89624a5185d37bc73c43d7bea730715ef5c99f6e7c2549d96171a04d22", "class_name": "RelatedNodeInfo"}}, "text": "866 Hyperparameter Optimization\n272for rung_index, rung inscheduler .all_observed_error_at_rungs .items():\nerrors =[xi[ 1]for xiinrung]\nd2l.plt.scatter([rung_index] *len(errors), errors)\nd2l.plt.xlim(min_number_of_epochs -0.5, max_number_of_epochs +0.5)\nd2l.plt.xticks(\nnp.arange(min_number_of_epochs, max_number_of_epochs +1),\nnp.arange(min_number_of_epochs, max_number_of_epochs +1)\n)\nd2l.plt.ylabel( \"validation error \")\nd2l.plt.xlabel( \"epochs \")\nText( 0.5,0,'epochs ')\nFinally,notesomeslightcomplexityinourimplementationof SuccessiveHalvingSched-\nuler. Say that a worker is free to run a job, and suggest is called when the current rung\nhasalmostbeencompletelyfilled,butanotherworkerisstillbusywithanevaluation. Since\nwelackthemetricvaluefromthisworker,wecannotdeterminethetop 1\u009d\ud835\udf02fractiontoopen\nup the next rung. On the other hand, we want to assign a job to our free worker, so it does\nnot remain idle. Our solution is to start a new round of successive halving and assign our\nworker to the first trial there. However, once a rung is completed in update, we make sure\nto insert new configurations at the beginning of the queue, so they take precedence over\nconfigurations from the next round.\n19.4.2Summary\nIn this section, we introduced the concept of multi-fidelity hyperparameter optimization,\nwhereweassumetohaveaccesstocheap-to-evaluateapproximationsoftheobjectivefunc-\ntion, such as validation error after a certain number of epochs of training as proxy to val-\nidation error after the full number of epochs. Multi-fidelity hyperparameter optimization\nallowstoreducetheoverallcomputationoftheHPOinsteadofjustreducingthewall-clock\ntime.\nWeimplementedandevaluatedsuccessivehalving,asimpleyetefficientmulti-fidelityHPO\nalgorithm.\nDiscussions272.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f562305-f063-46b8-99e4-a5190e1df54c": {"__data__": {"id_": "1f562305-f063-46b8-99e4-a5190e1df54c", "embedding": null, "metadata": {"page_label": "867", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0effb730-ac3e-43fe-9627-820435850fa1", "node_type": "4", "metadata": {"page_label": "867", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ebf68b9d5044589046f0bc3733749b72c2cd534dbeb593cf6b541d38a2c59947", "class_name": "RelatedNodeInfo"}}, "text": "867 Asynchronous Successive Halving\n19.5AsynchronousSuccessiveHalving\nAs we have seen in Section 19.3 , we can accelerate HPO by distributing the evaluation of\nhyperparameter configurations across either multiple instances or multiples CPUs / GPUs\non a single instance. However, compared to random search, it is not straightforward to\nrun successive halving (SH) asynchronously in a distributed setting. Before we can decide\nwhich configuration to run next, we first have to collect all observations at the current rung\nlevel. This requires to synchronize workers at each rung level. For example, for the lowest\nrung level\ud835\udc5fmin, we first have to evaluate all \ud835\udc41=\ud835\udf02\ud835\udc3econfigurations, before we can promote\nthe1\n\ud835\udf02of them to the next rung level.\nIn any distributed system, synchronization typically implies idle time for workers. First,\nwe often observe high variations in training time across hyperparameter configurations.\nFor example, assuming the number of filters per layer is a hyperparameter, then networks\nwith less filters finish training faster than networks with more filters, which implies idle\nworker time due to stragglers. Moreover, the number of slots in a rung level is not always a\nmultiple of the number of workers, in which case some workers may even sit idle for a full\nbatch.\nFigureFig. 19.5.1 shows the scheduling of synchronous SH with \ud835\udf02=2for four different\ntrials with two workers. We start with evaluating Trial-0 and Trial-1 for one epoch and\nimmediately continue with the next two trials once they are finished. We first have to wait\nuntil Trial-2 finishes, which takes substantially more time than the other trials, before we\ncan promote the best two trials, i.e., Trial-0 and Trial-3 to the next rung level. This causes\nidle time for Worker-1. Then, we continue with Rung 1. Also, here Trial-3 takes longer\nthanTrial-0,whichleadstoanadditionalidelingtimeofWorker-0. Once,wereachRung-2,\nonlythebesttrial,Trial-0,remainswhichoccupiesonlyoneworker. ToavoidthatWorker-1\nidles during that time, most implementaitons of SH continue already with the next round,\nand start evaluating new trials (e.g Trial-4) on the first rung.\nTrial-0\nTrial-1Trial-2\nTrial-3Trial-0\nTrial-3Synchronous Successive Halving\nWorker-0\nWorker-1\nRung 0Trial-0\nRung 1 Rung 2Trial-4\ntFig. 19.5.1 Synchronous successive halving with two workers.\nAsynchronous successive halving (ASHA) ( Lietal., 2018) adapts SH to the asynchronous\nparallel scenario. The main idea of ASHA is to promote configurations to the next rung\nlevelassoonaswecollectedatleast \ud835\udf02observationsonthecurrentrunglevel. Thisdecision\nrule may lead to suboptimal promotions: configurations can be promoted to the next rung\nlevel, which in hindsight do not compare favourably against most others at the same rung", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "85bb53d9-a52b-43a6-bb47-1a684aeb6b2f": {"__data__": {"id_": "85bb53d9-a52b-43a6-bb47-1a684aeb6b2f", "embedding": null, "metadata": {"page_label": "868", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6e68ab4-27fd-45ae-ad8a-e064ed829306", "node_type": "4", "metadata": {"page_label": "868", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2bd7f60fc55da5a1df04a8de87f4acf3b19e5d301713faaefd5256e92dd687ee", "class_name": "RelatedNodeInfo"}}, "text": "868 Hyperparameter Optimization\nlevel. Ontheotherhand, wegetridofallsynchronizationpointsthisway. Inpractice,such\nsuboptimalinitialpromotionshaveonlyamodestimpactonperformance,notonlybecause\nthe ranking of hyperparameter configurations is often fairly consistent across rung levels,\nbut also because rungs grow over time and reflect the distribution of metric values at this\nlevel better and better. If a worker is free, but no configuration can be promoted, we start a\nnew configuration with \ud835\udc5f=\ud835\udc5fmin, i.e the first rung level.\nFig. 19.5.2 shows the scheduling of the same configurations for ASHA. Once Trial-1 fin-\nishes, we collect the results of two trials (i.e Trial-0 and Trial-1) and immediately promote\nthe better of them (Trial-0) to the next rung level. After Trial-0 finishes on rung 1, there\nare too few trials there in order to support a further promotion. Hence, we continue with\nrung 0 and evaluate Trial-3. Once Trial-3 finishes, Trial-2 is still pending. At this point we\nhave 3 trials evaluated on rung 0 and one trial evaluated already on rung 1. Since Trial-3\nperforms worse than Trial-0 at rung 0, and \ud835\udf02=2, we cannot promote any new trial yet, and\nWorker-1 starts Trial-4 from scratch instead. However, once Trial-2 finishes and scores\nworse than Trial-3, the latter is promoted towards rung 1. Afterwards, we collected 2 eval-\nuations on rung 1, which means we can now promote Trial-0 towards rung 2. At the same\ntime, Worker-1 continues with evaluating new trials (i.e., Trial-5) on rung 0.\nTrial-0\nTrial-1Trial-2\nTrial-0 Trial-3Asynchronous Successive Halving\nWorker-0\nWorker-1Trial-3\nTrial-4Trial-0\nPromotion to Rung 1Promotion to Rung 1\nStart new trial on Rung 0Promotion to Rung 2\nTrial-5\nStart new trial on Rung 0\ntFig. 19.5.2 Asynchronous successive halving (ASHA) with two workers.\nimport logging\nfrom d2l import torch asd2l\nlogging .basicConfig(level =logging .INFO)\nimport matplotlib .pyplot asplt\nfrom syne_tune import StoppingCriterion, Tuner\nfrom syne_tune .backend .python_backend import PythonBackend\nfrom syne_tune .config_space import loguniform, randint\nfrom syne_tune .experiments import load_experiment\nfrom syne_tune .optimizer .baselines import ASHA\nINFO:root:SageMakerBackend isnot imported since dependencies are missing .You\u2423\n\u21a9!can install them with\npip install 'syne-tune[extra] '\nAWS dependencies are not imported since dependencies are missing .You can \u2423\n\u21a9!install them with\npip install 'syne-tune[aws] '\nor(for everything)\npip install 'syne-tune[extra] '\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97024dc6-f113-4144-9201-04d5090f1167": {"__data__": {"id_": "97024dc6-f113-4144-9201-04d5090f1167", "embedding": null, "metadata": {"page_label": "869", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a2c4ecac-c16e-459d-95e5-0ccd9444b343", "node_type": "4", "metadata": {"page_label": "869", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7d95bb90f2bbccecdb4e2730768c67488dcf5eb1d25571990550b2573c5806f6", "class_name": "RelatedNodeInfo"}}, "text": "869 Asynchronous Successive Halving\n(continued from previous page)\nAWS dependencies are not imported since dependencies are missing .You can \u2423\n\u21a9!install them with\npip install 'syne-tune[aws] '\nor(for everything)\npip install 'syne-tune[extra] '\nINFO:root:Ray Tune schedulers and searchers are not imported since \u2423\n\u21a9!dependencies are missing .You can install them with\npip install 'syne-tune[raytune] '\nor(for everything)\npip install 'syne-tune[extra] '\n19.5.1ObjectiveFunction\nWe will use Syne Tune with the same objective function as in Section 19.3 .\ndef hpo_objective_lenet_synetune (learning_rate, batch_size, max_epochs):\nfrom syne_tune import Reporter\nfrom d2l import torch asd2l\nmodel =d2l.LeNet(lr =learning_rate, num_classes =10)\ntrainer =d2l.HPOTrainer(max_epochs =1, num_gpus =1)\ndata =d2l.FashionMNIST(batch_size =batch_size)\nmodel .apply_init([ next (iter (data .get_dataloader( True )))[ 0]], d2l .init_cnn)\nreport =Reporter()\nfor epoch inrange (1, max_epochs +1):\nifepoch ==1:\n# Initialize the state of Trainer\ntrainer .fit(model =model, data =data)\nelse :\ntrainer .fit_epoch()\nvalidation_error =trainer .validation_error() .cpu() .detach() .numpy()\nreport(epoch =epoch, validation_error =float (validation_error))\nWe will also use the same configuration space as before:\nmin_number_of_epochs =2\nmax_number_of_epochs =10\neta =2\nconfig_space ={\n\"learning_rate \": loguniform( 1e-2 ,1),\n\"batch_size \": randint( 32,256),\n\"max_epochs \": max_number_of_epochs,\n}\ninitial_config ={\n\"learning_rate \":0.1,\n\"batch_size \":128,\n}\n19.5.2AsynchronousScheduler", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1558, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61549da7-5211-4da7-bc9b-3b26272a5681": {"__data__": {"id_": "61549da7-5211-4da7-bc9b-3b26272a5681", "embedding": null, "metadata": {"page_label": "870", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f2a23db9-93b8-4661-a0f1-37a4faafc8f4", "node_type": "4", "metadata": {"page_label": "870", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bb21964bb2734536b92a37cde4d8c428a16f657aae4101c90c822454e46ae9be", "class_name": "RelatedNodeInfo"}}, "text": "870 Hyperparameter Optimization\nFirst, we define the number of workers that evaluate trials concurrently. We also need to\nspecify how long we want to run random search, by defining an upper limit on the total\nwall-clock time.\nn_workers =2# Needs to be <= the number of available GPUs\nmax_wallclock_time =12*60 # 12 minutes\nThecodeforrunningASHAisasimplevariationofwhatwedidforasynchronousrandom\nsearch.\nmode =\"min\"\nmetric =\"validation_error \"\nresource_attr =\"epoch \"\nscheduler =ASHA(\nconfig_space,\nmetric =metric,\nmode =mode,\npoints_to_evaluate =[initial_config],\nmax_resource_attr =\"max_epochs \",\nresource_attr =resource_attr,\ngrace_period =min_number_of_epochs,\nreduction_factor =eta,\n)\nINFO:syne_tune .optimizer .schedulers .fifo:max_resource_level =10,asinferred \u2423\n\u21a9!from config_space\nINFO:syne_tune .optimizer .schedulers .fifo:Master random_seed =3140976097\nHere, metricandresource_attr specify the key names used with the reportcallback,\nandmax_resource_attr denoteswhichinputtotheobjectivefunctioncorrespondsto \ud835\udc5fmax.\nMoreover, grace_period provides\ud835\udc5fmin, and reduction_factor is\ud835\udf02. We can run Syne\nTune as before (this will take about 12 minutes):\ntrial_backend =PythonBackend(\ntune_function =hpo_objective_lenet_synetune,\nconfig_space =config_space,\n)\nstop_criterion =StoppingCriterion(max_wallclock_time =max_wallclock_time)\ntuner =Tuner(\ntrial_backend =trial_backend,\nscheduler =scheduler,\nstop_criterion =stop_criterion,\nn_workers =n_workers,\nprint_update_interval =int(max_wallclock_time *0.6),\n)\ntuner .run()", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "722f6dcd-5131-4050-94d5-a625d3f32955": {"__data__": {"id_": "722f6dcd-5131-4050-94d5-a625d3f32955", "embedding": null, "metadata": {"page_label": "871", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc824a43-68f6-49d9-93eb-6014ba1848b3", "node_type": "4", "metadata": {"page_label": "871", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "85bdb95457dd298c2d68b9a49f69a8aa0cfab467ab862dd7dbc3a01db99fb776", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60e4271f-b826-4e8d-ad41-8a2954dd5d71", "node_type": "1", "metadata": {}, "hash": "a3ce8bcc9f637564b6dec631c16a93c0f77c8a512c885e91eb94e4d4ce78af50", "class_name": "RelatedNodeInfo"}}, "text": "871 Asynchronous Successive Halving\nINFO:syne_tune .tuner:results of trials will be saved on /home /ci/syne -tune /\n\u21a9!python -entrypoint -2023 -08-18-20-01-52-046\nINFO:root:Detected 4GPUs\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.1 --batch_size 128 --max_epochs 10--tune_function_root \u2423\n\u21a9!/home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/tune_function --\n\u21a9!tune_function_hash e03d187e043d2a17cae636d6af164015 --st_checkpoint_dir /\n\u21a9!home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/0/checkpoints\nINFO:syne_tune .tuner:(trial 0)-scheduled config { 'learning_rate ':0.1,\n\u21a9!'batch_size ':128,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.44639554136672527 --batch_size 196 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!1/checkpoints\nINFO:syne_tune .tuner:(trial 1)-scheduled config { 'learning_rate ':0.\n\u21a9!44639554136672527 ,'batch_size ':196,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.011548051321691994 --batch_size 254 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!2/checkpoints\nINFO:syne_tune .tuner:(trial 2)-scheduled config { 'learning_rate ':0.\n\u21a9!011548051321691994 ,'batch_size ':254,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.14942487313193167 --batch_size 132 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!3/checkpoints\nINFO:syne_tune .tuner:(trial 3)-scheduled config { 'learning_rate ':0.\n\u21a9!14942487313193167 ,'batch_size ':132,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 1completed .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2748, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60e4271f-b826-4e8d-ad41-8a2954dd5d71": {"__data__": {"id_": "60e4271f-b826-4e8d-ad41-8a2954dd5d71", "embedding": null, "metadata": {"page_label": "871", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc824a43-68f6-49d9-93eb-6014ba1848b3", "node_type": "4", "metadata": {"page_label": "871", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "85bdb95457dd298c2d68b9a49f69a8aa0cfab467ab862dd7dbc3a01db99fb776", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "722f6dcd-5131-4050-94d5-a625d3f32955", "node_type": "1", "metadata": {"page_label": "871", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8c90b9a22a2ba8c90d4ec04812ebb74263874a58ebcf389befbacde39afe3dc6", "class_name": "RelatedNodeInfo"}}, "text": "\u21a9!py--learning_rate 0.14942487313193167 --batch_size 132 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!3/checkpoints\nINFO:syne_tune .tuner:(trial 3)-scheduled config { 'learning_rate ':0.\n\u21a9!14942487313193167 ,'batch_size ':132,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 1completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.06317157191455719 --batch_size 242 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!4/checkpoints\nINFO:syne_tune .tuner:(trial 4)-scheduled config { 'learning_rate ':0.\n\u21a9!06317157191455719 ,'batch_size ':242,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.48801815412811467 --batch_size 41--max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!5/checkpoints\nINFO:syne_tune .tuner:(trial 5)-scheduled config { 'learning_rate ':0.\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 2230, "end_char_idx": 3986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2fbcb017-27c6-4248-a54f-d983876c95df": {"__data__": {"id_": "2fbcb017-27c6-4248-a54f-d983876c95df", "embedding": null, "metadata": {"page_label": "872", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e36eff1a-6947-4f1b-b943-f7acecdc28e2", "node_type": "4", "metadata": {"page_label": "872", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "798a837b2fc1a84347d2e16eb55dd205b659a7fc6ef130de1d9daa93bfd2847c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fba9ba92-2d2d-4908-814f-9efe37937526", "node_type": "1", "metadata": {}, "hash": "81bb00c79fff15911532402a4b189810a83335272df17179a877e3db4c0ce827", "class_name": "RelatedNodeInfo"}}, "text": "872 Hyperparameter Optimization\n(continued from previous page)\n\u21a9!48801815412811467 ,'batch_size ':41,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.5904067586747807 --batch_size 244 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!6/checkpoints\nINFO:syne_tune .tuner:(trial 6)-scheduled config { 'learning_rate ':0.\n\u21a9!5904067586747807 ,'batch_size ':244,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.08812857364095393 --batch_size 148 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!7/checkpoints\nINFO:syne_tune .tuner:(trial 7)-scheduled config { 'learning_rate ':0.\n\u21a9!08812857364095393 ,'batch_size ':148,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.012271314788363914 --batch_size 235 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!8/checkpoints\nINFO:syne_tune .tuner:(trial 8)-scheduled config { 'learning_rate ':0.\n\u21a9!012271314788363914 ,'batch_size ':235,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 5completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.08845692598296777 --batch_size 236 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!9/checkpoints\nINFO:syne_tune .tuner:(trial 9)-scheduled config { 'learning_rate ':0.\n\u21a9!08845692598296777 ,'batch_size ':236,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fba9ba92-2d2d-4908-814f-9efe37937526": {"__data__": {"id_": "fba9ba92-2d2d-4908-814f-9efe37937526", "embedding": null, "metadata": {"page_label": "872", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e36eff1a-6947-4f1b-b943-f7acecdc28e2", "node_type": "4", "metadata": {"page_label": "872", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "798a837b2fc1a84347d2e16eb55dd205b659a7fc6ef130de1d9daa93bfd2847c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2fbcb017-27c6-4248-a54f-d983876c95df", "node_type": "1", "metadata": {"page_label": "872", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "584798bd90d71ab367db7083898016c75feeb5c1a2e3837c243545b735d60f20", "class_name": "RelatedNodeInfo"}}, "text": "\u21a9!08845692598296777 ,'batch_size ':236,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.0825770880068151 --batch_size 75--max_epochs 10--tune_\n\u21a9!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!10/checkpoints\nINFO:syne_tune .tuner:(trial 10)-scheduled config { 'learning_rate ':0.\n\u21a9!0825770880068151 ,'batch_size ':75,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.20235201406823256 --batch_size 65--max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!11/checkpoints\nINFO:syne_tune .tuner:(trial 11)-scheduled config { 'learning_rate ':0.\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 2656, "end_char_idx": 3949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f1e704a-4097-4c0d-9b2c-d12c0d1aba46": {"__data__": {"id_": "2f1e704a-4097-4c0d-9b2c-d12c0d1aba46", "embedding": null, "metadata": {"page_label": "873", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a47d563-871b-40b8-bfe0-71af3042e0c7", "node_type": "4", "metadata": {"page_label": "873", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5e829da2df56941d67203b3b882c7b1e99b138f1e0bb92d2e9b7246b7c377ef4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e8b5786c-e38c-4b64-9220-ebe75f58debb", "node_type": "1", "metadata": {}, "hash": "2215b6b47e9f77997322ac58a14508a850d41d8346274b4d9d418ddcfc9b147a", "class_name": "RelatedNodeInfo"}}, "text": "873 Asynchronous Successive Halving\n(continued from previous page)\n\u21a9!20235201406823256 ,'batch_size ':65,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.3359885631737537 --batch_size 58--max_epochs 10--tune_\n\u21a9!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!12/checkpoints\nINFO:syne_tune .tuner:(trial 12)-scheduled config { 'learning_rate ':0.\n\u21a9!3359885631737537 ,'batch_size ':58,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.7892434579795236 --batch_size 89--max_epochs 10--tune_\n\u21a9!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!13/checkpoints\nINFO:syne_tune .tuner:(trial 13)-scheduled config { 'learning_rate ':0.\n\u21a9!7892434579795236 ,'batch_size ':89,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.1233786579597858 --batch_size 176 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!14/checkpoints\nINFO:syne_tune .tuner:(trial 14)-scheduled config { 'learning_rate ':0.\n\u21a9!1233786579597858 ,'batch_size ':176,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 13completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.13707981127012328 --batch_size 141 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!15/checkpoints\nINFO:syne_tune .tuner:(trial 15)-scheduled config { 'learning_rate ':0.\n\u21a9!13707981127012328 ,'batch_size ':141,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e8b5786c-e38c-4b64-9220-ebe75f58debb": {"__data__": {"id_": "e8b5786c-e38c-4b64-9220-ebe75f58debb", "embedding": null, "metadata": {"page_label": "873", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a47d563-871b-40b8-bfe0-71af3042e0c7", "node_type": "4", "metadata": {"page_label": "873", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5e829da2df56941d67203b3b882c7b1e99b138f1e0bb92d2e9b7246b7c377ef4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f1e704a-4097-4c0d-9b2c-d12c0d1aba46", "node_type": "1", "metadata": {"page_label": "873", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8d50048c24d93a6281af00b5950139aa92f8d3c7ddf155e43507a7fe8bcc797c", "class_name": "RelatedNodeInfo"}}, "text": "\u21a9!13707981127012328 ,'batch_size ':141,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.02913976299993913 --batch_size 116 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!16/checkpoints\nINFO:syne_tune .tuner:(trial 16)-scheduled config { 'learning_rate ':0.\n\u21a9!02913976299993913 ,'batch_size ':116,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.033362897489792855 --batch_size 154 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!17/checkpoints\nINFO:syne_tune .tuner:(trial 17)-scheduled config { 'learning_rate ':0.\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 2657, "end_char_idx": 3958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a332080e-9ebd-4cd2-9b69-d2eb838c3362": {"__data__": {"id_": "a332080e-9ebd-4cd2-9b69-d2eb838c3362", "embedding": null, "metadata": {"page_label": "874", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc43ca0c-bd26-4be6-a8a5-c8f2102a86ee", "node_type": "4", "metadata": {"page_label": "874", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c1646dc77694ada68d08feda43f66e7c8aa71d631c826438d9c922b6f9469168", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ecd1c513-2e12-4122-bdd4-03e6d363e9e6", "node_type": "1", "metadata": {}, "hash": "9f3ee1fd8e457eb8adb432284c64fa66712a1d2d01086f7f1af357bde384d3f9", "class_name": "RelatedNodeInfo"}}, "text": "874 Hyperparameter Optimization\n(continued from previous page)\n\u21a9!033362897489792855 ,'batch_size ':154,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.29442952580755816 --batch_size 210 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!18/checkpoints\nINFO:syne_tune .tuner:(trial 18)-scheduled config { 'learning_rate ':0.\n\u21a9!29442952580755816 ,'batch_size ':210,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.10214259921521483 --batch_size 239 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!19/checkpoints\nINFO:syne_tune .tuner:(trial 19)-scheduled config { 'learning_rate ':0.\n\u21a9!10214259921521483 ,'batch_size ':239,'max_epochs ':10}\nINFO:syne_tune .tuner:tuning status (last metric isreported)\ntrial_id status iter learning_rate batch_size max_epochs epoch \u2423\n\u21a9!validation_error worker -time\n0 Stopped 4 0.100000 128 10 4.0 \u2423\n\u21a9! 0.430578 29.093798\n1Completed 10 0.446396 196 10 10.0 \u2423\n\u21a9! 0.205652 72.747496\n2 Stopped 2 0.011548 254 10 2.0 \u2423\n\u21a9! 0.900570 13.729115\n3 Stopped 8 0.149425 132 10 8.0 \u2423\n\u21a9! 0.259171 58.980305\n4 Stopped 4 0.063172 242 10 4.0 \u2423\n\u21a9! 0.900579 27.773950\n5Completed 10 0.488018 41 10 10.0 \u2423\n\u21a9! 0.140488 113.171314\n6 Stopped 10 0.590407 244 10 10.0 \u2423\n\u21a9! 0.193776 70.364757\n7 Stopped 2 0.088129 148 10 2.0 \u2423\n\u21a9! 0.899955 14.169738\n8 Stopped 2 0.012271 235 10 2.0 \u2423\n\u21a9! 0.899840 13.434274\n9 Stopped 2 0.088457 236 10 2.0 \u2423\n\u21a9! 0.899801 13.034437\n10 Stopped 4 0.082577 75 10 4.0 \u2423\n\u21a9! 0.385970 35.426524\n11 Stopped 4 0.202352 65 10 4.0 \u2423\n\u21a9! 0.543102 34.653495\n12 Stopped 10 0.335989 58 10 10.0 \u2423\n\u21a9!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ecd1c513-2e12-4122-bdd4-03e6d363e9e6": {"__data__": {"id_": "ecd1c513-2e12-4122-bdd4-03e6d363e9e6", "embedding": null, "metadata": {"page_label": "874", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc43ca0c-bd26-4be6-a8a5-c8f2102a86ee", "node_type": "4", "metadata": {"page_label": "874", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c1646dc77694ada68d08feda43f66e7c8aa71d631c826438d9c922b6f9469168", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a332080e-9ebd-4cd2-9b69-d2eb838c3362", "node_type": "1", "metadata": {"page_label": "874", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a013c7d3c8612581981f18015f5cad2de17babb37f051f06ec2044d4597c8145", "class_name": "RelatedNodeInfo"}}, "text": "0.899955 14.169738\n8 Stopped 2 0.012271 235 10 2.0 \u2423\n\u21a9! 0.899840 13.434274\n9 Stopped 2 0.088457 236 10 2.0 \u2423\n\u21a9! 0.899801 13.034437\n10 Stopped 4 0.082577 75 10 4.0 \u2423\n\u21a9! 0.385970 35.426524\n11 Stopped 4 0.202352 65 10 4.0 \u2423\n\u21a9! 0.543102 34.653495\n12 Stopped 10 0.335989 58 10 10.0 \u2423\n\u21a9! 0.149558 90.924182\n13 Completed 10 0.789243 89 10 10.0 \u2423\n\u21a9! 0.144887 77.365970\n14 Stopped 2 0.123379 176 10 2.0 \u2423\n\u21a9! 0.899987 12.422906\n15 Stopped 2 0.137080 141 10 2.0 \u2423\n\u21a9! 0.899983 13.395153\n16 Stopped 4 0.029140 116 10 4.0 \u2423\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 1992, "end_char_idx": 2526, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3ac1b31-0ede-450d-beda-33ece7bedca6": {"__data__": {"id_": "a3ac1b31-0ede-450d-beda-33ece7bedca6", "embedding": null, "metadata": {"page_label": "875", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bc13c2a-8305-4858-bb25-259f6f58e99d", "node_type": "4", "metadata": {"page_label": "875", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "073bdf552f081e40a561633765d0249b14fa91b70be30544b2e17495893525cd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "632d3bf8-166a-4d83-9dcb-cc29f0debe84", "node_type": "1", "metadata": {}, "hash": "7727a202cedf34e7f1134ef3134b5879775c0544babcb8abee39c764f2f23029", "class_name": "RelatedNodeInfo"}}, "text": "875 Asynchronous Successive Halving\n(continued from previous page)\n\u21a9! 0.900532 27.834111\n17 Stopped 2 0.033363 154 10 2.0 \u2423\n\u21a9! 0.899996 13.407285\n18InProgress 1 0.294430 210 10 1.0 \u2423\n\u21a9! 0.899878 6.126259\n19InProgress 0 0.102143 239 10 - \u2423\n\u21a9! - -\n2trials running, 18finished ( 3until the end), 437.07 s wallclock -time\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.02846298236356246 --batch_size 115 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!20/checkpoints\nINFO:syne_tune .tuner:(trial 20)-scheduled config { 'learning_rate ':0.\n\u21a9!02846298236356246 ,'batch_size ':115,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.037703019195187606 --batch_size 91--max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!21/checkpoints\nINFO:syne_tune .tuner:(trial 21)-scheduled config { 'learning_rate ':0.\n\u21a9!037703019195187606 ,'batch_size ':91,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.0741039859356903 --batch_size 192 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!22/checkpoints\nINFO:syne_tune .tuner:(trial 22)-scheduled config { 'learning_rate ':0.\n\u21a9!0741039859356903 ,'batch_size ':192,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2395, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "632d3bf8-166a-4d83-9dcb-cc29f0debe84": {"__data__": {"id_": "632d3bf8-166a-4d83-9dcb-cc29f0debe84", "embedding": null, "metadata": {"page_label": "875", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6bc13c2a-8305-4858-bb25-259f6f58e99d", "node_type": "4", "metadata": {"page_label": "875", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "073bdf552f081e40a561633765d0249b14fa91b70be30544b2e17495893525cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a3ac1b31-0ede-450d-beda-33ece7bedca6", "node_type": "1", "metadata": {"page_label": "875", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5085fa84bdddd70ad7ab8b7b05473382b792aa36ba1ad628cf73e3354a49cec2", "class_name": "RelatedNodeInfo"}}, "text": "\u21a9!0741039859356903 ,'batch_size ':192,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.3032613031191755 --batch_size 252 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!23/checkpoints\nINFO:syne_tune .tuner:(trial 23)-scheduled config { 'learning_rate ':0.\n\u21a9!3032613031191755 ,'batch_size ':252,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.019823425532533637 --batch_size 252 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!24/checkpoints\nINFO:syne_tune .tuner:(trial 24)-scheduled config { 'learning_rate ':0.\n\u21a9!019823425532533637 ,'batch_size ':252,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 2173, "end_char_idx": 3611, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "251443ec-7dcb-4c95-afcb-4759f06f78ce": {"__data__": {"id_": "251443ec-7dcb-4c95-afcb-4759f06f78ce", "embedding": null, "metadata": {"page_label": "876", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8158715b-cef2-4015-9562-b06aaaa04497", "node_type": "4", "metadata": {"page_label": "876", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c6459ffde1c75d41395bbe9accf38a67765638b53386b7b04f05ca4d47c664d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b01c3657-d3f7-4809-8f03-e30950548bbe", "node_type": "1", "metadata": {}, "hash": "d2d1ac3e4e81891127e8ff742a2048f22f9888c311c0cd75d51262229c71a83e", "class_name": "RelatedNodeInfo"}}, "text": "876 Hyperparameter Optimization\n(continued from previous page)\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.8203370335228594 --batch_size 77--max_epochs 10--tune_\n\u21a9!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!25/checkpoints\nINFO:syne_tune .tuner:(trial 25)-scheduled config { 'learning_rate ':0.\n\u21a9!8203370335228594 ,'batch_size ':77,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.2960420911378594 --batch_size 104 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!26/checkpoints\nINFO:syne_tune .tuner:(trial 26)-scheduled config { 'learning_rate ':0.\n\u21a9!2960420911378594 ,'batch_size ':104,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.2993874715754653 --batch_size 192 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!27/checkpoints\nINFO:syne_tune .tuner:(trial 27)-scheduled config { 'learning_rate ':0.\n\u21a9!2993874715754653 ,'batch_size ':192,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.08056711961080017 --batch_size 36--max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!28/checkpoints\nINFO:syne_tune .tuner:(trial 28)-scheduled config { 'learning_rate ':0.\n\u21a9!08056711961080017 ,'batch_size ':36,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b01c3657-d3f7-4809-8f03-e30950548bbe": {"__data__": {"id_": "b01c3657-d3f7-4809-8f03-e30950548bbe", "embedding": null, "metadata": {"page_label": "876", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8158715b-cef2-4015-9562-b06aaaa04497", "node_type": "4", "metadata": {"page_label": "876", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c6459ffde1c75d41395bbe9accf38a67765638b53386b7b04f05ca4d47c664d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "251443ec-7dcb-4c95-afcb-4759f06f78ce", "node_type": "1", "metadata": {"page_label": "876", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "93f30b8e55ecf0766071dca4643bdd12530c10417facea445fc0502112a840e0", "class_name": "RelatedNodeInfo"}}, "text": "\u21a9!08056711961080017 ,'batch_size ':36,'max_epochs ':10}\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.26868380288030347 --batch_size 151 --max_epochs 10--\n\u21a9!tune_function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-\n\u21a9!046/tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!29/checkpoints\nINFO:syne_tune .tuner:(trial 29)-scheduled config { 'learning_rate ':0.\n\u21a9!26868380288030347 ,'batch_size ':151,'max_epochs ':10}\nINFO:syne_tune .tuner:Trial trial_id 29completed .\nINFO:root:running subprocess with command: /usr/bin/python /home /ci/.local /lib/\n\u21a9!python3 .8/site -packages /syne_tune /backend /python_backend /python_entrypoint .\n\u21a9!py--learning_rate 0.9197404791177789 --batch_size 66--max_epochs 10--tune_\n\u21a9!function_root /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!tune_function --tune_function_hash e03d187e043d2a17cae636d6af164015 --st_\n\u21a9!checkpoint_dir /home /ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046/\n\u21a9!30/checkpoints\nINFO:syne_tune .tuner:(trial 30)-scheduled config { 'learning_rate ':0.\n\u21a9!9197404791177789 ,'batch_size ':66,'max_epochs ':10}\nINFO:syne_tune .stopping_criterion:reaching max wallclock time ( 720), stopping \u2423\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 2465, "end_char_idx": 3949, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bad50e03-44d3-4963-9db0-3a340a81ecbe": {"__data__": {"id_": "bad50e03-44d3-4963-9db0-3a340a81ecbe", "embedding": null, "metadata": {"page_label": "877", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "11672e81-c2ce-4630-90f3-30681ed5d73d", "node_type": "4", "metadata": {"page_label": "877", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cb7ab1d8c48391e002ad9fe32ef6b32b6db7e2d97165b991a211e0007553ceb3", "class_name": "RelatedNodeInfo"}}, "text": "877 Asynchronous Successive Halving\n(continued from previous page)\n\u21a9!there .\nINFO:syne_tune .tuner:Stopping trials that may still be running .\nINFO:syne_tune .tuner:Tuning finished, results of trials can be found on /home /\n\u21a9!ci/syne -tune /python -entrypoint -2023 -08-18-20-01-52-046\n--------------------\nResource summary (last result isreported):\ntrial_id status iter learning_rate batch_size max_epochs epoch \u2423\n\u21a9!validation_error worker -time\n0 Stopped 4 0.100000 128 10 4 \u2423\n\u21a9! 0.430578 29.093798\n1Completed 10 0.446396 196 10 10 \u2423\n\u21a9! 0.205652 72.747496\n2 Stopped 2 0.011548 254 10 2 \u2423\n\u21a9! 0.900570 13.729115\n3 Stopped 8 0.149425 132 10 8 \u2423\n\u21a9! 0.259171 58.980305\n4 Stopped 4 0.063172 242 10 4 \u2423\n\u21a9! 0.900579 27.773950\n5Completed 10 0.488018 41 10 10 \u2423\n\u21a9! 0.140488 113.171314\n6 Stopped 10 0.590407 244 10 10 \u2423\n\u21a9! 0.193776 70.364757\n7 Stopped 2 0.088129 148 10 2 \u2423\n\u21a9! 0.899955 14.169738\n8 Stopped 2 0.012271 235 10 2 \u2423\n\u21a9! 0.899840 13.434274\n9 Stopped 2 0.088457 236 10 2 \u2423\n\u21a9! 0.899801 13.034437\n10 Stopped 4 0.082577 75 10 4 \u2423\n\u21a9! 0.385970 35.426524\n11 Stopped 4 0.202352 65 10 4 \u2423\n\u21a9! 0.543102 34.653495\n12 Stopped 10 0.335989 58 10 10 \u2423\n\u21a9! 0.149558 90.924182\n13 Completed 10 0.789243 89 10 10 \u2423\n\u21a9! 0.144887 77.365970\n14 Stopped 2 0.123379 176 10 2 \u2423\n\u21a9! 0.899987 12.422906\n15 Stopped 2 0.137080 141 10 2 \u2423\n\u21a9! 0.899983 13.395153\n16 Stopped 4 0.029140 116 10 4 \u2423\n\u21a9! 0.900532 27.834111\n17 Stopped 2 0.033363 154 10 2 \u2423\n\u21a9! 0.899996 13.407285\n18 Stopped 8 0.294430 210 10 8 \u2423\n\u21a9! 0.241193 52.089688\n19 Stopped 2 0.102143 239 10 2 \u2423\n\u21a9! 0.900002 12.487762\n20 Stopped 2 0.028463 115 10 2 \u2423\n\u21a9! 0.899995 14.100359\n21 Stopped 2 0.037703 91 10 2 \u2423\n\u21a9! 0.900026 14.664848\n22 Stopped 2 0.074104 192 10 2 \u2423\n\u21a9! 0.901730 13.312770\n23 Stopped 2 0.303261 252 10 2 \u2423\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b756e03f-a448-44f3-bd46-128035ca97df": {"__data__": {"id_": "b756e03f-a448-44f3-bd46-128035ca97df", "embedding": null, "metadata": {"page_label": "878", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5204c427-8fd3-4b4d-9141-7e97fb38a56d", "node_type": "4", "metadata": {"page_label": "878", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "aee0c8827a07d2f094ea2a2f9abf450a3ea262f0a845ed52d4b4889fd4eb37b6", "class_name": "RelatedNodeInfo"}}, "text": "878 Hyperparameter Optimization\n(continued from previous page)\n\u21a9! 0.900009 12.725821\n24 Stopped 2 0.019823 252 10 2 \u2423\n\u21a9! 0.899917 12.533380\n25 Stopped 10 0.820337 77 10 10 \u2423\n\u21a9! 0.196842 81.816103\n26 Stopped 10 0.296042 104 10 10 \u2423\n\u21a9! 0.198453 81.121330\n27 Stopped 4 0.299387 192 10 4 \u2423\n\u21a9! 0.336183 24.610689\n28InProgress 9 0.080567 36 10 9 \u2423\n\u21a9! 0.203052 104.303746\n29 Completed 10 0.268684 151 10 10 \u2423\n\u21a9! 0.222814 68.217289\n30InProgress 1 0.919740 66 10 1 \u2423\n\u21a9! 0.900037 10.070776\n2trials running, 29finished ( 4until the end), 723.70 s wallclock -time\nvalidation_error: best 0.1404876708984375 for trial -id5\n--------------------\nNote that we are running a variant of ASHA where underperforming trials are stopped\nearly. This is different to our implementation in Section 19.4.1 , where each training job is\nstarted with a fixed max_epochs . In the latter case, a well-performing trial which reaches\nthe full 10 epochs, first needs to train 1, then 2, then 4, then 8 epochs, each time starting\nfromscratch. Thistypeofpause-and-resumeschedulingcanbeimplementedefficientlyby\ncheckpointing the training state after each epoch, but we avoid this extra complexity here.\nAfter the experiment has finished, we can retrieve and plot results.\nd2l.set_figsize()\ne=load_experiment(tuner .name)\ne.plot()\nWARNING:matplotlib .legend:No artists with labels found to put inlegend .Note \u2423\n\u21a9!that artists whose label start with an underscore are ignored when legend() \u2423\n\u21a9!iscalled with no argument .\n19.5.3Visualizethe Optimization Process", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1521, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93d10456-681b-4814-8c67-fdc2ab705ac9": {"__data__": {"id_": "93d10456-681b-4814-8c67-fdc2ab705ac9", "embedding": null, "metadata": {"page_label": "879", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64fa15ac-c027-4f53-89c9-0c1b5583502f", "node_type": "4", "metadata": {"page_label": "879", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d317ba36266ddcd9ef53b373fceb87dcd9eaf662966e35470b5acaf3b4c8a7df", "class_name": "RelatedNodeInfo"}}, "text": "879 Asynchronous Successive Halving\n273Oncemore,wevisualizethelearningcurvesofeverytrial(eachcolorintheplotrepresents\na trial). Compare this to asynchronous random search in Section 19.3 . As we have seen\nfor successive halving in Section 19.4 , most of the trials are stopped at 1 or 2 epochs ( \ud835\udc5fmin\nor\ud835\udf02\u0003\ud835\udc5fmin). However, trials do not stop at the same point, because they require different\namount of time per epoch. If we ran standard successive halving instead of ASHA, we\nwould need to synchronize our workers, before we can promote configurations to the next\nrung level.\nd2l.set_figsize([ 6,2.5])\nresults =e.results\nfor trial_id inresults .trial_id .unique():\ndf=results[results[ \"trial_id \"]==trial_id]\nd2l.plt.plot(\ndf[\"st_tuner_time \"],\ndf[\"validation_error \"],\nmarker =\"o\"\n)\nd2l.plt.xlabel( \"wall-clock time \")\nd2l.plt.ylabel( \"objective function \")\nText( 0,0.5,'objective function ')\n19.5.4Summary\nCompared to random search, successive halving is not quite as trivial to run in an asyn-\nchronous distributed setting. To avoid synchronisation points, we promote configurations\nas quickly as possible to the next rung level, even if this means promoting some wrong\nones. In practice, this usually does not hurt much, and the gains of asynchronous versus\nsynchronous scheduling are usually much higher than the loss of the suboptimal decision\nmaking.\nDiscussions273.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7a7362e-7807-4628-a066-e150af48a4a1": {"__data__": {"id_": "e7a7362e-7807-4628-a066-e150af48a4a1", "embedding": null, "metadata": {"page_label": "880", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "27541c14-f639-42c6-bd2b-3e1bfe34bee3", "node_type": "4", "metadata": {"page_label": "880", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9de9426356fb7f6ca2c4adb53f34561f35b957491ac0e8b225283c3c77469f2e", "class_name": "RelatedNodeInfo"}}, "text": "20 Generative Adversarial Networks\n20.1GenerativeAdversarialNetworks\nThroughout most of this book, we have talked about how to make predictions. In some\nform or another, we used deep neural networks to learn mappings from data examples to\nlabels. This kind of learning is called discriminative learning, as in, we\u2019d like to be able\nto discriminate between photos of cats and photos of dogs. Classifiers and regressors are\nboth examples of discriminative learning. And neural networks trained by backpropaga-\ntion have upended everything we thought we knew about discriminative learning on large\ncomplicated datasets. Classification accuracies on high-res images have gone from useless\ntohuman-level(withsomecaveats)injust5-6years. Wewillspareyouanotherspielabout\nalltheotherdiscriminativetaskswheredeepneuralnetworksdoastoundinglywell.\nBut there is more to machine learning than just solving discriminative tasks. For example,\ngiven a large dataset, without any labels, we might want to learn a model that concisely\ncaptures the characteristics of this data. Given such a model, we could sample synthetic\ndataexamplesthatresemblethedistributionofthetrainingdata. Forexample,givenalarge\ncorpus of photographs of faces, we might want to be able to generate a new photorealistic\nimage that looks like it might plausibly have come from the same dataset. This kind of\nlearning is called generative modeling.\nUntilrecently,wehadnomethodthatcouldsynthesizenovelphotorealisticimages. Butthe\nsuccess of deep neural networks for discriminative learning opened up new possibilities.\nOne big trend over the last three years has been the application of discriminative deep\nnets to overcome challenges in problems that we do not generally think of as supervised\nlearningproblems. Therecurrentneuralnetworklanguagemodelsareoneexampleofusing\na discriminative network (trained to predict the next character) that once trained can act as\na generative model.\nIn2014,abreakthroughpaperintroducedGenerativeadversarialnetworks(GANs)( Good-\nfellowetal., 2014), a clever new way to leverage the power of discriminative models to get\ngoodgenerativemodels. Attheirheart, GANsrelyontheideathatadatageneratorisgood\nif we cannot tell fake data apart from real data. In statistics, this is called a two-sample test\n- a test to answer the question whether datasets \ud835\udc4b=f\ud835\udc651,...,\ud835\udc65\ud835\udc5bgand\ud835\udc4b0=f\ud835\udc650\n1,...,\ud835\udc650\n\ud835\udc5bg\nweredrawnfromthesamedistribution. Themaindifferencebetweenmoststatisticspapers\n880", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ac8ffdc-d304-46c5-9acb-5060ce73d0f2": {"__data__": {"id_": "0ac8ffdc-d304-46c5-9acb-5060ce73d0f2", "embedding": null, "metadata": {"page_label": "881", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "664e8370-87bf-4cbe-9936-e1a4a96f7d10", "node_type": "4", "metadata": {"page_label": "881", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ead6ec7bc30d5f9169a9e1ba6d5545ca9f821cc6bfc1126266c9ea2cbf81ad30", "class_name": "RelatedNodeInfo"}}, "text": "881 Generative Adversarial Networks\n274and GANs is that the latter use this idea in a constructive way. In other words, rather than\njust training a model to say \u201chey, these two datasets do not look like they came from the\nsame distribution\u201d, they use the two-sample test274to provide training signals to a gener-\native model. This allows us to improve the data generator until it generates something that\nresembles the real data. At the very least, it needs to fool the classifier even if our classifier\nis a state of the art deep neural network.\ntFig. 20.1.1 Generative Adversarial Networks\nThe GAN architecture is illustrated in Fig. 20.1.1 . As you can see, there are two pieces\nin GAN architecture - first off, we need a device (say, a deep network but it really could\nbe anything, such as a game rendering engine) that might potentially be able to generate\ndata that looks just like the real thing. If we are dealing with images, this needs to generate\nimages. If we are dealing with speech, it needs to generate audio sequences, and so on.\nWe call this the generator network. The second component is the discriminator network. It\nattemptstodistinguishfakeandrealdatafromeachother. Bothnetworksareincompetition\nwith each other. The generator network attempts to fool the discriminator network. At that\npoint, the discriminator network adapts to the new fake data. This information, in turn is\nused to improve the generator network, and so on.\nThediscriminatorisabinaryclassifiertodistinguishiftheinput \ud835\udc65isreal(fromrealdata)or\nfake(fromthegenerator). Typically,thediscriminatoroutputsascalarprediction \ud835\udc5c2Rfor\ninputx, such as using a fully connected layer with hidden size 1, and then applies sigmoid\nfunction to obtain the predicted probability \ud835\udc37\u00b9x\u00ba=1\u009d\u00b91\u00b8\ud835\udc52\u0000\ud835\udc5c\u00ba. Assume the label \ud835\udc66\nfor the true data is 1and 0for the fake data. We train the discriminator to minimize the\ncross-entropy loss, i.e.,\nmin\n\ud835\udc37f\u0000\ud835\udc66log\ud835\udc37\u00b9x\u00ba\u0000\u00b9 1\u0000\ud835\udc66\u00balog\u00b91\u0000\ud835\udc37\u00b9x\u00ba\u00bag, (20.1.1)\nFor the generator, it first draws some parameter z2R\ud835\udc51from a source of randomness, e.g.,\na normal distribution z\u0018N\u00b9 0,1\u00ba. We often call zas the latent variable. It then applies\na function to generate x0=\ud835\udc3a\u00b9z\u00ba. The goal of the generator is to fool the discriminator\nto classify x0=\ud835\udc3a\u00b9z\u00baas true data, i.e., we want\ud835\udc37\u00b9\ud835\udc3a\u00b9z\u00ba\u00ba \u0019 1. In other words, for a\ngivendiscriminator \ud835\udc37,weupdatetheparametersofthegenerator \ud835\udc3atomaximizethecross-\nentropy loss when \ud835\udc66=0,i.e.,\nmax\n\ud835\udc3af\u0000\u00b91\u0000\ud835\udc66\u00balog\u00b91\u0000\ud835\udc37\u00b9\ud835\udc3a\u00b9z\u00ba\u00ba\u00bag=max\n\ud835\udc3af\u0000log\u00b91\u0000\ud835\udc37\u00b9\ud835\udc3a\u00b9z\u00ba\u00ba\u00bag. (20.1.2)\nIf the generator does a perfect job, then \ud835\udc37\u00b9x0\u00ba \u0019 1, so the above loss is near 0, which", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15f3d42e-d3ac-42fd-8090-04302d307e3d": {"__data__": {"id_": "15f3d42e-d3ac-42fd-8090-04302d307e3d", "embedding": null, "metadata": {"page_label": "882", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db8c9948-9ca5-4ae9-b2ac-12da3acc3447", "node_type": "4", "metadata": {"page_label": "882", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "97ece45bce89e795093e8c05fd230ae155cfde5e433da3d1bf432993944a0b51", "class_name": "RelatedNodeInfo"}}, "text": "882 Generative Adversarial Networks\nresults in the gradients that are too small to make good progress for the discriminator. So\ncommonly, we minimize the following loss:\nmin\n\ud835\udc3af\u0000\ud835\udc66log\u00b9\ud835\udc37\u00b9\ud835\udc3a\u00b9z\u00ba\u00ba\u00bag=min\n\ud835\udc3af\u0000log\u00b9\ud835\udc37\u00b9\ud835\udc3a\u00b9z\u00ba\u00ba\u00bag, (20.1.3)\nwhich is just feeding x0=\ud835\udc3a\u00b9z\u00bainto the discriminator but giving label \ud835\udc66=1.\nTo sum up, \ud835\udc37and\ud835\udc3aare playing a \u201cminimax\u201d game with the comprehensive objective\nfunction:\nmin\n\ud835\udc37max\n\ud835\udc3af\u0000\ud835\udc38\ud835\udc65\u0018Datalog\ud835\udc37\u00b9x\u00ba\u0000\ud835\udc38\ud835\udc67\u0018Noise log\u00b91\u0000\ud835\udc37\u00b9\ud835\udc3a\u00b9z\u00ba\u00ba\u00bag. (20.1.4)\nMany of the GANs applications are in the context of images. As a demonstration purpose,\nwe are going to content ourselves with fitting a much simpler distribution first. We will\nillustrate what happens if we use GANs to build the world\u2019s most inefficient estimator of\nparameters for a Gaussian. Let\u2019s get started.\n%matplotlib inline\nimport torch\nfrom torch import nn\nfrom d2l import torch asd2l\n20.1.1GenerateSome \u201cReal\u201dData\nSince this is going to be the world\u2019s lamest example, we simply generate data drawn from\na Gaussian.\nX=torch .normal( 0.0,1, (1000 ,2))\nA=torch .tensor([[ 1,2], [ -0.1,0.5]])\nb=torch .tensor([ 1,2])\ndata =torch .matmul(X, A) +b\nLet\u2019s see what we got. This should be a Gaussian shifted in some rather arbitrary way with\nmean\ud835\udc4fand covariance matrix \ud835\udc34\ud835\udc47\ud835\udc34.\nd2l.set_figsize()\nd2l.plt.scatter(data[: 100, (0)].detach() .numpy(), data[: 100, (1)].detach() .\n\u21a9!numpy());\nprint (f'The covariance matrix is \\n{torch .matmul(A .T,A)}')\nThe covariance matrix is\ntensor([[ 1.0100 ,1.9500 ],\n[1.9500 ,4.2500 ]])\nbatch_size =8\ndata_iter =d2l.load_array((data,), batch_size)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cb266fcb-337c-4c10-9ca7-62bf70ff2fd3": {"__data__": {"id_": "cb266fcb-337c-4c10-9ca7-62bf70ff2fd3", "embedding": null, "metadata": {"page_label": "883", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dc775fd5-8062-4090-98a0-b2e3300b9514", "node_type": "4", "metadata": {"page_label": "883", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2d7a570fc89a5b852f8c0dcaefd9c4391fa4de0144fa6aa9502161e36dcce678", "class_name": "RelatedNodeInfo"}}, "text": "883 Generative Adversarial Networks\n20.1.2Generator\nOur generator network will be the simplest network possible - a single layer linear model.\nThis is since we will be driving that linear network with a Gaussian data generator. Hence,\nit literally only needs to learn the parameters to fake things perfectly.\nnet_G =nn.Sequential(nn .Linear( 2,2))\n20.1.3Discriminator\nFor the discriminator we will be a bit more discriminating: we will use an MLP with 3\nlayers to make things a bit more interesting.\nnet_D =nn.Sequential(\nnn.Linear( 2,5), nn .Tanh(),\nnn.Linear( 5,3), nn .Tanh(),\nnn.Linear( 3,1))\n20.1.4Training\nFirst we define a function to update the discriminator.\n#@save\ndef update_D (X, Z, net_D, net_G, loss, trainer_D):\n\"\"\"Update discriminator.\"\"\"\nbatch_size =X.shape[ 0]\nones =torch .ones((batch_size,), device =X.device)\nzeros =torch .zeros((batch_size,), device =X.device)\ntrainer_D .zero_grad()\nreal_Y =net_D(X)\nfake_X =net_G(Z)\n# Do not need to compute gradient for `net_G`, detach it from\n# computing gradients.\nfake_Y =net_D(fake_X .detach())\nloss_D =(loss(real_Y, ones .reshape(real_Y .shape)) +\nloss(fake_Y, zeros .reshape(fake_Y .shape))) /2\nloss_D .backward()\ntrainer_D .step()\nreturn loss_D", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d79c9c1-069b-4797-8eaf-c01680bf7c5c": {"__data__": {"id_": "6d79c9c1-069b-4797-8eaf-c01680bf7c5c", "embedding": null, "metadata": {"page_label": "884", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ace730b1-1f39-463d-b1eb-7fd401550204", "node_type": "4", "metadata": {"page_label": "884", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9c8c7ab8fe33291d6630932c6c25f8cb5f55e8fa76bc548000702e1695bf0ab1", "class_name": "RelatedNodeInfo"}}, "text": "884 Generative Adversarial Networks\nThe generator is updated similarly. Here we reuse the cross-entropy loss but change the\nlabel of the fake data from 0to1.\n#@save\ndef update_G (Z, net_D, net_G, loss, trainer_G):\n\"\"\"Update generator.\"\"\"\nbatch_size =Z.shape[ 0]\nones =torch .ones((batch_size,), device =Z.device)\ntrainer_G .zero_grad()\n# We could reuse `fake_X` from `update_D` to save computation\nfake_X =net_G(Z)\n# Recomputing `fake_Y` is needed since `net_D` is changed\nfake_Y =net_D(fake_X)\nloss_G =loss(fake_Y, ones .reshape(fake_Y .shape))\nloss_G .backward()\ntrainer_G .step()\nreturn loss_G\nBoth the discriminator and the generator performs a binary logistic regression with the\ncross-entropy loss. We use Adam to smooth the training process. In each iteration, we first\nupdate the discriminator and then the generator. We visualize both losses and generated\nexamples.\ndef train (net_D, net_G, data_iter, num_epochs, lr_D, lr_G, latent_dim, data):\nloss =nn.BCEWithLogitsLoss(reduction ='sum')\nfor winnet_D .parameters():\nnn.init .normal_(w, 0,0.02 )\nfor winnet_G .parameters():\nnn.init .normal_(w, 0,0.02 )\ntrainer_D =torch .optim .Adam(net_D .parameters(), lr =lr_D)\ntrainer_G =torch .optim .Adam(net_G .parameters(), lr =lr_G)\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[1, num_epochs], nrows =2, figsize =(5,5),\nlegend =['discriminator ','generator '])\nanimator .fig.subplots_adjust(hspace =0.3)\nfor epoch inrange (num_epochs):\n# Train one epoch\ntimer =d2l.Timer()\nmetric =d2l.Accumulator( 3)# loss_D, loss_G, num_examples\nfor (X,) indata_iter:\nbatch_size =X.shape[ 0]\nZ=torch .normal( 0,1, size =(batch_size, latent_dim))\nmetric .add(update_D(X, Z, net_D, net_G, loss, trainer_D),\nupdate_G(Z, net_D, net_G, loss, trainer_G),\nbatch_size)\n# Visualize generated examples\nZ=torch .normal( 0,1, size =(100, latent_dim))\nfake_X =net_G(Z) .detach() .numpy()\nanimator .axes[ 1].cla()\nanimator .axes[ 1].scatter(data[:, 0], data[:, 1])\nanimator .axes[ 1].scatter(fake_X[:, 0], fake_X[:, 1])\nanimator .axes[ 1].legend([ 'real ','generated '])\n# Show the losses\nloss_D, loss_G =metric[ 0]/metric[ 2], metric[ 1]/metric[ 2]\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2167, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20910cd4-29e2-4d5e-935e-f389b86f469c": {"__data__": {"id_": "20910cd4-29e2-4d5e-935e-f389b86f469c", "embedding": null, "metadata": {"page_label": "885", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ef979d4f-4980-46d6-8414-dfeaa0eb1be8", "node_type": "4", "metadata": {"page_label": "885", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "64cef1bef8886aac40c516a61ce579c5726ce2d81b84de5fe77378ad7b18c2a1", "class_name": "RelatedNodeInfo"}}, "text": "885 Generative Adversarial Networks\n275(continued from previous page)\nanimator .add(epoch +1, (loss_D, loss_G))\nprint (f'loss_D {loss_D :.3f}, loss_G {loss_G :.3f},'\nf'{metric[ 2]/timer .stop() :.1f}examples/sec ')\nNow we specify the hyperparameters to fit the Gaussian distribution.\nlr_D, lr_G, latent_dim, num_epochs =0.05 ,0.005 ,2,20\ntrain(net_D, net_G, data_iter, num_epochs, lr_D, lr_G,\nlatent_dim, data[: 100].detach() .numpy())\nloss_D 0.693 , loss_G 0.693 ,1020.0 examples /sec\n20.1.5Summary\n\u000fGenerative adversarial networks (GANs) composes of two deep networks, the generator\nand the discriminator.\n\u000fThe generator generates the image as much closer to the true image as possible to fool\nthe discriminator, via maximizing the cross-entropy loss, i.e.,max log\u00b9\ud835\udc37\u00b9x0\u00ba\u00ba.\n\u000fThe discriminator tries to distinguish the generated images from the true images, via\nminimizing the cross-entropy loss, i.e.,min\u0000\ud835\udc66log\ud835\udc37\u00b9x\u00ba\u0000\u00b9 1\u0000\ud835\udc66\u00balog\u00b91\u0000\ud835\udc37\u00b9x\u00ba\u00ba.\n20.1.6Exercises\n\u000fDoesanequilibriumexistwherethegeneratorwins, i.e.thediscriminatorendsupunable\nto distinguish the two distributions on finite samples?\nDiscussions275.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1100, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "809f3c92-df81-432f-aa34-97c9af2131ea": {"__data__": {"id_": "809f3c92-df81-432f-aa34-97c9af2131ea", "embedding": null, "metadata": {"page_label": "886", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3757882-d8e2-4e62-bcfc-a2f5f3ac06bd", "node_type": "4", "metadata": {"page_label": "886", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e061e672d8b77245ea91fdee4d253b08756d8159c143f328f84f83c2199ca152", "class_name": "RelatedNodeInfo"}}, "text": "886 Generative Adversarial Networks\n27620.2DeepConvolutionalGenerativeAdversarial\nNetworks\nInSection 20.1 , we introduced the basic ideas behind how GANs work. We showed that\nthey can draw samples from some simple, easy-to-sample distribution, like a uniform or\nnormal distribution, and transform them into samples that appear to match the distribution\nof some dataset. And while our example of matching a 2D Gaussian distribution got the\npoint across, it is not especially exciting.\nIn this section, we will demonstrate how you can use GANs to generate photorealistic im-\nages. WewillbebasingourmodelsonthedeepconvolutionalGANs(DCGAN)introduced\nin Radford et al.(2015). We will borrow the convolutional architecture that have proven\nso successful for discriminative computer vision problems and show how via GANs, they\ncan be leveraged to generate photorealistic images.\nimport warnings\nimport torch\nimport torchvision\nfrom torch import nn\nfrom d2l import torch asd2l\n20.2.1The PokemonDataset\nThe dataset we will use is a collection of Pokemon sprites obtained from pokemondb276.\nFirst download, extract and load this dataset.\n#@save\nd2l.DATA_HUB[ 'pokemon ']=(d2l .DATA_URL +'pokemon.zip ',\n'c065c0e2593b8b161a2d7873e42418bf6a21106c ')\ndata_dir =d2l.download_extract( 'pokemon ')\npokemon =torchvision .datasets .ImageFolder(data_dir)\nDownloading ../data /pokemon .zip from http ://d2l-data .s3-accelerate .amazonaws .\n\u21a9!com/pokemon .zip...\nWe resize each image into 64\u000264. The ToTensor transformation will project the pixel\nvalueinto\u00bb0,1\u00bc,whileourgeneratorwillusethetanhfunctiontoobtainoutputsin \u00bb\u00001,1\u00bc.\nTherefore we normalize the data with 0.5mean and 0.5standard deviation to match the\nvalue range.\nbatch_size =256\ntransformer =torchvision .transforms .Compose([\ntorchvision .transforms .Resize(( 64,64)),\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1834, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f6130c1-a12a-4be6-9f0e-afa14a32757d": {"__data__": {"id_": "1f6130c1-a12a-4be6-9f0e-afa14a32757d", "embedding": null, "metadata": {"page_label": "887", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a85ae34-00d2-4d2d-aca3-0d894760a221", "node_type": "4", "metadata": {"page_label": "887", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cc66ce9b0d3d298c96c3d5cfee73c133df2ed5a323067ba47efd3e19285d3130", "class_name": "RelatedNodeInfo"}}, "text": "887 Deep Convolutional Generative Adversarial Networks\n(continued from previous page)\ntorchvision .transforms .ToTensor(),\ntorchvision .transforms .Normalize( 0.5,0.5)\n])\npokemon .transform =transformer\ndata_iter =torch .utils .data .DataLoader(\npokemon, batch_size =batch_size,\nshuffle =True , num_workers =d2l.get_dataloader_workers())\nLet\u2019s visualize the first 20 images.\nwarnings .filterwarnings( 'ignore ')\nd2l.set_figsize(( 4,4))\nfor X, y indata_iter:\nimgs =X[:20,:,:,:] .permute( 0,2,3,1)/2+0.5\nd2l.show_images(imgs, num_rows =4, num_cols =5)\nbreak\n20.2.2The Generator\nThe generator needs to map the noise variable z2R\ud835\udc51, a length-\ud835\udc51vector, to a RGB image\nwithwidthandheighttobe 64\u000264. InSection14.11 weintroducedthefullyconvolutional\nnetworkthatusestransposedconvolutionlayer(referto Section14.10 )toenlargeinputsize.\nThe basic block of the generator contains a transposed convolution layer followed by the\nbatch normalization and ReLU activation.\nclass G_block (nn.Module):\ndef __init__ (self , out_channels, in_channels =3, kernel_size =4, strides =2,\npadding =1,**kwargs):\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cda350f4-f66b-499c-a7ec-0c6088f77272": {"__data__": {"id_": "cda350f4-f66b-499c-a7ec-0c6088f77272", "embedding": null, "metadata": {"page_label": "888", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e5339f8-1724-4380-8282-3d6031fcd974", "node_type": "4", "metadata": {"page_label": "888", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1351b2e3b61f71f86857e2e80b494de10e2ef71572c2a35060d5732406a16507", "class_name": "RelatedNodeInfo"}}, "text": "888 Generative Adversarial Networks\n(continued from previous page)\nsuper (G_block, self ).__init__ (**kwargs)\nself .conv2d_trans =nn.ConvTranspose2d(in_channels, out_channels,\nkernel_size, strides, padding, bias =False )\nself .batch_norm =nn.BatchNorm2d(out_channels)\nself .activation =nn.ReLU()\ndef forward (self , X):\nreturn self .activation( self .batch_norm( self .conv2d_trans(X)))\nIn default, the transposed convolution layer uses a \ud835\udc58\u210e=\ud835\udc58\ud835\udc64=4kernel, a\ud835\udc60\u210e=\ud835\udc60\ud835\udc64=2\nstrides,anda \ud835\udc5d\u210e=\ud835\udc5d\ud835\udc64=1padding. Withainputshapeof \ud835\udc5b0\n\u210e\u0002\ud835\udc5b0\n\ud835\udc64=16\u000216,thegenerator\nblock will double input\u2019s width and height.\n\ud835\udc5b0\n\u210e\u0002\ud835\udc5b0\n\ud835\udc64=\u00bb\u00b9\ud835\udc5b\u210e\ud835\udc58\u210e\u0000\u00b9\ud835\udc5b\u210e\u00001\u00ba\u00b9\ud835\udc58\u210e\u0000\ud835\udc60\u210e\u00ba\u00002\ud835\udc5d\u210e\u00bc\u0002\u00bb\u00b9\ud835\udc5b\ud835\udc64\ud835\udc58\ud835\udc64\u0000\u00b9\ud835\udc5b\ud835\udc64\u00001\u00ba\u00b9\ud835\udc58\ud835\udc64\u0000\ud835\udc60\ud835\udc64\u00ba\u00002\ud835\udc5d\ud835\udc64\u00bc\n=\u00bb\u00b9\ud835\udc58\u210e\u00b8\ud835\udc60\u210e\u00b9\ud835\udc5b\u210e\u00001\u00ba\u00002\ud835\udc5d\u210e\u00bc\u0002\u00bb\u00b9\ud835\udc58\ud835\udc64\u00b8\ud835\udc60\ud835\udc64\u00b9\ud835\udc5b\ud835\udc64\u00001\u00ba\u00002\ud835\udc5d\ud835\udc64\u00bc\n=\u00bb\u00b94\u00b82\u0002\u00b916\u00001\u00ba\u00002\u00021\u00bc\u0002\u00bb\u00b9 4\u00b82\u0002\u00b916\u00001\u00ba\u00002\u00021\u00bc\n=32\u000232.\n(20.2.1)\nx=torch .zeros(( 2,3,16,16))\ng_blk =G_block( 20)\ng_blk(x) .shape\ntorch .Size([ 2,20,32,32])\nIfchangingthetransposedconvolutionlayertoa 4\u00024kernel, 1\u00021stridesandzeropadding.\nWith a input size of 1\u00021, the output will have its width and height increased by 3 respec-\ntively.\nx=torch .zeros(( 2,3,1,1))\ng_blk =G_block( 20, strides =1, padding =0)\ng_blk(x) .shape\ntorch .Size([ 2,20,4,4])\nThegeneratorconsistsoffourbasicblocksthatincreaseinput\u2019sbothwidthandheightfrom\n1 to 32. At the same time, it first projects the latent variable into 64\u00028channels, and then\nhalve the channels each time. At last, a transposed convolution layer is used to generate\nthe output. It further doubles the width and height to match the desired 64\u000264shape,\nand reduces the channel size to 3. The tanh activation function is applied to project output\nvalues into the\u00b9\u00001,1\u00barange.\nn_G =64\nnet_G =nn.Sequential(\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f47f31a-3ee9-43ad-86e7-fe98a4940e8e": {"__data__": {"id_": "2f47f31a-3ee9-43ad-86e7-fe98a4940e8e", "embedding": null, "metadata": {"page_label": "889", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "96edd470-d0a4-4e33-894b-30ca769bc094", "node_type": "4", "metadata": {"page_label": "889", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0c5a89ae6173532bba210ff413dbb449f81f0fe15f69769250c35b5aa51e6aa5", "class_name": "RelatedNodeInfo"}}, "text": "889 Deep Convolutional Generative Adversarial Networks\n(continued from previous page)\nG_block(in_channels =100, out_channels =n_G*8,\nstrides =1, padding =0), # Output: (64 * 8, 4, 4)\nG_block(in_channels =n_G*8, out_channels =n_G*4),# Output: (64 * 4, 8, 8)\nG_block(in_channels =n_G*4, out_channels =n_G*2),# Output: (64 * 2, 16, 16)\nG_block(in_channels =n_G*2, out_channels =n_G), # Output: (64, 32, 32)\nnn.ConvTranspose2d(in_channels =n_G, out_channels =3,\nkernel_size =4, stride =2, padding =1, bias =False ),\nnn.Tanh()) # Output: (3, 64, 64)\nGenerate a 100 dimensional latent variable to verify the generator\u2019s output shape.\nx=torch .zeros(( 1,100,1,1))\nnet_G(x) .shape\ntorch .Size([ 1,3,64,64])\n20.2.3Discriminator\nThe discriminator is a normal convolutional network network except that it uses a leaky\nReLU as its activation function. Given \ud835\udefc2\u00bb0,1\u00bc, its definition is\nleaky ReLU\u00b9\ud835\udc65\u00ba=(\n\ud835\udc65if\ud835\udc65 >0\n\ud835\udefc\ud835\udc65otherwise. (20.2.2)\nAs it can be seen, it is normal ReLU if \ud835\udefc=0, and an identity function if \ud835\udefc=1. For\n\ud835\udefc2\u00b90,1\u00ba, leaky ReLU is a nonlinear function that give a non-zero output for a negative\ninput. Itaimstofixthe\u201cdyingReLU\u201dproblemthataneuronmightalwaysoutputanegative\nvalue and therefore cannot make any progress since the gradient of ReLU is 0.\nalphas =[0,.2,.4,.6,.8,1]\nx=torch .arange( -2,1,0.1)\nY=[nn.LeakyReLU(alpha)(x) .detach() .numpy() for alpha inalphas]\nd2l.plot(x .detach() .numpy(), Y, 'x','y', alphas)\nThebasicblockofthediscriminatorisaconvolutionlayerfollowedbyabatchnormalization\nlayerandaleakyReLUactivation. Thehyperparametersoftheconvolutionlayeraresimilar\nto the transpose convolution layer in the generator block.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1630, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2d97601-bd16-4f8b-90f6-104b41ccb935": {"__data__": {"id_": "f2d97601-bd16-4f8b-90f6-104b41ccb935", "embedding": null, "metadata": {"page_label": "890", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d8e97e3-178f-42e9-a209-d5ddb5972831", "node_type": "4", "metadata": {"page_label": "890", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ef8719745ce2e30b0093e12c3a21c39be58aa06923a317f238de9ea3e4d0c25b", "class_name": "RelatedNodeInfo"}}, "text": "890 Generative Adversarial Networks\nclass D_block (nn.Module):\ndef __init__ (self , out_channels, in_channels =3, kernel_size =4, strides =2,\npadding =1, alpha =0.2,**kwargs):\nsuper (D_block, self ).__init__ (**kwargs)\nself .conv2d =nn.Conv2d(in_channels, out_channels, kernel_size,\nstrides, padding, bias =False )\nself .batch_norm =nn.BatchNorm2d(out_channels)\nself .activation =nn.LeakyReLU(alpha, inplace =True )\ndef forward (self , X):\nreturn self .activation( self .batch_norm( self .conv2d(X)))\nA basic block with default settings will halve the width and height of the inputs, as we\ndemonstratedin Section7.3 . Forexample,givenainputshape \ud835\udc5b\u210e=\ud835\udc5b\ud835\udc64=16,withakernel\nshape\ud835\udc58\u210e=\ud835\udc58\ud835\udc64=4, a stride shape \ud835\udc60\u210e=\ud835\udc60\ud835\udc64=2, and a padding shape \ud835\udc5d\u210e=\ud835\udc5d\ud835\udc64=1, the\noutput shape will be:\n\ud835\udc5b0\n\u210e\u0002\ud835\udc5b0\n\ud835\udc64=b\u00b9\ud835\udc5b\u210e\u0000\ud835\udc58\u210e\u00b82\ud835\udc5d\u210e\u00b8\ud835\udc60\u210e\u00ba\u009d\ud835\udc60\u210ec\u0002b\u00b9\ud835\udc5b\ud835\udc64\u0000\ud835\udc58\ud835\udc64\u00b82\ud835\udc5d\ud835\udc64\u00b8\ud835\udc60\ud835\udc64\u00ba\u009d\ud835\udc60\ud835\udc64c\n=b\u00b916\u00004\u00b82\u00021\u00b82\u00ba\u009d2c\u0002b\u00b9 16\u00004\u00b82\u00021\u00b82\u00ba\u009d2c\n=8\u00028.(20.2.3)\nx=torch .zeros(( 2,3,16,16))\nd_blk =D_block( 20)\nd_blk(x) .shape\ntorch .Size([ 2,20,8,8])\nThe discriminator is a mirror of the generator.\nn_D =64\nnet_D =nn.Sequential(\nD_block(n_D), # Output: (64, 32, 32)\nD_block(in_channels =n_D, out_channels =n_D*2), # Output: (64 * 2, 16, 16)\nD_block(in_channels =n_D*2, out_channels =n_D*4), # Output: (64 * 4, 8, 8)\nD_block(in_channels =n_D*4, out_channels =n_D*8), # Output: (64 * 8, 4, 4)\nnn.Conv2d(in_channels =n_D*8, out_channels =1,\nkernel_size =4, bias =False )) # Output: (1, 1, 1)\nItusesaconvolutionlayerwithoutputchannel 1asthelastlayertoobtainasingleprediction\nvalue.\nx=torch .zeros(( 1,3,64,64))\nnet_D(x) .shape\ntorch .Size([ 1,1,1,1])\n20.2.4Training", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "14e35c93-df0e-4104-9f27-bb087aa0ecf4": {"__data__": {"id_": "14e35c93-df0e-4104-9f27-bb087aa0ecf4", "embedding": null, "metadata": {"page_label": "891", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01db6d96-bc82-490d-8823-3e677c2905c1", "node_type": "4", "metadata": {"page_label": "891", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "568a83ddbaeea349f1f77061e08cf3950a46f5c8b8fc2083955126a00ac2713e", "class_name": "RelatedNodeInfo"}}, "text": "891 Deep Convolutional Generative Adversarial Networks\nCompared to the basic GAN in Section 20.1 , we use the same learning rate for both gen-\nerator and discriminator since they are similar to each other. In addition, we change \ud835\udefd1in\nAdam (Section 12.10 ) from 0.9to0.5. It decreases the smoothness of the momentum, the\nexponentiallyweightedmovingaverageofpastgradients,totakecareoftherapidchanging\ngradients because the generator and the discriminator fight with each other. Besides, the\nrandom generated noise Z, is a 4-D tensor and we are using GPU to accelerate the compu-\ntation.\ndef train (net_D, net_G, data_iter, num_epochs, lr, latent_dim,\ndevice =d2l.try_gpu()):\nloss =nn.BCEWithLogitsLoss(reduction ='sum')\nfor winnet_D .parameters():\nnn.init .normal_(w, 0,0.02 )\nfor winnet_G .parameters():\nnn.init .normal_(w, 0,0.02 )\nnet_D, net_G =net_D .to(device), net_G .to(device)\ntrainer_hp ={'lr': lr, 'betas ': [0.5,0.999 ]}\ntrainer_D =torch .optim .Adam(net_D .parameters(), **trainer_hp)\ntrainer_G =torch .optim .Adam(net_G .parameters(), **trainer_hp)\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[1, num_epochs], nrows =2, figsize =(5,5),\nlegend =['discriminator ','generator '])\nanimator .fig.subplots_adjust(hspace =0.3)\nfor epoch inrange (1, num_epochs +1):\n# Train one epoch\ntimer =d2l.Timer()\nmetric =d2l.Accumulator( 3)# loss_D, loss_G, num_examples\nfor X, _ indata_iter:\nbatch_size =X.shape[ 0]\nZ=torch .normal( 0,1, size =(batch_size, latent_dim, 1,1))\nX, Z =X.to(device), Z .to(device)\nmetric .add(d2l .update_D(X, Z, net_D, net_G, loss, trainer_D),\nd2l.update_G(Z, net_D, net_G, loss, trainer_G),\nbatch_size)\n# Show generated examples\nZ=torch .normal( 0,1, size =(21, latent_dim, 1,1), device =device)\n# Normalize the synthetic data to N(0, 1)\nfake_x =net_G(Z) .permute( 0,2,3,1)/2+0.5\nimgs =torch .cat(\n[torch .cat([\nfake_x[i *7+j].cpu() .detach() for jinrange (7)], dim =1)\nfor iinrange (len(fake_x) //7)], dim =0)\nanimator .axes[ 1].cla()\nanimator .axes[ 1].imshow(imgs)\n# Show the losses\nloss_D, loss_G =metric[ 0]/metric[ 2], metric[ 1]/metric[ 2]\nanimator .add(epoch, (loss_D, loss_G))\nprint (f'loss_D {loss_D :.3f}, loss_G {loss_G :.3f},'\nf'{metric[ 2]/timer .stop() :.1f}examples/sec on {str(device) }')\nWe train the model with a small number of epochs just for demonstration. For better per-\nformance, the variable num_epochs can be set to a larger number.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccf19eb6-f8ec-40ab-9d22-e89afca3131b": {"__data__": {"id_": "ccf19eb6-f8ec-40ab-9d22-e89afca3131b", "embedding": null, "metadata": {"page_label": "892", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a54b8dbe-4d11-4bc0-bd73-f1fdef0876ad", "node_type": "4", "metadata": {"page_label": "892", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "78e218f731a4682f3d50f8da5ec8569aa5a4c2836eafb216ef21a3676d0c95a1", "class_name": "RelatedNodeInfo"}}, "text": "892 Generative Adversarial Networks\n277latent_dim, lr, num_epochs =100,0.005 ,20\ntrain(net_D, net_G, data_iter, num_epochs, lr, latent_dim)\nloss_D 0.023 , loss_G 7.359 ,2292.7 examples /sec on cuda: 0\n20.2.5Summary\n\u000fDCGANarchitecturehasfourconvolutionallayersfortheDiscriminatorandfour\u201cfractionally-\nstrided\u201d convolutional layers for the Generator.\n\u000fThe Discriminator is a 4-layer strided convolutions with batch normalization (except its\ninput layer) and leaky ReLU activations.\n\u000fLeaky ReLU is a nonlinear function that give a non-zero output for a negative input. It\naims to fix the \u201cdying ReLU\u201d problem and helps the gradients flow easier through the\narchitecture.\n20.2.6Exercises\n1.What will happen if we use standard ReLU activation rather than leaky ReLU?\n2.ApplyDCGANonFashion-MNISTandseewhichcategoryworkswellandwhichdoes\nnot.\nDiscussions277.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bc7ec52-c798-4997-801a-e49c9e988c28": {"__data__": {"id_": "1bc7ec52-c798-4997-801a-e49c9e988c28", "embedding": null, "metadata": {"page_label": "893", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1067cad0-218b-4145-92a1-ef05d72d02f1", "node_type": "4", "metadata": {"page_label": "893", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "12afb85affb94a450868e2f0462da7afbcae52832b51705694f9dd5d7da30527", "class_name": "RelatedNodeInfo"}}, "text": "21 Recommender Systems\nShuaiZhang (Amazon),AstonZhang (Amazon), andYiTay(Google)\nRecommender systems are widely employed in industry and are ubiquitous in our daily\nlives. These systems are utilized in a number of areas such as online shopping sites (e.g.,\namazon.com), music/movie services site (e.g., Netflix and Spotify), mobile application\nstores (e.g., IOS app store and google play), online advertising, just to name a few.\nThe major goal of recommender systems is to help users discover relevant items such as\nmoviestowatch,texttoreadorproductstobuy,soastocreateadelightfuluserexperience.\nMoreover, recommender systems are among the most powerful machine learning systems\nthat online retailers implement in order to drive incremental revenue. Recommender sys-\ntems are replacements of search engines by reducing the efforts in proactive searches and\nsurprising users with offers they never searched for. Many companies managed to position\nthemselves ahead of their competitors with the help of more effective recommender sys-\ntems. As such, recommender systems are central to not only our everyday lives but also\nhighly indispensable in some industries.\nInthischapter,wewillcoverthefundamentalsandadvancementsofrecommendersystems,\nalongwithexploringsomecommonfundamentaltechniquesforbuildingrecommendersys-\ntemswith differentdata sources availableand theirimplementations. Specifically, youwill\nlearn how to predict the rating a user might give to a prospective item, how to generate a\nrecommendation list of items and how to predict the click-through rate from abundant fea-\ntures. These tasks are commonplace in real-world applications. By studying this chapter,\nyou will get hands-on experience pertaining to solving real world recommendation prob-\nlems with not only classical methods but the more advanced deep learning based models\nas well.\n21.1Overviewof RecommenderSystems\nIn the last decade, the Internet has evolved into a platform for large-scale online services,\nwhich profoundly changed the way we communicate, read news, buy products, and watch\nmovies. Inthemeanwhile,theunprecedentednumberofitems(weusetheterm itemtorefer\nto movies, news, books, and products.) offered online requires a system that can help us\ndiscoveritemsthatwepreferred. Recommendersystemsarethereforepowerfulinformation\n893", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2316, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0f44d0e-799a-4497-a1df-c42dea6401be": {"__data__": {"id_": "a0f44d0e-799a-4497-a1df-c42dea6401be", "embedding": null, "metadata": {"page_label": "894", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c8f2a83-1d10-4f97-9763-e6a7db2e948f", "node_type": "4", "metadata": {"page_label": "894", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f77adea67ed4e13253f75cfc9f5d1a67e12913d9918d720ec11dadd6bb3ffb7a", "class_name": "RelatedNodeInfo"}}, "text": "894 Recommender Systems\nfiltering tools that can facilitate personalized services and provide tailored experience to\nindividual users. In short, recommender systems play a pivotal role in utilizing the wealth\nof data available to make choices manageable. Nowadays, recommender systems are at\nthe core of a number of online services providers such as Amazon, Netflix, and YouTube.\nRecall the example of Deep learning books recommended by Amazon in Fig. 1.3.3 . The\nbenefits of employing recommender systems are two-folds: On the one hand, it can largely\nreduce users\u2019 effort in finding items and alleviate the issue of information overload. On\nthe other hand, it can add business value to online service providers and is an important\nsource of revenue. This chapter will introduce the fundamental concepts, classic models\nand recent advanceswith deep learning in the field of recommender systems, togetherwith\nimplemented examples.\ntFig. 21.1.1 Illustration of the Recommendation Process\n21.1.1CollaborativeFiltering\nWe start the journey with the important concept in recommender systems\u2014collaborative\nfiltering (CF), which was first coined by the Tapestry system ( Goldberg et al., 1992), re-\nferring to \u201cpeople collaborate to help one another perform the filtering process in order to\nhandlethelargeamountsofemailandmessagespostedtonewsgroups\u201d. Thistermhasbeen\nenriched with more senses. In a broad sense, it is the process of filtering for information or\npatterns using techniques involving collaboration among multiple users, agents, and data\nsources. CF has many forms and numerous CF methods proposed since its advent.\nOverall, CF techniques can be categorized into: memory-based CF, model-based CF, and\ntheir hybrid ( Su and Khoshgoftaar, 2009 ). Representative memory-based CF techniques\nare nearest neighbor-based CF such as user-based CF and item-based CF ( Sarwaret al.,\n2001). Latent factor models such as matrix factorization are examples of model-based CF.\nMemory-based CF has limitations in dealing with sparse and large-scale data since it com-\nputes the similarity values based on common items. Model-based methods become more\npopular with its better capability in dealing with sparsity and scalability. Many model-\nbased CF approaches can be extended with neural networks, leading to more flexible and\nscalable models with the computation acceleration in deep learning ( Zhanget al., 2019).\nIn general, CF only uses the user-item interaction data to make predictions and recom-\nmendations. Besides CF, content-based and context-based recommender systems are also\nuseful in incorporating the content descriptions of items/users and contextual signals such", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2671, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9f19840-b05b-41f3-bf86-ad080e9ebccc": {"__data__": {"id_": "e9f19840-b05b-41f3-bf86-ad080e9ebccc", "embedding": null, "metadata": {"page_label": "895", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e48ac36-3b63-4c9f-b356-0eba1c400395", "node_type": "4", "metadata": {"page_label": "895", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "71ab450c396dcd435f61e22fa916152f097cbb2b5bf907bf8ba5ad1d8531779f", "class_name": "RelatedNodeInfo"}}, "text": "895 Overview of Recommender Systems\n278as timestamps and locations. Obviously, we may need to adjust the model types/structures\nwhen different input data is available.\n21.1.2ExplicitFeedbackand ImplicitFeedback\nTolearnthepreferenceofusers,thesystemshallcollectfeedbackfromthem. Thefeedback\ncan be either explicit or implicit ( Huet al., 2008). For example, IMDb278collects star\nratings ranging from one to ten stars for movies. YouTube provides the thumbs-up and\nthumbs-down buttons for users to show their preferences. It is apparent that gathering\nexplicitfeedbackrequiresusersto indicate theirinterestsproactively. Nonetheless, explicit\nfeedback is not always readily available as many users may be reluctant to rate products.\nRelativelyspeaking,implicitfeedbackisoftenreadilyavailablesinceitismainlyconcerned\nwith modeling implicit behavior such as user clicks. As such, many recommender systems\narecenteredonimplicitfeedbackwhichindirectlyreflectsuser\u2019sopinionthroughobserving\nuser behavior. There are diverse forms of implicit feedback including purchase history,\nbrowsing history, watches and even mouse movements. For example, a user that purchased\nmany books by the same author probably likes that author. Note that implicit feedback is\ninherently noisy. We can only guesstheir preferences and true motives. A user watched a\nmovie does not necessarily indicate a positive view of that movie.\n21.1.3RecommendationTasks\nA number of recommendation tasks have been investigated in the past decades. Based\non the domain of applications, there are movies recommendation, news recommendations,\npoint-of-interest recommendation ( Yeet al., 2011) and so forth. It is also possible to dif-\nferentiate the tasks based on the types of feedback and input data, for example, the rating\nprediction task aims to predict the explicit ratings. Top- \ud835\udc5brecommendation (item ranking)\nranksallitemsforeachuserpersonallybasedontheimplicitfeedback. Iftime-stampinfor-\nmation is also included, we can build sequence-aware recommendation ( Quadrana et al.,\n2018). Another popular task is called click-through rate prediction, which is also based on\nimplicit feedback, but various categorical features can be utilized. Recommending for new\nusers and recommending new items to existing users are called cold-start recommendation\n(Scheinetal., 2002).\n21.1.4Summary\n\u000fRecommender systems are important for individual users and industries. Collaborative\nfiltering is a key concept in recommendation.\n\u000fThere are two types of feedbacks: implicit feedback and explicit feedback. A number of\nrecommendation tasks have been explored during the last decade.\n21.1.5Exercises\n1.Can you explain how recommender systems influence your daily life?\n2.What interesting recommendation tasks do you think can be investigated?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2786, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8c94ea6b-6624-441c-a44b-e85e5f4842e2": {"__data__": {"id_": "8c94ea6b-6624-441c-a44b-e85e5f4842e2", "embedding": null, "metadata": {"page_label": "896", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fbbfba3-46fc-46a7-9c7b-69f5140e7c79", "node_type": "4", "metadata": {"page_label": "896", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8f0efcf5629f560ee9477e7d70f4830fdcf656c75873f58b41578da71faac608", "class_name": "RelatedNodeInfo"}}, "text": "896 Recommender Systems\n279Discussions279.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 42, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "95d36ef0-7ab2-4476-b4ad-c07274d50388": {"__data__": {"id_": "95d36ef0-7ab2-4476-b4ad-c07274d50388", "embedding": null, "metadata": {"page_label": "897", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7000771f-fa4f-4b34-a811-d9b8ded08d68", "node_type": "4", "metadata": {"page_label": "897", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b7b417b2f6ef3d096a6f868d051e5e1374c0309afe894ecde83aec4119cfa280", "class_name": "RelatedNodeInfo"}}, "text": "AMathematics for Deep\nLearning\nBrentWerness (Amazon),RachelHu (Amazon), and authors of this book\nOne of the wonderful parts of modern deep learning is the fact that much of it can be\nunderstood and used without a full understanding of the mathematics below it. This is a\nsign that the field is maturing. Just as most software developers no longer need to worry\nabout the theory of computable functions, neither should deep learning practitioners need\nto worry about the theoretical foundations of maximum likelihood learning.\nBut, we are not quite there yet.\nIn practice, you will sometimes need to understand how architectural choices influence\ngradientflow,ortheimplicitassumptionsyoumakebytrainingwithacertainlossfunction.\nYou might need to know what in the world entropy measures, and how it can help you\nunderstand exactly what bits-per-character means in your model. These all require deeper\nmathematical understanding.\nThis appendix aims to provide you the mathematical background you need to understand\nthe core theory of modern deep learning, but it is not exhaustive. We will begin with\nexamininglinearalgebraingreaterdepth. Wedevelopageometricunderstandingofallthe\ncommon linear algebraic objects and operations that will enable us to visualize the effects\nof various transformations on our data. A key element is the development of the basics of\neigen-decompositions.\nWenextdevelopthetheoryofdifferentialcalculustothepointthatwecanfullyunderstand\nwhy the gradient is the direction of steepest descent, and why back-propagation takes the\nform it does. Integral calculus is then discussed to the degree needed to support our next\ntopic, probability theory.\nProblemsencounteredinpracticefrequentlyarenotcertain,andthusweneedalanguageto\nspeakaboutuncertainthings. Wereviewthetheoryofrandomvariablesandthemostcom-\nmonlyencountereddistributionssowemaydiscussmodelsprobabilistically. Thisprovides\nthefoundationforthenaiveBayesclassifier,aprobabilisticclassificationtechnique.\nClosely related to probability theory is the study of statistics. While statistics is far too\nlargeafield todo justiceina shortsection, wewillintroduce fundamentalconcepts thatall\nmachinelearningpractitionersshouldbeawareof,inparticular: evaluatingandcomparing\nestimators, conducting hypothesis tests, and constructing confidence intervals.\nLast, we turn to the topic of information theory, which is the mathematical study of infor-\n897", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2417, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fae7402a-d577-4023-84c4-c1a0ebe10ffd": {"__data__": {"id_": "fae7402a-d577-4023-84c4-c1a0ebe10ffd", "embedding": null, "metadata": {"page_label": "898", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bfd95a86-26e2-4148-b364-865713db1d51", "node_type": "4", "metadata": {"page_label": "898", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2dab82ec78cfeef38b79d579ef1a860e3e9ad7829d7725b5f186155af0f51f9a", "class_name": "RelatedNodeInfo"}}, "text": "898 Mathematics for Deep Learning\nmationstorageandtransmission. Thisprovidesthecorelanguagebywhichwemaydiscuss\nquantitatively how much information a model holds on a domain of discourse.\nTakentogether,theseformthecoreofthemathematicalconceptsneededtobegindownthe\npath towards a deep understanding of deep learning.\nA.1Geometryand Linear AlgebraicOperations\nInSection 2.3 , we encountered the basics of linear algebra and saw how it could be used\nto express common operations for transforming our data. Linear algebra is one of the key\nmathematical pillars underlying much of the work that we do in deep learning and in ma-\nchine learning more broadly. While Section 2.3 contained enough machinery to commu-\nnicate the mechanics of modern deep learning models, there is a lot more to the subject.\nIn this section, we will go deeper, highlighting some geometric interpretations of linear\nalgebra operations, and introducing a few fundamental concepts, including of eigenvalues\nand eigenvectors.\nA.1.1Geometryof Vectors\nFirst, we need to discuss the two common geometric interpretations of vectors, as either\npoints or directions in space. Fundamentally, a vector is a list of numbers such as the\nPython list below.\nv=[1,7,0,1]\nMathematiciansmostoftenwritethisaseithera columnorrowvector,whichistosayeither\nas\nx=266666641\n7\n0\n137777775, (A.1)\nor\nx>=\u0002\n1 7 0 1\u0003\n. (A.2)\nThese often have different interpretations, where data examples are column vectors and\nweights used to form weighted sums are row vectors. However, it can be beneficial to be\nflexible. Aswehavedescribedin Section2.3 ,thoughasinglevector\u2019sdefaultorientationis\na column vector, for any matrix representing a tabular dataset, treating each data example\nas a row vector in the matrix is more conventional.\nGiven a vector, the first interpretation that we should give it is as a point in space. In two\nor three dimensions, we can visualize these points by using the components of the vectors", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1951, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2328051a-1fa0-47ff-b173-5106686eb2a4": {"__data__": {"id_": "2328051a-1fa0-47ff-b173-5106686eb2a4", "embedding": null, "metadata": {"page_label": "899", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "25447448-a2e5-4b64-bfe2-63be4984a1af", "node_type": "4", "metadata": {"page_label": "899", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bc0afc73eaa749c4421d1fce7af41f139f5869ba702aecba1227a4c40db8f266", "class_name": "RelatedNodeInfo"}}, "text": "899 Geometry and Linear Algebraic Operations\nto define the location of the points in space compared to a fixed reference called the origin.\nThis can be seen in Fig. A.1.\ntFig. A.1 An illustration of visualizing vectors as points in the plane. The \ufb01rst component of the\nvector gives the x-coordinate, the second component gives the y-coordinate. Higher\ndimensions are analogous, although much harder to visualize.\nThisgeometricpointofviewallowsustoconsidertheproblemonamoreabstractlevel. No\nlongerfacedwithsomeinsurmountableseemingproblemlikeclassifyingpicturesaseither\ncats or dogs, we can start considering tasks abstractly as collections of points in space and\npicturing the task as discovering how to separate two distinct clusters of points.\nIn parallel, there is a second point of view that people often take of vectors: as directions\nin space. Not onlycan we think of the vector v=\u00bb3,2\u00bc>as the location 3units to the right\nand2units up from the origin, we can also think of it as the direction itself to take 3steps\nto the right and 2steps up. In this way, we consider all the vectors in figure Fig. A.2 the\nsame.\ntFig. A.2 Any vector can be visualized as an arrow in the plane. In this case, every vector drawn is a\nrepresentation of the vector \u00b93,2\u00ba>.\nOneofthebenefitsofthisshiftisthatwecanmakevisualsenseoftheactofvectoraddition.\nIn particular, we follow the directions given by one vector, and then follow the directions\ngiven by the other, as is seen in Fig. A.3.\nVector subtraction has a similar interpretation. By considering the identity that u=v\u00b8\n\u00b9u\u0000v\u00ba, we see that the vector u\u0000vis the direction that takes us from the point vto the\npointu.\nA.1.2DotProductsand Angles", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e38bb4e-f293-4093-a863-ed4b94dca12e": {"__data__": {"id_": "9e38bb4e-f293-4093-a863-ed4b94dca12e", "embedding": null, "metadata": {"page_label": "900", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "17a136d1-14dd-4305-924f-c5a3579e5699", "node_type": "4", "metadata": {"page_label": "900", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "aa5505502623106e4c1aaacbb1a56e46cebf8fbfa58c7c269cc95e62bea1e787", "class_name": "RelatedNodeInfo"}}, "text": "900 Mathematics for Deep Learning\ntFig. A.3 We can visualize vector addition by \ufb01rst following one vector, and then another.\nAs we saw in Section 2.3 , if we take two column vectors uandv, we can form their dot\nproduct by computing:\nu>v=\u00d5\n\ud835\udc56\ud835\udc62\ud835\udc56\u0001\ud835\udc63\ud835\udc56.(A.3)\nBecause (A.3)is symmetric, we will mirror the notation of classical multiplication and\nwrite\nu\u0001v=u>v=v>u, (A.4)\ntohighlightthefactthatexchangingtheorderofthevectorswillyieldthesameanswer.\nThe dot product (A.3)also admits a geometric interpretation: it is closely related to the\nangle between two vectors. Consider the angle shown in Fig. A.4.\ntFig. A.4 Between any two vectors in the plane there is a well de\ufb01ned angle \ud835\udf03. We will see this\nangle is intimately tied to the dot product.\nTo start, let\u2019s consider two specific vectors:\nv=\u00b9\ud835\udc5f,0\u00baandw=\u00b9\ud835\udc60cos\u00b9\ud835\udf03\u00ba,\ud835\udc60sin\u00b9\ud835\udf03\u00ba\u00ba. (A.5)\nThe vector vis length\ud835\udc5fand runs parallel to the \ud835\udc65-axis, and the vector wis of length \ud835\udc60\nand at angle \ud835\udf03with the\ud835\udc65-axis. If we compute the dot product of these two vectors, we see\nthat\nv\u0001w=\ud835\udc5f\ud835\udc60cos\u00b9\ud835\udf03\u00ba=kvkkwkcos\u00b9\ud835\udf03\u00ba. (A.6)\nWith some simple algebraic manipulation, we can rearrange terms to obtain\n\ud835\udf03=arccos\u0012v\u0001w\nkvkkwk\u0013\n. (A.7)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1135, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "826174a6-72d6-41ec-854b-654d4a3d4f03": {"__data__": {"id_": "826174a6-72d6-41ec-854b-654d4a3d4f03", "embedding": null, "metadata": {"page_label": "901", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88084533-c476-427c-8d28-250771cfefc1", "node_type": "4", "metadata": {"page_label": "901", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2ab7987ae099146ff90b470e65ed11a031440fd5f96009ba8554c1111b7d2ef2", "class_name": "RelatedNodeInfo"}}, "text": "901 Geometry and Linear Algebraic Operations\nIn short, for these two specific vectors, the dot product combined with the norms tell us\nthe angle between the two vectors. This same fact is true in general. We will not derive\nthe expression here, however, if we consider writing kv\u0000wk2in two ways: one with\nthe dot product, and the other geometrically using the law of cosines, we can obtain the\nfull relationship. Indeed, for any two vectors vandw, the angle between the two vectors\nis\n\ud835\udf03=arccos\u0012v\u0001w\nkvkkwk\u0013\n. (A.8)\nThis is a nice result since nothing in the computation references two-dimensions. Indeed,\nwe can use this in three or three million dimensions without issue.\nAs a simple example, let\u2019s see how to compute the angle between a pair of vectors:\n%matplotlib inline\nimport torch\nimport torchvision\nfrom IPython import display\nfrom torchvision import transforms\nfrom d2l import torch asd2l\ndef angle (v, w):\nreturn torch .acos(v .dot(w) /(torch .norm(v) *torch .norm(w)))\nangle(torch .tensor([ 0,1,2], dtype =torch .float32), torch .tensor([ 2.0,3,4]))\ntensor( 0.4190 )\nWe will not use it right now, but it is useful to know that we will refer to vectors for which\nthe angle is \ud835\udf0b\u009d2(or equivalently 90\u000e) as being orthogonal . By examining the equation\nabove, we see that this happens when \ud835\udf03=\ud835\udf0b\u009d2, which is the same thing as cos\u00b9\ud835\udf03\u00ba=0. The\nonly way this can happen is if the dot product itself is zero, and two vectors are orthogonal\nifandonlyif v\u0001w=0. Thiswillprovetobeahelpfulformulawhenunderstandingobjects\ngeometrically.\nIt is reasonable to ask: why is computing the angle useful? The answer comes in the kind\nof invariance we expect data to have. Consider an image, and a duplicate image, where\nevery pixel value is the same but 10%the brightness. The values of the individual pixels\nare in general far from the original values. Thus, if one computed the distance between\nthe original image and the darker one, the distance can be large. However, for most ML\napplications,the contentisthesame\u2014itisstillanimageofacatasfarasacat/dogclassifier\nis concerned. However, if we consider the angle, it is not hard to see that for any vector v,\nthe angle between vand0.1\u0001vis zero. This corresponds to the fact that scaling vectors\nkeepsthesamedirectionandjustchangesthelength. Theangleconsidersthedarkerimage\nidentical.\nExamples like this are everywhere. In text, we might want the topic being discussed to\nnot change if we write twice as long of document that says the same thing. For some", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ca65cd8-c9c5-4181-8520-ad31103ebce3": {"__data__": {"id_": "4ca65cd8-c9c5-4181-8520-ad31103ebce3", "embedding": null, "metadata": {"page_label": "902", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f19184bc-33fe-4325-a440-1dc077fc720c", "node_type": "4", "metadata": {"page_label": "902", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cd13c5ac48017245582a15c6acdbcbaf0defe7672055dfef69314634bc5c3377", "class_name": "RelatedNodeInfo"}}, "text": "902 Mathematics for Deep Learning\nencoding (such as counting the number of occurrences of words in some vocabulary), this\ncorresponds to a doubling of the vector encoding the document, so again we can use the\nangle.\nCosine Similarity\nIn ML contextswhere the angle is employed to measure the closeness of twovectors, prac-\ntitioners adopt the term cosinesimilarity to refer to the portion\ncos\u00b9\ud835\udf03\u00ba=v\u0001w\nkvkkwk. (A.9)\nThe cosine takes a maximum value of 1when the two vectors point in the same direction,\na minimum value of \u00001when they point in opposite directions, and a value of 0when the\ntwo vectors are orthogonal. Note that if the components of high-dimensional vectors are\nsampled randomly with mean 0, their cosine will nearly always be close to 0.\nA.1.3Hyperplanes\nIn addition to working with vectors, another key object that you must understand to go far\nin linear algebra is the hyperplane , a generalization to higher dimensions of a line (two di-\nmensions)orofaplane(threedimensions). Inan \ud835\udc51-dimensionalvectorspace,ahyperplane\nhas\ud835\udc51\u00001dimensions and divides the space into two half-spaces.\nLet\u2019s start with an example. Suppose that we have a column vector w=\u00bb2,1\u00bc>. We want\nto know, \u201cwhat are the points vwithw\u0001v=1?\u201d By recalling the connection between dot\nproducts and angles above (A.8), we can see that this is equivalent to\nkvkkwkcos\u00b9\ud835\udf03\u00ba=1() k vkcos\u00b9\ud835\udf03\u00ba=1\nkwk=1p\n5. (A.10)\ntFig. A.5 Recalling trigonometry, we see the formula kvkcos\u00b9\ud835\udf03\u00bais the length of the projection of\nthe vector vonto the direction of w\nIf we consider the geometric meaning of this expression, we see that this is equivalent to\nsaying that the length of the projection of vonto the direction of wis exactly 1\u009dkwk, as\nis shown in Fig. A.5. The set of all points where this is true is a line at right angles to the\nvectorw. If we wanted, we could find the equation for this line and see that it is 2\ud835\udc65\u00b8\ud835\udc66=1\nor equivalently \ud835\udc66=1\u00002\ud835\udc65.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d369a3f7-414a-4e14-9aed-70c27582456d": {"__data__": {"id_": "d369a3f7-414a-4e14-9aed-70c27582456d", "embedding": null, "metadata": {"page_label": "903", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b0df48ad-3cb4-4ea4-a6d1-cc4211fab607", "node_type": "4", "metadata": {"page_label": "903", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e92739c1f3ffae7f9d1455b281fc33903e164d638c1602043baac422cc56bc89", "class_name": "RelatedNodeInfo"}}, "text": "903 Geometry and Linear Algebraic Operations\nIf we now look at what happens when we ask about the set of points with w\u0001v>1or\nw\u0001v<1, we can see that these are cases where the projections are longer or shorter than\n1\u009dkwk, respectively. Thus, thosetwoinequalitiesdefineeithersideoftheline. Inthisway,\nwe have found a way to cut our space into two halves, where all the points on one side have\ndot product below a threshold, and the other side above as we see in Fig. A.6.\ntFig. A.6 If we now consider the inequality version of the expression, we see that our hyperplane (in\nthis case: just a line) separates the space into two halves.\nThe story in higher dimension is much the same. If we now take w=\u00bb1,2,3\u00bc>and ask\naboutthepointsinthreedimensionswith w\u0001v=1, weobtainaplane atrightanglestothe\ngiven vector w. The two inequalities again define the two sides of the plane as is shown in\nFig. A.7.\ntFig. A.7 Hyperplanes in any dimension separate the space into two halves.\nWhile our ability to visualize runs out at this point, nothing stops us from doing this in\ntens, hundreds, or billions of dimensions. This occurs often when thinking about machine\nlearned models. For instance, we can understand linear classification models like those\nfromSection 4.1 , as methods to find hyperplanes that separate the different target classes.\nIn this context, such hyperplanes are often referred to as decision planes . The majority of\ndeep learned classification models end with a linear layer fed into a softmax, so one can\ninterpret the role of the deep neural network to be to find a non-linear embedding such that\nthe target classes can be separated cleanly by hyperplanes.\nTo give a hand-built example, notice that we can produce a reasonable model to classify\ntiny images of t-shirts and trousers from the Fashion-MNIST dataset (seen in Section 4.2 )\nby just taking the vector between their means to define the decision plane and eyeball a\ncrude threshold. First we will load the data and compute the averages.\n# Load in the dataset\ntrans =[]\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c42f0acf-5c83-4942-989b-cc77266bfd9b": {"__data__": {"id_": "c42f0acf-5c83-4942-989b-cc77266bfd9b", "embedding": null, "metadata": {"page_label": "904", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb6cea9e-2448-4dfe-acaa-9532e1e68cf2", "node_type": "4", "metadata": {"page_label": "904", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1e86c0a6d74ed381fbc47d9326002b46cf3500bbd259b9500e4f1b288fb9517c", "class_name": "RelatedNodeInfo"}}, "text": "904 Mathematics for Deep Learning\n(continued from previous page)\ntrans .append(transforms .ToTensor())\ntrans =transforms .Compose(trans)\ntrain =torchvision .datasets .FashionMNIST(root =\"../data \", transform =trans,\ntrain =True , download =True )\ntest =torchvision .datasets .FashionMNIST(root =\"../data \", transform =trans,\ntrain =False , download =True )\nX_train_0 =torch .stack(\n[x[0]*256 for xintrain ifx[1]==0]).type(torch .float32)\nX_train_1 =torch .stack(\n[x[0]*256 for xintrain ifx[1]==1]).type(torch .float32)\nX_test =torch .stack(\n[x[0]*256 for xintest ifx[1]==0orx[1]==1]).type(torch .float32)\ny_test =torch .stack([torch .tensor(x[ 1])for xintest\nifx[1]==0orx[1]==1]).type(torch .float32)\n# Compute averages\nave_0 =torch .mean(X_train_0, axis =0)\nave_1 =torch .mean(X_train_1, axis =0)\nIt can be informative to examine these averages in detail, so let\u2019s plot what they look like.\nIn this case, we see that the average indeed resembles a blurry image of a t-shirt.\n# Plot average t-shirt\nd2l.set_figsize()\nd2l.plt.imshow(ave_0 .reshape( 28,28).tolist(), cmap ='Greys ')\nd2l.plt.show()\nInthesecondcase,weagainseethattheaverageresemblesablurryimageoftrousers.\n# Plot average trousers\nd2l.plt.imshow(ave_1 .reshape( 28,28).tolist(), cmap ='Greys ')\nd2l.plt.show()\nIn a fully machine learned solution, we would learn the threshold from the dataset. In this\ncase, I simply eyeballed a threshold that looked good on the training data by hand.\n# Print test set accuracy with eyeballed threshold\nw=(ave_1 -ave_0) .T\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f88d2c5-d9df-428d-b946-fd7e7f215871": {"__data__": {"id_": "6f88d2c5-d9df-428d-b946-fd7e7f215871", "embedding": null, "metadata": {"page_label": "905", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "655ebe1c-6034-4a99-9087-9f55318f1adc", "node_type": "4", "metadata": {"page_label": "905", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f1f55a5f5bd435dd1d15232aba69d1234175f8f1d372018191e7d6a60c453905", "class_name": "RelatedNodeInfo"}}, "text": "905 Geometry and Linear Algebraic Operations\n(continued from previous page)\n# '@' is Matrix Multiplication operator in pytorch.\npredictions =X_test .reshape( 2000 ,-1)@(w.flatten()) >-1500000\n# Accuracy\ntorch .mean((predictions .type(y_test .dtype) ==y_test) .float(), dtype =torch .\n\u21a9!float64)\ntensor( 0.7870 , dtype =torch .float64)\nA.1.4Geometryof Linear Transformations\nThrough Section2.3 andtheabovediscussions,wehaveasolidunderstandingofthegeom-\netryofvectors,lengths,andangles. However,thereisoneimportantobjectwehaveomitted\ndiscussing, and that is a geometric understanding of linear transformations represented by\nmatrices. Fully internalizing what matrices can do to transform data between two poten-\ntially different high dimensional spaces takes significant practice, and is beyond the scope\nof this appendix. However, we can start building up intuition in two dimensions.\nSuppose that we have some matrix:\nA=\u0014\ud835\udc4e \ud835\udc4f\n\ud835\udc50 \ud835\udc51\u0015\n. (A.11)\nIf we want to apply this to an arbitrary vector v=\u00bb\ud835\udc65,\ud835\udc66\u00bc>, we multiply and see that\nAv=\u0014\ud835\udc4e \ud835\udc4f\n\ud835\udc50 \ud835\udc51\u0015 \u0014\ud835\udc65\n\ud835\udc66\u0015\n=\u0014\ud835\udc4e\ud835\udc65\u00b8\ud835\udc4f\ud835\udc66\n\ud835\udc50\ud835\udc65\u00b8\ud835\udc51\ud835\udc66\u0015\n=\ud835\udc65\u0014\ud835\udc4e\n\ud835\udc50\u0015\n\u00b8\ud835\udc66\u0014\ud835\udc4f\n\ud835\udc51\u0015\n=\ud835\udc65\u001a\nA\u00141\n0\u0015\u001b\n\u00b8\ud835\udc66\u001a\nA\u00140\n1\u0015\u001b\n.(A.12)\nThis may seem like an odd computation, where something clear became somewhat impen-\netrable. However, it tells us that we can write the way that a matrix transforms anyvector", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6b5cd206-2a5b-4ebe-9304-968d60a8d9a4": {"__data__": {"id_": "6b5cd206-2a5b-4ebe-9304-968d60a8d9a4", "embedding": null, "metadata": {"page_label": "906", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf6c459c-f5d8-48a3-bb57-9a1a481da80f", "node_type": "4", "metadata": {"page_label": "906", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9c3143b5dc9063218a8ecfa11c265dc76839dbda4b4744f3516e1aafe4dfeee0", "class_name": "RelatedNodeInfo"}}, "text": "906 Mathematics for Deep Learning\nin terms of how it transforms two specific vectors :\u00bb1,0\u00bc>and\u00bb0,1\u00bc>. This is worth con-\nsidering for a moment. We have essentially reduced an infinite problem (what happens to\nany pair of real numbers) to a finite one (what happens to these specific vectors). These\nvectors are an example a basis, where we can write any vector in our space as a weighted\nsum of these basis vectors .\nLet\u2019s draw what happens when we use the specific matrix\nA=\u00141 2\n\u00001 3\u0015\n. (A.13)\nIf we look at the specific vector v=\u00bb2,\u00001\u00bc>, we see this is 2\u0001\u00bb1,0\u00bc>\u00b8\u00001\u0001\u00bb0,1\u00bc>, and\nthusweknowthatthematrix \ud835\udc34willsendthisto 2\u00b9A\u00bb1,0\u00bc>\u00ba\u00b8\u00001\u00b9A\u00bb0,1\u00bc\u00ba>=2\u00bb1,\u00001\u00bc>\u0000\n\u00bb2,3\u00bc>=\u00bb0,\u00005\u00bc>. If we follow this logic through carefully, say by considering the grid\nof all integer pairs of points, we see that what happens is that the matrix multiplication\ncan skew, rotate, and scale the grid, but the grid structure must remain as you see in Fig.\nA.8.\ntFig. A.8 The matrix Aacting on the given basis vectors. Notice how the entire grid is transported\nalong with it.\nThis is the most important intuitive point to internalize about linear transformations rep-\nresented by matrices. Matrices are incapable of distorting some parts of space differently\nthan others. All they can do is take the original coordinates on our space and skew, rotate,\nand scale them.\nSome distortions can be severe. For instance the matrix\nB=\u00142\u00001\n4\u00002\u0015\n, (A.14)\ncompressestheentiretwo-dimensionalplanedowntoasingleline. Identifyingandworking\nwith such transformations are the topic of a later section, but geometrically we can see\nthat this is fundamentally different from the types of transformations we saw above. For\ninstance, theresultfrommatrix Acanbe\u201cbentback\u201dtotheoriginalgrid. Theresultsfrom\nmatrix Bcannot because we will never know where the vector \u00bb1,2\u00bc>came from\u2014was it\n\u00bb1,1\u00bc>or\u00bb0,\u00001\u00bc>?\nWhilethispicturewasfora 2\u00022matrix,nothingpreventsusfromtakingthelessonslearned\nintohigherdimensions. Ifwetakesimilarbasisvectorslike \u00bb1,0,..., 0\u00bcandseewhereour\nmatrix sends them, we can start to get a feeling for how the matrix multiplication distorts\nthe entire space in whatever dimension space we are dealing with.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2165, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d593b936-92cd-4459-9b35-ece89cd6177c": {"__data__": {"id_": "d593b936-92cd-4459-9b35-ece89cd6177c", "embedding": null, "metadata": {"page_label": "907", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1d42eace-8e1d-4d0a-af6c-e3a8da4b7f8c", "node_type": "4", "metadata": {"page_label": "907", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c9dc328b6bf112d9f88337c60eff31018622590508fe5b3b815a01785a124663", "class_name": "RelatedNodeInfo"}}, "text": "907 Geometry and Linear Algebraic Operations\nA.1.5LinearDependence\nConsider again the matrix\nB=\u00142\u00001\n4\u00002\u0015\n. (A.15)\nThis compresses the entire plane down to live on the single line \ud835\udc66=2\ud835\udc65. The question now\narises: is there some way we can detect this just looking at the matrix itself? The answer is\nthat indeed we can. Let\u2019s take b1=\u00bb2,4\u00bc>andb2=\u00bb\u00001,\u00002\u00bc>be the two columns of B.\nRemember that we can write everything transformed bythe matrix Bas a weighted sum of\nthe columns of the matrix: like \ud835\udc4e1b1\u00b8\ud835\udc4e2b2. We call this a linear combination . The fact\nthatb1=\u00002\u0001b2means that we can write any linear combination of those two columns\nentirely in terms of say b2since\n\ud835\udc4e1b1\u00b8\ud835\udc4e2b2=\u00002\ud835\udc4e1b2\u00b8\ud835\udc4e2b2=\u00b9\ud835\udc4e2\u00002\ud835\udc4e1\u00bab2. (A.16)\nThis means that one of the columns is, in a sense, redundant because it does not define a\nunique direction in space. This should not surprise us too much since we already saw that\nthis matrix collapses the entire plane down into a single line. Moreover, we see that the\nlinear dependence b1=\u00002\u0001b2captures this. To make this more symmetrical between the\ntwo vectors, we will write this as\nb1\u00b82\u0001b2=0. (A.17)\nIngeneral,wewillsaythatacollectionofvectors v1,...,v\ud835\udc58arelinearlydependent ifthere\nexist coefficients \ud835\udc4e1,...,\ud835\udc4e\ud835\udc58notall equaltozero so that\n\ud835\udc58\u00d5\n\ud835\udc56=1\ud835\udc4e\ud835\udc56vi=0. (A.18)\nInthiscase, wecansolveforoneofthevectorsintermsofsomecombinationoftheothers,\nand effectively render it redundant. Thus, a linear dependence in the columns of a matrix\nis a witness to the fact that our matrix is compressing the space down to some lower di-\nmension. If there is no linear dependence we say the vectors are linearly independent . If\nthe columns of a matrix are linearly independent, no compression occurs and the operation\ncan be undone.\nA.1.6Rank\nIf we have a general \ud835\udc5b\u0002\ud835\udc5amatrix, it is reasonable to ask what dimension space the matrix\nmaps into. A concept known as the rankwill be our answer. In the previous section, we\nnotedthatalineardependencebearswitnesstocompressionofspaceintoalowerdimension\nand so we will be able to use this to define the notion of rank. In particular, the rank of\na matrix Ais the largest number of linearly independent columns amongst all subsets of\ncolumns. For example, the matrix\nB=\u00142 4\n\u00001\u00002\u0015\n, (A.19)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2213, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "aa11f417-4fe2-4846-ab61-0f20bcab9492": {"__data__": {"id_": "aa11f417-4fe2-4846-ab61-0f20bcab9492", "embedding": null, "metadata": {"page_label": "908", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bec61c2e-e6de-4e2b-833d-06eb5deae25b", "node_type": "4", "metadata": {"page_label": "908", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "15ca51e90ec9bb38e0d3642f9eb2d6a0f25787ba4a5e234638c4de9a8bcf93be", "class_name": "RelatedNodeInfo"}}, "text": "908 Mathematics for Deep Learning\nhas rank\u00b9\ud835\udc35\u00ba=1, since the two columns are linearly dependent, but either column by itself\nis not linearly dependent. For a more challenging example, we can consider\nC=266666641 3 0\u00001 0\n\u00001 0 1 1\u00001\n0 3 1 0\u00001\n2 3\u00001\u00002 137777775, (A.20)\nand show that Chas rank two since, for instance, the first two columns are linearly inde-\npendent, however any of the four collections of three columns are dependent.\nThis procedure, as described, is very inefficient. It requires looking at every subset of the\ncolumnsofourgivenmatrix, andthusispotentiallyexponentialinthenumberofcolumns.\nLater we will see a more computationally efficient way to compute the rank of a matrix,\nbut for now, this is sufficient to see that the concept is well defined and understand the\nmeaning.\nA.1.7Invertibility\nWehaveseenabovethatmultiplicationbyamatrixwithlinearlydependentcolumnscannot\nbe undone, i.e., there is no inverse operation that can always recover the input. However,\nmultiplication by a full-rank matrix (i.e., some Athat is\ud835\udc5b\u0002\ud835\udc5bmatrix with rank \ud835\udc5b), we\nshould always be able to undo it. Consider the matrix\nI=2666666641 0\u0001\u0001\u0001 0\n0 1\u0001\u0001\u0001 0\n............\n0 0\u0001\u0001\u0001 1377777775. (A.21)\nwhich is the matrix with ones along the diagonal, and zeros elsewhere. We call this the\nidentitymatrix. It is the matrix which leaves our data unchanged when applied. To find\na matrix which undoes what our matrix Ahas done, we want to find a matrix A\u00001such\nthat\nA\u00001A=AA\u00001=I. (A.22)\nIf we look at this as a system, we have \ud835\udc5b\u0002\ud835\udc5bunknowns (the entries of A\u00001) and\ud835\udc5b\u0002\ud835\udc5b\nequations (the equality that needs to hold between every entry of the product A\u00001Aand\nevery entry of I) so we should generically expect a solution to exist. Indeed, in the next\nsectionwewillseeaquantitycalledthe determinant ,whichhasthepropertythataslongas\nthe determinant is not zero, we can find a solution. We call such a matrix A\u00001theinverse\nmatrix. As an example, if Ais the general 2\u00022matrix\nA=\u0014\ud835\udc4e \ud835\udc4f\n\ud835\udc50 \ud835\udc51\u0015\n, (A.23)\nthen we can see that the inverse is\n1\n\ud835\udc4e\ud835\udc51\u0000\ud835\udc4f\ud835\udc50\u0014\ud835\udc51\u0000\ud835\udc4f\n\u0000\ud835\udc50 \ud835\udc4e\u0015\n. (A.24)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2028, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7b839114-8ef7-42b1-b4f1-351783de4038": {"__data__": {"id_": "7b839114-8ef7-42b1-b4f1-351783de4038", "embedding": null, "metadata": {"page_label": "909", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dcf74ee0-abdb-45c0-9e06-35a3b586e2cc", "node_type": "4", "metadata": {"page_label": "909", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8c1a6202c07a6c0b9b9855712154acfc3bde33357544e67ef5f6e8edc3b2a6d5", "class_name": "RelatedNodeInfo"}}, "text": "909 Geometry and Linear Algebraic Operations\nWe can test to see this by seeing that multiplying by the inverse given by the formula above\nworks in practice.\nM=torch .tensor([[ 1,2], [ 1,4]], dtype =torch .float32)\nM_inv =torch .tensor([[ 2,-1], [ -0.5,0.5]])\nM_inv @M\ntensor([[ 1.,0.],\n[0.,1.]])\nNumericalIssues\nWhile the inverse of a matrix is useful in theory, we must say that most of the time we do\nnot wish to usethe matrix inverse to solve a problem in practice. In general, there are far\nmore numerically stable algorithms for solving linear equations like\nAx=b, (A.25)\nthan computing the inverse and multiplying to get\nx=A\u00001b. (A.26)\nJust as division by a small number can lead to numerical instability, so can inversion of a\nmatrix which is close to having low rank.\nMoreover, it is common that the matrix Aissparse, which is to say that it contains only a\nsmall number of non-zero values. If we were to explore examples, we would see that this\ndoesnotmeantheinverseissparse. Evenif Awasa 1millionby 1millionmatrixwithonly\n5million non-zero entries (and thus we need only store those 5million), the inverse will\ntypically have almost every entry non-negative, requiring us to store all 1M2entries\u2014that\nis1trillion entries!\nWhile we do not have time to dive all the way into the thorny numerical issues frequently\nencounteredwhenworkingwithlinearalgebra,wewanttoprovideyouwithsomeintuition\nabout when to proceed with caution, and generally avoiding inversion in practice is a good\nrule of thumb.\nA.1.8Determinant\nThe geometric view of linear algebra gives an intuitive way to interpret a fundamental\nquantity known as the determinant . Consider the grid image from before, but now with a\nhighlighted region ( Fig. A.9).\nLook at the highlighted square. This is a square with edges given by \u00b90,1\u00baand\u00b91,0\u00baand\nthusithasareaone. After Atransformsthissquare,weseethatitbecomesaparallelogram.\nThere is no reason this parallelogram should have the same area that we started with, and", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd4ff1b2-b667-4c9f-863b-f7c781c461e4": {"__data__": {"id_": "fd4ff1b2-b667-4c9f-863b-f7c781c461e4", "embedding": null, "metadata": {"page_label": "910", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bfa176a-1f95-4e51-9b3a-316ce84a22c7", "node_type": "4", "metadata": {"page_label": "910", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c58575831cb8ae8fde683c24d0aa2739a69d0f619fef5c323a47df082a846ef5", "class_name": "RelatedNodeInfo"}}, "text": "910 Mathematics for Deep Learning\ntFig. A.9 The matrix Aagain distorting the grid. This time, I want to draw particular attention to\nwhat happens to the highlighted square.\nindeed in the specific case shown here of\nA=\u00141 2\n\u00001 3\u0015\n, (A.27)\nitisanexerciseincoordinategeometrytocomputetheareaofthisparallelogramandobtain\nthat the area is 5.\nIn general, if we have a matrix\nA=\u0014\ud835\udc4e \ud835\udc4f\n\ud835\udc50 \ud835\udc51\u0015\n, (A.28)\nwe can see with some computation that the area of the resulting parallelogram is \ud835\udc4e\ud835\udc51\u0000\ud835\udc4f\ud835\udc50.\nThis area is referred to as the determinant .\nLet\u2019s check this quickly with some example code.\ntorch .det(torch .tensor([[ 1,-1], [ 2,3]], dtype =torch .float32))\ntensor( 5.)\nThe eagle-eyed amongst us will notice that this expression can be zero or even negative.\nFor the negative term, this is a matter of convention taken generally in mathematics: if the\nmatrix flips the figure, we say the area is negated. Let\u2019s see now that when the determinant\nis zero, we learn more.\nLet\u2019s consider\nB=\u00142 4\n\u00001\u00002\u0015\n. (A.29)\nIf we compute the determinant of this matrix, we get 2\u0001\u00b9\u00002\u00ba\u00004\u0001\u00b9\u00001\u00ba=0. Given our\nunderstanding above, this makes sense. Bcompresses the square from the original image\ndown to a line segment, which has zero area. And indeed, being compressed into a lower\ndimensional space is the only way to have zero area after the transformation. Thus we see\nthe following result is true: a matrix \ud835\udc34is invertible if and only if the determinant is not\nequal to zero.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "714c0367-e257-48e9-ab67-57cee81a2d05": {"__data__": {"id_": "714c0367-e257-48e9-ab67-57cee81a2d05", "embedding": null, "metadata": {"page_label": "911", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f8c2413-171e-4f98-8d19-12346440980a", "node_type": "4", "metadata": {"page_label": "911", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "38083a0701724d1d73c7aa05ab836c005680d165710f8b53d063df6c3582b047", "class_name": "RelatedNodeInfo"}}, "text": "911 Geometry and Linear Algebraic Operations\nAs a final comment, imagine that we have any figure drawn on the plane. Thinking like\ncomputer scientists, we can decompose that figure into a collection of little squares so that\nthe area of the figure is in essence just the number of squares in the decomposition. If we\nnowtransformthatfigurebyamatrix,wesendeachofthesesquarestoparallelograms,each\noneofwhichhasareagivenbythedeterminant. Weseethatforanyfigure,thedeterminant\ngives the (signed) number that a matrix scales the area of any figure.\nComputing determinants for larger matrices can be laborious, but the intuition is the same.\nThedeterminantremainsthefactorthat \ud835\udc5b\u0002\ud835\udc5bmatricesscale \ud835\udc5b-dimensionalvolumes.\nA.1.9Tensorsand Common Linear AlgebraOperations\nInSection 2.3 the concept of tensors was introduced. In this section, we will dive more\ndeeply into tensor contractions (the tensor equivalent of matrix multiplication), and see\nhow it can provide a unified view on a number of matrix and vector operations.\nWith matrices and vectors we knew how to multiply them to transform data. We need\nto have a similar definition for tensors if they are to be useful to us. Think about matrix\nmultiplication:\nC=AB, (A.30)\nor equivalently\n\ud835\udc50\ud835\udc56,\ud835\udc57=\u00d5\n\ud835\udc58\ud835\udc4e\ud835\udc56,\ud835\udc58\ud835\udc4f\ud835\udc58,\ud835\udc57.(A.31)\nThis pattern is one we can repeat for tensors. For tensors, there is no one case of what to\nsum over that can be universally chosen, so we need specify exactly which indices we want\nto sum over. For instance we could consider\n\ud835\udc66\ud835\udc56\ud835\udc59=\u00d5\n\ud835\udc57\ud835\udc58\ud835\udc65\ud835\udc56\ud835\udc57\ud835\udc58\ud835\udc59\ud835\udc4e\ud835\udc57\ud835\udc58.(A.32)\nSuch a transformation is called a tensor contraction . It can represent a far more flexible\nfamily of transformations that matrix multiplication alone.\nAs a often-used notational simplification, we can notice that the sum is over exactly those\nindices that occur more than once in the expression, thus people often work with Einstein\nnotation, where the summation is implicitly taken over all repeated indices. This gives the\ncompact expression:\n\ud835\udc66\ud835\udc56\ud835\udc59=\ud835\udc65\ud835\udc56\ud835\udc57\ud835\udc58\ud835\udc59\ud835\udc4e\ud835\udc57\ud835\udc58. (A.33)\nCommon ExamplesfromLinear Algebra\nLet\u2019sseehowmanyofthelinearalgebraicdefinitionswehaveseenbeforecanbeexpressed\nin this compressed tensor notation:\n\u000fv\u0001w=\u00cd\n\ud835\udc56\ud835\udc63\ud835\udc56\ud835\udc64\ud835\udc56\n\u000fkvk2\n2=\u00cd\n\ud835\udc56\ud835\udc63\ud835\udc56\ud835\udc63\ud835\udc56", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67abb207-16af-4e1d-b5bc-210e4490f591": {"__data__": {"id_": "67abb207-16af-4e1d-b5bc-210e4490f591", "embedding": null, "metadata": {"page_label": "912", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c56c00d1-d371-46a7-b3ec-e569cbaa408d", "node_type": "4", "metadata": {"page_label": "912", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8eb57986a71ed87037488c659fdef020c9cd011ea95519448f9d5194bb839fd7", "class_name": "RelatedNodeInfo"}}, "text": "912 Mathematics for Deep Learning\n\u000f\u00b9Av\u00ba\ud835\udc56=\u00cd\n\ud835\udc57\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc63\ud835\udc57\n\u000f\u00b9AB\u00ba\ud835\udc56\ud835\udc58=\u00cd\n\ud835\udc57\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc4f\ud835\udc57\ud835\udc58\n\u000ftr\u00b9A\u00ba=\u00cd\n\ud835\udc56\ud835\udc4e\ud835\udc56\ud835\udc56\nIn this way, we can replace a myriad of specialized notations with short tensor expres-\nsions.\nExpressingin Code\nTensors may flexibly be operated on in code as well. As seen in Section 2.3 , we can create\ntensors as is shown below.\n# Define tensors\nB=torch .tensor([[[ 1,2,3], [ 4,5,6]], [[ 7,8,9], [ 10,11,12]]])\nA=torch .tensor([[ 1,2], [ 3,4]])\nv=torch .tensor([ 1,2])\n# Print out the shapes\nA.shape, B .shape, v .shape\n(torch .Size([ 2,2]), torch .Size([ 2,2,3]), torch .Size([ 2]))\nEinstein summation has been implemented directly. The indices that occurs in the Einstein\nsummationcanbepassedasastring,followedbythetensorsthatarebeingactedupon. For\ninstance,toimplementmatrixmultiplication,wecanconsidertheEinsteinsummationseen\nabove( Av=\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc63\ud835\udc57)andstripouttheindicesthemselvestogettheimplementation:\n# Reimplement matrix multiplication\ntorch .einsum( \"ij, j -> i \", A, v), A @v\n(tensor([ 5,11]), tensor([ 5,11]))\nThis is a highly flexible notation. For instance if we want to compute what would be tradi-\ntionally written as\n\ud835\udc50\ud835\udc58\ud835\udc59=\u00d5\n\ud835\udc56\ud835\udc57b\ud835\udc56\ud835\udc57\ud835\udc58a\ud835\udc56\ud835\udc59\ud835\udc63\ud835\udc57.(A.34)\nit can be implemented via Einstein summation as:\ntorch .einsum( \"ijk, il, j -> kl \", B, A, v)\ntensor([[ 90,126],\n[102,144],\n[114,162]])", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1271, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a74591b-165a-45f1-ba40-8f92a5552061": {"__data__": {"id_": "1a74591b-165a-45f1-ba40-8f92a5552061", "embedding": null, "metadata": {"page_label": "913", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "da85d089-01fd-40c4-8a45-99c788db6dc7", "node_type": "4", "metadata": {"page_label": "913", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d5fb4e8b9a74a590fe18ff3b3d15de55c0be589f3262aa4c0c3a0878ae65c37c", "class_name": "RelatedNodeInfo"}}, "text": "913 Geometry and Linear Algebraic Operations\nThisnotationisreadableandefficientforhumans,howeverbulkyifforwhateverreasonwe\nneed to generate a tensor contraction programmatically. For this reason, einsumprovides\nan alternative notation by providing integer indices for each tensor. For example, the same\ntensor contraction can also be written as:\n# PyTorch does not support this type of notation.\nEither notation allows for concise and efficient representation of tensor contractions in\ncode.\nA.1.10Summary\n\u000fVectors can be interpreted geometrically as either points or directions in space.\n\u000fDot products define the notion of angle to arbitrarily high-dimensional spaces.\n\u000fHyperplanesarehigh-dimensionalgeneralizationsoflinesandplanes. Theycanbeused\nto define decision planes that are often used as the last step in a classification task.\n\u000fMatrix multiplication can be geometrically interpreted as uniform distortions of the un-\nderlying coordinates. They represent a very restricted, but mathematically clean, way\nto transform vectors.\n\u000fLineardependenceisawaytotellwhenacollectionofvectorsareinalowerdimensional\nspace than we would expect (say you have 3vectors living in a 2-dimensional space).\nThe rank of a matrix is the size of the largest subset of its columns that are linearly\nindependent.\n\u000fWhenamatrix\u2019sinverseisdefined,matrixinversionallowsustofindanothermatrixthat\nundoes the action of the first. Matrix inversion is useful in theory, but requires care in\npractice owing to numerical instability.\n\u000fDeterminants allow us to measure how much a matrix expands or contracts a space. A\nnonzero determinant implies an invertible (non-singular) matrix and a zero-valued\ndeterminant means that the matrix is non-invertible (singular).\n\u000fTensor contractions and Einstein summation provide for a neat and clean notation for\nexpressing many of the computations that are seen in machine learning.\nA.1.11Exercises\n1.What is the angle between\n\u00ae\ud835\udc631=266666641\n0\n\u00001\n237777775,\u00ae\ud835\udc632=266666643\n1\n0\n137777775? (A.35)\n2.True or false:\u00141 2\n0 1\u0015\nand\u00141\u00002\n0 1\u0015\nare inverses of one another?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2069, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed037b07-5946-47f9-b1b6-c6e585b7fad8": {"__data__": {"id_": "ed037b07-5946-47f9-b1b6-c6e585b7fad8", "embedding": null, "metadata": {"page_label": "914", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58ddc913-545a-4626-88bb-0dab82093ead", "node_type": "4", "metadata": {"page_label": "914", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8cbe35ea5880ef97278178c0ba1502f0f082a9878d5ef44a00b6c479ee1248db", "class_name": "RelatedNodeInfo"}}, "text": "914 Mathematics for Deep Learning\n2803.Suppose that we draw a shape in the plane with area 100m2. What is the area after\ntransforming the figure by the matrix\n\u00142 3\n1 2\u0015\n. (A.36)\n4.Which of the following sets of vectors are linearly independent?\n\u000f8>>> <\n>>>:\u00a9\u00ad\u00ad\n\u00ab1\n0\n\u00001\u00aa\u00ae\u00ae\n\u00ac,\u00a9\u00ad\u00ad\n\u00ab2\n1\n\u00001\u00aa\u00ae\u00ae\n\u00ac,\u00a9\u00ad\u00ad\n\u00ab3\n1\n1\u00aa\u00ae\u00ae\n\u00ac9>>> =\n>>>;\n\u000f8>>> <\n>>>:\u00a9\u00ad\u00ad\n\u00ab3\n1\n1\u00aa\u00ae\u00ae\n\u00ac,\u00a9\u00ad\u00ad\n\u00ab1\n1\n1\u00aa\u00ae\u00ae\n\u00ac,\u00a9\u00ad\u00ad\n\u00ab0\n0\n0\u00aa\u00ae\u00ae\n\u00ac9>>> =\n>>>;\n\u000f8>>> <\n>>>:\u00a9\u00ad\u00ad\n\u00ab1\n1\n0\u00aa\u00ae\u00ae\n\u00ac,\u00a9\u00ad\u00ad\n\u00ab0\n1\n\u00001\u00aa\u00ae\u00ae\n\u00ac,\u00a9\u00ad\u00ad\n\u00ab1\n0\n1\u00aa\u00ae\u00ae\n\u00ac9>>> =\n>>>;\n5.Suppose that you have a matrix written as \ud835\udc34=\u0014\ud835\udc50\n\ud835\udc51\u0015\n\u0001\u0002\n\ud835\udc4e \ud835\udc4f\u0003\nfor some choice of values\n\ud835\udc4e,\ud835\udc4f,\ud835\udc50, and\ud835\udc51. True or false: the determinant of such a matrix is always 0?\n6.The vectors\ud835\udc521=\u00141\n0\u0015\nand\ud835\udc522=\u00140\n1\u0015\nare orthogonal. What is the condition on a matrix \ud835\udc34\nso that\ud835\udc34\ud835\udc521and\ud835\udc34\ud835\udc522are orthogonal?\n7.How can you write tr \u00b9A4\u00bain Einstein notation for an arbitrary matrix \ud835\udc34?\nDiscussions280.\nA.2Eigendecompositions\nEigenvaluesareoftenoneofthemostusefulnotionswewillencounterwhenstudyinglinear\nalgebra,however,asabeginner,itiseasytooverlooktheirimportance. Below,weintroduce\neigendecomposition and try to convey some sense of just why it is so important.\nSuppose that we have a matrix \ud835\udc34with the following entries:\nA=\u00142 0\n0\u00001\u0015\n. (A.1)\nIf we apply\ud835\udc34to any vector v=\u00bb\ud835\udc65,\ud835\udc66\u00bc>, we obtain a vector Av=\u00bb2\ud835\udc65,\u0000\ud835\udc66\u00bc>. This has an\nintuitive interpretation: stretch the vector to be twice as wide in the \ud835\udc65-direction, and then\nflip it in the\ud835\udc66-direction.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4994a402-6792-4c8b-a35b-d7c8d7582edf": {"__data__": {"id_": "4994a402-6792-4c8b-a35b-d7c8d7582edf", "embedding": null, "metadata": {"page_label": "915", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8bf92c09-77c6-4be5-8579-06705122c6f1", "node_type": "4", "metadata": {"page_label": "915", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "47c82363c03da7b4bec699a34464a828be37c17270517d6bcb6286978bc8c8df", "class_name": "RelatedNodeInfo"}}, "text": "915 Eigendecompositions\nHowever, there are somevectors for which something remains unchanged. Namely \u00bb1,0\u00bc>\ngets sent to\u00bb2,0\u00bc>and\u00bb0,1\u00bc>gets sent to\u00bb0,\u00001\u00bc>. These vectors are still in the same\nline, and the only modification is that the matrix stretches them by a factor of 2and\u00001\nrespectively. We call such vectors eigenvectors and the factor they are stretched by eigen-\nvalues.\nIn general, if we can find a number \ud835\udf06and a vector vsuch that\nAv=\ud835\udf06v. (A.2)\nWe say that vis an eigenvector for \ud835\udc34and\ud835\udf06is an eigenvalue.\nA.2.1Finding Eigenvalues\nLet\u2019s figure out how to find them. By subtracting off the \ud835\udf06vfrom both sides, and then\nfactoring out the vector, we see the above is equivalent to:\n\u00b9A\u0000\ud835\udf06I\u00bav=0. (A.3)\nFor(A.3)to happen, we see that \u00b9A\u0000\ud835\udf06I\u00bamust compress some direction down to zero,\nhenceitisnotinvertible,andthusthedeterminantiszero. Thus,wecanfindthe eigenvalues\nby finding for what \ud835\udf06isdet\u00b9A\u0000\ud835\udf06I\u00ba=0. Once we find the eigenvalues, we can solve\nAv=\ud835\udf06vto find the associated eigenvector(s) .\nAnExample\nLet\u2019s see this with a more challenging matrix\nA=\u00142 1\n2 3\u0015\n. (A.4)\nIf we consider det\u00b9A\u0000\ud835\udf06I\u00ba=0, we see this is equivalent to the polynomial equation\n0=\u00b92\u0000\ud835\udf06\u00ba\u00b93\u0000\ud835\udf06\u00ba\u00002=\u00b94\u0000\ud835\udf06\u00ba\u00b91\u0000\ud835\udf06\u00ba. Thus, two eigenvalues are 4and1. To find the\nassociated vectors, we then need to solve\n\u00142 1\n2 3\u0015 \u0014\ud835\udc65\n\ud835\udc66\u0015\n=\u0014\ud835\udc65\n\ud835\udc66\u0015\nand\u00142 1\n2 3\u0015 \u0014\ud835\udc65\n\ud835\udc66\u0015\n=\u00144\ud835\udc65\n4\ud835\udc66\u0015\n. (A.5)\nWe can solve this with the vectors \u00bb1,\u00001\u00bc>and\u00bb1,2\u00bc>respectively.\nWe can check this in code using the built-in numpy.linalg.eig routine.\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom d2l import torch asd2l\ntorch .linalg .eig(torch .tensor([[ 2,1], [ 2,3]], dtype =torch .float64))", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1615, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53f3f03f-c6c0-45d5-954e-329e7fe23f04": {"__data__": {"id_": "53f3f03f-c6c0-45d5-954e-329e7fe23f04", "embedding": null, "metadata": {"page_label": "916", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a350b4db-e07a-41f3-b2f3-a47e9fa9d6b1", "node_type": "4", "metadata": {"page_label": "916", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9eefb1f4db37da3de4938d81dae60cc011bd60ebf525ec8ca0d146e1b3da14ad", "class_name": "RelatedNodeInfo"}}, "text": "916 Mathematics for Deep Learning\ntorch .return_types .linalg_eig(\neigenvalues =tensor([ 1.+0.j,4.+0.j], dtype =torch .complex128),\neigenvectors =tensor([[ -0.7071 +0.j,-0.4472 +0.j],\n[0.7071 +0.j,-0.8944 +0.j]], dtype =torch .complex128))\nNote that numpynormalizes the eigenvectors to be of length one, whereas we took ours to\nbe of arbitrary length. Additionally, the choice of sign is arbitrary. However, the vectors\ncomputed are parallel to the ones we found by hand with the same eigenvalues.\nA.2.2DecomposingMatrices\nLet\u2019s continue the previous example one step further. Let\nW=\u00141 1\n\u00001 2\u0015\n, (A.6)\nbe the matrix where the columns are the eigenvectors of the matrix A. Let\n\ud835\udeba=\u00141 0\n0 4\u0015\n, (A.7)\nbethematrixwiththeassociatedeigenvaluesonthediagonal. Thenthedefinitionofeigen-\nvalues and eigenvectors tells us that\nAW=W\ud835\udeba. (A.8)\nThe matrix\ud835\udc4ais invertible, so we may multiply both sides by \ud835\udc4a\u00001on the right, we see that\nwe may write\nA=W\ud835\udebaW\u00001. (A.9)\nIn the next section we will see some nice consequences of this, but for now we need only\nknowthatsuchadecompositionwillexistaslongaswecanfindafullcollectionoflinearly\nindependent eigenvectors (so that \ud835\udc4ais invertible).\nA.2.3Operationson Eigendecompositions\nOne nice thing about eigendecompositions (A.9)is that we can write many operations we\nusually encounter cleanly in terms of the eigendecomposition. As a first example, con-\nsider:\nA\ud835\udc5b=\ud835\udc5btimesz   }|   {\nA\u0001\u0001\u0001A=\ud835\udc5btimesz                              }|                              {\n\u00b9W\ud835\udebaW\u00001\u00ba\u0001\u0001\u0001\u00b9W\ud835\udebaW\u00001\u00ba=W\ud835\udc5btimesz  }|  {\n\ud835\udeba\u0001\u0001\u0001\ud835\udebaW\u00001=W\ud835\udeba\ud835\udc5bW\u00001.(A.10)\nThis tells us that for any positive power of a matrix, the eigendecomposition is obtained by\njustraisingtheeigenvaluestothesamepower. Thesamecanbeshownfornegativepowers,\nso if we want to invert a matrix we need only consider\nA\u00001=W\ud835\udeba\u00001W\u00001, (A.11)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba197a59-a379-4f42-a9b0-bf2df0d229b7": {"__data__": {"id_": "ba197a59-a379-4f42-a9b0-bf2df0d229b7", "embedding": null, "metadata": {"page_label": "917", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c87b58e4-1df5-48a4-aa2b-4127c47028ac", "node_type": "4", "metadata": {"page_label": "917", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "940996b0b57cc7615fcc1f31063f9336f155d71c2b979895b577fbe21cf733a2", "class_name": "RelatedNodeInfo"}}, "text": "917 Eigendecompositions\nor in other words, just invert each eigenvalue. This will work as long as each eigenvalue is\nnon-zero, so we see that invertible is the same as having no zero eigenvalues.\nIndeed, additional work can show that if \ud835\udf061,...,\ud835\udf06\ud835\udc5bare the eigenvalues of a matrix, then\nthe determinant of that matrix is\ndet\u00b9A\u00ba=\ud835\udf061\u0001\u0001\u0001\ud835\udf06\ud835\udc5b, (A.12)\northeproductofalltheeigenvalues. Thismakessenseintuitivelybecausewhateverstretch-\ningWdoes,\ud835\udc4a\u00001undoes it, so in the end the only stretching that happens is by multipli-\ncation by the diagonal matrix \ud835\udeba, which stretches volumes by the product of the diagonal\nelements.\nFinally, recall that the rank was the maximum number of linearly independent columns of\nyour matrix. By examining the eigendecomposition closely, we can see that the rank is the\nsame as the number of non-zero eigenvalues of A.\nThe examples could continue, but hopefully the point is clear: eigendecomposition can\nsimplify many linear-algebraic computations and is a fundamental operation underlying\nmany numerical algorithms and much of the analysis that we do in linear algebra.\nA.2.4Eigendecompositionsof SymmetricMatrices\nIt is not always possible to find enough linearly independent eigenvectors for the above\nprocess to work. For instance the matrix\nA=\u00141 1\n0 1\u0015\n, (A.13)\nhas only a single eigenvector, namely \u00b91,0\u00ba>. To handle such matrices, we require more\nadvancedtechniquesthanwecancover(suchastheJordanNormalForm,orSingularValue\nDecomposition). We will often need to restrict our attention to those matrices where we\ncan guarantee the existence of a full set of eigenvectors.\nThe most commonly encountered family are the symmetric matrices , which are those ma-\ntrices where A=A>. In this case, we may take \ud835\udc4ato be anorthogonal matrix \u2014a matrix\nwhose columns are all length one vectors that are at right angles to one another, where\nW>=W\u00001\u2014and all the eigenvalues will be real. Thus, in this special case, we can write\n(A.9)as\nA=W\ud835\udebaW>. (A.14)\nA.2.5GershgorinCircleTheorem\nEigenvalues are often difficult to reason with intuitively. If presented an arbitrary matrix,\nthereislittlethatcanbesaidaboutwhattheeigenvaluesarewithoutcomputingthem. There\nis, however, one theorem that can make it easy to approximate well if the largest values are\non the diagonal.\nLetA=\u00b9\ud835\udc4e\ud835\udc56\ud835\udc57\u00babe any square matrix ( \ud835\udc5b\u0002\ud835\udc5b). We will define \ud835\udc5f\ud835\udc56=\u00cd\n\ud835\udc57\u2260\ud835\udc56j\ud835\udc4e\ud835\udc56\ud835\udc57j. LetD\ud835\udc56", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2344, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0cb4bd28-addc-47fd-b0ea-57a58e1472ca": {"__data__": {"id_": "0cb4bd28-addc-47fd-b0ea-57a58e1472ca", "embedding": null, "metadata": {"page_label": "918", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78753eb2-0c5c-4969-ac3a-542bf1993ec3", "node_type": "4", "metadata": {"page_label": "918", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b9702b4270fe6e062daa0841ee98ebe1d7314ea92c544d54153eb2d53599a674", "class_name": "RelatedNodeInfo"}}, "text": "918 Mathematics for Deep Learning\nrepresent the disc in the complex plane with center \ud835\udc4e\ud835\udc56\ud835\udc56radius\ud835\udc5f\ud835\udc56. Then, every eigenvalue of\nAis contained in one of the D\ud835\udc56.\nThis can be a bit to unpack, so let\u2019s look at an example. Consider the matrix:\nA=266666641.0 0.1 0.1 0.1\n0.1 3.0 0.2 0.3\n0.1 0.2 5.0 0.5\n0.1 0.3 0.5 9.037777775. (A.15)\nWe have\ud835\udc5f1=0.3,\ud835\udc5f2=0.6,\ud835\udc5f3=0.8and\ud835\udc5f4=0.9. The matrix is symmetric, so all\neigenvalues are real. This means that all of our eigenvalues will be in one of the ranges\nof\n\u00bb\ud835\udc4e11\u0000\ud835\udc5f1,\ud835\udc4e11\u00b8\ud835\udc5f1\u00bc=\u00bb0.7,1.3\u00bc, (A.16)\n\u00bb\ud835\udc4e22\u0000\ud835\udc5f2,\ud835\udc4e22\u00b8\ud835\udc5f2\u00bc=\u00bb2.4,3.6\u00bc, (A.17)\n\u00bb\ud835\udc4e33\u0000\ud835\udc5f3,\ud835\udc4e33\u00b8\ud835\udc5f3\u00bc=\u00bb4.2,5.8\u00bc, (A.18)\n\u00bb\ud835\udc4e44\u0000\ud835\udc5f4,\ud835\udc4e44\u00b8\ud835\udc5f4\u00bc=\u00bb8.1,9.9\u00bc. (A.19)\nPerforming the numerical computation shows that the eigenvalues are approximately 0.99,\n2.97,4.95,9.08, all comfortably inside the ranges provided.\nA=torch .tensor([[ 1.0,0.1,0.1,0.1],\n[0.1,3.0,0.2,0.3],\n[0.1,0.2,5.0,0.5],\n[0.1,0.3,0.5,9.0]])\nv, _ =torch .linalg .eig(A)\nv\ntensor([ 0.9923 +0.j,9.0803 +0.j,4.9539 +0.j,2.9734 +0.j])\nInthisway,eigenvaluescanbeapproximated,andtheapproximationswillbefairlyaccurate\nin the case that the diagonal is significantly larger than all the other elements.\nIt is a small thing, but with a complex and subtle topic like eigendecomposition, it is good\nto get any intuitive grasp we can.\nA.2.6AUseful Application: The Growthof Iterated Maps\nNow that we understand what eigenvectors are in principle, let\u2019s see how they can be used\nto provide a deep understanding of a problem central to neural network behavior: proper\nweight initialization.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1501, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67805cde-6ee4-497b-9d96-6f611ad78265": {"__data__": {"id_": "67805cde-6ee4-497b-9d96-6f611ad78265", "embedding": null, "metadata": {"page_label": "919", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fb00c216-81ba-4c6e-bc60-747457efe92f", "node_type": "4", "metadata": {"page_label": "919", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b8e10794055cf1ac65ec57494f106277345607a341e882a08d4983dd37cfa591", "class_name": "RelatedNodeInfo"}}, "text": "919 Eigendecompositions\nEigenvectorsas Long TermBehavior\nThe full mathematical investigation of the initialization of deep neural networks is beyond\nthe scope of the text, but we can see a toy version here to understand how eigenvalues can\nhelp us see how these models work. As we know, neural networks operate by interspersing\nlayers of linear transformations with non-linear operations. For simplicity here, we will\nassumethatthereisnonon-linearity,andthatthetransformationisasinglerepeatedmatrix\noperation\ud835\udc34, so that the output of our model is\nv\ud835\udc5c\ud835\udc62\ud835\udc61=A\u0001A\u0001\u0001\u0001Av\ud835\udc56\ud835\udc5b=A\ud835\udc41v\ud835\udc56\ud835\udc5b. (A.20)\nWhen these models are initialized, \ud835\udc34is taken to be a random matrix with Gaussian entries,\nsolet\u2019smakeoneofthose. Tobeconcrete,westartwithameanzero,varianceoneGaussian\ndistributed 5\u00025matrix.\ntorch .manual_seed( 42)\nk=5\nA=torch .randn(k, k, dtype =torch .float64)\nA\ntensor([[ 0.2996 ,0.2424 ,0.2832 ,-0.2329 ,0.6712 ],\n[0.7818 ,-1.7903 ,-1.7484 ,0.1735 ,-0.1182 ],\n[-1.7446 ,-0.4695 ,0.4573 ,0.5177 ,-0.2771 ],\n[-0.6641 ,0.6551 ,0.2616 ,-1.5265 ,-0.3311 ],\n[-0.6378 ,0.1072 ,0.7096 ,0.3009 ,-0.2869 ]], dtype =torch .float64)\nBehavioron Random Data\nFor simplicity in our toy model, we will assume that the data vector we feed in v\ud835\udc56\ud835\udc5bis a\nrandom five dimensional Gaussian vector. Let\u2019s think about what we want to have happen.\nForcontext,letsthinkofagenericMLproblem,wherewearetryingtoturninputdata,like\nan image, into a prediction, like the probability the image is a picture of a cat. If repeated\napplication of Astretches a random vector out to be very long, then small changes in input\nwillbeamplifiedintolargechangesinoutput\u2014tinymodificationsoftheinputimagewould\nlead to vastly different predictions. This does not seem right!\nOntheflipside,if Ashrinksrandomvectorstobeshorter,thenafterrunningthroughmany\nlayers, the vector will essentially shrink to nothing, and the output will not depend on the\ninput. This is also clearly not right either!\nWe need to walk the narrow line between growth and decay to make sure that our output\nchanges depending on our input, but not much!\nLet\u2019s see what happens when we repeatedly multiply our matrix Aagainst a random input\nvector, and keep track of the norm.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6e7373b5-b01b-4d75-906d-d82dc989740b": {"__data__": {"id_": "6e7373b5-b01b-4d75-906d-d82dc989740b", "embedding": null, "metadata": {"page_label": "920", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "508d3bd4-596c-461f-9c79-52a21b0565e3", "node_type": "4", "metadata": {"page_label": "920", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "80997b55431da1bb00308f32dcd73d93fa3fe6e111a6281112c082d8849abd9f", "class_name": "RelatedNodeInfo"}}, "text": "920 Mathematics for Deep Learning\n# Calculate the sequence of norms after repeatedly applying `A`\nv_in =torch .randn(k, 1, dtype =torch .float64)\nnorm_list =[torch .norm(v_in) .item()]\nfor iinrange (1,100):\nv_in =A@v_in\nnorm_list .append(torch .norm(v_in) .item())\nd2l.plot(torch .arange( 0,100), norm_list, 'Iteration ','Value ')\nThe norm is growing uncontrollably! Indeed if we take the list of quotients, we will see a\npattern.\n# Compute the scaling factor of the norms\nnorm_ratio_list =[]\nfor iinrange (1,100):\nnorm_ratio_list .append(norm_list[i] /norm_list[i -1])\nd2l.plot(torch .arange( 1,100), norm_ratio_list, 'Iteration ','Ratio ')\nIf we look at the last portion of the above computation, we see that the random vector is\nstretched by a factor of 1.974459321485[...] , where the portion at the end shifts a little,\nbut the stretching factor is stable.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24004148-918a-4956-8844-f18279dd1818": {"__data__": {"id_": "24004148-918a-4956-8844-f18279dd1818", "embedding": null, "metadata": {"page_label": "921", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dacd34ff-0234-4b63-99dd-4e662e8506b5", "node_type": "4", "metadata": {"page_label": "921", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "241713b2f418a0c2ad7a09bb832bd22c6b8a35c2bf5611822f2eb14e1ff1ab9a", "class_name": "RelatedNodeInfo"}}, "text": "921 Eigendecompositions\nRelatingBackto Eigenvectors\nWe have seen that eigenvectors and eigenvalues correspond to the amount something is\nstretched, but that was for specific vectors, and specific stretches. Let\u2019s take a look at what\nthey are for A. A bit of a caveat here: it turns out that to see them all, we will need to go\nto complex numbers. You can think of these as stretches and rotations. By taking the norm\nof the complex number (square root of the sums of squares of real and imaginary parts) we\ncan measure that stretching factor. Let\u2019s also sort them.\n# Compute the eigenvalues\neigs =torch .linalg .eig(A) .eigenvalues .tolist()\nnorm_eigs =[torch .abs(torch .tensor(x)) for xineigs]\nnorm_eigs .sort()\nprint (f'norms of eigenvalues: {norm_eigs }')\nnorms of eigenvalues: [tensor( 0.3490 ), tensor( 1.1296 ), tensor( 1.1296 ),\u2423\n\u21a9!tensor( 1.1828 ), tensor( 2.4532 )]\nAnObservation\nWeseesomethingabitunexpectedhappeninghere: thatnumberweidentifiedbeforeforthe\nlongtermstretchingofourmatrix Aappliedtoarandomvectoris exactly(accuratetothir-\nteendecimalplaces!) thelargesteigenvalueof A. Thisisclearlynotacoincidence!\nBut,ifwenowthinkaboutwhatishappeninggeometrically,thisstartstomakesense. Con-\nsiderarandomvector. Thisrandomvectorpointsalittleineverydirection,soinparticular,\nit points at least a little bit in the same direction as the eigenvector of Aassociated with\nthe largest eigenvalue. This is so important that it is called the principle eigenvalue and\nprinciple eigenvector . After applying A, our random vector gets stretched in every possi-\nble direction, as is associated with every possible eigenvector, but it is stretched most of\nall in the direction associated with this principle eigenvector. What this means is that after\napply in\ud835\udc34, our random vector is longer, and points in a direction closer to being aligned\nwith the principle eigenvector. After applying the matrix many times, the alignment with\nthe principle eigenvector becomes closer and closer until, for all practical purposes, our\nrandom vector has been transformed into the principle eigenvector! Indeed this algorithm\nis the basis for what is known as the power iteration for finding the largest eigenvalue and\neigenvector of a matrix. For details see, for example, ( Golub and Van Loan, 1996 ).\nFixing the Normalization\nNow, from above discussions, we concluded that we do not want a random vector to be\nstretched or squished at all, we would like random vectors to stay about the same size\nthroughout the entire process. To do so, we now rescale our matrix by this principle eigen-\nvalue so that the largest eigenvalue is instead now just one. Let\u2019s see what happens in this\ncase.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2671, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ee663ba-ca7b-48d6-b22a-3996d2500b73": {"__data__": {"id_": "2ee663ba-ca7b-48d6-b22a-3996d2500b73", "embedding": null, "metadata": {"page_label": "922", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5dd81624-a8cb-4f8b-8721-a22720e67c6c", "node_type": "4", "metadata": {"page_label": "922", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "69c1613a5723864e0a76da66fad999367989a61f4da48fc8705a2e23e7423c57", "class_name": "RelatedNodeInfo"}}, "text": "922 Mathematics for Deep Learning\n# Rescale the matrix `A`\nA/=norm_eigs[ -1]\n# Do the same experiment again\nv_in =torch .randn(k, 1, dtype =torch .float64)\nnorm_list =[torch .norm(v_in) .item()]\nfor iinrange (1,100):\nv_in =A@v_in\nnorm_list .append(torch .norm(v_in) .item())\nd2l.plot(torch .arange( 0,100), norm_list, 'Iteration ','Value ')\nWe can also plot the ratio between consecutive norms as before and see that indeed it sta-\nbilizes.\n# Also plot the ratio\nnorm_ratio_list =[]\nfor iinrange (1,100):\nnorm_ratio_list .append(norm_list[i] /norm_list[i -1])\nd2l.plot(torch .arange( 1,100), norm_ratio_list, 'Iteration ','Ratio ')\nA.2.7Discussion\nWe now see exactly what we hoped for! After normalizing the matrices by the principal\neigenvalue, we see that the random data does not explode as before, but rather eventually\nequilibrates to a specific value. It would be nice to be able to do these things from first", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b09708e-dc0a-4045-ae22-67752f0e6a06": {"__data__": {"id_": "1b09708e-dc0a-4045-ae22-67752f0e6a06", "embedding": null, "metadata": {"page_label": "923", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bcc98809-e744-4dbe-9d47-643307221871", "node_type": "4", "metadata": {"page_label": "923", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "74317011b87e0288d515f248d816af066c7655f79db61d71ff3280a887e14d98", "class_name": "RelatedNodeInfo"}}, "text": "923 Eigendecompositions\n281principles, and it turns out that if we look deeply at the mathematics of it, we can see that\nthe largest eigenvalue of a large random matrix with independent mean zero, variance one\nGaussian entries is on average aboutp\ud835\udc5b, or in our casep\n5\u00192.2, due to a fascinating fact\nknown as the circularlaw (Ginibre, 1965 ). The relationship between the eigenvalues (and\na related object called singular values) of random matrices has been shown to have deep\nconnections to proper initialization of neural networks as was discussed in Pennington et\nal.(2017) and subsequent works.\nA.2.8Summary\n\u000fEigenvectors are vectors which are stretched by a matrix without changing direction.\n\u000fEigenvalues are the amount that the eigenvectors are stretched by the application of the\nmatrix.\n\u000fThe eigendecomposition of a matrix can allow for many operations to be reduced to\noperations on the eigenvalues.\n\u000fThe Gershgorin Circle Theorem can provide approximate values for the eigenvalues of\na matrix.\n\u000fThebehaviorofiteratedmatrixpowersdependsprimarilyonthesizeofthelargesteigen-\nvalue. This understanding has many applications in the theory of neural network ini-\ntialization.\nA.2.9Exercises\n1.What are the eigenvalues and eigenvectors of\nA=\u00142 1\n1 2\u0015\n? (A.21)\n2.What are the eigenvalues and eigenvectors of the following matrix, and what is strange\nabout this example compared to the previous one?\nA=\u00142 1\n0 2\u0015\n. (A.22)\n3.Without computing the eigenvalues, is it possible that the smallest eigenvalue of the\nfollowing matrix is less that 0.5?Note: this problem can be done in your head.\nA=266666643.0 0.1 0.3 1.0\n0.1 1.0 0.1 0.2\n0.3 0.1 5.0 0.0\n1.0 0.2 0.0 1.837777775. (A.23)\nDiscussions281.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1694, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "084b72bb-9a1b-41b1-af62-f9e3c2072c02": {"__data__": {"id_": "084b72bb-9a1b-41b1-af62-f9e3c2072c02", "embedding": null, "metadata": {"page_label": "924", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a84966a9-6e4c-4a5a-84b5-57a368c79c36", "node_type": "4", "metadata": {"page_label": "924", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "427c143a8dfa4bd0cf7c2f51e9842fce945127e5f2899b97669484e105397c45", "class_name": "RelatedNodeInfo"}}, "text": "924 Mathematics for Deep Learning\nA.3SingleVariableCalculus\nInSection 2.4 , we saw the basic elements of differential calculus. This section takes a\ndeeper dive into the fundamentals of calculus and how we can understand and apply it in\nthe context of machine learning.\nA.3.1DifferentialCalculus\nDifferentialcalculusisfundamentallythestudyofhowfunctionsbehaveundersmallchanges.\nTo see why this is so core to deep learning, let\u2019s consider an example.\nSuppose that we have a deep neural network where the weights are, for convenience, con-\ncatenated into a single vector w=\u00b9\ud835\udc641,...,\ud835\udc64\ud835\udc5b\u00ba. Given a training dataset, we consider the\nloss of our neural network on this dataset, which we will write as L\u00b9w\u00ba.\nThis function is extraordinarily complex, encoding the performance of all possible models\nofthegivenarchitectureonthisdataset,soitisnearlyimpossibletotellwhatsetofweights\nwwill minimize the loss. Thus, in practice, we often start by initializing our weights ran-\ndomly, and then iteratively take small steps in the direction which makes the loss decrease\nas rapidly as possible.\nThe question then becomes something that on the surface is no easier: how do we find\nthe direction which makes the weights decrease as quickly as possible? To dig into this,\nlet\u2019s first examine the case with only a single weight: \ud835\udc3f\u00b9w\u00ba=\ud835\udc3f\u00b9\ud835\udc65\u00bafor a single real value\n\ud835\udc65.\nLet\u2019s take\ud835\udc65and try to understand what happens when we change it by a small amount to\n\ud835\udc65\u00b8\ud835\udf16. If you wish to be concrete, think a number like \ud835\udf16=0.0000001 . To help us visualize\nwhat happens, let\u2019s graph an example function, \ud835\udc53\u00b9\ud835\udc65\u00ba=sin\u00b9\ud835\udc65\ud835\udc65\u00ba, over the\u00bb0,3\u00bc.\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom d2l import torch asd2l\ntorch .pi=torch .acos(torch .zeros( 1)).item() *2# Define pi in torch\n# Plot a function in a normal range\nx_big =torch .arange( 0.01 ,3.01 ,0.01 )\nys=torch .sin(x_big **x_big)\nd2l.plot(x_big, ys, 'x','f(x) ')\nAtthislargescale,thefunction\u2019sbehaviorisnotsimple. However,ifwereduceourrangeto\nsomething smaller like \u00bb1.75,2.25\u00bc, we see that the graph becomes much simpler.\n# Plot a the same function in a tiny range\nx_med =torch .arange( 1.75 ,2.25 ,0.001 )\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2159, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77df227b-fd7b-4820-9305-162fc0f7888f": {"__data__": {"id_": "77df227b-fd7b-4820-9305-162fc0f7888f", "embedding": null, "metadata": {"page_label": "925", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6629f19-b56d-4506-8c48-e9ad2de85b52", "node_type": "4", "metadata": {"page_label": "925", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b9b43189cb65af9afcae5cd94bd5efb48ab714d31314c5488b2d13903a5f83b2", "class_name": "RelatedNodeInfo"}}, "text": "925 Single Variable Calculus\n(continued from previous page)\nys=torch .sin(x_med **x_med)\nd2l.plot(x_med, ys, 'x','f(x) ')\nTakingthistoanextreme,ifwezoomintoatinysegment,thebehaviorbecomesfarsimpler:\nit is just a straight line.\n# Plot a the same function in a tiny range\nx_small =torch .arange( 2.0,2.01 ,0.0001 )\nys=torch .sin(x_small **x_small)\nd2l.plot(x_small, ys, 'x','f(x) ')\nThis is the key observation of single variable calculus: the behavior of familiar functions\ncan be modeled by a line in a small enough range. This means that for most functions, it\nis reasonable to expect that as we shift the \ud835\udc65value of the function by a little bit, the output\n\ud835\udc53\u00b9\ud835\udc65\u00bawillalsobeshiftedbyalittlebit. Theonlyquestionweneedtoansweris, \u201cHowlarge", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f4f3e6c-7e05-47ab-88bd-ca2daa0c8e64": {"__data__": {"id_": "2f4f3e6c-7e05-47ab-88bd-ca2daa0c8e64", "embedding": null, "metadata": {"page_label": "926", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "926f1fcf-b6ae-48e5-9a1c-1032cb961b1d", "node_type": "4", "metadata": {"page_label": "926", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7b8df688f16dfa0bfda637ef2e75c11dca32458d74a967ec029060af21696287", "class_name": "RelatedNodeInfo"}}, "text": "926 Mathematics for Deep Learning\nis the change in the output compared to the change in the input? Is it half as large? Twice\nas large?\u201d\nThus, we can consider the ratio of the change in the output of a function for a small change\nin the input of the function. We can write this formally as\n\ud835\udc3f\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0000\ud835\udc3f\u00b9\ud835\udc65\u00ba\n\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0000\ud835\udc65=\ud835\udc3f\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0000\ud835\udc3f\u00b9\ud835\udc65\u00ba\n\ud835\udf16. (A.1)\nThis is already enough to start to play around with in code. For instance, suppose that we\nknow that\ud835\udc3f\u00b9\ud835\udc65\u00ba=\ud835\udc652\u00b81701\u00b9\ud835\udc65\u00004\u00ba3, then we can see how large this value is at the point\n\ud835\udc65=4as follows.\n# Define our function\ndef L(x):\nreturn x**2+1701 *(x-4)**3\n# Print the difference divided by epsilon for several epsilon\nfor epsilon in[0.1,0.001 ,0.0001 ,0.00001 ]:\nprint (f'epsilon = {epsilon :.5f}->{(L(4+epsilon) -L(4))/epsilon :.5f}')\nepsilon =0.10000 ->25.11000\nepsilon =0.00100 ->8.00270\nepsilon =0.00010 ->8.00012\nepsilon =0.00001 ->8.00001\nNow,ifweareobservant,wewillnoticethattheoutputofthisnumberissuspiciouslyclose\nto8. Indeed,ifwedecrease \ud835\udf16,wewillseevaluebecomesprogressivelycloserto 8. Thuswe\nmay conclude, correctly, that the value we seek (the degree a change in the input changes\nthe output) should be 8at the point\ud835\udc65=4. The way that a mathematician encodes this fact\nis\nlim\n\ud835\udf16!0\ud835\udc3f\u00b94\u00b8\ud835\udf16\u00ba\u0000\ud835\udc3f\u00b94\u00ba\n\ud835\udf16=8. (A.2)\nAs a bit of a historical digression: in the first few decades of neural network research, sci-\nentists used this algorithm (the methodoffinitedifferences ) to evaluate how a loss function\nchanged under small perturbation: just change the weights and see how the loss changed.\nThisiscomputationallyinefficient,requiringtwoevaluationsofthelossfunctiontoseehow\na single change of one variable influenced the loss. If we tried to do this with even a pal-\ntry few thousand parameters, it would require several thousand evaluations of the network\nover the entire dataset! It was not solved until 1986 that the backpropagation algorithm\nintroduced in Rumelhart et al.(1988) provided a way to calculate how anychange of the\nweightstogetherwouldchangethelossinthesamecomputationtimeasasingleprediction\nof the network over the dataset.\nBack in our example, this value 8is different for different values of \ud835\udc65, so it makes sense to\ndefine it as a function of \ud835\udc65. More formally, this value dependent rate of change is referred\nto as thederivative which is written as\n\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba=lim\n\ud835\udf16!0\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0000\ud835\udc53\u00b9\ud835\udc65\u00ba\n\ud835\udf16. (A.3)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2324, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b09cbb34-8cd8-4661-b852-7c0fc8f3122a": {"__data__": {"id_": "b09cbb34-8cd8-4661-b852-7c0fc8f3122a", "embedding": null, "metadata": {"page_label": "927", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81487b45-8413-4208-8041-c8aa1f1e063e", "node_type": "4", "metadata": {"page_label": "927", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9bceda43a9a10888ef2bf20784a5e1d7a756ef90e695bdf41ac0a98d46962756", "class_name": "RelatedNodeInfo"}}, "text": "927 Single Variable Calculus\nDifferent texts will use different notations for the derivative. For instance, all of the below\nnotations indicate the same thing:\n\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65=\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc53=\ud835\udc530=r\ud835\udc65\ud835\udc53=\ud835\udc37\ud835\udc65\ud835\udc53=\ud835\udc53\ud835\udc65. (A.4)\nMost authors will pick a single notation and stick with it, however even that is not guaran-\nteed. It is best to be familiar with all of these. We will use the notation\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65throughout this\ntext, unless we want to take the derivative of a complex expression, in which case we will\nuse\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc53to write expressions like\n\ud835\udc51\n\ud835\udc51\ud835\udc65\u0014\n\ud835\udc654\u00b8cos\u0012\ud835\udc652\u00b81\n2\ud835\udc65\u00001\u0013\u0015\n. (A.5)\nOftentimes, it is intuitively useful to unravel the definition of derivative (A.3)again to see\nhow a function changes when we make a small change of \ud835\udc65:\n\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba=lim\n\ud835\udf16!0\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0000\ud835\udc53\u00b9\ud835\udc65\u00ba\n\ud835\udf16=)\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u0019\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0000\ud835\udc53\u00b9\ud835\udc65\u00ba\n\ud835\udf16\n=)\ud835\udf16\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u0019\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0000\ud835\udc53\u00b9\ud835\udc65\u00ba\n=)\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0019\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba.(A.6)\nThe last equation is worth explicitly calling out. It tells us that if you take any function and\nchange the input by a small amount, the output would change by that small amount scaled\nby the derivative.\nIn this way, we can understand the derivative as the scaling factor that tells us how large of\nchange we get in the output from a change in the input.\nA.3.2Rulesof Calculus\nWe now turn to the task of understanding how to compute the derivative of an explicit\nfunction. A full formal treatment of calculus would derive everything from first principles.\nWe will not indulge in this temptation here, but rather provide an understanding of the\ncommon rules encountered.\nCommon Derivatives\nAs was seen in Section 2.4 , when computing derivatives one can oftentimes use a series of\nrules to reduce the computation to a few core functions. We repeat them here for ease of\nreference.\n\u000fDerivativeof constants.\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc50=0.\n\u000fDerivativeof linear functions.\ud835\udc51\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc4e\ud835\udc65\u00ba=\ud835\udc4e.\n\u000fPowerrule.\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc65\ud835\udc5b=\ud835\udc5b\ud835\udc65\ud835\udc5b\u00001.\n\u000fDerivativeof exponentials.\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc52\ud835\udc65=\ud835\udc52\ud835\udc65.\n\u000fDerivativeof the logarithm.\ud835\udc51\n\ud835\udc51\ud835\udc65log\u00b9\ud835\udc65\u00ba=1\n\ud835\udc65.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "366cd19e-63b4-4590-8615-829132669bb4": {"__data__": {"id_": "366cd19e-63b4-4590-8615-829132669bb4", "embedding": null, "metadata": {"page_label": "928", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51c128ec-97f0-48f8-a988-147c654b51d1", "node_type": "4", "metadata": {"page_label": "928", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "809c8d27b870cd02e40602c9ede9ac842581b490952040af5386457fa3e3527d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "658a896d-4e6c-4d84-babe-127997a58c4d", "node_type": "1", "metadata": {}, "hash": "63710df15cb55b9290360b4413eab19c24e6626b31dbdf59e4867f0aa5aa47e1", "class_name": "RelatedNodeInfo"}}, "text": "928 Mathematics for Deep Learning\nDerivativeRules\nIf every derivative needed to be separately computed and stored in a table, differential cal-\nculus would be near impossible. It is a gift of mathematics that we can generalize the\nabove derivatives and compute more complex derivatives like finding the derivative of\n\ud835\udc53\u00b9\ud835\udc65\u00ba=log\u00001\u00b8\u00b9\ud835\udc65\u00001\u00ba10\u0001. As was mentioned in Section 2.4 , the key to doing so is to\ncodify what happens when we take functions and combine them in various ways, most\nimportantly: sums, products, and compositions.\n\u000fSum rule.\ud835\udc51\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc54\u00b9\ud835\udc65\u00ba\u00b8\u210e\u00b9\ud835\udc65\u00ba\u00ba=\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba.\n\u000fProductrule.\ud835\udc51\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc54\u00b9\ud835\udc65\u00ba\u0001\u210e\u00b9\ud835\udc65\u00ba\u00ba=\ud835\udc54\u00b9\ud835\udc65\u00ba\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u210e\u00b9\ud835\udc65\u00ba.\n\u000fChain rule.\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc54\u00b9\u210e\u00b9\ud835\udc65\u00ba\u00ba=\ud835\udc51\ud835\udc54\n\ud835\udc51\u210e\u00b9\u210e\u00b9\ud835\udc65\u00ba\u00ba\u0001\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba.\nLet\u2019s see how we may use (A.6)to understand these rules. For the sum rule, consider\nfollowing chain of reasoning:\n\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba=\ud835\udc54\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u00b8\u210e\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\n\u0019\ud835\udc54\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u00b8\u210e\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\n=\ud835\udc54\u00b9\ud835\udc65\u00ba\u00b8\u210e\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\u0012\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u0013\n=\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\u0012\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u0013\n.(A.7)\nBy comparing this result with the fact that \ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0019\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba, we see that\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba=\n\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00baasdesired. Theintuitionhereis: whenwechangetheinput \ud835\udc65,\ud835\udc54and\u210ejointly\ncontribute to the change of the output by\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00baand\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba.\nThe product is more subtle, and will require a new observation about how to work with\nthese expressions.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "658a896d-4e6c-4d84-babe-127997a58c4d": {"__data__": {"id_": "658a896d-4e6c-4d84-babe-127997a58c4d", "embedding": null, "metadata": {"page_label": "928", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51c128ec-97f0-48f8-a988-147c654b51d1", "node_type": "4", "metadata": {"page_label": "928", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "809c8d27b870cd02e40602c9ede9ac842581b490952040af5386457fa3e3527d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "366cd19e-63b4-4590-8615-829132669bb4", "node_type": "1", "metadata": {"page_label": "928", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4c571a66345a92aae208aff75adad29cd2264f2cd044017cf2ad3c90489dd0d0", "class_name": "RelatedNodeInfo"}}, "text": "Theintuitionhereis: whenwechangetheinput \ud835\udc65,\ud835\udc54and\u210ejointly\ncontribute to the change of the output by\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00baand\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba.\nThe product is more subtle, and will require a new observation about how to work with\nthese expressions. We will begin as before using (A.6):\n\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba=\ud835\udc54\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0001\u210e\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\n\u0019\u0012\n\ud835\udc54\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u0013\n\u0001\u0012\n\u210e\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u0013\n=\ud835\udc54\u00b9\ud835\udc65\u00ba\u0001\u210e\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\u0012\n\ud835\udc54\u00b9\ud835\udc65\u00ba\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u210e\u00b9\ud835\udc65\u00ba\u0013\n\u00b8\ud835\udf162\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\n=\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\u0012\n\ud835\udc54\u00b9\ud835\udc65\u00ba\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u210e\u00b9\ud835\udc65\u00ba\u0013\n\u00b8\ud835\udf162\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba.(A.8)\nThis resembles the computation done above, and indeed we see our answer (\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba=\n\ud835\udc54\u00b9\ud835\udc65\u00ba\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u210e\u00b9\ud835\udc65\u00ba) sitting next to \ud835\udf16, but there is the issue of that term of size \ud835\udf162.\nWe will refer to this as a higher-orderterm , since the power of \ud835\udf162is higher than the power\nof\ud835\udf161. We will see in a later section that we will sometimes want to keep track of these,\nhowever for now observe that if \ud835\udf16=0.0000001 , then\ud835\udf162=0.0000000000001 , which is\nvastly smaller. As we send \ud835\udf16!0, we may safely ignore the higher order terms. As a\ngeneral convention in this appendix, we will use \u201c \u0019\u201d to denote that the two terms are equal", "mimetype": "text/plain", "start_char_idx": 1022, "end_char_idx": 2076, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b073cf29-47ac-4500-8c23-8d93c9e7f736": {"__data__": {"id_": "b073cf29-47ac-4500-8c23-8d93c9e7f736", "embedding": null, "metadata": {"page_label": "929", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3731a957-0fe7-4560-994c-e56c8892a4aa", "node_type": "4", "metadata": {"page_label": "929", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "16fd53adb29e83a66895f00c36442d6d6c49b754e1673045a95b413dca780c9e", "class_name": "RelatedNodeInfo"}}, "text": "929 Single Variable Calculus\nup to higher order terms. However, if we wish to be more formal we may examine the\ndifference quotient\n\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0000\ud835\udc53\u00b9\ud835\udc65\u00ba\n\ud835\udf16=\ud835\udc54\u00b9\ud835\udc65\u00ba\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u210e\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba, (A.9)\nand see that as we send \ud835\udf16!0, the right hand term goes to zero as well.\nFinally,withthechainrule,wecanagainprogressasbeforeusing (A.6)andseethat\n\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba=\ud835\udc54\u00b9\u210e\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u00ba\n\u0019\ud835\udc54\u0012\n\u210e\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\u0013\n\u0019\ud835\udc54\u00b9\u210e\u00b9\ud835\udc65\u00ba\u00ba\u00b8\ud835\udf16\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc54\n\ud835\udc51\u210e\u00b9\u210e\u00b9\ud835\udc65\u00ba\u00ba\n=\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc51\ud835\udc54\n\ud835\udc51\u210e\u00b9\u210e\u00b9\ud835\udc65\u00ba\u00ba\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba,(A.10)\nwhere in the second line we view the function \ud835\udc54as having its input ( \u210e\u00b9\ud835\udc65\u00ba) shifted by the\ntiny quantity \ud835\udf16\ud835\udc51\u210e\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba.\nThese rule provide us with a flexible set of tools to compute essentially any expression\ndesired. For instance,\n\ud835\udc51\n\ud835\udc51\ud835\udc65h\nlog\u0010\n1\u00b8\u00b9\ud835\udc65\u00001\u00ba10\u0011i\n=\u0010\n1\u00b8\u00b9\ud835\udc65\u00001\u00ba10\u0011\u00001\ud835\udc51\n\ud835\udc51\ud835\udc65\u0002\n1\u00b8\u00b9\ud835\udc65\u00001\u00ba10\u0003\n=\u0010\n1\u00b8\u00b9\ud835\udc65\u00001\u00ba10\u0011\u00001\u0012\ud835\udc51\n\ud835\udc51\ud835\udc65\u00bb1\u00bc\u00b8\ud835\udc51\n\ud835\udc51\ud835\udc65\u00bb\u00b9\ud835\udc65\u00001\u00ba10\u00bc\u0013\n=\u0010\n1\u00b8\u00b9\ud835\udc65\u00001\u00ba10\u0011\u00001\u0012\n0\u00b810\u00b9\ud835\udc65\u00001\u00ba9\ud835\udc51\n\ud835\udc51\ud835\udc65\u00bb\ud835\udc65\u00001\u00bc\u0013\n=10\u0010\n1\u00b8\u00b9\ud835\udc65\u00001\u00ba10\u0011\u00001\n\u00b9\ud835\udc65\u00001\u00ba9\n=10\u00b9\ud835\udc65\u00001\u00ba9\n1\u00b8\u00b9\ud835\udc65\u00001\u00ba10.(A.11)\nWhere each line has used the following rules:\n1.The chain rule and derivative of logarithm.\n2.The sum rule.\n3.The derivative of constants, chain rule, and power rule.\n4.The sum rule, derivative of linear functions, derivative of constants.\nTwo things should be clear after doing this example:\n1.Anyfunctionwecanwritedownusingsums,products,constants,powers,exponentials,\nand logarithms can have its derivate computed mechanically by following these rules.\n2.Having a human follow these rules can be tedious and error prone!\nThankfully, these two facts together hint towards a way forward: this is a perfect candidate", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92fef7a1-d40b-4c85-9f10-43706761fefe": {"__data__": {"id_": "92fef7a1-d40b-4c85-9f10-43706761fefe", "embedding": null, "metadata": {"page_label": "930", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a6886040-9fb9-4382-a229-a5d5cfcf8528", "node_type": "4", "metadata": {"page_label": "930", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f74835aa53b2d4064d76c0f575af9bc20b7440e0a0eb5276b37bfb8594925aa6", "class_name": "RelatedNodeInfo"}}, "text": "930 Mathematics for Deep Learning\nfor mechanization! Indeed backpropagation, which we will revisit later in this section, is\nexactly that.\nLinear Approximation\nWhenworkingwithderivatives, itis oftenusefultogeometricallyinterprettheapproxima-\ntion used above. In particular, note that the equation\n\ud835\udc53\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0019\ud835\udc53\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba, (A.12)\napproximates the value of \ud835\udc53by a line which passes through the point \u00b9\ud835\udc65, \ud835\udc53\u00b9\ud835\udc65\u00ba\u00baand has\nslope\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba. In this way we say that the derivative gives a linear approximation to the\nfunction\ud835\udc53, as illustrated below:\n# Compute sin\nxs=torch .arange( -torch .pi, torch .pi, 0.01 )\nplots =[torch .sin(xs)]\n# Compute some linear approximations. Use d(sin(x))/dx = cos(x)\nfor x0in[-1.5,0.0,2.0]:\nplots .append(torch .sin(torch .tensor(x0)) +(xs -x0) *\ntorch .cos(torch .tensor(x0)))\nd2l.plot(xs, plots, 'x','f(x) ', ylim =[-1.5,1.5])\nHigherOrderDerivatives\nLet\u2019s now do something that may on the surface seem strange. Take a function \ud835\udc53and\ncompute the derivative\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65. This gives us the rate of change of \ud835\udc53at any point.\nHowever, the derivative,\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65, can be viewed as a function itself, so nothing stops us from\ncomputing the derivative of\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65to get\ud835\udc512\ud835\udc53\n\ud835\udc51\ud835\udc652=\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u0010\n\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u0011\n. We will call this the second deriva-\ntive of\ud835\udc53. This function is the rate of change of the rate of change of \ud835\udc53, or in other words,\nhow the rate of change is changing. We may apply the derivative any number of times to\nobtain what is called the \ud835\udc5b-th derivative. To keep the notation clean, we will denote the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "50133199-caee-47d4-a702-34ff49ce8b49": {"__data__": {"id_": "50133199-caee-47d4-a702-34ff49ce8b49", "embedding": null, "metadata": {"page_label": "931", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2bb89aa9-f4a4-4aee-b358-eb0db25c2f54", "node_type": "4", "metadata": {"page_label": "931", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "01d5ef2183763f304709d01b71e77bf96c500fbe7b502ca842d6bf5734fd023e", "class_name": "RelatedNodeInfo"}}, "text": "931 Single Variable Calculus\n\ud835\udc5b-th derivative as\n\ud835\udc53\u00b9\ud835\udc5b\u00ba\u00b9\ud835\udc65\u00ba=\ud835\udc51\ud835\udc5b\ud835\udc53\n\ud835\udc51\ud835\udc65\ud835\udc5b=\u0012\ud835\udc51\n\ud835\udc51\ud835\udc65\u0013\ud835\udc5b\n\ud835\udc53. (A.13)\nLet\u2019s try to understand whythis is a useful notion. Below, we visualize \ud835\udc53\u00b92\u00ba\u00b9\ud835\udc65\u00ba,\ud835\udc53\u00b91\u00ba\u00b9\ud835\udc65\u00ba,\nand\ud835\udc53\u00b9\ud835\udc65\u00ba.\nFirst,considerthecasethatthesecondderivative \ud835\udc53\u00b92\u00ba\u00b9\ud835\udc65\u00baisapositiveconstant. Thismeans\nthat the slope of the first derivative is positive. As a result, the first derivative \ud835\udc53\u00b91\u00ba\u00b9\ud835\udc65\u00bamay\nstart out negative, becomes zero at a point, and then becomes positive in the end. This tells\nustheslopeofouroriginalfunction \ud835\udc53andtherefore,thefunction \ud835\udc53itselfdecreases,flattens\nout, then increases. In other words, the function \ud835\udc53curves up, and has a single minimum as\nis shown in Fig. A.1.\ntFig. A.1 If we assume the second derivative is a positive constant, then the \ufb01st derivative in\nincreasing, which implies the function itself has a minimum.\nSecond, if the second derivative is a negative constant, that means that the first derivative\nis decreasing. This implies the first derivative may start out positive, becomes zero at a\npoint, and then becomes negative. Hence, the function \ud835\udc53itself increases, flattens out, then\ndecreases. In other words, the function \ud835\udc53curves down, and has a single maximum as is\nshown in Fig. A.2.\ntFig. A.2 If we assume the second derivative is a negative constant, then the \ufb01st derivative in\ndecreasing, which implies the function itself has a maximum.\nThird,ifthesecondderivativeisaalwayszero,thenthefirstderivativewillneverchange\u2014\nit is constant! This means that \ud835\udc53increases (or decreases) at a fixed rate, and \ud835\udc53is itself a\nstraight line as is shown in Fig. A.3.\nTosummarize,thesecondderivativecanbeinterpretedasdescribingthewaythatthefunc-\ntion\ud835\udc53curves. A positive second derivative leads to a upwards curve, while a negative sec-\nond derivative means that \ud835\udc53curves downwards, and a zero second derivative means that \ud835\udc53\ndoes not curve at all.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1833, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e09db88b-e6b2-40a9-9eba-64ac79b4004e": {"__data__": {"id_": "e09db88b-e6b2-40a9-9eba-64ac79b4004e", "embedding": null, "metadata": {"page_label": "932", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "834756d3-064a-45f1-a3e8-55f68788240a", "node_type": "4", "metadata": {"page_label": "932", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cd75374e4e64cc5924543586c591f86cca39b72974fa454553cd9192b0b3beda", "class_name": "RelatedNodeInfo"}}, "text": "932 Mathematics for Deep Learning\ntFig. A.3 If we assume the second derivative is zero, then the \ufb01st derivative is constant, which\nimplies the function itself is a straight line.\nLet\u2019s take this one step further. Consider the function \ud835\udc54\u00b9\ud835\udc65\u00ba=\ud835\udc4e\ud835\udc652\u00b8\ud835\udc4f\ud835\udc65\u00b8\ud835\udc50. We can then\ncompute that\n\ud835\udc51\ud835\udc54\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba=2\ud835\udc4e\ud835\udc65\u00b8\ud835\udc4f\n\ud835\udc512\ud835\udc54\n\ud835\udc51\ud835\udc652\u00b9\ud835\udc65\u00ba=2\ud835\udc4e.(A.14)\nIf we have some original function \ud835\udc53\u00b9\ud835\udc65\u00bain mind, we may compute the first two derivatives\nand find the values for \ud835\udc4e,\ud835\udc4f, and\ud835\udc50that make them match this computation. Similarly to\nthe previous section where we saw that the first derivative gave the best approximation\nwith a straight line, this construction provides the best approximation by a quadratic. Let\u2019s\nvisualize this for \ud835\udc53\u00b9\ud835\udc65\u00ba=sin\u00b9\ud835\udc65\u00ba.\n# Compute sin\nxs=torch .arange( -torch .pi, torch .pi, 0.01 )\nplots =[torch .sin(xs)]\n# Compute some quadratic approximations. Use d(sin(x)) / dx = cos(x)\nfor x0in[-1.5,0.0,2.0]:\nplots .append(torch .sin(torch .tensor(x0)) +(xs -x0) *\ntorch .cos(torch .tensor(x0)) -(xs -x0)**2*\ntorch .sin(torch .tensor(x0)) /2)\nd2l.plot(xs, plots, 'x','f(x) ', ylim =[-1.5,1.5])\nWe will extend this idea to the idea of a Taylorseries in the next section.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1137, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2f51572-f016-4a1a-82e6-4e53b68435d6": {"__data__": {"id_": "e2f51572-f016-4a1a-82e6-4e53b68435d6", "embedding": null, "metadata": {"page_label": "933", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a1aeb747-1ec0-4b4f-a55b-8e841ae01abc", "node_type": "4", "metadata": {"page_label": "933", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5b7cd5223129c2b6ff00379f9eb669663e7baa9d7e41b004479268c657505e42", "class_name": "RelatedNodeInfo"}}, "text": "933 Single Variable Calculus\nTaylorSeries\nTheTaylorseries providesamethodtoapproximatethefunction \ud835\udc53\u00b9\ud835\udc65\u00baifwearegivenvalues\nfor the first\ud835\udc5bderivatives at a point \ud835\udc650, i.e.,\b\n\ud835\udc53\u00b9\ud835\udc650\u00ba, \ud835\udc53\u00b91\u00ba\u00b9\ud835\udc650\u00ba, \ud835\udc53\u00b92\u00ba\u00b9\ud835\udc650\u00ba,..., \ud835\udc53\u00b9\ud835\udc5b\u00ba\u00b9\ud835\udc650\u00ba\t\n. The\nideawillbetofindadegree \ud835\udc5bpolynomialthatmatchesallthegivenderivativesat \ud835\udc650.\nWe saw the case of \ud835\udc5b=2in the previous section and a little algebra shows this is\n\ud835\udc53\u00b9\ud835\udc65\u00ba\u00191\n2\ud835\udc512\ud835\udc53\n\ud835\udc51\ud835\udc652\u00b9\ud835\udc650\u00ba\u00b9\ud835\udc65\u0000\ud835\udc650\u00ba2\u00b8\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc650\u00ba\u00b9\ud835\udc65\u0000\ud835\udc650\u00ba\u00b8\ud835\udc53\u00b9\ud835\udc650\u00ba. (A.15)\nAs we can see above, the denominator of 2is there to cancel out the 2we get when we take\ntwo derivatives of \ud835\udc652, while the other terms are all zero. Same logic applies for the first\nderivative and the value itself.\nIf we push the logic further to \ud835\udc5b=3, we will conclude that\n\ud835\udc53\u00b9\ud835\udc65\u00ba\u0019\ud835\udc513\ud835\udc53\n\ud835\udc51\ud835\udc653\u00b9\ud835\udc650\u00ba\n6\u00b9\ud835\udc65\u0000\ud835\udc650\u00ba3\u00b8\ud835\udc512\ud835\udc53\n\ud835\udc51\ud835\udc652\u00b9\ud835\udc650\u00ba\n2\u00b9\ud835\udc65\u0000\ud835\udc650\u00ba2\u00b8\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc650\u00ba\u00b9\ud835\udc65\u0000\ud835\udc650\u00ba\u00b8\ud835\udc53\u00b9\ud835\udc650\u00ba.(A.16)\nwherethe 6=3\u00022=3!comesfromtheconstantwegetinfrontifwetakethreederivatives\nof\ud835\udc653.\nFurthermore, we can get a degree \ud835\udc5bpolynomial by\n\ud835\udc43\ud835\udc5b\u00b9\ud835\udc65\u00ba=\ud835\udc5b\u00d5\n\ud835\udc56=0\ud835\udc53\u00b9\ud835\udc56\u00ba\u00b9\ud835\udc650\u00ba\n\ud835\udc56!\u00b9\ud835\udc65\u0000\ud835\udc650\u00ba\ud835\udc56. (A.17)\nwhere the notation\n\ud835\udc53\u00b9\ud835\udc5b\u00ba\u00b9\ud835\udc65\u00ba=\ud835\udc51\ud835\udc5b\ud835\udc53\n\ud835\udc51\ud835\udc65\ud835\udc5b=\u0012\ud835\udc51\n\ud835\udc51\ud835\udc65\u0013\ud835\udc5b\n\ud835\udc53. (A.18)\nIndeed,\ud835\udc43\ud835\udc5b\u00b9\ud835\udc65\u00bacanbeviewedasthebest \ud835\udc5b-thdegreepolynomialapproximationtoourfunc-\ntion\ud835\udc53\u00b9\ud835\udc65\u00ba.\nWhile we are not going to dive all the way into the error of the above approximations, it\nis worth mentioning the infinite limit. In this case, for well behaved functions (known as\nrealanalyticfunctions)like cos\u00b9\ud835\udc65\u00baor\ud835\udc52\ud835\udc65, wecanwriteouttheinfinitenumberoftermsand\napproximate the exactly same function\n\ud835\udc53\u00b9\ud835\udc65\u00ba=1\u00d5\n\ud835\udc5b=0\ud835\udc53\u00b9\ud835\udc5b\u00ba\u00b9\ud835\udc650\u00ba\n\ud835\udc5b!\u00b9\ud835\udc65\u0000\ud835\udc650\u00ba\ud835\udc5b. (A.19)\nTake\ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc52\ud835\udc65as am example. Since \ud835\udc52\ud835\udc65is its own derivative, we know that \ud835\udc53\u00b9\ud835\udc5b\u00ba\u00b9\ud835\udc65\u00ba=\ud835\udc52\ud835\udc65.\nTherefore,\ud835\udc52\ud835\udc65can be reconstructed by taking the Taylor series at \ud835\udc650=0, i.e.,\n\ud835\udc52\ud835\udc65=1\u00d5\n\ud835\udc5b=0\ud835\udc65\ud835\udc5b\n\ud835\udc5b!=1\u00b8\ud835\udc65\u00b8\ud835\udc652\n2\u00b8\ud835\udc653\n6\u00b8\u0001\u0001\u0001. (A.20)\nLet\u2019s see how this works in code and observe how increasing the degree of the Taylor\napproximation brings us closer to the desired function \ud835\udc52\ud835\udc65.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1764, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "615bb5d2-444d-4b63-8b0d-eda0a988d0ec": {"__data__": {"id_": "615bb5d2-444d-4b63-8b0d-eda0a988d0ec", "embedding": null, "metadata": {"page_label": "934", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7027798f-38c2-413d-bc7a-ebea06b4da2e", "node_type": "4", "metadata": {"page_label": "934", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "96a6c5c8a99d9dd9060a96169c8d4b1511bd0a3aab4f41d7d0c7e768449d1b95", "class_name": "RelatedNodeInfo"}}, "text": "934 Mathematics for Deep Learning\n# Compute the exponential function\nxs=torch .arange( 0,3,0.01 )\nys=torch .exp(xs)\n# Compute a few Taylor series approximations\nP1=1+xs\nP2=1+xs+xs**2/2\nP5=1+xs+xs**2/2+xs**3/6+xs**4/24+xs**5/120\nd2l.plot(xs, [ys, P1, P2, P5], 'x','f(x) ', legend =[\n\"Exponential \",\"Degree 1 Taylor Series \",\"Degree 2 Taylor Series \",\n\"Degree 5 Taylor Series \"])\nTaylor series have two primary applications:\n1.Theoretical applications : Often when we try to understand a too complex function,\nusingTaylorseriesenablesustoturnitintoapolynomialthatwecanworkwithdirectly.\n2.Numerical applications : Some functions like \ud835\udc52\ud835\udc65orcos\u00b9\ud835\udc65\u00baare difficult for machines to\ncompute. Theycanstoretablesofvaluesatafixedprecision(andthisisoftendone),but\nit still leaves open questions like \u201cWhat is the 1000-th digit of cos\u00b91\u00ba?\u201d Taylor series\nare often helpful to answer such questions.\nA.3.3Summary\n\u000fDerivatives can be used to express how functions change when we change the input by a\nsmall amount.\n\u000fElementaryderivativescanbecombinedusingderivativerulestocreatearbitrarilycom-\nplex derivatives.\n\u000fDerivatives can be iterated to get second or higher order derivatives. Each increase in\norder provides more fine grained information on the behavior of the function.\n\u000fUsing information in the derivatives of a single data example, we can approximate well\nbehaved functions by polynomials obtained from the Taylor series.\nA.3.4Exercises\n1.What is the derivative of \ud835\udc653\u00004\ud835\udc65\u00b81?", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1464, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c80c011-772b-4d40-9040-950c099266b0": {"__data__": {"id_": "3c80c011-772b-4d40-9040-950c099266b0", "embedding": null, "metadata": {"page_label": "935", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "98e00c25-ecb9-470d-bef3-bea7c8fafd91", "node_type": "4", "metadata": {"page_label": "935", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4ad942abb307cfe882eabbf16cdcdca227cd7e7c722c85fc32ef3015bf3a7abd", "class_name": "RelatedNodeInfo"}}, "text": "935 Multivariable Calculus\n2822.What is the derivative of log\u00b91\n\ud835\udc65\u00ba?\n3.True or False: If \ud835\udc530\u00b9\ud835\udc65\u00ba=0then\ud835\udc53has a maximum or minimum at \ud835\udc65?\n4.Where is the minimum of \ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc65log\u00b9\ud835\udc65\u00bafor\ud835\udc65\u00150(where we assume that \ud835\udc53takes the\nlimiting value of 0at\ud835\udc53\u00b90\u00ba)?\nDiscussions282.\nA.4MultivariableCalculus\nNow that we have a fairly strong understanding of derivatives of a function of a single\nvariable, let\u2019s return to our original question where we were considering a loss function of\npotentially billions of weights.\nA.4.1Higher-DimensionalDifferentiation\nWhatSection A.3 tells us is that if we change a single one of these billions of weights\nleaving every other one fixed, we know what will happen! This is nothing more than a\nfunction of a single variable, so we can write\n\ud835\udc3f\u00b9\ud835\udc641\u00b8\ud835\udf161,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba\u0019\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba\u00b8\ud835\udf161\ud835\udc51\n\ud835\udc51\ud835\udc64 1\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba.(A.1)\nWewillcallthederivativeinonevariablewhilefixingtheothervariablesthe partialderiva-\ntive, and we will use the notation\ud835\udf15\n\ud835\udf15\ud835\udc64 1for the derivative in (A.1).\nNow, let\u2019s take this and change \ud835\udc642a little bit to \ud835\udc642\u00b8\ud835\udf162:\n\ud835\udc3f\u00b9\ud835\udc641\u00b8\ud835\udf161,\ud835\udc642\u00b8\ud835\udf162,...,\ud835\udc64\ud835\udc41\u00ba\u0019\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642\u00b8\ud835\udf162,...,\ud835\udc64\ud835\udc41\u00ba\u00b8\ud835\udf161\ud835\udf15\n\ud835\udf15\ud835\udc64 1\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642\u00b8\ud835\udf162,...,\ud835\udc64\ud835\udc41\u00b8\ud835\udf16\ud835\udc41\u00ba\n\u0019\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba\n\u00b8\ud835\udf162\ud835\udf15\n\ud835\udf15\ud835\udc64 2\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba\n\u00b8\ud835\udf161\ud835\udf15\n\ud835\udf15\ud835\udc64 1\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba\n\u00b8\ud835\udf161\ud835\udf162\ud835\udf15\n\ud835\udf15\ud835\udc64 2\ud835\udf15\n\ud835\udf15\ud835\udc64 1\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba\n\u0019\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba\n\u00b8\ud835\udf162\ud835\udf15\n\ud835\udf15\ud835\udc64 2\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba\n\u00b8\ud835\udf161\ud835\udf15\n\ud835\udf15\ud835\udc64 1\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba.\n(A.2)\nWehaveagainusedtheideathat \ud835\udf161\ud835\udf162isahigherordertermthatwecandiscardinthesame", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1343, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1209d05-bdd2-48e8-a0ad-ad144b07f1c7": {"__data__": {"id_": "f1209d05-bdd2-48e8-a0ad-ad144b07f1c7", "embedding": null, "metadata": {"page_label": "936", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4463b90-911f-495e-afa2-52228065ed4d", "node_type": "4", "metadata": {"page_label": "936", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "780a634bc6e25eba3acd2e9e43a721959ea3e2564cfe04eeec77b015cae26d8d", "class_name": "RelatedNodeInfo"}}, "text": "936 Mathematics for Deep Learning\nway we could discard \ud835\udf162in the previous section, along with what we saw in (A.1). By\ncontinuing in this manner, we may write that\n\ud835\udc3f\u00b9\ud835\udc641\u00b8\ud835\udf161,\ud835\udc642\u00b8\ud835\udf162,...,\ud835\udc64\ud835\udc41\u00b8\ud835\udf16\ud835\udc41\u00ba\u0019\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba\u00b8\u00d5\n\ud835\udc56\ud835\udf16\ud835\udc56\ud835\udf15\n\ud835\udf15\ud835\udc64\ud835\udc56\ud835\udc3f\u00b9\ud835\udc641,\ud835\udc642,...,\ud835\udc64\ud835\udc41\u00ba.\n(A.3)\nThis may look like a mess, but we can make this more familiar by noting that the sum on\nthe right looks exactly like a dot product, so if we let\n\ud835\udf50=\u00bb\ud835\udf161,...,\ud835\udf16\ud835\udc41\u00bc>andrx\ud835\udc3f=\u0014\ud835\udf15\ud835\udc3f\n\ud835\udf15\ud835\udc651,...,\ud835\udf15\ud835\udc3f\n\ud835\udf15\ud835\udc65\ud835\udc41\u0015>\n, (A.4)\nthen\n\ud835\udc3f\u00b9w\u00b8\ud835\udf50\u00ba\u0019\ud835\udc3f\u00b9w\u00ba\u00b8\ud835\udf50\u0001rw\ud835\udc3f\u00b9w\u00ba. (A.5)\nWe will call the vector rw\ud835\udc3fthegradient of\ud835\udc3f.\nEquation (A.5)isworthponderingforamoment. Ithasexactlytheformatthatweencoun-\ntered in one dimension, just we have converted everything to vectors and dot products.\nIt allows us to tell approximately how the function \ud835\udc3fwill change given any perturbation\nto the input. As we will see in the next section, this will provide us with an important\ntool in understanding geometrically how we can learn using information contained in the\ngradient.\nButfirst,let\u2019sseethisapproximationatworkwithanexample. Supposethatweareworking\nwith the function\n\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba=log\u00b9\ud835\udc52\ud835\udc65\u00b8\ud835\udc52\ud835\udc66\u00bawith gradientr\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba=\u0014\ud835\udc52\ud835\udc65\n\ud835\udc52\ud835\udc65\u00b8\ud835\udc52\ud835\udc66,\ud835\udc52\ud835\udc66\n\ud835\udc52\ud835\udc65\u00b8\ud835\udc52\ud835\udc66\u0015\n. (A.6)\nIf we look at a point like \u00b90,log\u00b92\u00ba\u00ba, we see that\n\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba=log\u00b93\u00bawith gradientr\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba=\u00141\n3,2\n3\u0015\n. (A.7)\nThus, if we want to approximate \ud835\udc53at\u00b9\ud835\udf161,log\u00b92\u00ba\u00b8\ud835\udf162\u00ba, we see that we should have the\nspecific instance of (A.5):\n\ud835\udc53\u00b9\ud835\udf161,log\u00b92\u00ba\u00b8\ud835\udf162\u00ba\u0019log\u00b93\u00ba\u00b81\n3\ud835\udf161\u00b82\n3\ud835\udf162. (A.8)\nWe can test this in code to see how good the approximation is.\n%matplotlib inline\nimport numpy asnp\nimport torch\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch asd2l\ndef f(x, y):\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1605, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e590f85-b193-4a1c-b5ab-98e5d1fe727d": {"__data__": {"id_": "2e590f85-b193-4a1c-b5ab-98e5d1fe727d", "embedding": null, "metadata": {"page_label": "937", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "438eab76-7f80-4a5d-9f52-a8cc82a88881", "node_type": "4", "metadata": {"page_label": "937", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6a642aec4609ac927243c1f93d5c0630ab242b37ff6d1cb285638f537aca674a", "class_name": "RelatedNodeInfo"}}, "text": "937 Multivariable Calculus\n(continued from previous page)\nreturn torch .log(torch .exp(x) +torch .exp(y))\ndef grad_f (x, y):\nreturn torch .tensor([torch .exp(x) /(torch .exp(x) +torch .exp(y)),\ntorch .exp(y) /(torch .exp(x) +torch .exp(y))])\nepsilon =torch .tensor([ 0.01 ,-0.03 ])\ngrad_approx =f(torch .tensor([ 0.]), torch .log(\ntorch .tensor([ 2.]))) +epsilon .dot(\ngrad_f(torch .tensor([ 0.]), torch .log(torch .tensor( 2.))))\ntrue_value =f(torch .tensor([ 0.])+epsilon[ 0], torch .log(\ntorch .tensor([ 2.])) +epsilon[ 1])\nf'approximation: {grad_approx }, true Value: {true_value }'\n'approximation: tensor([1.0819]), true Value: tensor([1.0821]) '\nA.4.2Geometryof Gradients and Gradient Descent\nConsider the expression from (A.5)again:\n\ud835\udc3f\u00b9w\u00b8\ud835\udf50\u00ba\u0019\ud835\udc3f\u00b9w\u00ba\u00b8\ud835\udf50\u0001rw\ud835\udc3f\u00b9w\u00ba. (A.9)\nLet\u2019s suppose that I want to use this to help minimize our loss \ud835\udc3f. Let\u2019s understand geomet-\nrically the algorithm of gradient descent first described in Section 2.5 . What we will do is\nthe following:\n1.Start with a random choice for the initial parameters w.\n2.Find the direction vthat makes\ud835\udc3fdecrease the most rapidly at w.\n3.Take a small step in that direction: w!w\u00b8\ud835\udf16v.\n4.Repeat.\nThe only thing we do not know exactly how to do is to compute the vector vin the second\nstep. We will call such a direction the direction of steepest descent . Using the geometric\nunderstandingofdotproductsfrom SectionA.1 ,weseethatwecanrewrite (A.5)as\n\ud835\udc3f\u00b9w\u00b8v\u00ba\u0019\ud835\udc3f\u00b9w\u00ba\u00b8v\u0001rw\ud835\udc3f\u00b9w\u00ba=\ud835\udc3f\u00b9w\u00ba\u00b8kr w\ud835\udc3f\u00b9w\u00bakcos\u00b9\ud835\udf03\u00ba. (A.10)\nNote that we have taken our direction to have length one for convenience, and used \ud835\udf03for\nthe angle between vandrw\ud835\udc3f\u00b9w\u00ba. If we want to find the direction that decreases \ud835\udc3fas\nrapidly as possible, we want to make this expression as negative as possible. The only way\nthe direction we pick enters into this equation is through cos\u00b9\ud835\udf03\u00ba, and thus we wish to make\nthis cosine as negative as possible. Now, recalling the shape of cosine, we can make this as\nnegative as possible by making cos\u00b9\ud835\udf03\u00ba=\u00001or equivalently making the angle between the\ngradient and our chosen direction to be \ud835\udf0bradians, or equivalently 180degrees. The only\nway to achieve this is to head in the exact opposite direction: pick vto point in the exact\nopposite direction to rw\ud835\udc3f\u00b9w\u00ba!\nThis brings us to one of the most important mathematical concepts in machine learning:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "93900cca-5ec3-4e1b-a8c5-f3ed7bc486f1": {"__data__": {"id_": "93900cca-5ec3-4e1b-a8c5-f3ed7bc486f1", "embedding": null, "metadata": {"page_label": "938", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "618b35e2-f2b2-4622-87aa-6a58f48b1d3a", "node_type": "4", "metadata": {"page_label": "938", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "839b68373776a64586a6d00f6c8c99331cbe5df74d8f2f21890f6ac9dd98e30b", "class_name": "RelatedNodeInfo"}}, "text": "938 Mathematics for Deep Learning\nthe direction of steepest decent points in the direction of \u0000rw\ud835\udc3f\u00b9w\u00ba. Thus our informal\nalgorithm can be rewritten as follows.\n1.Start with a random choice for the initial parameters w.\n2.Computerw\ud835\udc3f\u00b9w\u00ba.\n3.Take a small step in the opposite of that direction: w w\u0000\ud835\udf16rw\ud835\udc3f\u00b9w\u00ba.\n4.Repeat.\nThis basic algorithm has been modified and adapted many ways by many researchers, but\nthecore conceptremains the samein allof them. Use thegradient tofind the directionthat\ndecreases the loss as rapidly as possible, and update the parameters to take a step in that\ndirection.\nA.4.3A Noteon Mathematical Optimization\nThroughoutthisbook,wefocussquarelyonnumericaloptimizationtechniquesfortheprac-\ntical reason that all functions we encounter in the deep learning setting are too complex to\nminimize explicitly.\nHowever, it is a useful exercise to consider what the geometric understanding we obtained\nabove tells us about optimizing functions directly.\nSuppose that we wish to find the value of x0which minimizes some function \ud835\udc3f\u00b9x\u00ba. Let\u2019s\nsuppose that moreover someone gives us a value and tells us that it is the value that mini-\nmizes\ud835\udc3f. Is there anything we can check to see if their answer is even plausible?\nAgain consider (A.5):\n\ud835\udc3f\u00b9x0\u00b8\ud835\udf50\u00ba\u0019\ud835\udc3f\u00b9x0\u00ba\u00b8\ud835\udf50\u0001rx\ud835\udc3f\u00b9x0\u00ba. (A.11)\nIf the gradient is not zero, we know that we can take a step in the direction \u0000\ud835\udf16rx\ud835\udc3f\u00b9x0\u00bato\nfind a value of \ud835\udc3fthat is smaller. Thus, if we truly are at a minimum, this cannot be the\ncase! We can conclude that if x0is a minimum, then rx\ud835\udc3f\u00b9x0\u00ba=0. We call points with\nrx\ud835\udc3f\u00b9x0\u00ba=0critical points .\nThis is nice, because in some rare settings, we canexplicitly find all the points where the\ngradient is zero, and find the one with the smallest value.\nFor a concrete example, consider the function\n\ud835\udc53\u00b9\ud835\udc65\u00ba=3\ud835\udc654\u00004\ud835\udc653\u000012\ud835\udc652. (A.12)\nThis function has derivative\n\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65=12\ud835\udc653\u000012\ud835\udc652\u000024\ud835\udc65=12\ud835\udc65\u00b9\ud835\udc65\u00002\u00ba\u00b9\ud835\udc65\u00b81\u00ba. (A.13)\nThe only possible location of minima are at \ud835\udc65=\u00001,0,2, where the function takes the\nvalues\u00005,0,\u000032respectively, and thus we can conclude that we minimize our function\nwhen\ud835\udc65=2. A quick plot confirms this.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "24686375-678e-4794-99ea-f79390176955": {"__data__": {"id_": "24686375-678e-4794-99ea-f79390176955", "embedding": null, "metadata": {"page_label": "939", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "124a7aed-b02e-47bc-876b-63637c59811a", "node_type": "4", "metadata": {"page_label": "939", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3f63b713068c5a37dd7eb58bad7c11f63b334bca0bc50ae18dd0656ac38e33a2", "class_name": "RelatedNodeInfo"}}, "text": "939 Multivariable Calculus\nx=torch .arange( -2,3,0.01 )\nf=(3*x**4)-(4*x**3)-(12*x**2)\nd2l.plot(x, f, 'x','f(x) ')\nThishighlightsanimportantfacttoknowwhenworkingeithertheoreticallyornumerically:\ntheonlypossiblepointswherewecanminimize(ormaximize)afunctionwillhavegradient\nequal to zero, however, not every point with gradient zero is the true globalminimum (or\nmaximum).\nA.4.4MultivariateChain Rule\nLet\u2019s suppose that we have a function of four variables ( \ud835\udc64,\ud835\udc65,\ud835\udc66, and\ud835\udc67) which we can make\nby composing many terms:\n\ud835\udc53\u00b9\ud835\udc62,\ud835\udc63\u00ba=\u00b9\ud835\udc62\u00b8\ud835\udc63\u00ba2\n\ud835\udc62\u00b9\ud835\udc4e,\ud835\udc4f\u00ba=\u00b9\ud835\udc4e\u00b8\ud835\udc4f\u00ba2, \ud835\udc63\u00b9\ud835\udc4e,\ud835\udc4f\u00ba=\u00b9\ud835\udc4e\u0000\ud835\udc4f\u00ba2,\n\ud835\udc4e\u00b9\ud835\udc64,\ud835\udc65,\ud835\udc66,\ud835\udc67\u00ba=\u00b9\ud835\udc64\u00b8\ud835\udc65\u00b8\ud835\udc66\u00b8\ud835\udc67\u00ba2, \ud835\udc4f\u00b9\ud835\udc64,\ud835\udc65,\ud835\udc66,\ud835\udc67\u00ba=\u00b9\ud835\udc64\u00b8\ud835\udc65\u0000\ud835\udc66\u0000\ud835\udc67\u00ba2.(A.14)\nSuch chains of equations are common when working with neural networks, so trying to\nunderstand how to compute gradients of such functions is key. We can start to see visual\nhints of this connection in Fig. A.1 if we take a look at what variables directly relate to one\nanother.\ntFig. A.1 The function relations above where nodes represent values and edges show functional\ndependence.\nNothing stops us from just composing everything from (A.14 )and writing out that\n\ud835\udc53\u00b9\ud835\udc64,\ud835\udc65,\ud835\udc66,\ud835\udc67\u00ba=\u0012\u0010\n\u00b9\ud835\udc64\u00b8\ud835\udc65\u00b8\ud835\udc66\u00b8\ud835\udc67\u00ba2\u00b8\u00b9\ud835\udc64\u00b8\ud835\udc65\u0000\ud835\udc66\u0000\ud835\udc67\u00ba2\u00112\n\u00b8\u0010\n\u00b9\ud835\udc64\u00b8\ud835\udc65\u00b8\ud835\udc66\u00b8\ud835\udc67\u00ba2\u0000\u00b9\ud835\udc64\u00b8\ud835\udc65\u0000\ud835\udc66\u0000\ud835\udc67\u00ba2\u00112\u00132\n.\n(A.15)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ea7fb8a8-e8d3-4610-af9a-a0ea2dd76200": {"__data__": {"id_": "ea7fb8a8-e8d3-4610-af9a-a0ea2dd76200", "embedding": null, "metadata": {"page_label": "940", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1669e80-5bf3-4b94-925c-9d6557068806", "node_type": "4", "metadata": {"page_label": "940", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e387c1c41dd54aa81c6b3150d6f05d10c180518fae9d7bc47a7a33cde5e09a4b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d54636b7-e96a-4c8e-8948-0ddddd27d117", "node_type": "1", "metadata": {}, "hash": "ca8a147723227cf293d9b0a935589b7917449cade7772350be5db6877039cdd4", "class_name": "RelatedNodeInfo"}}, "text": "940 Mathematics for Deep Learning\nWe may then take the derivative by just using single variable derivatives, but if we did that\nwe would quickly find ourself swamped with terms, many of which are repeats! Indeed,\none can see that, for instance:\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc64=2\u0010\n2\u00b92\u00b9\ud835\udc64\u00b8\ud835\udc65\u00b8\ud835\udc66\u00b8\ud835\udc67\u00ba\u00002\u00b9\ud835\udc64\u00b8\ud835\udc65\u0000\ud835\udc66\u0000\ud835\udc67\u00ba\u00ba\u0010\n\u00b9\ud835\udc64\u00b8\ud835\udc65\u00b8\ud835\udc66\u00b8\ud835\udc67\u00ba2\u0000\u00b9\ud835\udc64\u00b8\ud835\udc65\u0000\ud835\udc66\u0000\ud835\udc67\u00ba2\u0011\n\u00b8\n2\u00b92\u00b9\ud835\udc64\u00b8\ud835\udc65\u0000\ud835\udc66\u0000\ud835\udc67\u00ba\u00b82\u00b9\ud835\udc64\u00b8\ud835\udc65\u00b8\ud835\udc66\u00b8\ud835\udc67\u00ba\u00ba\u0010\n\u00b9\ud835\udc64\u00b8\ud835\udc65\u0000\ud835\udc66\u0000\ud835\udc67\u00ba2\u00b8\u00b9\ud835\udc64\u00b8\ud835\udc65\u00b8\ud835\udc66\u00b8\ud835\udc67\u00ba2\u0011\u0011\n\u0002\n\u0012\u0010\n\u00b9\ud835\udc64\u00b8\ud835\udc65\u00b8\ud835\udc66\u00b8\ud835\udc67\u00ba2\u0000\u00b9\ud835\udc64\u00b8\ud835\udc65\u0000\ud835\udc66\u0000\ud835\udc67\u00ba2\u00112\n\u00b8\u0010\n\u00b9\ud835\udc64\u00b8\ud835\udc65\u0000\ud835\udc66\u0000\ud835\udc67\u00ba2\u00b8\u00b9\ud835\udc64\u00b8\ud835\udc65\u00b8\ud835\udc66\u00b8\ud835\udc67\u00ba2\u00112\u0013\n.\n(A.16)\nIf we then also wanted to compute\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc65, we would end up with a similar equation again with\nmany repeated terms, and many sharedrepeated terms between the two derivatives. This\nrepresentsamassivequantityofwastedwork, andifweneededtocomputederivativesthis\nway, the whole deep learning revolution would have stalled out before it began!\nLet\u2019s break up the problem. We will start by trying to understand how \ud835\udc53changes when we\nchange\ud835\udc4e, essentially assuming that \ud835\udc64,\ud835\udc65,\ud835\udc66, and\ud835\udc67all do not exist. We will reason as we\ndid back when we worked with the gradient for the first time. Let\u2019s take \ud835\udc4eand add a small\namount\ud835\udf16to it.\n\ud835\udc53\u00b9\ud835\udc62\u00b9\ud835\udc4e\u00b8\ud835\udf16,\ud835\udc4f\u00ba,\ud835\udc63\u00b9\ud835\udc4e\u00b8\ud835\udf16,\ud835\udc4f\u00ba\u00ba\n\u0019\ud835\udc53\u0012\n\ud835\udc62\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\u00b8\ud835\udf16\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc4e\u00b9\ud835\udc4e,\ud835\udc4f\u00ba,\ud835\udc63\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\u00b8\ud835\udf16\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc4e\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\u0013\n\u0019\ud835\udc53\u00b9\ud835\udc62\u00b9\ud835\udc4e,\ud835\udc4f\u00ba,\ud835\udc63\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\u00ba\u00b8\ud835\udf16\u0014\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc62\u00b9\ud835\udc62\u00b9\ud835\udc4e,\ud835\udc4f\u00ba,\ud835\udc63\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\u00ba\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc4e\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc63\u00b9\ud835\udc62\u00b9\ud835\udc4e,\ud835\udc4f\u00ba,\ud835\udc63\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\u00ba\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc4e\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\u0015\n.\n(A.17)\nThe first line follows from the definition of partial derivative, and the second follows from\nthe definition of gradient. It is notationally burdensome to track exactly where we evaluate\nevery derivative, as in the expression\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc62\u00b9\ud835\udc62\u00b9\ud835\udc4e,\ud835\udc4f\u00ba,\ud835\udc63\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\u00ba, so we often abbreviate this to\nthe much more memorable\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4e=\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc62\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc4e\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc63\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc4e.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1517, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d54636b7-e96a-4c8e-8948-0ddddd27d117": {"__data__": {"id_": "d54636b7-e96a-4c8e-8948-0ddddd27d117", "embedding": null, "metadata": {"page_label": "940", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1669e80-5bf3-4b94-925c-9d6557068806", "node_type": "4", "metadata": {"page_label": "940", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e387c1c41dd54aa81c6b3150d6f05d10c180518fae9d7bc47a7a33cde5e09a4b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ea7fb8a8-e8d3-4610-af9a-a0ea2dd76200", "node_type": "1", "metadata": {"page_label": "940", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "65cdf1d77797aaeff08334a5acffde715b65c1f60bed6668859ac5ac6cab1dbb", "class_name": "RelatedNodeInfo"}}, "text": "(A.17)\nThe first line follows from the definition of partial derivative, and the second follows from\nthe definition of gradient. It is notationally burdensome to track exactly where we evaluate\nevery derivative, as in the expression\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc62\u00b9\ud835\udc62\u00b9\ud835\udc4e,\ud835\udc4f\u00ba,\ud835\udc63\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\u00ba, so we often abbreviate this to\nthe much more memorable\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4e=\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc62\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc4e\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc63\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc4e. (A.18)\nIt is useful to think about the meaning of the process. We are trying to understand how\na function of the form \ud835\udc53\u00b9\ud835\udc62\u00b9\ud835\udc4e,\ud835\udc4f\u00ba,\ud835\udc63\u00b9\ud835\udc4e,\ud835\udc4f\u00ba\u00bachanges its value with a change in \ud835\udc4e. There\nare two pathways this can occur: there is the pathway where \ud835\udc4e!\ud835\udc62!\ud835\udc53and where\n\ud835\udc4e!\ud835\udc63!\ud835\udc53. We can compute both of these contributions via the chain rule:\ud835\udf15\ud835\udc64\n\ud835\udf15\ud835\udc62\u0001\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc65and\n\ud835\udf15\ud835\udc64\n\ud835\udf15\ud835\udc63\u0001\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc65respectively, and added up.\nImagine we have a different network of functions where the functions on the right depend\non those that are connected to on the left as is shown in Fig. A.2.\ntFig. A.2 Another more subtle example of the chain rule.", "mimetype": "text/plain", "start_char_idx": 1180, "end_char_idx": 2107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "294056e6-c100-4864-8c30-afe02ef48173": {"__data__": {"id_": "294056e6-c100-4864-8c30-afe02ef48173", "embedding": null, "metadata": {"page_label": "941", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a2e1732-fd51-4c0a-8340-1b06f8dc74e7", "node_type": "4", "metadata": {"page_label": "941", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "39345caa381c7ff9dcf7493324f920de6b4f02aac057f3b9ac405c1f7b1bda70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be308fa9-4744-4784-9e41-2b055da0443c", "node_type": "1", "metadata": {}, "hash": "3961c34490ced66517031ac703f8d361769b6c72d60c80e36f2645ffe7929965", "class_name": "RelatedNodeInfo"}}, "text": "941 Multivariable Calculus\nTo compute something like\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc66, we need to sum over all (in this case 3) paths from \ud835\udc66to\ud835\udc53\ngiving\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc66=\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4e\ud835\udf15\ud835\udc4e\n\ud835\udf15\ud835\udc62\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc66\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc62\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc66\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4f\ud835\udf15\ud835\udc4f\n\ud835\udf15\ud835\udc63\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc66. (A.19)\nUnderstandingthechainruleinthiswaywillpaygreatdividendswhentryingtounderstand\nhow gradients flow through networks, and why various architectural choices like those in\nLSTMs ( Section 10.1 ) or residual layers ( Section 8.6 ) can help shape the learning process\nby controlling gradient flow.\nA.4.5TheBackpropagationAlgorithm\nLet\u2019s return to the example of (A.14 )the previous section where\n\ud835\udc53\u00b9\ud835\udc62,\ud835\udc63\u00ba=\u00b9\ud835\udc62\u00b8\ud835\udc63\u00ba2\n\ud835\udc62\u00b9\ud835\udc4e,\ud835\udc4f\u00ba=\u00b9\ud835\udc4e\u00b8\ud835\udc4f\u00ba2, \ud835\udc63\u00b9\ud835\udc4e,\ud835\udc4f\u00ba=\u00b9\ud835\udc4e\u0000\ud835\udc4f\u00ba2,\n\ud835\udc4e\u00b9\ud835\udc64,\ud835\udc65,\ud835\udc66,\ud835\udc67\u00ba=\u00b9\ud835\udc64\u00b8\ud835\udc65\u00b8\ud835\udc66\u00b8\ud835\udc67\u00ba2, \ud835\udc4f\u00b9\ud835\udc64,\ud835\udc65,\ud835\udc66,\ud835\udc67\u00ba=\u00b9\ud835\udc64\u00b8\ud835\udc65\u0000\ud835\udc66\u0000\ud835\udc67\u00ba2.(A.20)\nIf we want to compute say\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc64we may apply the multi-variate chain rule to see:\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc64=\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc62\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc64\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc63\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc64,\n\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc64=\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc4e\ud835\udf15\ud835\udc4e\n\ud835\udf15\ud835\udc64\u00b8\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc4f\ud835\udf15\ud835\udc4f\n\ud835\udf15\ud835\udc64,\n\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc64=\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc4e\ud835\udf15\ud835\udc4e\n\ud835\udf15\ud835\udc64\u00b8\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc4f\ud835\udf15\ud835\udc4f\n\ud835\udf15\ud835\udc64.(A.21)\nLet\u2019s try using this decomposition to compute\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc64. Notice that all we need here are the\nvarious single step partials:\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc62=2\u00b9\ud835\udc62\u00b8\ud835\udc63\u00ba,\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc63=2\u00b9\ud835\udc62\u00b8\ud835\udc63\u00ba,\n\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc4e=2\u00b9\ud835\udc4e\u00b8\ud835\udc4f\u00ba,\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc4f=2\u00b9\ud835\udc4e\u00b8\ud835\udc4f\u00ba,\n\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc4e=2\u00b9\ud835\udc4e\u0000\ud835\udc4f\u00ba,\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc4f=\u00002\u00b9\ud835\udc4e\u0000\ud835\udc4f\u00ba,\n\ud835\udf15\ud835\udc4e\n\ud835\udf15\ud835\udc64=2\u00b9\ud835\udc64\u00b8\ud835\udc65\u00b8\ud835\udc66\u00b8\ud835\udc67\u00ba,\ud835\udf15\ud835\udc4f\n\ud835\udf15\ud835\udc64=2\u00b9\ud835\udc64\u00b8\ud835\udc65\u0000\ud835\udc66\u0000\ud835\udc67\u00ba.(A.22)\nIf we write this out into code this becomes a fairly manageable expression.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1155, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be308fa9-4744-4784-9e41-2b055da0443c": {"__data__": {"id_": "be308fa9-4744-4784-9e41-2b055da0443c", "embedding": null, "metadata": {"page_label": "941", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a2e1732-fd51-4c0a-8340-1b06f8dc74e7", "node_type": "4", "metadata": {"page_label": "941", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "39345caa381c7ff9dcf7493324f920de6b4f02aac057f3b9ac405c1f7b1bda70", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "294056e6-c100-4864-8c30-afe02ef48173", "node_type": "1", "metadata": {"page_label": "941", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "baaeba0e4d460390f4bfbe854bc80d6c391e2ecbe04dbb084ddefa3461522824", "class_name": "RelatedNodeInfo"}}, "text": "(A.22)\nIf we write this out into code this becomes a fairly manageable expression.\n# Compute the value of the function from inputs to outputs\nw, x, y, z =-1,0,-2,1\na, b =(w+x+y+z)**2, (w +x-y-z)**2\nu, v =(a+b)**2, (a -b)**2\nf=(u+v)**2\nprint (f' f at {w},{x},{y},{z}is{f}')\n# Compute the single step partials\ndf_du, df_dv =2*(u+v), 2*(u+v)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 1073, "end_char_idx": 1436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b6cf1349-3614-4733-b85b-89eb48dd669f": {"__data__": {"id_": "b6cf1349-3614-4733-b85b-89eb48dd669f", "embedding": null, "metadata": {"page_label": "942", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f342970e-0e28-4f0c-b773-4d31759ae4b3", "node_type": "4", "metadata": {"page_label": "942", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3bef34185d24d2b91e06192cb9abe8b866b4635a74253d2ff4374854b358b253", "class_name": "RelatedNodeInfo"}}, "text": "942 Mathematics for Deep Learning\n(continued from previous page)\ndu_da, du_db, dv_da, dv_db =2*(a+b), 2*(a+b), 2*(a-b), -2*(a-b)\nda_dw, db_dw =2*(w+x+y+z), 2*(w+x-y-z)\n# Compute the final result from inputs to outputs\ndu_dw, dv_dw =du_da *da_dw +du_db *db_dw, dv_da *da_dw +dv_db *db_dw\ndf_dw =df_du *du_dw +df_dv *dv_dw\nprint (f'df/dw at {w},{x},{y},{z}is{df_dw }')\nf at -1,0,-2,1is1024\ndf/dw at -1,0,-2,1is-4096\nHowever, note that this still does not make it easy to compute something like\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc65. The\nreasonforthatisthe waywechosetoapplythechainrule. Ifwelookatwhatwedidabove,\nwe always kept \ud835\udf15\ud835\udc64in the denominator when we could. In this way, we chose to apply the\nchainruleseeinghow \ud835\udc64changedeveryothervariable. Ifthatiswhatwewanted,thiswould\nbe a good idea. However, think back to our motivation from deep learning: we want to see\nhow every parameter changes the loss. In essence, we want to apply the chain rule keeping\n\ud835\udf15\ud835\udc53in the numerator whenever we can!\nTo be more explicit, note that we can write\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc64=\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4e\ud835\udf15\ud835\udc4e\n\ud835\udf15\ud835\udc64\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4f\ud835\udf15\ud835\udc4f\n\ud835\udf15\ud835\udc64,\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4e=\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc62\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc4e\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc63\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc4e,\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4f=\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc62\ud835\udf15\ud835\udc62\n\ud835\udf15\ud835\udc4f\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc63\ud835\udf15\ud835\udc63\n\ud835\udf15\ud835\udc4f.(A.23)\nNotethatthisapplicationofthechainrulehasusexplicitlycompute\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc62,\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc63,\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4e,\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4f,and\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc64.\nNothing stops us from also including the equations:\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc65=\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4e\ud835\udf15\ud835\udc4e\n\ud835\udf15\ud835\udc65\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4f\ud835\udf15\ud835\udc4f\n\ud835\udf15\ud835\udc65,\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc66=\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4e\ud835\udf15\ud835\udc4e\n\ud835\udf15\ud835\udc66\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4f\ud835\udf15\ud835\udc4f\n\ud835\udf15\ud835\udc66,\n\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc67=\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4e\ud835\udf15\ud835\udc4e\n\ud835\udf15\ud835\udc67\u00b8\ud835\udf15\ud835\udc53\n\ud835\udf15\ud835\udc4f\ud835\udf15\ud835\udc4f\n\ud835\udf15\ud835\udc67.(A.24)\nand then keeping track of how \ud835\udc53changes when we change anynode in the entire network.\nLet\u2019s implement it.\n# Compute the value of the function from inputs to outputs\nw, x, y, z =-1,0,-2,1\na, b =(w+x+y+z)**2, (w +x-y-z)**2\nu, v =(a+b)**2, (a -b)**2\nf=(u+v)**2\nprint (f'f at {w},{x},{y},{z}is{f}')\n# Compute the derivative using the decomposition above\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1705, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3f751bd9-1ee9-4fce-acc0-20df115a0f42": {"__data__": {"id_": "3f751bd9-1ee9-4fce-acc0-20df115a0f42", "embedding": null, "metadata": {"page_label": "943", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d5f4a4e-86da-463d-9d7b-18e22090acf4", "node_type": "4", "metadata": {"page_label": "943", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2009258bbc99e9353e06fc0c506ac345a99dc6827c167f06bda63a3750268365", "class_name": "RelatedNodeInfo"}}, "text": "943 Multivariable Calculus\n(continued from previous page)\n# First compute the single step partials\ndf_du, df_dv =2*(u+v), 2*(u+v)\ndu_da, du_db, dv_da, dv_db =2*(a+b), 2*(a+b), 2*(a-b), -2*(a-b)\nda_dw, db_dw =2*(w+x+y+z), 2*(w+x-y-z)\nda_dx, db_dx =2*(w+x+y+z), 2*(w+x-y-z)\nda_dy, db_dy =2*(w+x+y+z), -2*(w+x-y-z)\nda_dz, db_dz =2*(w+x+y+z), -2*(w+x-y-z)\n# Now compute how f changes when we change any value from output to input\ndf_da, df_db =df_du *du_da +df_dv *dv_da, df_du *du_db +df_dv *dv_db\ndf_dw, df_dx =df_da *da_dw +df_db *db_dw, df_da *da_dx +df_db *db_dx\ndf_dy, df_dz =df_da *da_dy +df_db *db_dy, df_da *da_dz +df_db *db_dz\nprint (f'df/dw at {w},{x},{y},{z}is{df_dw }')\nprint (f'df/dx at {w},{x},{y},{z}is{df_dx }')\nprint (f'df/dy at {w},{x},{y},{z}is{df_dy }')\nprint (f'df/dz at {w},{x},{y},{z}is{df_dz }')\nf at -1,0,-2,1is1024\ndf/dw at -1,0,-2,1is-4096\ndf/dx at -1,0,-2,1is-4096\ndf/dy at -1,0,-2,1is-4096\ndf/dz at -1,0,-2,1is-4096\nThe fact that we compute derivatives from \ud835\udc53back towards the inputs rather than from the\ninputs forward to the outputs (as we did in the first code snippet above) is what gives this\nalgorithm its name: backpropagation . Note that there are two steps: 1. Compute the value\nof the function, and the single step partials from front to back. While not done above, this\ncan be combined into a single forward pass . 2. Compute the gradient of \ud835\udc53from back to\nfront. We call this the backwardspass .\nThis is precisely what every deep learning algorithm implements to allow the computation\nof the gradient of the loss with respect to every weight in the network at one pass. It is an\nastonishing fact that we have such a decomposition.\nTo see how to encapsulated this, let\u2019s take a quick look at this example.\n# Initialize as ndarrays, then attach gradients\nw=torch .tensor([ -1.], requires_grad =True )\nx=torch .tensor([ 0.], requires_grad =True )\ny=torch .tensor([ -2.], requires_grad =True )\nz=torch .tensor([ 1.], requires_grad =True )\n# Do the computation like usual, tracking gradients\na, b =(w+x+y+z)**2, (w +x-y-z)**2\nu, v =(a+b)**2, (a -b)**2\nf=(u+v)**2\n# Execute backward pass\nf.backward()\nprint (f'df/dw at {w.data .item() },{x.data .item() },{y.data .item() },'\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "538c2ad7-1046-402a-814e-467b18ff41de": {"__data__": {"id_": "538c2ad7-1046-402a-814e-467b18ff41de", "embedding": null, "metadata": {"page_label": "944", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3131d840-5cb3-4056-972b-9ed0f6588974", "node_type": "4", "metadata": {"page_label": "944", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e04e2c16092d276689fbaaae675520ccae63efc5f57f3ec3ce58605110055157", "class_name": "RelatedNodeInfo"}}, "text": "944 Mathematics for Deep Learning\n(continued from previous page)\nf'{z.data .item() }is{w.grad .data .item() }')\nprint (f'df/dx at {w.data .item() },{x.data .item() },{y.data .item() },'\nf'{z.data .item() }is{x.grad .data .item() }')\nprint (f'df/dy at {w.data .item() },{x.data .item() },{y.data .item() },'\nf'{z.data .item() }is{y.grad .data .item() }')\nprint (f'df/dz at {w.data .item() },{x.data .item() },{y.data .item() },'\nf'{z.data .item() }is{z.grad .data .item() }')\ndf/dw at -1.0,0.0,-2.0,1.0 is-4096.0\ndf/dx at -1.0,0.0,-2.0,1.0 is-4096.0\ndf/dy at -1.0,0.0,-2.0,1.0 is-4096.0\ndf/dz at -1.0,0.0,-2.0,1.0 is-4096.0\nAll of what we did above can be done automatically by calling f.backwards() .\nA.4.6Hessians\nAs with single variable calculus, it is useful to consider higher-order derivatives in order\nto get a handle on how we can obtain a better approximation to a function than using the\ngradient alone.\nThere is one immediate problem one encounters when working with higher order deriva-\ntives of functions of several variables, and that is there are a large number of them. If we\nhave a function \ud835\udc53\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc5b\u00baof\ud835\udc5bvariables, then we can take \ud835\udc5b2many second derivatives,\nnamely for any choice of \ud835\udc56and\ud835\udc57:\n\ud835\udc512\ud835\udc53\n\ud835\udc51\ud835\udc65\ud835\udc56\ud835\udc51\ud835\udc65\ud835\udc57=\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc56\u0012\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc57\ud835\udc53\u0013\n. (A.25)\nThis is traditionally assembled into a matrix called the Hessian:\nH\ud835\udc53=26666664\ud835\udc512\ud835\udc53\n\ud835\udc51\ud835\udc651\ud835\udc51\ud835\udc651\u0001\u0001\u0001\ud835\udc512\ud835\udc53\n\ud835\udc51\ud835\udc651\ud835\udc51\ud835\udc65\ud835\udc5b.........\n\ud835\udc512\ud835\udc53\n\ud835\udc51\ud835\udc65\ud835\udc5b\ud835\udc51\ud835\udc651\u0001\u0001\u0001\ud835\udc512\ud835\udc53\n\ud835\udc51\ud835\udc65\ud835\udc5b\ud835\udc51\ud835\udc65\ud835\udc5b37777775. (A.26)\nNot every entry of this matrix is independent. Indeed, we can show that as long as both\nmixed partials (partial derivatives with respect to more than one variable) exist and are\ncontinuous, we can say that for any \ud835\udc56, and\ud835\udc57,\n\ud835\udc512\ud835\udc53\n\ud835\udc51\ud835\udc65\ud835\udc56\ud835\udc51\ud835\udc65\ud835\udc57=\ud835\udc512\ud835\udc53\n\ud835\udc51\ud835\udc65\ud835\udc57\ud835\udc51\ud835\udc65\ud835\udc56. (A.27)\nThis follows by considering first perturbing a function in the direction of \ud835\udc65\ud835\udc56, and then per-\nturbing it in\ud835\udc65\ud835\udc57and then comparing the result of that with what happens if we perturb first\n\ud835\udc65\ud835\udc57and then\ud835\udc65\ud835\udc56, with the knowledge that both of these orders lead to the same final change\nin the output of \ud835\udc53.\nAs with single variables, we can use these derivatives to get a far better idea of how the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3782df88-f77b-4264-893c-b8dabf6f7371": {"__data__": {"id_": "3782df88-f77b-4264-893c-b8dabf6f7371", "embedding": null, "metadata": {"page_label": "945", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c60d7e0-5ce9-4d9b-b874-642cd0715938", "node_type": "4", "metadata": {"page_label": "945", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fdd2f792de1e6a0031a5a345151ba444fbeac733adf2bf7c38ffbd69cd75032e", "class_name": "RelatedNodeInfo"}}, "text": "945 Multivariable Calculus\nfunction behaves near a point. In particular, we can use it to find the best fitting quadratic\nnear a point x0, as we saw in a single variable.\nLet\u2019s see an example. Suppose that \ud835\udc53\u00b9\ud835\udc651,\ud835\udc652\u00ba=\ud835\udc4e\u00b8\ud835\udc4f1\ud835\udc651\u00b8\ud835\udc4f2\ud835\udc652\u00b8\ud835\udc5011\ud835\udc652\n1\u00b8\ud835\udc5012\ud835\udc651\ud835\udc652\u00b8\ud835\udc5022\ud835\udc652\n2.\nThis is the general form for a quadratic in two variables. If we look at the value of the\nfunction, its gradient, and its Hessian (A.26 ), all at the point zero:\n\ud835\udc53\u00b90,0\u00ba=\ud835\udc4e,\nr\ud835\udc53\u00b90,0\u00ba=\u0014\ud835\udc4f1\n\ud835\udc4f2\u0015\n,\nH\ud835\udc53\u00b90,0\u00ba=\u00142\ud835\udc5011\ud835\udc5012\n\ud835\udc5012 2\ud835\udc5022\u0015\n,(A.28)\nwe can get our original polynomial back by saying\n\ud835\udc53\u00b9x\u00ba=\ud835\udc53\u00b90\u00ba\u00b8r\ud835\udc53\u00b90\u00ba\u0001x\u00b81\n2x>H\ud835\udc53\u00b90\u00bax. (A.29)\nIn general, if we computed this expansion any point x0, we see that\n\ud835\udc53\u00b9x\u00ba=\ud835\udc53\u00b9x0\u00ba\u00b8r\ud835\udc53\u00b9x0\u00ba\u0001\u00b9x\u0000x0\u00ba\u00b81\n2\u00b9x\u0000x0\u00ba>H\ud835\udc53\u00b9x0\u00ba\u00b9x\u0000x0\u00ba. (A.30)\nThisworksforanydimensionalinput,andprovidesthebestapproximatingquadratictoany\nfunction at a point. To give an example, let\u2019s plot the function\n\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba=\ud835\udc65\ud835\udc52\u0000\ud835\udc652\u0000\ud835\udc662. (A.31)\nOne can compute that the gradient and Hessian are\nr\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba=\ud835\udc52\u0000\ud835\udc652\u0000\ud835\udc662\u00121\u00002\ud835\udc652\n\u00002\ud835\udc65\ud835\udc66\u0013\nandH\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba=\ud835\udc52\u0000\ud835\udc652\u0000\ud835\udc662\u00124\ud835\udc653\u00006\ud835\udc65 4\ud835\udc652\ud835\udc66\u00002\ud835\udc66\n4\ud835\udc652\ud835\udc66\u00002\ud835\udc664\ud835\udc65\ud835\udc662\u00002\ud835\udc65\u0013\n.\n(A.32)\nAnd thus, with a little algebra, see that the approximating quadratic at \u00bb\u00001,0\u00bc>is\n\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba\u0019\ud835\udc52\u00001\u0010\n\u00001\u0000\u00b9\ud835\udc65\u00b81\u00ba\u00b8\u00b9\ud835\udc65\u00b81\u00ba2\u00b8\ud835\udc662\u0011\n. (A.33)\n# Construct grid and compute function\nx, y =torch .meshgrid(torch .linspace( -2,2,101),\ntorch .linspace( -2,2,101))\nz=x*torch .exp( -x**2-y**2)\n# Compute approximating quadratic with gradient and Hessian at (1, 0)\nw=torch .exp(torch .tensor([ -1.]))*(-1-(x+1)+2*(x+1)**2+2*y**2)\n# Plot function\nax=d2l.plt.figure() .add_subplot( 111, projection ='3d')\nax.plot_wireframe(x .numpy(), y .numpy(), z .numpy(),\n**{'rstride ':10,'cstride ':10})\nax.plot_wireframe(x .numpy(), y .numpy(), w .numpy(),\n**{'rstride ':10,'cstride ':10}, color ='purple ')\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6aedb46f-717f-4b11-b88a-091cbcc56a4d": {"__data__": {"id_": "6aedb46f-717f-4b11-b88a-091cbcc56a4d", "embedding": null, "metadata": {"page_label": "946", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6593fb55-8dcc-41d1-af99-c1ff1d2685ee", "node_type": "4", "metadata": {"page_label": "946", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0b42772738b01f04f69e52bd043636db5ffe65157557b7b0cf9f33866021b04b", "class_name": "RelatedNodeInfo"}}, "text": "946 Mathematics for Deep Learning\n(continued from previous page)\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'y')\nd2l.set_figsize()\nax.set_xlim( -2,2)\nax.set_ylim( -2,2)\nax.set_zlim( -1,1)\nax.dist =12\nThis forms the basis for Newton\u2019s Algorithm discussed in Section 12.3 , where we perform\nnumerical optimization iteratively finding the best fitting quadratic, and then exactly mini-\nmizing that quadratic.\nA.4.7ALittle Matrix Calculus\nDerivatives of functions involving matrices turn out to be particularly nice. This section\ncanbecomenotationallyheavy,somaybeskippedinafirstreading,butitisusefultoknow\nhow derivatives of functions involving common matrix operations are often much cleaner\nthan one might initially anticipate, particularly given how central matrix operations are to\ndeep learning applications.\nLet\u2019s begin with an example. Suppose that we have some fixed column vector \ud835\udf37, and we\nwanttotaketheproductfunction \ud835\udc53\u00b9x\u00ba=\ud835\udf37>x,andunderstandhowthedotproductchanges\nwhen we change x.\nA bit of notation that will be useful when working with matrix derivatives in ML is called\nthedenominator layout matrix derivative where we assemble our partial derivatives into\nthe shape of whatever vector, matrix, or tensor is in the denominator of the differential. In\nthis case, we will write\n\ud835\udc51\ud835\udc53\n\ud835\udc51x=26666664\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc651...\n\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\ud835\udc5b37777775, (A.34)\nwhere we matched the shape of the column vector x.\nIf we write out our function into components this is\n\ud835\udc53\u00b9x\u00ba=\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udefd\ud835\udc56\ud835\udc65\ud835\udc56=\ud835\udefd1\ud835\udc651\u00b8\u0001\u0001\u0001\u00b8\ud835\udefd\ud835\udc5b\ud835\udc65\ud835\udc5b. (A.35)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f58d150a-1790-482b-b36a-2c4480f9e3cf": {"__data__": {"id_": "f58d150a-1790-482b-b36a-2c4480f9e3cf", "embedding": null, "metadata": {"page_label": "947", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5fe9c554-fae6-43a5-b274-712b68c702b3", "node_type": "4", "metadata": {"page_label": "947", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2265796f112673896decf436cdcdfc2b3c326ebdfcbbdcf1dc20372e7b558785", "class_name": "RelatedNodeInfo"}}, "text": "947 Multivariable Calculus\nIf we now take the partial derivative with respect to say \ud835\udefd1, note that everything is zero but\nthe first term, which is just \ud835\udc651multiplied by \ud835\udefd1, so we obtain that\n\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc651=\ud835\udefd1, (A.36)\nor more generally that\n\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\ud835\udc56=\ud835\udefd\ud835\udc56. (A.37)\nWe can now reassemble this into a matrix to see\n\ud835\udc51\ud835\udc53\n\ud835\udc51x=26666664\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc651...\n\ud835\udc51\ud835\udc53\n\ud835\udc51\ud835\udc65\ud835\udc5b37777775=2666664\ud835\udefd1\n...\n\ud835\udefd\ud835\udc5b3777775=\ud835\udf37. (A.38)\nThis illustrates a few factors about matrix calculus that we will often counter throughout\nthis section:\n\u000fFirst, The computations will get rather involved.\n\u000fSecond, The final results are much cleaner than the intermediate process, and will al-\nways look similar to the single variable case. In this case, note that\ud835\udc51\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc4f\ud835\udc65\u00ba=\ud835\udc4fand\n\ud835\udc51\n\ud835\udc51x\u00b9\ud835\udf37>x\u00ba=\ud835\udf37are both similar.\n\u000fThird, transposes can often appear seemingly from nowhere. The core reason for this is\nthe convention that we match the shape of the denominator, thus when we multiply\nmatrices, we will need to take transposes to match back to the shape of the original\nterm.\nTo keep building intuition, let\u2019s try a computation that is a little harder. Suppose that we\nhave a column vector x, and a square matrix \ud835\udc34and we want to compute\n\ud835\udc51\n\ud835\udc51x\u00b9x>\ud835\udc34x\u00ba. (A.39)\nTo drive towards easier to manipulate notation, let\u2019s consider this problem using Einstein\nnotation. In this case we can write the function as\nx>\ud835\udc34x=\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc65\ud835\udc57. (A.40)\nTo compute our derivative, we need to understand for every \ud835\udc58, what is the value of\n\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc58\u00b9x>\ud835\udc34x\u00ba=\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc58\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc65\ud835\udc57. (A.41)\nBy the product rule, this is\n\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc58\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc65\ud835\udc57=\ud835\udc51\ud835\udc65\ud835\udc56\n\ud835\udc51\ud835\udc65\ud835\udc58\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc65\ud835\udc57\u00b8\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc51\ud835\udc65\ud835\udc57\n\ud835\udc51\ud835\udc65\ud835\udc58. (A.42)\nFor a term like\ud835\udc51\ud835\udc65\ud835\udc56\n\ud835\udc51\ud835\udc65\ud835\udc58, it is not hard to see that this is one when \ud835\udc56=\ud835\udc58and zero otherwise.\nThis means that every term where \ud835\udc56and\ud835\udc58are different vanish from this sum, so the only", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b3ad059-49bf-473d-a659-f70a03aff5bd": {"__data__": {"id_": "1b3ad059-49bf-473d-a659-f70a03aff5bd", "embedding": null, "metadata": {"page_label": "948", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aa20d37c-ddf9-4710-ae59-64d7253f0e77", "node_type": "4", "metadata": {"page_label": "948", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a598999fe0c8d1de2a974269aba1876b31c0f3a9db149a7a3ffa6caee9805706", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e29e7ff-a941-4280-a840-a095a20f6444", "node_type": "1", "metadata": {}, "hash": "548adfd67fc4a038ae25ad70537ae7d93d00429177ab71976aa75f22ce359157", "class_name": "RelatedNodeInfo"}}, "text": "948 Mathematics for Deep Learning\nterms that remain in that first sum are the ones where \ud835\udc56=\ud835\udc58. The same reasoning holds for\nthe second term where we need \ud835\udc57=\ud835\udc58. This gives\n\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc58\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc65\ud835\udc57=\ud835\udc4e\ud835\udc58\ud835\udc57\ud835\udc65\ud835\udc57\u00b8\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc56\ud835\udc58. (A.43)\nNow, the names of the indices in Einstein notation are arbitrary\u2014the fact that \ud835\udc56and\ud835\udc57are\ndifferentisimmaterialtothiscomputationatthispoint,sowecanre-indexsothattheyboth\nuse\ud835\udc56to see that\n\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc58\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc65\ud835\udc57=\ud835\udc4e\ud835\udc58\ud835\udc56\ud835\udc65\ud835\udc56\u00b8\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc56\ud835\udc58=\u00b9\ud835\udc4e\ud835\udc58\ud835\udc56\u00b8\ud835\udc4e\ud835\udc56\ud835\udc58\u00ba\ud835\udc65\ud835\udc56. (A.44)\nNow, here is where we start to need some practice to go further. Let\u2019s try and identify this\noutcome in terms of matrix operations. \ud835\udc4e\ud835\udc58\ud835\udc56\u00b8\ud835\udc4e\ud835\udc56\ud835\udc58is the\ud835\udc58,\ud835\udc56-th component of A\u00b8A>. This\ngives\n\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc58\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc65\ud835\udc57=\u00bbA\u00b8A>\u00bc\ud835\udc58\ud835\udc56\ud835\udc65\ud835\udc56. (A.45)\nSimilarly, this term is now the product of the matrix A\u00b8A>by the vector x, so we see\nthat\u0014\ud835\udc51\n\ud835\udc51x\u00b9x>\ud835\udc34x\u00ba\u0015\n\ud835\udc58=\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc58\ud835\udc65\ud835\udc56\ud835\udc4e\ud835\udc56\ud835\udc57\ud835\udc65\ud835\udc57=\u00bb\u00b9A\u00b8A>\u00bax\u00bc\ud835\udc58. (A.46)\nThus, we see that the \ud835\udc58-th entry of the desired derivative from (A.39 )is just the\ud835\udc58-th entry\nof the vector on the right, and thus the two are the same. Thus yields\n\ud835\udc51\n\ud835\udc51x\u00b9x>\ud835\udc34x\u00ba=\u00b9A\u00b8A>\u00bax. (A.47)\nThis required significantly more work than our last one, but the final result is small. More\nthanthat,considerthefollowingcomputationfortraditionalsinglevariablederivatives:\n\ud835\udc51\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\ud835\udc4e\ud835\udc65\u00ba=\ud835\udc51\ud835\udc65\n\ud835\udc51\ud835\udc65\ud835\udc4e\ud835\udc65\u00b8\ud835\udc65\ud835\udc4e\ud835\udc51\ud835\udc65\n\ud835\udc51\ud835\udc65=\u00b9\ud835\udc4e\u00b8\ud835\udc4e\u00ba\ud835\udc65. (A.48)\nEquivalently\ud835\udc51\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc4e\ud835\udc652\u00ba=2\ud835\udc4e\ud835\udc65=\u00b9\ud835\udc4e\u00b8\ud835\udc4e\u00ba\ud835\udc65. Again, we get a result that looks rather like the\nsingle variable result but with a transpose tossed in.\nAt this point, the pattern should be looking rather suspicious, so let\u2019s try to figure out\nwhy. When we take matrix derivatives like this, let\u2019s first assume that the expression we\nget will be another matrix expression: an expression we can write it in terms of products\nand sums of matrices and their transposes. If such an expression exists, it will need to be\ntrue for all matrices. In particular, it will need to be true of 1\u00021matrices, in which case\nthe matrix product is just the product of the numbers, the matrix sum is just the sum, and\nthe transpose does nothing at all! In other words, whatever expression we get mustmatch\nthe single variable expression. This means that, with some practice, one can often guess\nmatrixderivativesjustbyknowingwhattheassociatedsinglevariableexpressionmustlook\nlike!\nLet\u2019s try this out.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e29e7ff-a941-4280-a840-a095a20f6444": {"__data__": {"id_": "1e29e7ff-a941-4280-a840-a095a20f6444", "embedding": null, "metadata": {"page_label": "948", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "aa20d37c-ddf9-4710-ae59-64d7253f0e77", "node_type": "4", "metadata": {"page_label": "948", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a598999fe0c8d1de2a974269aba1876b31c0f3a9db149a7a3ffa6caee9805706", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b3ad059-49bf-473d-a659-f70a03aff5bd", "node_type": "1", "metadata": {"page_label": "948", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a0cb47b19da50d8a3247dc47d18807393d32beaf87dc3e8cf488f3693f478cd3", "class_name": "RelatedNodeInfo"}}, "text": "At this point, the pattern should be looking rather suspicious, so let\u2019s try to figure out\nwhy. When we take matrix derivatives like this, let\u2019s first assume that the expression we\nget will be another matrix expression: an expression we can write it in terms of products\nand sums of matrices and their transposes. If such an expression exists, it will need to be\ntrue for all matrices. In particular, it will need to be true of 1\u00021matrices, in which case\nthe matrix product is just the product of the numbers, the matrix sum is just the sum, and\nthe transpose does nothing at all! In other words, whatever expression we get mustmatch\nthe single variable expression. This means that, with some practice, one can often guess\nmatrixderivativesjustbyknowingwhattheassociatedsinglevariableexpressionmustlook\nlike!\nLet\u2019s try this out. Suppose that Xis a\ud835\udc5b\u0002\ud835\udc5amatrix, Uis an\ud835\udc5b\u0002\ud835\udc5fandVis an\ud835\udc5f\u0002\ud835\udc5a. Let\u2019s\ntry to compute\n\ud835\udc51\n\ud835\udc51VkX\u0000UVk2\n2=? (A.49)", "mimetype": "text/plain", "start_char_idx": 1326, "end_char_idx": 2250, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90f27983-51e9-4dff-ae73-581a5bd0ee32": {"__data__": {"id_": "90f27983-51e9-4dff-ae73-581a5bd0ee32", "embedding": null, "metadata": {"page_label": "949", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c410adb9-1403-47d3-859b-c4ad48a30e8f", "node_type": "4", "metadata": {"page_label": "949", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "98b3c8d02287914cf3b246689f667591847fb87659adeb9dd58ca96071c37f30", "class_name": "RelatedNodeInfo"}}, "text": "949 Multivariable Calculus\nThis computation is important in an area called matrix factorization. For us, however, it is\njust a derivative to compute. Let\u2019s try to imagine what this would be for 1\u00021matrices. In\nthat case, we get the expression\n\ud835\udc51\n\ud835\udc51\ud835\udc63\u00b9\ud835\udc65\u0000\ud835\udc62\ud835\udc63\u00ba2=\u00002\u00b9\ud835\udc65\u0000\ud835\udc62\ud835\udc63\u00ba\ud835\udc62, (A.50)\nwhere, the derivative is rather standard. If we try to convert this back into a matrix expres-\nsion we get\n\ud835\udc51\n\ud835\udc51VkX\u0000UVk2\n2=\u00002\u00b9X\u0000UV\u00baU. (A.51)\nHowever, if we look at this it does not quite work. Recall that Xis\ud835\udc5b\u0002\ud835\udc5a, as isUV, so the\nmatrix 2\u00b9X\u0000UV\u00bais\ud835\udc5b\u0002\ud835\udc5a. On the other hand Uis\ud835\udc5b\u0002\ud835\udc5f, and we cannot multiply a \ud835\udc5b\u0002\ud835\udc5a\nand a\ud835\udc5b\u0002\ud835\udc5fmatrix since the dimensions do not match!\nWe want to get\ud835\udc51\n\ud835\udc51V, which is the same shape as V, which is\ud835\udc5f\u0002\ud835\udc5a. So somehow we need\nto take a\ud835\udc5b\u0002\ud835\udc5amatrix and a \ud835\udc5b\u0002\ud835\udc5fmatrix, multiply them together (perhaps with some\ntransposes) to get a \ud835\udc5f\u0002\ud835\udc5a. We can do this by multiplying \ud835\udc48>by\u00b9X\u0000UV\u00ba. Thus, we can\nguess the solution to (A.49 )is\n\ud835\udc51\n\ud835\udc51VkX\u0000UVk2\n2=\u00002U>\u00b9X\u0000UV\u00ba. (A.52)\nTo show that this works, we would be remiss to not provide a detailed computation. If\nwe already believe that this rule-of-thumb works, feel free to skip past this derivation. To\ncompute\n\ud835\udc51\n\ud835\udc51VkX\u0000UVk2\n2, (A.53)\nwe must find for every \ud835\udc4e, and\ud835\udc4f\n\ud835\udc51\n\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc4fkX\u0000UVk2\n2=\ud835\udc51\n\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc4f\u00d5\n\ud835\udc56,\ud835\udc57 \n\ud835\udc65\ud835\udc56\ud835\udc57\u0000\u00d5\n\ud835\udc58\ud835\udc62\ud835\udc56\ud835\udc58\ud835\udc63\ud835\udc58\ud835\udc57!2\n. (A.54)\nRecalling that all entries of XandUare constants as far as\ud835\udc51\n\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc4fis concerned, we may\npush the derivative inside the sum, and apply the chain rule to the square to get\n\ud835\udc51\n\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc4fkX\u0000UVk2\n2=\u00d5\n\ud835\udc56,\ud835\udc572 \n\ud835\udc65\ud835\udc56\ud835\udc57\u0000\u00d5\n\ud835\udc58\ud835\udc62\ud835\udc56\ud835\udc58\ud835\udc63\ud835\udc58\ud835\udc57!  \n\u0000\u00d5\n\ud835\udc58\ud835\udc62\ud835\udc56\ud835\udc58\ud835\udc51\ud835\udc63\ud835\udc58\ud835\udc57\n\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc4f!\n. (A.55)\nAs in the previous derivation, we may note that\ud835\udc51\ud835\udc63\ud835\udc58 \ud835\udc57\n\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc4fis only non-zero if the \ud835\udc58=\ud835\udc4eand\n\ud835\udc57=\ud835\udc4f. If either of those conditions do not hold, the term in the sum is zero, and we may\nfreely discard it. We see that\n\ud835\udc51\n\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc4fkX\u0000UVk2\n2=\u00002\u00d5\n\ud835\udc56 \n\ud835\udc65\ud835\udc56\ud835\udc4f\u0000\u00d5\n\ud835\udc58\ud835\udc62\ud835\udc56\ud835\udc58\ud835\udc63\ud835\udc58\ud835\udc4f!\n\ud835\udc62\ud835\udc56\ud835\udc4e. (A.56)\nAn important subtlety here is that the requirement that \ud835\udc58=\ud835\udc4edoes not occur inside the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd7d0ce9-ded7-4889-a786-3316e372f22a": {"__data__": {"id_": "dd7d0ce9-ded7-4889-a786-3316e372f22a", "embedding": null, "metadata": {"page_label": "950", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "16692cac-39ed-4da8-85cc-43d5493b1090", "node_type": "4", "metadata": {"page_label": "950", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "00efeec21b168714d4c40aae89b22678f383d848b3763e1b6dfe7422eb604d9b", "class_name": "RelatedNodeInfo"}}, "text": "950 Mathematics for Deep Learning\ninner sum since that \ud835\udc58is a dummy variable which we are summing over inside the inner\nterm. For a notationally cleaner example, consider why\n\ud835\udc51\n\ud835\udc51\ud835\udc651 \u00d5\n\ud835\udc56\ud835\udc65\ud835\udc56!2\n=2 \u00d5\n\ud835\udc56\ud835\udc65\ud835\udc56!\n. (A.57)\nFrom this point, we may start identifying components of the sum. First,\n\u00d5\n\ud835\udc58\ud835\udc62\ud835\udc56\ud835\udc58\ud835\udc63\ud835\udc58\ud835\udc4f=\u00bbUV\u00bc\ud835\udc56\ud835\udc4f.(A.58)\nSo the entire expression in the inside of the sum is\n\ud835\udc65\ud835\udc56\ud835\udc4f\u0000\u00d5\n\ud835\udc58\ud835\udc62\ud835\udc56\ud835\udc58\ud835\udc63\ud835\udc58\ud835\udc4f=\u00bbX\u0000UV\u00bc\ud835\udc56\ud835\udc4f.(A.59)\nThis means we may now write our derivative as\n\ud835\udc51\n\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc4fkX\u0000UVk2\n2=\u00002\u00d5\n\ud835\udc56\u00bbX\u0000UV\u00bc\ud835\udc56\ud835\udc4f\ud835\udc62\ud835\udc56\ud835\udc4e. (A.60)\nWe want this to look like the \ud835\udc4e,\ud835\udc4felement of a matrix so we can use the technique as in the\nprevious example to arrive at a matrix expression, which means that we need to exchange\nthe order of the indices on \ud835\udc62\ud835\udc56\ud835\udc4e. If we notice that \ud835\udc62\ud835\udc56\ud835\udc4e=\u00bbU>\u00bc\ud835\udc4e\ud835\udc56, we can then write\n\ud835\udc51\n\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc4fkX\u0000UVk2\n2=\u00002\u00d5\n\ud835\udc56\u00bbU>\u00bc\ud835\udc4e\ud835\udc56\u00bbX\u0000UV\u00bc\ud835\udc56\ud835\udc4f. (A.61)\nThis is a matrix product, and thus we can conclude that\n\ud835\udc51\n\ud835\udc51\ud835\udc63\ud835\udc4e\ud835\udc4fkX\u0000UVk2\n2=\u00002\u00bbU>\u00b9X\u0000UV\u00ba\u00bc\ud835\udc4e\ud835\udc4f. (A.62)\nand thus we may write the solution to (A.49 )\n\ud835\udc51\n\ud835\udc51VkX\u0000UVk2\n2=\u00002U>\u00b9X\u0000UV\u00ba. (A.63)\nThis matches the solution we guessed above!\nIt is reasonable to ask at this point, \u201cWhy can I not just write down matrix versions of\nall the calculus rules I have learned? It is clear this is still mechanical. Why do we not\njust get it over with!\u201d And indeed there are such rules and ( Petersen and Pedersen, 2008 )\nprovides an excellent summary. However, due to the plethora of ways matrix operations\ncan be combined compared to single values, there are many more matrix derivative rules\nthansinglevariableones. Itisoftenthecasethatitisbesttoworkwiththeindices,orleave\nit up to automatic differentiation when appropriate.\nA.4.8Summary\n\u000fIn higher dimensions, we can define gradients which serve the same purpose as deriva-\ntives in one dimension. These allow us to see how a multi-variable function changes\nwhen we make an arbitrary small change to the inputs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1811, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc60643f-f9fd-49e8-a5e6-5f289fddf027": {"__data__": {"id_": "dc60643f-f9fd-49e8-a5e6-5f289fddf027", "embedding": null, "metadata": {"page_label": "951", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "69ddaa41-fbd5-4111-8888-fde555a1cd99", "node_type": "4", "metadata": {"page_label": "951", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1543fd248b7c8c965ea72e0c02e2123edb720cdbdbdff2bec10618d341729e49", "class_name": "RelatedNodeInfo"}}, "text": "951 Integral Calculus\n283\u000fThe backpropagation algorithm can be seen to be a method of organizing the multi-\nvariable chain rule to allow for the efficient computation of many partial derivatives.\n\u000fMatrix calculus allows us to write the derivatives of matrix expressions in concise ways.\nA.4.9Exercises\n1.Givenacolumnvector \ud835\udf37,computethederivativesofboth \ud835\udc53\u00b9x\u00ba=\ud835\udf37>xand\ud835\udc54\u00b9x\u00ba=x>\ud835\udf37.\nWhy do you get the same answer?\n2.Letvbe an\ud835\udc5bdimension vector. What is\ud835\udf15\n\ud835\udf15vkvk2?\n3.Let\ud835\udc3f\u00b9\ud835\udc65,\ud835\udc66\u00ba=log\u00b9\ud835\udc52\ud835\udc65\u00b8\ud835\udc52\ud835\udc66\u00ba. Compute the gradient. What is the sum of the components\nof the gradient?\n4.Let\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba=\ud835\udc652\ud835\udc66\u00b8\ud835\udc65\ud835\udc662. Show that the only critical point is \u00b90,0\u00ba. By considering\n\ud835\udc53\u00b9\ud835\udc65,\ud835\udc65\u00ba, determine if\u00b90,0\u00bais a maximum, minimum, or neither.\n5.Suppose that we are minimizing a function \ud835\udc53\u00b9x\u00ba=\ud835\udc54\u00b9x\u00ba\u00b8\u210e\u00b9x\u00ba. How can we geomet-\nrically interpret the condition of r\ud835\udc53=0in terms of\ud835\udc54and\u210e?\nDiscussions283.\nA.5IntegralCalculus\nDifferentiation only makes up half of the content of a traditional calculus education. The\nother pillar, integration, starts out seeming a rather disjoint question, \u201cWhat is the area\nunderneath this curve?\u201d While seemingly unrelated, integration is tightly intertwined with\nthe differentiation via what is known as the fundamentaltheoremof calculus .\nAtthelevelofmachinelearningwediscussinthisbook,wewillnotneedadeepunderstand-\ning of integration. However, we will provide a brief introduction to lay the groundwork for\nany further applications we will encounter later on.\nA.5.1GeometricInterpretation\nSupposethatwehaveafunction \ud835\udc53\u00b9\ud835\udc65\u00ba. Forsimplicity,let\u2019sassumethat \ud835\udc53\u00b9\ud835\udc65\u00baisnon-negative\n(never takes a value less than zero). What we want to try and understand is: what is the\narea contained between \ud835\udc53\u00b9\ud835\udc65\u00baand the\ud835\udc65-axis?\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom mpl_toolkits import mplot3d\nfrom d2l import torch asd2l\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1822, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7db5ddab-0d7e-41cf-b98f-708becca1aad": {"__data__": {"id_": "7db5ddab-0d7e-41cf-b98f-708becca1aad", "embedding": null, "metadata": {"page_label": "952", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c64dd00c-5a71-461b-a7aa-24a07610b74c", "node_type": "4", "metadata": {"page_label": "952", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "72ac52b147fe59b16d64c59db120b40131bc0c31156cab3815cccb9c5e00b434", "class_name": "RelatedNodeInfo"}}, "text": "952 Mathematics for Deep Learning\n(continued from previous page)\nx=torch .arange( -2,2,0.01 )\nf=torch .exp( -x**2)\nd2l.set_figsize()\nd2l.plt.plot(x, f, color ='black ')\nd2l.plt.fill_between(x .tolist(), f .tolist())\nd2l.plt.show()\nIn most cases, this area will be infinite or undefined (consider the area under \ud835\udc53\u00b9\ud835\udc65\u00ba=\ud835\udc652),\nso people will often talk about the area between a pair of ends, say \ud835\udc4eand\ud835\udc4f.\nx=torch .arange( -2,2,0.01 )\nf=torch .exp( -x**2)\nd2l.set_figsize()\nd2l.plt.plot(x, f, color ='black ')\nd2l.plt.fill_between(x .tolist()[ 50:250], f .tolist()[ 50:250])\nd2l.plt.show()\nWe will denote this area by the integral symbol below:\nArea\u00b9A\u00ba=\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65. (A.1)\nThe inner variable is a dummy variable, much like the index of a sum in a\u00cd, and so this\ncan be equivalently written with any inner value we like:\n\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65=\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc53\u00b9\ud835\udc67\u00ba\ud835\udc51\ud835\udc67. (A.2)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5a71e95-442a-4a83-942e-8b08b2dc9830": {"__data__": {"id_": "d5a71e95-442a-4a83-942e-8b08b2dc9830", "embedding": null, "metadata": {"page_label": "953", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43b0e7df-4912-4def-90b2-60604db225b6", "node_type": "4", "metadata": {"page_label": "953", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2715870278225e82a82d7198708b61a855fc948e087817577b10d2b7e469fbea", "class_name": "RelatedNodeInfo"}}, "text": "953 Integral Calculus\nThere is a traditional way to try and understand how we might try to approximate such\nintegrals: we can imagine taking the region in-between \ud835\udc4eand\ud835\udc4fand chopping it into \ud835\udc41\nvertical slices. If \ud835\udc41is large, we can approximate the area of each slice by a rectangle, and\nthen add up the areas to get the total area under the curve. Let\u2019s take a look at an example\ndoing this in code. We will see how to get the true value in a later section.\nepsilon =0.05\na=0\nb=2\nx=torch .arange(a, b, epsilon)\nf=x/(1+x**2)\napprox =torch .sum(epsilon *f)\ntrue =torch .log(torch .tensor([ 5.])) /2\nd2l.set_figsize()\nd2l.plt.bar(x, f, width =epsilon, align ='edge ')\nd2l.plt.plot(x, f, color ='black ')\nd2l.plt.ylim([ 0,1])\nd2l.plt.show()\nf'approximation: {approx }, truth: {true }'\n'approximation: 0.7944855690002441, truth: tensor([0.8047]) '\nThe issue is that while it can be done numerically, we can do this approach analytically for\nonly the simplest functions like\n\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc65 \ud835\udc51\ud835\udc65. (A.3)\nAnything somewhat more complex like our example from the code above\n\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc65\n1\u00b8\ud835\udc652\ud835\udc51\ud835\udc65. (A.4)\nis beyond what we can solve with such a direct method.\nWe will instead take a different approach. We will work intuitively with the notion of the\narea,andlearnthemaincomputationaltoolusedtofindintegrals: the fundamentaltheorem\nofcalculus . This will be the basis for our study of integration.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e239d1ba-cea9-4a53-b51e-5abacd7d6a8e": {"__data__": {"id_": "e239d1ba-cea9-4a53-b51e-5abacd7d6a8e", "embedding": null, "metadata": {"page_label": "954", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ca28fa14-4437-48e2-ae23-9f732ed14586", "node_type": "4", "metadata": {"page_label": "954", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "66210e949e62e954a2331fe12a24409069e188cf94b6897efc883c4a41fe0659", "class_name": "RelatedNodeInfo"}}, "text": "954 Mathematics for Deep Learning\nA.5.2The Fundamental Theoremof Calculus\nTo dive deeper into the theory of integration, let\u2019s introduce a function\n\ud835\udc39\u00b9\ud835\udc65\u00ba=\u00b9\ud835\udc65\n0\ud835\udc53\u00b9\ud835\udc66\u00ba\ud835\udc51\ud835\udc66. (A.5)\nThis function measures the area between 0and\ud835\udc65depending on how we change \ud835\udc65. Notice\nthat this is everything we need since\n\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65=\ud835\udc39\u00b9\ud835\udc4f\u00ba\u0000\ud835\udc39\u00b9\ud835\udc4e\u00ba. (A.6)\nThis is a mathematical encoding of the fact that we can measure the area out to the far end-\npoint and then subtract off the area to the near end point as indicated in Fig. A.1.\ntFig. A.1 Visualizing why we may reduce the problem of computing the area under a curve between\ntwo points to computing the area to the left of a point.\nThus, we can figure out what the integral over any interval is by figuring out what \ud835\udc39\u00b9\ud835\udc65\u00ba\nis.\nTo do so, let\u2019s consider an experiment. As we often do in calculus, let\u2019s imagine what hap-\npens when we shift the value by a tiny bit. From the comment above, we know that\n\ud835\udc39\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0000\ud835\udc39\u00b9\ud835\udc65\u00ba=\u00b9\ud835\udc65\u00b8\ud835\udf16\n\ud835\udc65\ud835\udc53\u00b9\ud835\udc66\u00ba\ud835\udc51\ud835\udc66. (A.7)\nThis tells us that the function changes by the area under a tiny sliver of a function.\nThisis the point at whichwemakean approximation. If welookat a tinysliverofarea like\nthis, it looks like this area is close to the rectangular area with height the value of \ud835\udc53\u00b9\ud835\udc65\u00baand\nthe base width \ud835\udf16. Indeed, one can show that as \ud835\udf16!0this approximation becomes better\nand better. Thus we can conclude:\n\ud835\udc39\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0000\ud835\udc39\u00b9\ud835\udc65\u00ba\u0019\ud835\udf16\ud835\udc53\u00b9\ud835\udc65\u00ba. (A.8)\nHowever, we can now notice: this is exactly the pattern we expect if we were computing\nthe derivative of \ud835\udc39! Thus we see the following rather surprising fact:\n\ud835\udc51\ud835\udc39\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba=\ud835\udc53\u00b9\ud835\udc65\u00ba. (A.9)\nThis is the fundamentaltheoremof calculus . We may write it in expanded form as\n\ud835\udc51\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\n0\ud835\udc53\u00b9\ud835\udc66\u00ba\ud835\udc51\ud835\udc66=\ud835\udc53\u00b9\ud835\udc65\u00ba. (A.10)\nIt takes the concept of finding areas ( a priori rather hard), and reduces it to a statement\nderivatives (something much more completely understood). One last comment that we", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1820, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97b8d7a9-0656-4f20-b577-a1247826d991": {"__data__": {"id_": "97b8d7a9-0656-4f20-b577-a1247826d991", "embedding": null, "metadata": {"page_label": "955", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bacddf08-1901-4cbc-8828-930cbabc11fd", "node_type": "4", "metadata": {"page_label": "955", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5d090d085223a1dc883bdec77a93a6f97c4a7a3f8697156e2e542fe8f46218a6", "class_name": "RelatedNodeInfo"}}, "text": "955 Integral Calculus\nmust make is that this does not tell us exactly what \ud835\udc39\u00b9\ud835\udc65\u00bais. Indeed\ud835\udc39\u00b9\ud835\udc65\u00ba\u00b8\ud835\udc36for any\ud835\udc36has\nthe same derivative. This is a fact-of-life in the theory of integration. Thankfully, notice\nthat when working with definite integrals, the constants drop out, and thus are irrelevant to\nthe outcome.\n\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc53\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65=\u00b9\ud835\udc39\u00b9\ud835\udc4f\u00ba\u00b8\ud835\udc36\u00ba\u0000\u00b9\ud835\udc39\u00b9\ud835\udc4e\u00ba\u00b8\ud835\udc36\u00ba=\ud835\udc39\u00b9\ud835\udc4f\u00ba\u0000\ud835\udc39\u00b9\ud835\udc4e\u00ba. (A.11)\nThismayseemlikeabstractnon-sense,butlet\u2019stakeamomenttoappreciatethatithasgiven\nus a whole new perspective on computing integrals. Our goal is no-longer to do some sort\nof chop-and-sum process to try and recover the area, rather we need only find a function\nwhose derivative is the function we have! This is incredible since we can now list many\nrather difficult integrals by just reversing the table from Section A.3.2 . For instance, we\nknow that the derivative of \ud835\udc65\ud835\udc5bis\ud835\udc5b\ud835\udc65\ud835\udc5b\u00001. Thus, we can say using the fundamental theorem\n(A.10 )that\n\u00b9\ud835\udc65\n0\ud835\udc5b\ud835\udc66\ud835\udc5b\u00001\ud835\udc51\ud835\udc66=\ud835\udc65\ud835\udc5b\u00000\ud835\udc5b=\ud835\udc65\ud835\udc5b. (A.12)\nSimilarly, we know that the derivative of \ud835\udc52\ud835\udc65is itself, so that means\n\u00b9\ud835\udc65\n0\ud835\udc52\ud835\udc65\ud835\udc51\ud835\udc65=\ud835\udc52\ud835\udc65\u0000\ud835\udc520=\ud835\udc52\ud835\udc65\u00001. (A.13)\nInthisway,wecandeveloptheentiretheoryofintegrationleveragingideasfromdifferential\ncalculus freely. Every integration rule derives from this one fact.\nA.5.3Changeof Variables\nJust as with differentiation, there are a number of rules which make the computation of\nintegrals more tractable. In fact, every rule of differential calculus (like the product rule,\nsumrule,andchainrule)hasacorrespondingruleforintegralcalculus(integrationbyparts,\nlinearity of integration, and the change of variables formula respectively). In this section,\nwe will dive into what is arguably the most important from the list: the change of variables\nformula.\nFirst, suppose that we have a function which is itself an integral:\n\ud835\udc39\u00b9\ud835\udc65\u00ba=\u00b9\ud835\udc65\n0\ud835\udc53\u00b9\ud835\udc66\u00ba\ud835\udc51\ud835\udc66. (A.14)\nLet\u2019s suppose that we want to know how this function looks when we compose it with\nanother to obtain \ud835\udc39\u00b9\ud835\udc62\u00b9\ud835\udc65\u00ba\u00ba. By the chain rule, we know\n\ud835\udc51\n\ud835\udc51\ud835\udc65\ud835\udc39\u00b9\ud835\udc62\u00b9\ud835\udc65\u00ba\u00ba=\ud835\udc51\ud835\udc39\n\ud835\udc51\ud835\udc62\u00b9\ud835\udc62\u00b9\ud835\udc65\u00ba\u00ba\u0001\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc65. (A.15)\nWe can turn this into a statement about integration by using the fundamental theorem\n(A.10 )as above. This gives\n\ud835\udc39\u00b9\ud835\udc62\u00b9\ud835\udc65\u00ba\u00ba\u0000\ud835\udc39\u00b9\ud835\udc62\u00b90\u00ba\u00ba=\u00b9\ud835\udc65\n0\ud835\udc51\ud835\udc39\n\ud835\udc51\ud835\udc62\u00b9\ud835\udc62\u00b9\ud835\udc66\u00ba\u00ba\u0001\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc66\ud835\udc51\ud835\udc66. (A.16)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2078, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07e28ca8-6908-4963-bb94-3c4153470361": {"__data__": {"id_": "07e28ca8-6908-4963-bb94-3c4153470361", "embedding": null, "metadata": {"page_label": "956", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c97ea8d4-f522-4bed-a453-9eb1cb8cfa85", "node_type": "4", "metadata": {"page_label": "956", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "49662ccd6a43d4a7ccaf0b33cf748e8addd02bd6e2a60720d489f53dc42b0ff5", "class_name": "RelatedNodeInfo"}}, "text": "956 Mathematics for Deep Learning\nRecalling that \ud835\udc39is itself an integral gives that the left hand side may be rewritten to\nbe\n\u00b9\ud835\udc62\u00b9\ud835\udc65\u00ba\n\ud835\udc62\u00b90\u00ba\ud835\udc53\u00b9\ud835\udc66\u00ba\ud835\udc51\ud835\udc66=\u00b9\ud835\udc65\n0\ud835\udc51\ud835\udc39\n\ud835\udc51\ud835\udc62\u00b9\ud835\udc62\u00b9\ud835\udc66\u00ba\u00ba\u0001\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc66\ud835\udc51\ud835\udc66. (A.17)\nSimilarly, recalling that \ud835\udc39is an integral allows us to recognize that\ud835\udc51\ud835\udc39\n\ud835\udc51\ud835\udc65=\ud835\udc53using the\nfundamental theorem (A.10 ), and thus we may conclude\n\u00b9\ud835\udc62\u00b9\ud835\udc65\u00ba\n\ud835\udc62\u00b90\u00ba\ud835\udc53\u00b9\ud835\udc66\u00ba\ud835\udc51\ud835\udc66=\u00b9\ud835\udc65\n0\ud835\udc53\u00b9\ud835\udc62\u00b9\ud835\udc66\u00ba\u00ba\u0001\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc66\ud835\udc51\ud835\udc66. (A.18)\nThis is the changeof variables formula.\nForamoreintuitivederivation,considerwhathappenswhenwetakeanintegralof \ud835\udc53\u00b9\ud835\udc62\u00b9\ud835\udc65\u00ba\u00ba\nbetween\ud835\udc65and\ud835\udc65\u00b8\ud835\udf16. For a small \ud835\udf16, this integral is approximately \ud835\udf16\ud835\udc53\u00b9\ud835\udc62\u00b9\ud835\udc65\u00ba\u00ba, the area of\nthe associated rectangle. Now, let\u2019s compare this with the integral of \ud835\udc53\u00b9\ud835\udc66\u00bafrom\ud835\udc62\u00b9\ud835\udc65\u00bato\n\ud835\udc62\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba. We know that \ud835\udc62\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\u0019\ud835\udc62\u00b9\ud835\udc65\u00ba\u00b8\ud835\udf16\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba, so the area of this rectangle is approx-\nimately\ud835\udf16\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba\ud835\udc53\u00b9\ud835\udc62\u00b9\ud835\udc65\u00ba\u00ba. Thus, to make the area of these two rectangles to agree, we need\nto multiply the first one by\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00baas is illustrated in Fig. A.2.\ntFig. A.2 Visualizing the transformation of a single thin rectangle under the change of variables.\nThis tells us that\n\u00b9\ud835\udc65\u00b8\ud835\udf16\n\ud835\udc65\ud835\udc53\u00b9\ud835\udc62\u00b9\ud835\udc66\u00ba\u00ba\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc66\u00b9\ud835\udc66\u00ba\ud835\udc51\ud835\udc66=\u00b9\ud835\udc62\u00b9\ud835\udc65\u00b8\ud835\udf16\u00ba\n\ud835\udc62\u00b9\ud835\udc65\u00ba\ud835\udc53\u00b9\ud835\udc66\u00ba\ud835\udc51\ud835\udc66. (A.19)\nThis is the change of variables formula expressed for a single small rectangle.\nIf\ud835\udc62\u00b9\ud835\udc65\u00baand\ud835\udc53\u00b9\ud835\udc65\u00baare properly chosen, this can allow for the computation of incredibly\ncomplex integrals. For instance, if we even chose \ud835\udc53\u00b9\ud835\udc66\u00ba=1and\ud835\udc62\u00b9\ud835\udc65\u00ba=\ud835\udc52\u0000\ud835\udc652(which means\n\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00ba=\u00002\ud835\udc65\ud835\udc52\u0000\ud835\udc652), this can show for instance that\n\ud835\udc52\u00001\u00001=\u00b9\ud835\udc52\u00001\n\ud835\udc52\u000001\ud835\udc51\ud835\udc66=\u00002\u00b91\n0\ud835\udc66\ud835\udc52\u0000\ud835\udc662\ud835\udc51\ud835\udc66, (A.20)\nand thus by rearranging that\n\u00b91\n0\ud835\udc66\ud835\udc52\u0000\ud835\udc662\ud835\udc51\ud835\udc66=1\u0000\ud835\udc52\u00001\n2. (A.21)\nA.5.4A Comment on Sign Conventions", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1484, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fadfbaf-bbe8-435e-bd03-b5ede69b4b64": {"__data__": {"id_": "3fadfbaf-bbe8-435e-bd03-b5ede69b4b64", "embedding": null, "metadata": {"page_label": "957", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec216030-44b3-4a56-8685-bb7fc93f4d2d", "node_type": "4", "metadata": {"page_label": "957", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "19b489abeae1ffe4c22c9d7a7f6c05110f8b3f113232a3351790881caad8b9ad", "class_name": "RelatedNodeInfo"}}, "text": "957 Integral Calculus\nKeen-eyed readers will observe something strange about the computations above. Namely,\ncomputations like\n\u00b9\ud835\udc52\u00001\n\ud835\udc52\u000001\ud835\udc51\ud835\udc66=\ud835\udc52\u00001\u00001<0, (A.22)\ncan produce negative numbers. When thinking about areas, it can be strange to see a neg-\native value, and so it is worth digging into what the convention is.\nMathematicians take the notion of signed areas. This manifests itself in two ways. First, if\nwe consider a function \ud835\udc53\u00b9\ud835\udc65\u00bawhich is sometimes less than zero, then the area will also be\nnegative. So for instance\n\u00b91\n0\u00b9\u00001\u00ba\ud835\udc51\ud835\udc65=\u00001. (A.23)\nSimilarly, integrals which progress from right to left, rather than left to right are also taken\nto be negative areas\n\u00b9\u00001\n01\ud835\udc51\ud835\udc65=\u00001. (A.24)\nThe standard area (from left to right of a positive function) is always positive. Anything\nobtained by flipping it (say flipping over the \ud835\udc65-axis to get the integral of a negative number,\nor flipping over the \ud835\udc66-axis to get an integral in the wrong order) will produce a negative\narea. And indeed, flipping twice will give a pair of negative signs that cancel out to have\npositive area\n\u00b9\u00001\n0\u00b9\u00001\u00ba\ud835\udc51\ud835\udc65=1. (A.25)\nIf this discussion sounds familiar, it is! In Section A.1 we discussed how the determinant\nrepresented the signed area in much the same way.\nA.5.5MultipleIntegrals\nIn some cases, we will need to work in higher dimensions. For instance, suppose that we\nhave a function of two variables, like \ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00baand we want to know the volume under \ud835\udc53\nwhen\ud835\udc65ranges over\u00bb\ud835\udc4e,\ud835\udc4f\u00bcand\ud835\udc66ranges over\u00bb\ud835\udc50,\ud835\udc51\u00bc.\n# Construct grid and compute function\nx, y =torch .meshgrid(torch .linspace( -2,2,101), torch .linspace( -2,2,101))\nz=torch .exp( -x**2-y**2)\n# Plot function\nax=d2l.plt.figure() .add_subplot( 111, projection ='3d')\nax.plot_wireframe(x, y, z)\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'y')\nd2l.plt.xticks([ -2,-1,0,1,2])\nd2l.plt.yticks([ -2,-1,0,1,2])\nd2l.set_figsize()\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1854, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a7423643-0738-46a7-8d01-4b63fa4b7bb2": {"__data__": {"id_": "a7423643-0738-46a7-8d01-4b63fa4b7bb2", "embedding": null, "metadata": {"page_label": "958", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1d2415a-60aa-4f68-9d93-aa266d0da7be", "node_type": "4", "metadata": {"page_label": "958", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "334a09935ed7b6c72ff6336bd3716ce924bb427f669e2eefa1eb6e887275da9e", "class_name": "RelatedNodeInfo"}}, "text": "958 Mathematics for Deep Learning\n(continued from previous page)\nax.set_xlim( -2,2)\nax.set_ylim( -2,2)\nax.set_zlim( 0,1)\nax.dist =12\nWe write this as\u00b9\n\u00bb\ud835\udc4e,\ud835\udc4f\u00bc\u0002\u00bb\ud835\udc50,\ud835\udc51\u00bc\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66. (A.26)\nSupposethatwewishtocomputethisintegral. Myclaimisthatwecandothisbyiteratively\ncomputing first the integral in \ud835\udc65and then shifting to the integral in \ud835\udc66, that is to say\n\u00b9\n\u00bb\ud835\udc4e,\ud835\udc4f\u00bc\u0002\u00bb\ud835\udc50,\ud835\udc51\u00bc\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66 =\u00b9\ud835\udc51\n\ud835\udc50\u0012\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc65\u0013\n\ud835\udc51\ud835\udc66. (A.27)\nLet\u2019s see why this is.\nConsiderthefigureabovewherewehavesplitthefunctioninto \ud835\udf16\u0002\ud835\udf16squareswhichwewill\nindex with integer coordinates \ud835\udc56,\ud835\udc57. In this case, our integral is approximately\n\u00d5\n\ud835\udc56,\ud835\udc57\ud835\udf162\ud835\udc53\u00b9\ud835\udf16\ud835\udc56,\ud835\udf16\ud835\udc57\u00ba.(A.28)\nOnce we discretize the problem, we may add up the values on these squares in whatever\norder we like, and not worry about changing the values. This is illustrated in Fig. A.3. In\nparticular, we can say that\n\u00d5\n\ud835\udc57\ud835\udf16 \u00d5\n\ud835\udc56\ud835\udf16\ud835\udc53\u00b9\ud835\udf16\ud835\udc56,\ud835\udf16\ud835\udc57\u00ba!\n. (A.29)\ntFig. A.3 Illustrating how to decompose a sum over many squares as a sum over \ufb01rst the columns\n(1), then adding the column sums together (2).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f38b6920-b358-482b-87d5-85c675588da0": {"__data__": {"id_": "f38b6920-b358-482b-87d5-85c675588da0", "embedding": null, "metadata": {"page_label": "959", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0669d3f-69d2-4384-af77-e2384c7f6750", "node_type": "4", "metadata": {"page_label": "959", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "523aeddd430ff923315f16cfcaf314aae9d0651b401007a7c6a5ed40ba5af64a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c0a62fd9-a516-466e-8c6d-f3a21952a78c", "node_type": "1", "metadata": {}, "hash": "56e3eaaf5ded1d75977aff6c653bec7ff81c5783079a4be9ba17c9f709a5d140", "class_name": "RelatedNodeInfo"}}, "text": "959 Integral Calculus\nThe sum on the inside is precisely the discretization of the integral\n\ud835\udc3a\u00b9\ud835\udf16\ud835\udc57\u00ba=\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc53\u00b9\ud835\udc65,\ud835\udf16\ud835\udc57\u00ba\ud835\udc51\ud835\udc65. (A.30)\nFinally, notice that if we combine these two expressions we get\n\u00d5\n\ud835\udc57\ud835\udf16\ud835\udc3a\u00b9\ud835\udf16\ud835\udc57\u00ba\u0019\u00b9\ud835\udc51\n\ud835\udc50\ud835\udc3a\u00b9\ud835\udc66\u00ba\ud835\udc51\ud835\udc66=\u00b9\n\u00bb\ud835\udc4e,\ud835\udc4f\u00bc\u0002\u00bb\ud835\udc50,\ud835\udc51\u00bc\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66. (A.31)\nThus putting it all together, we have that\n\u00b9\n\u00bb\ud835\udc4e,\ud835\udc4f\u00bc\u0002\u00bb\ud835\udc50,\ud835\udc51\u00bc\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66 =\u00b9\ud835\udc51\n\ud835\udc50\u0012\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc65\u0013\n\ud835\udc51\ud835\udc66. (A.32)\nNotice that, once discretized, all we did was rearrange the order in which we added a list\nof numbers. This may make it seem like it is nothing, however this result (called Fubini\u2019s\nTheorem ) is not always true! For the type of mathematics encountered when doing ma-\nchine learning (continuous functions), there is no concern, however it is possible to create\nexampleswhereitfails(forexamplethefunction \ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba=\ud835\udc65\ud835\udc66\u00b9\ud835\udc652\u0000\ud835\udc662\u00ba\u009d\u00b9\ud835\udc652\u00b8\ud835\udc662\u00ba3overthe\nrectangle\u00bb0,2\u00bc\u0002\u00bb 0,1\u00bc).\nNotethatthechoicetodotheintegralin \ud835\udc65first, andthentheintegralin \ud835\udc66wasarbitrary. We\ncould have equally well chosen to do \ud835\udc66first and then \ud835\udc65to see\n\u00b9\n\u00bb\ud835\udc4e,\ud835\udc4f\u00bc\u0002\u00bb\ud835\udc50,\ud835\udc51\u00bc\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66 =\u00b9\ud835\udc4f\n\ud835\udc4e\u0012\u00b9\ud835\udc51\n\ud835\udc50\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc66\u0013\n\ud835\udc51\ud835\udc65. (A.33)\nOften times, we will condense down to vector notation, and say that for \ud835\udc48=\u00bb\ud835\udc4e,\ud835\udc4f\u00bc\u0002\u00bb\ud835\udc50,\ud835\udc51\u00bc\nthis is\u00b9\n\ud835\udc48\ud835\udc53\u00b9x\u00ba\ud835\udc51x. (A.34)\nA.5.6Changeof Variablesin Multiple Integrals\nAs with single variables in (A.18 ), the ability to change variables inside a higher dimen-\nsional integral is a key tool. Let\u2019s summarize the result without derivation.\nWe need a function that reparametrizes our domain of integration. We can take this to be\n\ud835\udf19:R\ud835\udc5b!R\ud835\udc5b, that is any function which takes in \ud835\udc5breal variables and returns another \ud835\udc5b. To\nkeep the expressions clean, we will assume that \ud835\udf19isinjective which is to say it never folds\nover itself (\ud835\udf19\u00b9x\u00ba=\ud835\udf19\u00b9y\u00ba=)x=y).\nIn this case, we can say that\n\u00b9\n\ud835\udf19\u00b9\ud835\udc48\u00ba\ud835\udc53\u00b9x\u00ba\ud835\udc51x=\u00b9\n\ud835\udc48\ud835\udc53\u00b9\ud835\udf19\u00b9x\u00ba\u00bajdet\u00b9\ud835\udc37\ud835\udf19\u00b9x\u00ba\u00baj\ud835\udc51x.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1688, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0a62fd9-a516-466e-8c6d-f3a21952a78c": {"__data__": {"id_": "c0a62fd9-a516-466e-8c6d-f3a21952a78c", "embedding": null, "metadata": {"page_label": "959", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0669d3f-69d2-4384-af77-e2384c7f6750", "node_type": "4", "metadata": {"page_label": "959", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "523aeddd430ff923315f16cfcaf314aae9d0651b401007a7c6a5ed40ba5af64a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f38b6920-b358-482b-87d5-85c675588da0", "node_type": "1", "metadata": {"page_label": "959", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ed6870a6c8ab937d06e88160ec46055c2dd507d6d12714772c3ca86dda398153", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s summarize the result without derivation.\nWe need a function that reparametrizes our domain of integration. We can take this to be\n\ud835\udf19:R\ud835\udc5b!R\ud835\udc5b, that is any function which takes in \ud835\udc5breal variables and returns another \ud835\udc5b. To\nkeep the expressions clean, we will assume that \ud835\udf19isinjective which is to say it never folds\nover itself (\ud835\udf19\u00b9x\u00ba=\ud835\udf19\u00b9y\u00ba=)x=y).\nIn this case, we can say that\n\u00b9\n\ud835\udf19\u00b9\ud835\udc48\u00ba\ud835\udc53\u00b9x\u00ba\ud835\udc51x=\u00b9\n\ud835\udc48\ud835\udc53\u00b9\ud835\udf19\u00b9x\u00ba\u00bajdet\u00b9\ud835\udc37\ud835\udf19\u00b9x\u00ba\u00baj\ud835\udc51x. (A.35)\nwhere\ud835\udc37\ud835\udf19istheJacobian of\ud835\udf19,whichisthematrixofpartialderivativesof \ud835\udf53=\u00b9\ud835\udf191\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc5b\u00ba,...,\ud835\udf19\ud835\udc5b\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc5b\u00ba\u00ba,\n\ud835\udc37\ud835\udf53=26666664\ud835\udf15\ud835\udf19 1\n\ud835\udf15\ud835\udc651\u0001\u0001\u0001\ud835\udf15\ud835\udf19 1\n\ud835\udf15\ud835\udc65\ud835\udc5b.........\n\ud835\udf15\ud835\udf19\ud835\udc5b\n\ud835\udf15\ud835\udc651\u0001\u0001\u0001\ud835\udf15\ud835\udf19\ud835\udc5b\n\ud835\udf15\ud835\udc65\ud835\udc5b37777775. (A.36)", "mimetype": "text/plain", "start_char_idx": 1275, "end_char_idx": 1870, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4b870d2-3473-4d64-af6d-4c7fce205f6f": {"__data__": {"id_": "e4b870d2-3473-4d64-af6d-4c7fce205f6f", "embedding": null, "metadata": {"page_label": "960", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "71ef1115-d0e5-44e1-a28c-a8a65590dc07", "node_type": "4", "metadata": {"page_label": "960", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1951ecaec9484876443ba333cc594979aebd70e8c4ce6c1c2cd13bb986b71ed1", "class_name": "RelatedNodeInfo"}}, "text": "960 Mathematics for Deep Learning\nLooking closely, we see that this is similar to the single variable chain rule (A.18 ), except\nwe have replaced the term\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00bawithjdet\u00b9\ud835\udc37\ud835\udf19\u00b9x\u00ba\u00baj. Let\u2019s see how we can to interpret\nthis term. Recall that the\ud835\udc51\ud835\udc62\n\ud835\udc51\ud835\udc65\u00b9\ud835\udc65\u00baterm existed to say how much we stretched our \ud835\udc65-axis by\napplying\ud835\udc62. The same process in higher dimensions is to determine how much we stretch\nthearea(orvolume,orhyper-volume)ofalittlesquare(orlittle hyper-cube )byapplying \ud835\udf53.\nIf\ud835\udf53was the multiplication by a matrix, then we know how the determinant already gives\nthe answer.\nWith some work, one can show that the Jacobian provides the best approximation to a\nmultivariable function \ud835\udf53at a point by a matrix in the same way we could approximate by\nlinesorplaneswithderivativesandgradients. ThusthedeterminantoftheJacobianexactly\nmirrors the scaling factor we identified in one dimension.\nIt takes some work to fill in the details to this, so do not worry if they are not clear now.\nLet\u2019s see at least one example we will make use of later on. Consider the integral\n\u00b91\n\u00001\u00b91\n\u00001\ud835\udc52\u0000\ud835\udc652\u0000\ud835\udc662\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66. (A.37)\nPlaying with this integral directly will get us no-where, but if we change variables, we can\nmake significant progress. If we let \ud835\udf53\u00b9\ud835\udc5f,\ud835\udf03\u00ba=\u00b9\ud835\udc5fcos\u00b9\ud835\udf03\u00ba,\ud835\udc5fsin\u00b9\ud835\udf03\u00ba\u00ba(which is to say that\n\ud835\udc65=\ud835\udc5fcos\u00b9\ud835\udf03\u00ba,\ud835\udc66=\ud835\udc5fsin\u00b9\ud835\udf03\u00ba), then we can apply the change of variable formula to see that\nthis is the same thing as\n\u00b91\n0\u00b92\ud835\udf0b\n0\ud835\udc52\u0000\ud835\udc5f2jdet\u00b9\ud835\udc37\u0152\u00b9x\u00ba\u00baj\ud835\udc51\ud835\udf03 \ud835\udc51\ud835\udc5f, (A.38)\nwhere\njdet\u00b9\ud835\udc37\u0152\u00b9x\u00ba\u00baj=\f\f\f\fdet\u0014cos\u00b9\ud835\udf03\u00ba \u0000\ud835\udc5fsin\u00b9\ud835\udf03\u00ba\nsin\u00b9\ud835\udf03\u00ba\ud835\udc5fcos\u00b9\ud835\udf03\u00ba\u0015\f\f\f\f=\ud835\udc5f\u00b9cos2\u00b9\ud835\udf03\u00ba\u00b8sin2\u00b9\ud835\udf03\u00ba\u00ba=\ud835\udc5f. (A.39)\nThus, the integral is\n\u00b91\n0\u00b92\ud835\udf0b\n0\ud835\udc5f\ud835\udc52\u0000\ud835\udc5f2\ud835\udc51\ud835\udf03 \ud835\udc51\ud835\udc5f =2\ud835\udf0b\u00b91\n0\ud835\udc5f\ud835\udc52\u0000\ud835\udc5f2\ud835\udc51\ud835\udc5f=\ud835\udf0b, (A.40)\nwhere the final equality follows by the same computation that we used in section Section\nA.5.3.\nWe will meet this integral again when we study continuous random variables in Section\nA.6.\nA.5.7Summary\n\u000fThe theory of integration allows us to answer questions about areas or volumes.\n\u000fThe fundamental theorem of calculus allows us to leverage knowledge about derivatives\nto compute areas via the observation that the derivative of the area up to some point\nis given by the value of the function being integrated.\n\u000fIntegrals in higher dimensions can be computed by iterating single variable integrals.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2168, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "632b18f6-6fce-457a-b513-a9a6ac1c8f7d": {"__data__": {"id_": "632b18f6-6fce-457a-b513-a9a6ac1c8f7d", "embedding": null, "metadata": {"page_label": "961", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "225e3686-c73a-4a0c-8918-a60289e2da04", "node_type": "4", "metadata": {"page_label": "961", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f985bcb1090ae974b38aaa56c476f8b39efed59cebd6238458b1e1e3e9396c20", "class_name": "RelatedNodeInfo"}}, "text": "961 Random Variables\n284A.5.8Exercises\n1.What is\u00af2\n11\n\ud835\udc65\ud835\udc51\ud835\udc65?\n2.Use the change of variables formula to integrate\u00afp\ud835\udf0b\n0\ud835\udc65sin\u00b9\ud835\udc652\u00ba\ud835\udc51\ud835\udc65.\n3.What is\u00af\n\u00bb0,1\u00bc2\ud835\udc65\ud835\udc66 \ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66 ?\n4.Use the change of variables formula to compute\u00af2\n0\u00af1\n0\ud835\udc65\ud835\udc66\u00b9\ud835\udc652\u0000\ud835\udc662\u00ba\u009d\u00b9\ud835\udc652\u00b8\ud835\udc662\u00ba3\ud835\udc51\ud835\udc66 \ud835\udc51\ud835\udc65\nand\u00af1\n0\u00af2\n0\ud835\udc53\u00b9\ud835\udc65,\ud835\udc66\u00ba=\ud835\udc65\ud835\udc66\u00b9\ud835\udc652\u0000\ud835\udc662\u00ba\u009d\u00b9\ud835\udc652\u00b8\ud835\udc662\u00ba3\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66to see they are different.\nDiscussions284.\nA.6Random Variables\nInSection 2.6 we saw the basics of how to work with discrete random variables, which in\nour case refer to those random variables which take either a finite set of possible values, or\nthe integers. In this section, we develop the theory of continuousrandomvariables , which\nare random variables which can take on any real value.\nA.6.1Continuous Random Variables\nContinuous random variables are a significantly more subtle topic than discrete random\nvariables. A fair analogy to make is that the technical jump is comparable to the jump\nbetween adding lists of numbers and integrating functions. As such, we will need to take\nsome time to develop the theory.\nFromDiscreteto Continuous\nTo understand the additional technical challenges encountered when working with contin-\nuous random variables, let\u2019s perform a thought experiment. Suppose that we are throwing\na dart at the dart board, and we want to know the probability that it hits exactly 2cm from\nthe center of the board.\nTo start with, we imagine measuring a single digit of accuracy, that is to say with bins for\n0cm,1cm,2cm, and so on. We throw say 100darts at the dart board, and if 20of them fall\ninto the bin for 2cm we conclude that 20%of the darts we throw hit the board 2cm away\nfrom the center.\nHowever,whenwelookcloser,thisdoesnotmatchourquestion! Wewantedexactequality,\nwhereas these bins hold all that fell between say 1.5cm and 2.5cm.\nUndeterred, we continue further. We measure even more precisely, say 1.9cm, 2.0cm,\n2.1cm, and now see that perhaps 3of the 100darts hit the board in the 2.0cm bucket. Thus\nwe conclude the probability is 3%.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "582f7517-d580-440f-a65e-459cfc99a4d1": {"__data__": {"id_": "582f7517-d580-440f-a65e-459cfc99a4d1", "embedding": null, "metadata": {"page_label": "962", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df079df9-43ca-4eb6-8a5f-ee9118bda107", "node_type": "4", "metadata": {"page_label": "962", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "61008047f86df26c369e0e7198b912fbf760238f705407011347a049c8e7ca83", "class_name": "RelatedNodeInfo"}}, "text": "962 Mathematics for Deep Learning\nHowever,thisdoesnotsolveanything! Wehavejustpushedtheissuedownonedigitfurther.\nLet\u2019s abstract a bit. Imagine we know the probability that the first \ud835\udc58digits match with\n2.00000...and we want to know the probability it matches for the first \ud835\udc58\u00b81digits. It is\nfairly reasonable to assume that the \ud835\udc58\u00b81thdigit is essentially a random choice from the\nsetf0,1,2,..., 9g. At least, we cannot conceive of a physically meaningful process which\nwould force the number of micrometers away form the center to prefer to end in a 7vs a\n3.\nWhat this means is that in essence each additional digit of accuracy we require should\ndecrease probability of matching by a factor of 10. Or put another way, we would expect\nthat\n\ud835\udc43\u00b9distance is 2.00...,to\ud835\udc58digits\u00ba\u0019\ud835\udc5d\u000110\u0000\ud835\udc58. (A.1)\nThevalue\ud835\udc5dessentiallyencodeswhathappenswiththefirstfewdigits,andthe 10\u0000\ud835\udc58handles\nthe rest.\nNotice that if we know the position accurate to \ud835\udc58=4digits after the decimal, that means\nwe know the value falls within the interval say \u00bb1.99995,2.00005\u00bcwhich is an interval of\nlength 2.00005\u00001.99995 =10\u00004. Thus, if we call the length of this interval \ud835\udf16, we can\nsay\n\ud835\udc43\u00b9distance is in an \ud835\udf16-sized interval around 2\u00ba\u0019\ud835\udf16\u0001\ud835\udc5d. (A.2)\nLet\u2019s take this one final step further. We have been thinking about the point 2the entire\ntime, but never thinking about other points. Nothing is different there fundamentally, but\nit is the case that the value \ud835\udc5dwill likely be different. We would at least hope that a dart\nthrower was more likely to hit a point near the center, like 2cm rather than 20cm. Thus, the\nvalue\ud835\udc5dis not fixed, but rather should depend on the point \ud835\udc65. This tells us that we should\nexpect\n\ud835\udc43\u00b9distance is in an \ud835\udf16-sized interval around \ud835\udc65\u00ba\u0019\ud835\udf16\u0001\ud835\udc5d\u00b9\ud835\udc65\u00ba. (A.3)\nIndeed, (A.3)preciselydefinesthe probabilitydensityfunction . Itisafunction \ud835\udc5d\u00b9\ud835\udc65\u00bawhich\nencodes the relative probability of hitting near one point vs. another. Let\u2019s visualize what\nsuch a function might look like.\n%matplotlib inline\nimport torch\nfrom IPython import display\nfrom d2l import torch asd2l\ntorch .pi=torch .acos(torch .zeros( 1)).item() *2# Define pi in torch\n# Plot the probability density function for some random variable\nx=torch .arange( -5,5,0.01 )\np=0.2*torch .exp( -(x-3)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi)) +\\\n0.8*torch .exp( -(x+1)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi))\nd2l.plot(x, p, 'x','Density ')", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2359, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6d6af9f-05d8-4977-9194-4dedc4078784": {"__data__": {"id_": "a6d6af9f-05d8-4977-9194-4dedc4078784", "embedding": null, "metadata": {"page_label": "963", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0ae27a9-c425-4183-ada0-09aea59be0ba", "node_type": "4", "metadata": {"page_label": "963", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b4a091700e8121139cc016eaff89d9521daa08d83218c0f13bdebb23411123af", "class_name": "RelatedNodeInfo"}}, "text": "963 Random Variables\nThe locations where the function value is large indicates regions where we are more likely\nto find the random value. The low portions are areas where we are unlikely to find the\nrandom value.\nProbabilityDensity Functions\nLet\u2019snowinvestigatethisfurther. Wehavealreadyseenwhataprobabilitydensityfunction\nis intuitively for a random variable \ud835\udc4b, namely the density function is a function \ud835\udc5d\u00b9\ud835\udc65\u00baso\nthat\n\ud835\udc43\u00b9\ud835\udc4bis in an\ud835\udf16-sized interval around \ud835\udc65\u00ba\u0019\ud835\udf16\u0001\ud835\udc5d\u00b9\ud835\udc65\u00ba. (A.4)\nBut what does this imply for the properties of \ud835\udc5d\u00b9\ud835\udc65\u00ba?\nFirst, probabilities are never negative, thus we should expect that \ud835\udc5d\u00b9\ud835\udc65\u00ba\u00150as well.\nSecond, let\u2019s imagine that we slice up the Rinto an infinite number of slices which are \ud835\udf16\nwide,saywithslices \u00b9\ud835\udf16\u0001\ud835\udc56,\ud835\udf16\u0001\u00b9\ud835\udc56\u00b81\u00ba\u00bc. Foreachofthese,weknowfrom (A.4)theprobability\nis approximately\n\ud835\udc43\u00b9\ud835\udc4bis in an\ud835\udf16-sized interval around \ud835\udc65\u00ba\u0019\ud835\udf16\u0001\ud835\udc5d\u00b9\ud835\udf16\u0001\ud835\udc56\u00ba, (A.5)\nso summed over all of them it should be\n\ud835\udc43\u00b9\ud835\udc4b2R\u00ba\u0019\u00d5\n\ud835\udc56\ud835\udf16\u0001\ud835\udc5d\u00b9\ud835\udf16\u0001\ud835\udc56\u00ba.(A.6)\nThis is nothing more than the approximation of an integral discussed in Section A.5 , thus\nwe can say that\n\ud835\udc43\u00b9\ud835\udc4b2R\u00ba=\u00b91\n\u00001\ud835\udc5d\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65. (A.7)\nWe know that \ud835\udc43\u00b9\ud835\udc4b2R\u00ba=1, since the random variable must take on somenumber, we\ncan conclude that for any density\n\u00b91\n\u00001\ud835\udc5d\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65=1. (A.8)\nIndeed, digging into this further shows that for any \ud835\udc4e, and\ud835\udc4f, we see that\n\ud835\udc43\u00b9\ud835\udc4b2\u00b9\ud835\udc4e,\ud835\udc4f\u00bc\u00ba=\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc5d\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65. (A.9)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d308549a-cf83-4b95-90a4-a8300017f518": {"__data__": {"id_": "d308549a-cf83-4b95-90a4-a8300017f518", "embedding": null, "metadata": {"page_label": "964", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a0651f0e-0784-437d-ae8c-c7bec54e1752", "node_type": "4", "metadata": {"page_label": "964", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c2f0d3001363d1b09da2d7896b95563bad511c637f27140dfe237dac5921339d", "class_name": "RelatedNodeInfo"}}, "text": "964 Mathematics for Deep Learning\nWe may approximate this in code by using the same discrete approximation methods as\nbefore. Inthiscasewecanapproximatetheprobabilityoffallingintheblueregion.\n# Approximate probability using numerical integration\nepsilon =0.01\nx=torch .arange( -5,5,0.01 )\np=0.2*torch .exp( -(x-3)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi)) +\\\n0.8*torch .exp( -(x+1)**2/2)/torch .sqrt( 2*torch .tensor(torch .pi))\nd2l.set_figsize()\nd2l.plt.plot(x, p, color ='black ')\nd2l.plt.fill_between(x .tolist()[ 300:800], p .tolist()[ 300:800])\nd2l.plt.show()\nf'approximate Probability: {torch .sum(epsilon *p[300:800])}'\n'approximate Probability: 0.773617148399353 '\nIt turns out that these two properties describe exactly the space of possible probability\ndensity functions (or p.d.f.\u2019s for the commonly encountered abbreviation). They are non-\nnegative functions \ud835\udc5d\u00b9\ud835\udc65\u00ba\u00150such that\n\u00b91\n\u00001\ud835\udc5d\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65=1. (A.10)\nWeinterpretthisfunctionbyusingintegrationtoobtaintheprobabilityourrandomvariable\nis in a specific interval:\n\ud835\udc43\u00b9\ud835\udc4b2\u00b9\ud835\udc4e,\ud835\udc4f\u00bc\u00ba=\u00b9\ud835\udc4f\n\ud835\udc4e\ud835\udc5d\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65. (A.11)\nInSection A.8 we will see a number of common distributions, but let\u2019s continue working\nin the abstract.\nCumulativeDistribution Functions\nIn the previous section, we saw the notion of the p.d.f. In practice, this is a commonly en-\ncounteredmethodtodiscusscontinuousrandomvariables,butithasonesignificantpitfall:\nthat the values of the p.d.f. are not themselves probabilities, but rather a function that we\nmust integrate to yield probabilities. There is nothing wrong with a density being larger", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ef087df-59fc-48a4-bd99-73dfaed60dad": {"__data__": {"id_": "8ef087df-59fc-48a4-bd99-73dfaed60dad", "embedding": null, "metadata": {"page_label": "965", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b52f1475-6b60-455a-86f1-a3d49ec46177", "node_type": "4", "metadata": {"page_label": "965", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5bb4ab51ee40afc9e4820d81432ffbc427d792ee53aebdad41dcf05e655617cc", "class_name": "RelatedNodeInfo"}}, "text": "965 Random Variables\nthan 10, as long as it is not larger than 10for more than an interval of length 1\u009d10. This\ncan be counter-intuitive, so people often also think in terms of the cumulative distribution\nfunction, or c.d.f., which isa probability.\nIn particular, by using (A.11 ), we define the c.d.f. for a random variable \ud835\udc4bwith density\n\ud835\udc5d\u00b9\ud835\udc65\u00baby\n\ud835\udc39\u00b9\ud835\udc65\u00ba=\u00b9\ud835\udc65\n\u00001\ud835\udc5d\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65=\ud835\udc43\u00b9\ud835\udc4b\u0014\ud835\udc65\u00ba. (A.12)\nLet\u2019s observe a few properties.\n\u000f\ud835\udc39\u00b9\ud835\udc65\u00ba! 0as\ud835\udc65!\u00001.\n\u000f\ud835\udc39\u00b9\ud835\udc65\u00ba! 1as\ud835\udc65!1.\n\u000f\ud835\udc39\u00b9\ud835\udc65\u00bais non-decreasing ( \ud835\udc66 >\ud835\udc65 =)\ud835\udc39\u00b9\ud835\udc66\u00ba\u0015\ud835\udc39\u00b9\ud835\udc65\u00ba).\n\u000f\ud835\udc39\u00b9\ud835\udc65\u00bais continuous (has no jumps) if \ud835\udc4bis a continuous random variable.\nWith the fourth bullet point, note that this would not be true if \ud835\udc4bwere discrete, say taking\nthe values 0and1both with probability 1\u009d2. In that case\n\ud835\udc39\u00b9\ud835\udc65\u00ba=8>>> <\n>>>:0\ud835\udc65 <0,\n1\n2\ud835\udc65 <1,\n1\ud835\udc65\u00151.(A.13)\nIn this example, we see one of the benefits of working with the c.d.f., the ability to deal\nwith continuous or discrete random variables in the same framework, or indeed mixtures\nof the two (flip a coin: if heads return the roll of a die, if tails return the distance of a dart\nthrow from the center of a dart board).\nMeans\nSuppose that we are dealing with a random variables \ud835\udc4b. The distribution itself can be hard\nto interpret. It is often useful to be able to summarize the behavior of a random variable\nconcisely. Numbers that help us capture the behavior of a random variable are called sum-\nmarystatistics . The most commonly encountered ones are the mean, thevariance , and the\nstandarddeviation .\nThemeanencodes the average value of a random variable. If we have a discrete random\nvariable\ud835\udc4b, which takes the values \ud835\udc65\ud835\udc56with probabilities \ud835\udc5d\ud835\udc56, then the mean is given by the\nweighted average: sum the values times the probability that the random variable takes on\nthat value:\n\ud835\udf07\ud835\udc4b=\ud835\udc38\u00bb\ud835\udc4b\u00bc=\u00d5\n\ud835\udc56\ud835\udc65\ud835\udc56\ud835\udc5d\ud835\udc56.(A.14)\nThe way we should interpret the mean (albeit with caution) is that it tells us essentially\nwhere the random variable tends to be located.\nAs a minimalistic example that we will examine throughout this section, let\u2019s take \ud835\udc4bto be", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1977, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "27ce6b65-4acb-4795-befb-74a42148a334": {"__data__": {"id_": "27ce6b65-4acb-4795-befb-74a42148a334", "embedding": null, "metadata": {"page_label": "966", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bba584d-9758-4f6e-a11c-5595973d85b3", "node_type": "4", "metadata": {"page_label": "966", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5e505a5f18abfc99335f50b6f4cb56494b81fc87a5ae8a54c66451a3ce930760", "class_name": "RelatedNodeInfo"}}, "text": "966 Mathematics for Deep Learning\ntherandomvariablewhichtakesthevalue \ud835\udc4e\u00002withprobability \ud835\udc5d,\ud835\udc4e\u00b82withprobability \ud835\udc5d\nand\ud835\udc4ewith probability 1\u00002\ud835\udc5d. We can compute using (A.14 )that, for any possible choice\nof\ud835\udc4eand\ud835\udc5d, the mean is\n\ud835\udf07\ud835\udc4b=\ud835\udc38\u00bb\ud835\udc4b\u00bc=\u00d5\n\ud835\udc56\ud835\udc65\ud835\udc56\ud835\udc5d\ud835\udc56=\u00b9\ud835\udc4e\u00002\u00ba\ud835\udc5d\u00b8\ud835\udc4e\u00b91\u00002\ud835\udc5d\u00ba\u00b8\u00b9\ud835\udc4e\u00b82\u00ba\ud835\udc5d=\ud835\udc4e.(A.15)\nThus we see that the mean is \ud835\udc4e. This matches the intuition since \ud835\udc4eis the location around\nwhich we centered our random variable.\nBecause they are helpful, let\u2019s summarize a few properties.\n\u000fFor any random variable \ud835\udc4band numbers \ud835\udc4eand\ud835\udc4f, we have that \ud835\udf07\ud835\udc4e\ud835\udc4b\u00b8\ud835\udc4f=\ud835\udc4e\ud835\udf07\ud835\udc4b\u00b8\ud835\udc4f.\n\u000fIf we have two random variables \ud835\udc4band\ud835\udc4c, we have\ud835\udf07\ud835\udc4b\u00b8\ud835\udc4c=\ud835\udf07\ud835\udc4b\u00b8\ud835\udf07\ud835\udc4c.\nMeansareusefulforunderstandingtheaveragebehaviorofarandomvariable,howeverthe\nmeanisnotsufficienttoevenhaveafullintuitiveunderstanding. Makingaprofitof $10\u0006$1\nper sale is very different from making $10\u0006$15per sale despite having the same average\nvalue. The second one has a much larger degree of fluctuation, and thus represents a much\nlargerrisk. Thus,tounderstandthebehaviorofarandomvariable,wewillneedatminimum\none more measure: some measure of how widely a random variable fluctuates.\nVariances\nThis leads us to consider the variance of a random variable. This is a quantitative measure\nof how far a random variable deviates from the mean. Consider the expression \ud835\udc4b\u0000\ud835\udf07\ud835\udc4b.\nThis is the deviation of the random variable from its mean. This value can be positive\nor negative, so we need to do something to make it positive so that we are measuring the\nmagnitude of the deviation.\nA reasonable thing to try is to look at j\ud835\udc4b\u0000\ud835\udf07\ud835\udc4bj, and indeed this leads to a useful quan-\ntity called the mean absolute deviation , however due to connections with other areas of\nmathematics and statistics, people often use a different solution.\nInparticular,theylookat \u00b9\ud835\udc4b\u0000\ud835\udf07\ud835\udc4b\u00ba2.Ifwelookatthetypicalsizeofthisquantitybytaking\nthe mean, we arrive at the variance\n\ud835\udf0e2\n\ud835\udc4b=Var\u00b9\ud835\udc4b\u00ba=\ud835\udc38\u0002\n\u00b9\ud835\udc4b\u0000\ud835\udf07\ud835\udc4b\u00ba2\u0003\n=\ud835\udc38\u00bb\ud835\udc4b2\u00bc\u0000\ud835\udf072\n\ud835\udc4b. (A.16)\nThelastequalityin (A.16 )holdsbyexpandingoutthedefinitioninthemiddle,andapplying\nthe properties of expectation.\nLet\u2019s look at our example where \ud835\udc4bis the random variable which takes the value \ud835\udc4e\u00002with\nprobability\ud835\udc5d,\ud835\udc4e\u00b82with probability \ud835\udc5dand\ud835\udc4ewith probability 1\u00002\ud835\udc5d. In this case \ud835\udf07\ud835\udc4b=\ud835\udc4e,\nso all we need to compute is \ud835\udc38\u0002\n\ud835\udc4b2\u0003\n. This can readily be done:\n\ud835\udc38\u0002\n\ud835\udc4b2\u0003\n=\u00b9\ud835\udc4e\u00002\u00ba2\ud835\udc5d\u00b8\ud835\udc4e2\u00b91\u00002\ud835\udc5d\u00ba\u00b8\u00b9\ud835\udc4e\u00b82\u00ba2\ud835\udc5d=\ud835\udc4e2\u00b88\ud835\udc5d. (A.17)\nThus, we see that by (A.16 )our variance is\n\ud835\udf0e2\n\ud835\udc4b=Var\u00b9\ud835\udc4b\u00ba=\ud835\udc38\u00bb\ud835\udc4b2\u00bc\u0000\ud835\udf072\n\ud835\udc4b=\ud835\udc4e2\u00b88\ud835\udc5d\u0000\ud835\udc4e2=8\ud835\udc5d. (A.18)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "197c227d-d7a9-463f-96a6-87925b1079b7": {"__data__": {"id_": "197c227d-d7a9-463f-96a6-87925b1079b7", "embedding": null, "metadata": {"page_label": "967", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7cf99aae-4b90-4823-a1af-72e4dd60279d", "node_type": "4", "metadata": {"page_label": "967", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "78748dd70cbc145530fc414e48a628d14d0dd4d76d2d5175658e6bbb31cfe300", "class_name": "RelatedNodeInfo"}}, "text": "967 Random Variables\nThis result again makes sense. The largest \ud835\udc5dcan be is 1\u009d2which corresponds to picking\n\ud835\udc4e\u00002or\ud835\udc4e\u00b82with a coin flip. The variance of this being 4corresponds to the fact that\nboth\ud835\udc4e\u00002and\ud835\udc4e\u00b82are2units away from the mean, and 22=4. On the other end of the\nspectrum, if \ud835\udc5d=0, this random variable always takes the value 0and so it has no variance\nat all.\nWe will list a few properties of variance below:\n\u000fFor any random variable \ud835\udc4b, Var\u00b9\ud835\udc4b\u00ba\u00150, with Var\u00b9\ud835\udc4b\u00ba=0if and only if \ud835\udc4bis a constant.\n\u000fForanyrandomvariable \ud835\udc4bandnumbers \ud835\udc4eand\ud835\udc4f,wehavethatVar\u00b9\ud835\udc4e\ud835\udc4b\u00b8\ud835\udc4f\u00ba=\ud835\udc4e2Var\u00b9\ud835\udc4b\u00ba.\n\u000fIf we have two independent random variables \ud835\udc4band\ud835\udc4c, we have Var\u00b9\ud835\udc4b\u00b8\ud835\udc4c\u00ba=Var\u00b9\ud835\udc4b\u00ba\u00b8\nVar\u00b9\ud835\udc4c\u00ba.\nWhen interpreting these values, there can be a bit of a hiccup. In particular, let\u2019s try imag-\niningwhathappensifwekeeptrackofunitsthroughthiscomputation. Supposethatweare\nworkingwiththestarratingassignedtoaproductonthewebpage. Then \ud835\udc4e,\ud835\udc4e\u00002, and\ud835\udc4e\u00b82\nare all measured in units of stars. Similarly, the mean \ud835\udf07\ud835\udc4bis then also measured in stars\n(being a weighted average). However, if we get to the variance, we immediately encounter\nan issue, which is we want to look at \u00b9\ud835\udc4b\u0000\ud835\udf07\ud835\udc4b\u00ba2, which is in units of squared stars . This\nmeans that the variance itself is not comparable to the original measurements. To make it\ninterpretable, we will need to return to our original units.\nStandardDeviations\nThissummarystatisticscanalwaysbededucedfromthevariancebytakingthesquareroot!\nThus we define the standarddeviation to be\n\ud835\udf0e\ud835\udc4b=p\nVar\u00b9\ud835\udc4b\u00ba. (A.19)\nIn our example, this means we now have the standard deviation is \ud835\udf0e\ud835\udc4b=2p\n2\ud835\udc5d. If we are\ndealing with units of stars for our review example, \ud835\udf0e\ud835\udc4bis again in units of stars.\nThe properties we had for the variance can be restated for the standard deviation.\n\u000fFor any random variable \ud835\udc4b,\ud835\udf0e\ud835\udc4b\u00150.\n\u000fFor any random variable \ud835\udc4band numbers \ud835\udc4eand\ud835\udc4f, we have that \ud835\udf0e\ud835\udc4e\ud835\udc4b\u00b8\ud835\udc4f=j\ud835\udc4ej\ud835\udf0e\ud835\udc4b\n\u000fIf we have two independent random variables \ud835\udc4band\ud835\udc4c, we have\ud835\udf0e\ud835\udc4b\u00b8\ud835\udc4c=q\n\ud835\udf0e2\n\ud835\udc4b\u00b8\ud835\udf0e2\n\ud835\udc4c.\nIt is natural at this moment to ask, \u201cIf the standard deviation is in the units of our original\nrandom variable, does it represent something we can draw with regards to that random\nvariable?\u201d The answer is a resounding yes! Indeed much like the mean told us the typical\nlocation of our random variable, the standard deviation gives the typical range of variation\nof that random variable. We can make this rigorous with what is known as Chebyshev\u2019s\ninequality:\n\ud835\udc43\u00b9\ud835\udc4b\u2209\u00bb\ud835\udf07\ud835\udc4b\u0000\ud835\udefc\ud835\udf0e\ud835\udc4b,\ud835\udf07\ud835\udc4b\u00b8\ud835\udefc\ud835\udf0e\ud835\udc4b\u00bc\u00ba\u00141\n\ud835\udefc2. (A.20)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dbd7f710-f45a-4cff-931e-b6e8407dd47a": {"__data__": {"id_": "dbd7f710-f45a-4cff-931e-b6e8407dd47a", "embedding": null, "metadata": {"page_label": "968", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d67dcd29-2774-498e-95d5-b544c108feae", "node_type": "4", "metadata": {"page_label": "968", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6da374008bcde4c3e0453a4af28f31fd687d58daaa5d9e55200dc3b47e073bb0", "class_name": "RelatedNodeInfo"}}, "text": "968 Mathematics for Deep Learning\nOr to state it verbally in the case of \ud835\udefc=10,99%of the samples from any random variable\nfall within 10standard deviations of the mean. This gives an immediate interpretation to\nour standard summary statistics.\nTo see how this statement is rather subtle, let\u2019s take a look at our running example again\nwhere\ud835\udc4bis the random variable which takes the value \ud835\udc4e\u00002with probability \ud835\udc5d,\ud835\udc4e\u00b82with\nprobability\ud835\udc5dand\ud835\udc4ewith probability 1\u00002\ud835\udc5d. We saw that the mean was \ud835\udc4eand the standard\ndeviation was 2p\n2\ud835\udc5d. This means, if we take Chebyshev\u2019s inequality (A.20 )with\ud835\udefc=2,\nwe see that the expression is\n\ud835\udc43\u0010\n\ud835\udc4b\u2209\u00bb\ud835\udc4e\u00004p\n2\ud835\udc5d,\ud835\udc4e\u00b84p\n2\ud835\udc5d\u00bc\u0011\n\u00141\n4. (A.21)\nThis means that 75%of the time, this random variable will fall within this interval for any\nvalue of\ud835\udc5d. Now, notice that as \ud835\udc5d!0, this interval also converges to the single point \ud835\udc4e.\nButweknowthatourrandomvariabletakesthevalues \ud835\udc4e\u00002,\ud835\udc4e,and\ud835\udc4e\u00b82onlysoeventually\nwe can be certain \ud835\udc4e\u00002and\ud835\udc4e\u00b82will fall outside the interval! The question is, at what \ud835\udc5d\ndoes that happen. So we want to solve: for what \ud835\udc5ddoes\ud835\udc4e\u00b84p\n2\ud835\udc5d=\ud835\udc4e\u00b82, which is solved\nwhen\ud835\udc5d=1\u009d8,whichis exactlythefirst\ud835\udc5dwhereitcouldpossiblyhappenwithoutviolating\nour claim that no more than 1\u009d4of samples from the distribution would fall outside the\ninterval ( 1\u009d8to the left, and 1\u009d8to the right).\nLet\u2019svisualizethis. Wewillshowtheprobabilityofgettingthethreevaluesasthreevertical\nbars with height proportional to the probability. The interval will be drawn as a horizontal\nline in the middle. The first plot shows what happens for \ud835\udc5d > 1\u009d8where the interval safely\ncontains all points.\n# Define a helper to plot these figures\ndef plot_chebyshev (a, p):\nd2l.set_figsize()\nd2l.plt.stem([a -2, a, a +2], [p, 1-2*p, p], use_line_collection =True )\nd2l.plt.xlim([ -4,4])\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.hlines( 0.5, a -4*torch .sqrt( 2*p),\na+4*torch .sqrt( 2*p), 'black ', lw =4)\nd2l.plt.vlines(a -4*torch .sqrt( 2*p), 0.53 ,0.47 ,'black ', lw =1)\nd2l.plt.vlines(a +4*torch .sqrt( 2*p), 0.53 ,0.47 ,'black ', lw =1)\nd2l.plt.title( f'p = {p:.3f}')\nd2l.plt.show()\n# Plot interval when p > 1/8\nplot_chebyshev( 0.0, torch .tensor( 0.2))\nThe second shows that at \ud835\udc5d=1\u009d8, the interval exactly touches the two points. This shows\nthat the inequality is sharp, since no smaller interval could be taken while keeping the\ninequality true.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ff618b8-9e78-4eef-b760-9473611787f7": {"__data__": {"id_": "7ff618b8-9e78-4eef-b760-9473611787f7", "embedding": null, "metadata": {"page_label": "969", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "84273060-b30d-4298-8b78-dff3d59d10aa", "node_type": "4", "metadata": {"page_label": "969", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "617e9bf876a7b721fc4f24de2ae3649ae61817070dc3ad3f0ed88bd3e6ecfa9c", "class_name": "RelatedNodeInfo"}}, "text": "969 Random Variables\n# Plot interval when p = 1/8\nplot_chebyshev( 0.0, torch .tensor( 0.125 ))\nThethirdshowsthatfor \ud835\udc5d < 1\u009d8theintervalonlycontainsthecenter. Thisdoesnotinvali-\ndate the inequality since we only needed to ensure that no more than 1\u009d4of the probability\nfalls outside the interval, which means that once \ud835\udc5d < 1\u009d8, the two points at \ud835\udc4e\u00002and\ud835\udc4e\u00b82\ncan be discarded.\n# Plot interval when p < 1/8\nplot_chebyshev( 0.0, torch .tensor( 0.05 ))", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 444, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9971160d-74af-4728-aa34-0926c603b710": {"__data__": {"id_": "9971160d-74af-4728-aa34-0926c603b710", "embedding": null, "metadata": {"page_label": "970", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3a560e9-2cf1-47e8-9c38-b793af3d8272", "node_type": "4", "metadata": {"page_label": "970", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ce4d52b1e86ddf75da0cd371abe40db93af3938146a85983ab53a91793c46a4b", "class_name": "RelatedNodeInfo"}}, "text": "970 Mathematics for Deep Learning\nMeans and Variancesin the Continuum\nThis has all been in terms of discrete random variables, but the case of continuous random\nvariables is similar. To intuitively understand how this works, imagine that we split the\nreal number line into intervals of length \ud835\udf16given by\u00b9\ud835\udf16\ud835\udc56,\ud835\udf16\u00b9\ud835\udc56\u00b81\u00ba\u00bc. Once we do this, our\ncontinuousrandomvariablehasbeenmadediscreteandwecanuse (A.14 )saythat\n\ud835\udf07\ud835\udc4b\u0019\u00d5\n\ud835\udc56\u00b9\ud835\udf16\ud835\udc56\u00ba\ud835\udc43\u00b9\ud835\udc4b2\u00b9\ud835\udf16\ud835\udc56,\ud835\udf16\u00b9\ud835\udc56\u00b81\u00ba\u00bc\u00ba\n\u0019\u00d5\n\ud835\udc56\u00b9\ud835\udf16\ud835\udc56\u00ba\ud835\udc5d\ud835\udc4b\u00b9\ud835\udf16\ud835\udc56\u00ba\ud835\udf16,(A.22)\nwhere\ud835\udc5d\ud835\udc4bis the density of \ud835\udc4b. This is an approximation to the integral of \ud835\udc65\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba, so we\ncan conclude that\n\ud835\udf07\ud835\udc4b=\u00b91\n\u00001\ud835\udc65\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65. (A.23)\nSimilarly, using (A.16 )the variance can be written as\n\ud835\udf0e2\n\ud835\udc4b=\ud835\udc38\u00bb\ud835\udc4b2\u00bc\u0000\ud835\udf072\n\ud835\udc4b=\u00b91\n\u00001\ud835\udc652\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65\u0000\u0012\u00b91\n\u00001\ud835\udc65\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65\u00132\n. (A.24)\nEverything stated above about the mean, the variance, and the standard deviation still ap-\nplies in this case. For instance, if we consider the random variable with density\n\ud835\udc5d\u00b9\ud835\udc65\u00ba=(\n1\ud835\udc652\u00bb0,1\u00bc,\n0otherwise.(A.25)\nwe can compute\n\ud835\udf07\ud835\udc4b=\u00b91\n\u00001\ud835\udc65\ud835\udc5d\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65=\u00b91\n0\ud835\udc65 \ud835\udc51\ud835\udc65=1\n2. (A.26)\nand\n\ud835\udf0e2\n\ud835\udc4b=\u00b91\n\u00001\ud835\udc652\ud835\udc5d\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65\u0000\u00121\n2\u00132\n=1\n3\u00001\n4=1\n12. (A.27)\nAs a warning, let\u2019s examine one more example, known as the Cauchy distribution . This is\nthe distribution with p.d.f. given by\n\ud835\udc5d\u00b9\ud835\udc65\u00ba=1\n1\u00b8\ud835\udc652. (A.28)\n# Plot the Cauchy distribution p.d.f.\nx=torch .arange( -5,5,0.01 )\np=1/(1+x**2)\nd2l.plot(x, p, 'x','p.d.f. ')\nThis function looks innocent, and indeed consulting a table of integrals will show it has\narea one under it, and thus it defines a continuous random variable.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6d5edac-2a60-46bf-a1c5-ad1c3fabb8e7": {"__data__": {"id_": "a6d5edac-2a60-46bf-a1c5-ad1c3fabb8e7", "embedding": null, "metadata": {"page_label": "971", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "df9d599a-bb9c-47ec-ba1f-07091b411110", "node_type": "4", "metadata": {"page_label": "971", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d7f0a918410c39fc0201eec40ab7f983c328f6e236189bfa37389a60d5e66a30", "class_name": "RelatedNodeInfo"}}, "text": "971 Random Variables\nTo see what goes astray, let\u2019s try to compute the variance of this. This would involve using\n(A.16 )computing\n\u00b91\n\u00001\ud835\udc652\n1\u00b8\ud835\udc652\ud835\udc51\ud835\udc65. (A.29)\nThe function on the inside looks like this:\n# Plot the integrand needed to compute the variance\nx=torch .arange( -20,20,0.01 )\np=x**2/(1+x**2)\nd2l.plot(x, p, 'x','integrand ')\nThis function clearly has infinite area under it since it is essentially the constant one with a\nsmall dip near zero, and indeed we could show that\n\u00b91\n\u00001\ud835\udc652\n1\u00b8\ud835\udc652\ud835\udc51\ud835\udc65=1. (A.30)\nThis means it does not have a well-defined finite variance.\nHowever, looking deeper shows an even more disturbing result. Let\u2019s try to compute the\nmean using (A.14 ). Using the change of variables formula, we see\n\ud835\udf07\ud835\udc4b=\u00b91\n\u00001\ud835\udc65\n1\u00b8\ud835\udc652\ud835\udc51\ud835\udc65=1\n2\u00b91\n11\n\ud835\udc62\ud835\udc51\ud835\udc62. (A.31)\nThe integral inside is the definition of the logarithm, so this is in essence log\u00b91\u00ba=1, so\nthere is no well-defined average value either!", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "45e271a8-dfb3-434f-b696-4ed8a986bdcc": {"__data__": {"id_": "45e271a8-dfb3-434f-b696-4ed8a986bdcc", "embedding": null, "metadata": {"page_label": "972", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5492367-ff90-4338-a4bd-6e51c8408b41", "node_type": "4", "metadata": {"page_label": "972", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f5d31977c3d016f9b60ab1fe20bb735fecbe0d310472acebceef7bfdb77bd4a0", "class_name": "RelatedNodeInfo"}}, "text": "972 Mathematics for Deep Learning\nMachine learning scientists define their models so that we most often do not need to deal\nwith these issues, and will in the vast majority of cases deal with random variables with\nwell-defined means and variances. However, every so often random variables with heavy\ntails(thatisthoserandomvariableswheretheprobabilitiesofgettinglargevaluesarelarge\nenoughtomakethingslikethemeanorvarianceundefined)arehelpfulinmodelingphysical\nsystems, thus it is worth knowing that they exist.\nJoint Density Functions\nTheaboveworkallassumesweareworkingwithasinglerealvaluedrandomvariable. But\nwhat if we are dealing with two or more potentially highly correlated random variables?\nThis circumstance is the norm in machine learning: imagine random variables like \ud835\udc45\ud835\udc56,\ud835\udc57\nwhich encode the red value of the pixel at the \u00b9\ud835\udc56,\ud835\udc57\u00bacoordinate in an image, or \ud835\udc43\ud835\udc61which is\narandomvariablegivenbyastockpriceattime \ud835\udc61. Nearbypixelstendtohavesimilarcolor,\nand nearby times tend to have similar prices. We cannot treat them as separate random\nvariables, and expect to create a successful model (we will see in Section A.9 a model that\nunder-performsduetosuchanassumption). Weneedtodevelopthemathematicallanguage\nto handle these correlated continuous random variables.\nThankfully, with the multiple integrals in Section A.5 we can develop such a language.\nSuppose that we have, for simplicity, two random variables \ud835\udc4b,\ud835\udc4cwhich can be correlated.\nThen, similar to the case of a single variable, we can ask the question:\n\ud835\udc43\u00b9\ud835\udc4bis in an\ud835\udf16-sized interval around \ud835\udc65and\ud835\udc4cis in an\ud835\udf16-sized interval around \ud835\udc66\u00ba.(A.32)\nSimilarreasoningtothesinglevariablecaseshowsthatthisshouldbeapproximately\n\ud835\udc43\u00b9\ud835\udc4bis in an\ud835\udf16-sized interval around \ud835\udc65and\ud835\udc4cis in an\ud835\udf16-sized interval around \ud835\udc66\u00ba\u0019\ud835\udf162\ud835\udc5d\u00b9\ud835\udc65,\ud835\udc66\u00ba,\n(A.33)\nfor some function \ud835\udc5d\u00b9\ud835\udc65,\ud835\udc66\u00ba. This is referred to as the joint density of \ud835\udc4band\ud835\udc4c. Similar\nproperties are true for this as we saw in the single variable case. Namely:\n\u000f\ud835\udc5d\u00b9\ud835\udc65,\ud835\udc66\u00ba\u00150;\n\u000f\u00af\nR2\ud835\udc5d\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66 =1;\n\u000f\ud835\udc43\u00b9\u00b9\ud835\udc4b,\ud835\udc4c\u00ba2D\u00ba =\u00af\nD\ud835\udc5d\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66.\nIn this way, we can deal with multiple, potentially correlated random variables. If we wish\nto work with more than two random variables, we can extend the multivariate density to as\nmany coordinates as desired by considering \ud835\udc5d\u00b9x\u00ba=\ud835\udc5d\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc5b\u00ba. The same properties of\nbeing non-negative, and having total integral of one still hold.\nMarginal Distributions\nWhen dealing with multiple variables, we oftentimes want to be able to ignore the rela-\ntionships and ask, \u201chow is this one variable distributed?\u201d Such a distribution is called a\nmarginaldistribution .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2537, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "66d6f3d0-4ad1-445b-a986-e779601fafc1": {"__data__": {"id_": "66d6f3d0-4ad1-445b-a986-e779601fafc1", "embedding": null, "metadata": {"page_label": "973", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c120afc-9622-4ac3-8db5-de67672b4743", "node_type": "4", "metadata": {"page_label": "973", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0d31b487ba96c4536a6cddb3ebe0c42073d829026740d78b3e5b409f97dd70ba", "class_name": "RelatedNodeInfo"}}, "text": "973 Random Variables\nTo be concrete, let\u2019s suppose that we have two random variables \ud835\udc4b,\ud835\udc4cwith joint density\ngiven by\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba. We will be using the subscript to indicate what random variables the\ndensity is for. The question of finding the marginal distribution is taking this function, and\nusing it to find \ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba.\nAs with most things, it is best to return to the intuitive picture to figure out what should be\ntrue. Recall that the density is the function \ud835\udc5d\ud835\udc4bso that\n\ud835\udc43\u00b9\ud835\udc4b2\u00bb\ud835\udc65,\ud835\udc65\u00b8\ud835\udf16\u00bc\u00ba\u0019\ud835\udf16\u0001\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba. (A.34)\nThere is no mention of \ud835\udc4c, but if all we are given is \ud835\udc5d\ud835\udc4b,\ud835\udc4c, we need to include \ud835\udc4csomehow.\nWe can first observe that this is the same as\n\ud835\udc43\u00b9\ud835\udc4b2\u00bb\ud835\udc65,\ud835\udc65\u00b8\ud835\udf16\u00bc, and\ud835\udc4c2R\u00ba\u0019\ud835\udf16\u0001\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba. (A.35)\nOur density does not directly tell us about what happens in this case, we need to split into\nsmall intervals in \ud835\udc66as well, so we can write this as\n\ud835\udf16\u0001\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba\u0019\u00d5\n\ud835\udc56\ud835\udc43\u00b9\ud835\udc4b2\u00bb\ud835\udc65,\ud835\udc65\u00b8\ud835\udf16\u00bc, and\ud835\udc4c2\u00bb\ud835\udf16\u0001\ud835\udc56,\ud835\udf16\u0001\u00b9\ud835\udc56\u00b81\u00ba\u00bc\u00ba\n\u0019\u00d5\n\ud835\udc56\ud835\udf162\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udf16\u0001\ud835\udc56\u00ba.(A.36)\ntFig. A.1 By summing along the columns of our array of probabilities, we are able to obtain the\nmarginal distribution for just the random variable represented along the x-axis.\nThistellsustoaddupthevalueofthedensityalongaseriesofsquaresinalineasisshown\ninFig. A.1. Indeed, after canceling one factor of epsilon from both sides, and recognizing\nthe sum on the right is the integral over \ud835\udc66, we can conclude that\n\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba\u0019\u00d5\n\ud835\udc56\ud835\udf16\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udf16\u0001\ud835\udc56\u00ba\n\u0019\u00b91\n\u00001\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc66.(A.37)\nThus we see\n\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba=\u00b91\n\u00001\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc66. (A.38)\nThis tells us that to get a marginal distribution, we integrate over the variables we do not", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1473, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4ca407d9-efb0-489c-8165-ccf7402cf0b6": {"__data__": {"id_": "4ca407d9-efb0-489c-8165-ccf7402cf0b6", "embedding": null, "metadata": {"page_label": "974", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2ab75a7-5582-45bd-b8d6-8da6c499d32e", "node_type": "4", "metadata": {"page_label": "974", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7902e25ee8574803302568c190746e3a3a638b22c6aea489916d96aee1a770a4", "class_name": "RelatedNodeInfo"}}, "text": "974 Mathematics for Deep Learning\ncare about. This process is often referred to as integrating out ormarginalized out the\nunneeded variables.\nCovariance\nWhen dealing with multiple random variables, there is one additional summary statistic\nwhich is helpful to know: the covariance . This measures the degree that two random vari-\nable fluctuate together.\nSuppose that we have two random variables \ud835\udc4band\ud835\udc4c, to begin with, let\u2019s suppose they\nare discrete, taking on values \u00b9\ud835\udc65\ud835\udc56,\ud835\udc66\ud835\udc57\u00bawith probability \ud835\udc5d\ud835\udc56\ud835\udc57. In this case, the covariance is\ndefined as\n\ud835\udf0e\ud835\udc4b\ud835\udc4c=Cov\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\u00d5\n\ud835\udc56,\ud835\udc57\u00b9\ud835\udc65\ud835\udc56\u0000\ud835\udf07\ud835\udc4b\u00ba\u00b9\ud835\udc66\ud835\udc57\u0000\ud835\udf07\ud835\udc4c\u00ba\ud835\udc5d\ud835\udc56\ud835\udc57.=\ud835\udc38\u00bb\ud835\udc4b\ud835\udc4c\u00bc\u0000\ud835\udc38\u00bb\ud835\udc4b\u00bc\ud835\udc38\u00bb\ud835\udc4c\u00bc.(A.39)\nTo think about this intuitively: consider the following pair of random variables. Suppose\nthat\ud835\udc4btakes the values 1and3, and\ud835\udc4ctakes the values\u00001and3. Suppose that we have the\nfollowing probabilities\n\ud835\udc43\u00b9\ud835\udc4b=1and\ud835\udc4c=\u00001\u00ba=\ud835\udc5d\n2,\n\ud835\udc43\u00b9\ud835\udc4b=1and\ud835\udc4c=3\u00ba=1\u0000\ud835\udc5d\n2,\n\ud835\udc43\u00b9\ud835\udc4b=3and\ud835\udc4c=\u00001\u00ba=1\u0000\ud835\udc5d\n2,\n\ud835\udc43\u00b9\ud835\udc4b=3and\ud835\udc4c=3\u00ba=\ud835\udc5d\n2,(A.40)\nwhere\ud835\udc5disaparameterin\u00bb0,1\u00bcwegettopick. Noticethatif \ud835\udc5d=1thentheyarebothalways\ntheirminimumormaximumvaluessimultaneously,andif \ud835\udc5d=0theyareguaranteedtotake\ntheir flipped values simultaneously (one is large when the other is small and vice versa).\nIf\ud835\udc5d=1\u009d2, then the four possibilities are all equally likely, and neither should be related.\nLet\u2019s compute the covariance. First, note \ud835\udf07\ud835\udc4b=2and\ud835\udf07\ud835\udc4c=1, so we may compute using\n(A.39 ):\nCov\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\u00d5\n\ud835\udc56,\ud835\udc57\u00b9\ud835\udc65\ud835\udc56\u0000\ud835\udf07\ud835\udc4b\u00ba\u00b9\ud835\udc66\ud835\udc57\u0000\ud835\udf07\ud835\udc4c\u00ba\ud835\udc5d\ud835\udc56\ud835\udc57\n=\u00b91\u00002\u00ba\u00b9\u00001\u00001\u00ba\ud835\udc5d\n2\u00b8\u00b91\u00002\u00ba\u00b93\u00001\u00ba1\u0000\ud835\udc5d\n2\u00b8\u00b93\u00002\u00ba\u00b9\u00001\u00001\u00ba1\u0000\ud835\udc5d\n2\u00b8\u00b93\u00002\u00ba\u00b93\u00001\u00ba\ud835\udc5d\n2\n=4\ud835\udc5d\u00002.\n(A.41)\nWhen\ud835\udc5d=1(thecasewheretheyarebothmaximallypositiveornegativeatthesametime)\nhas a covariance of 2. When\ud835\udc5d=0(the case where they are flipped) the covariance is \u00002.\nFinally, when \ud835\udc5d=1\u009d2(the case where they are unrelated), the covariance is 0. Thus we\nsee that the covariance measures how these two random variables are related.\nA quick note on the covariance is that it only measures these linear relationships. More\ncomplex relationships like \ud835\udc4b=\ud835\udc4c2where\ud835\udc4cis randomly chosen from f\u00002,\u00001,0,1,2gwith", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1899, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e3f3009-4b65-486f-83d9-540d0efc69da": {"__data__": {"id_": "9e3f3009-4b65-486f-83d9-540d0efc69da", "embedding": null, "metadata": {"page_label": "975", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7b9a7816-9df1-4087-b9d3-fce837bbb5f0", "node_type": "4", "metadata": {"page_label": "975", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f4dcfbe811e6c5556e3248d61b877cd43df3432b43a6ba5308d7fa6f45631e9a", "class_name": "RelatedNodeInfo"}}, "text": "975 Random Variables\nequalprobabilitycanbemissed. Indeedaquickcomputationshowsthattheserandomvari-\nables have covariance zero, despite one being a deterministic function of the other.\nFor continuous random variables, much the same story holds. At this point, we are pretty\ncomfortable with doing the transition between discrete and continuous, so we will provide\nthe continuous analogue of (A.39 )without any derivation.\n\ud835\udf0e\ud835\udc4b\ud835\udc4c=\u00b9\nR2\u00b9\ud835\udc65\u0000\ud835\udf07\ud835\udc4b\u00ba\u00b9\ud835\udc66\u0000\ud835\udf07\ud835\udc4c\u00ba\ud835\udc5d\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66. (A.42)\nFor visualization, let\u2019s take a look at a collection of random variables with tunable covari-\nance.\n# Plot a few random variables adjustable covariance\ncovs =[-0.9,0.0,1.2]\nd2l.plt.figure(figsize =(12,3))\nfor iinrange (3):\nX=torch .randn( 500)\nY=covs[i] *X+torch .randn( 500)\nd2l.plt.subplot( 1,4, i+1)\nd2l.plt.scatter(X .numpy(), Y .numpy())\nd2l.plt.xlabel( 'X')\nd2l.plt.ylabel( 'Y')\nd2l.plt.title( f'cov = {covs[i] }')\nd2l.plt.show()\nLet\u2019s see some properties of covariances:\n\u000fFor any random variable \ud835\udc4b, Cov\u00b9\ud835\udc4b,\ud835\udc4b\u00ba=Var\u00b9\ud835\udc4b\u00ba.\n\u000fForanyrandomvariables \ud835\udc4b,\ud835\udc4candnumbers \ud835\udc4eand\ud835\udc4f,Cov\u00b9\ud835\udc4e\ud835\udc4b\u00b8\ud835\udc4f,\ud835\udc4c\u00ba=Cov\u00b9\ud835\udc4b,\ud835\udc4e\ud835\udc4c\u00b8\ud835\udc4f\u00ba=\n\ud835\udc4eCov\u00b9\ud835\udc4b,\ud835\udc4c\u00ba.\n\u000fIf\ud835\udc4band\ud835\udc4care independent then Cov \u00b9\ud835\udc4b,\ud835\udc4c\u00ba=0.\nIn addition, we can use the covariance to expand a relationship we saw before. Recall that\nis\ud835\udc4band\ud835\udc4care two independent random variables then\nVar\u00b9\ud835\udc4b\u00b8\ud835\udc4c\u00ba=Var\u00b9\ud835\udc4b\u00ba\u00b8Var\u00b9\ud835\udc4c\u00ba. (A.43)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e16bdc3-8b51-409f-be1d-7029b8cd680b": {"__data__": {"id_": "9e16bdc3-8b51-409f-be1d-7029b8cd680b", "embedding": null, "metadata": {"page_label": "976", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3700e140-0b5b-49f5-84c0-5188fe804335", "node_type": "4", "metadata": {"page_label": "976", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "117a07a9dabe2b78fc19a63c475b4cba25b80bdafe742ab621e3fecda8670761", "class_name": "RelatedNodeInfo"}}, "text": "976 Mathematics for Deep Learning\nWithknowledgeofcovariances,wecanexpandthisrelationship. Indeed,somealgebracan\nshow that in general,\nVar\u00b9\ud835\udc4b\u00b8\ud835\udc4c\u00ba=Var\u00b9\ud835\udc4b\u00ba\u00b8Var\u00b9\ud835\udc4c\u00ba\u00b82Cov\u00b9\ud835\udc4b,\ud835\udc4c\u00ba. (A.44)\nThisallowsustogeneralizethevariancesummationruleforcorrelatedrandomvariables.\nCorrelation\nAs we did in the case of means and variances, let\u2019s now consider units. If \ud835\udc4bis measured in\noneunit(sayinches),and \ud835\udc4cismeasuredinanother(saydollars),thecovarianceismeasured\nintheproductofthesetwounitsinches \u0002dollars. Theseunitscanbehardtointerpret. What\nwe will often want in this case is a unit-less measurement of relatedness. Indeed, often we\ndo not care about exact quantitative correlation, but rather ask if the correlation is in the\nsame direction, and how strong the relationship is.\nTo see what makes sense, let\u2019s perform a thought experiment. Suppose that we convert\nour random variables in inches and dollars to be in inches and cents. In this case the ran-\ndom variable \ud835\udc4cis multiplied by 100. If we work through the definition, this means that\nCov\u00b9\ud835\udc4b,\ud835\udc4c\u00bawillbemultipliedby 100. Thusweseethatinthiscaseachangeofunitschange\nthe covariance by a factor of 100. Thus, to find our unit-invariant measure of correlation,\nwe will need to divide by something else that also gets scaled by 100. Indeed we have a\nclear candidate, the standard deviation! Indeed if we define the correlation coe\ufb00icient to\nbe\n\ud835\udf0c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=Cov\u00b9\ud835\udc4b,\ud835\udc4c\u00ba\n\ud835\udf0e\ud835\udc4b\ud835\udf0e\ud835\udc4c, (A.45)\nwe see that this is a unit-less value. A little mathematics can show that this number is\nbetween\u00001and 1with 1meaning maximally positively correlated, whereas \u00001means\nmaximally negatively correlated.\nReturning to our explicit discrete example above, we can see that \ud835\udf0e\ud835\udc4b=1and\ud835\udf0e\ud835\udc4c=2,\nso we can compute the correlation between the two random variables using (A.45 )to see\nthat\n\ud835\udf0c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=4\ud835\udc5d\u00002\n1\u00012=2\ud835\udc5d\u00001. (A.46)\nThisnowrangesbetween \u00001and1withtheexpectedbehaviorof 1meaningmostcorrelated,\nand\u00001meaning minimally correlated.\nAs another example, consider \ud835\udc4bas any random variable, and \ud835\udc4c=\ud835\udc4e\ud835\udc4b\u00b8\ud835\udc4fas any linear\ndeterministic function of \ud835\udc4b. Then, one can compute that\n\ud835\udf0e\ud835\udc4c=\ud835\udf0e\ud835\udc4e\ud835\udc4b\u00b8\ud835\udc4f=j\ud835\udc4ej\ud835\udf0e\ud835\udc4b, (A.47)\nCov\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=Cov\u00b9\ud835\udc4b,\ud835\udc4e\ud835\udc4b\u00b8\ud835\udc4f\u00ba=\ud835\udc4eCov\u00b9\ud835\udc4b,\ud835\udc4b\u00ba=\ud835\udc4eVar\u00b9\ud835\udc4b\u00ba, (A.48)\nand thus by (A.45 )that\n\ud835\udf0c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\ud835\udc4eVar\u00b9\ud835\udc4b\u00ba\nj\ud835\udc4ej\ud835\udf0e2\n\ud835\udc4b=\ud835\udc4e\nj\ud835\udc4ej=sign\u00b9\ud835\udc4e\u00ba. (A.49)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55bba3ab-2576-44ac-bb3f-5cfa7d31e8c6": {"__data__": {"id_": "55bba3ab-2576-44ac-bb3f-5cfa7d31e8c6", "embedding": null, "metadata": {"page_label": "977", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a4b4fa7-907d-42d2-9f07-aae78a62ca34", "node_type": "4", "metadata": {"page_label": "977", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "55ab602c4c144d174a6e6b0e7fcb94af773b9fdf3c96924b4307f6a738cdfe44", "class_name": "RelatedNodeInfo"}}, "text": "977 Random Variables\nThus we see that the correlation is \u00b81for any\ud835\udc4e > 0, and\u00001for any\ud835\udc4e < 0illustrating that\ncorrelation measures the degree and directionality the two random variables are related,\nnot the scale that the variation takes.\nLet\u2019s again plot a collection of random variables with tunable correlation.\n# Plot a few random variables adjustable correlations\ncors =[-0.9,0.0,1.0]\nd2l.plt.figure(figsize =(12,3))\nfor iinrange (3):\nX=torch .randn( 500)\nY=cors[i] *X+torch .sqrt(torch .tensor( 1)-\ncors[i] **2)*torch .randn( 500)\nd2l.plt.subplot( 1,4, i +1)\nd2l.plt.scatter(X .numpy(), Y .numpy())\nd2l.plt.xlabel( 'X')\nd2l.plt.ylabel( 'Y')\nd2l.plt.title( f'cor = {cors[i] }')\nd2l.plt.show()\nLet\u2019s list a few properties of the correlation below.\n\u000fFor any random variable \ud835\udc4b,\ud835\udf0c\u00b9\ud835\udc4b,\ud835\udc4b\u00ba=1.\n\u000fFor any random variables \ud835\udc4b,\ud835\udc4cand numbers \ud835\udc4eand\ud835\udc4f,\ud835\udf0c\u00b9\ud835\udc4e\ud835\udc4b\u00b8\ud835\udc4f,\ud835\udc4c\u00ba=\ud835\udf0c\u00b9\ud835\udc4b,\ud835\udc4e\ud835\udc4c\u00b8\ud835\udc4f\u00ba=\n\ud835\udf0c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba.\n\u000fIf\ud835\udc4band\ud835\udc4care independent with non-zero variance then \ud835\udf0c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=0.\nAsafinalnote, youmayfeellikesomeoftheseformulaearefamiliar. Indeed, ifweexpand\neverything out assuming that \ud835\udf07\ud835\udc4b=\ud835\udf07\ud835\udc4c=0, we see that this is\n\ud835\udf0c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\u00cd\n\ud835\udc56,\ud835\udc57\ud835\udc65\ud835\udc56\ud835\udc66\ud835\udc56\ud835\udc5d\ud835\udc56\ud835\udc57q\u00cd\n\ud835\udc56,\ud835\udc57\ud835\udc652\n\ud835\udc56\ud835\udc5d\ud835\udc56\ud835\udc57q\u00cd\n\ud835\udc56,\ud835\udc57\ud835\udc662\n\ud835\udc57\ud835\udc5d\ud835\udc56\ud835\udc57.(A.50)\nThis looks like a sum of a product of terms divided by the square root of sums of terms.\nThis is exactly the formula for the cosine of the angle between two vectors v,wwith the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1291, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a8c631b-b9d2-4e97-ac9f-75f3bfa35893": {"__data__": {"id_": "6a8c631b-b9d2-4e97-ac9f-75f3bfa35893", "embedding": null, "metadata": {"page_label": "978", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3d53c93-3816-4a3d-b988-89de6678d2a9", "node_type": "4", "metadata": {"page_label": "978", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "943d7a8653cffb69326bca16df9f8e15a4c202c171f3cf7050ac9874e8cdbeaf", "class_name": "RelatedNodeInfo"}}, "text": "978 Mathematics for Deep Learning\ndifferent coordinates weighted by \ud835\udc5d\ud835\udc56\ud835\udc57:\ncos\u00b9\ud835\udf03\u00ba=v\u0001w\nkvkkwk=\u00cd\n\ud835\udc56\ud835\udc63\ud835\udc56\ud835\udc64\ud835\udc56q\u00cd\n\ud835\udc56\ud835\udc632\n\ud835\udc56q\u00cd\n\ud835\udc56\ud835\udc642\n\ud835\udc56.(A.51)\nIndeed if we think of norms as being related to standard deviations, and correlations as\nbeing cosines of angles, much of the intuition we have from geometry can be applied to\nthinking about random variables.\nA.6.2Summary\n\u000fContinuous random variables are random variables that can take on a continuum of val-\nues. They have some technical difficulties that make them more challenging to work\nwith compared to discrete random variables.\n\u000fThe probability density function allows us to work with continuous random variables by\ngivingafunctionwheretheareaunderthecurveonsomeintervalgivestheprobability\nof finding a sample point in that interval.\n\u000fThecumulativedistributionfunctionistheprobabilityofobservingtherandomvariable\nto be less than a given threshold. It can provide a useful alternate viewpoint which\nunifies discrete and continuous variables.\n\u000fThe mean is the average value of a random variable.\n\u000fThe variance is the expected square of the difference between the random variable and\nits mean.\n\u000fThe standard deviation is the square root of the variance. It can be thought of as mea-\nsuring the range of values the random variable may take.\n\u000fChebyshev\u2019s inequality allows us to make this intuition rigorous by giving an explicit\ninterval that contains the random variable most of the time.\n\u000fJoint densities allow us to work with correlated random variables. We may marginalize\njoint densities by integrating over unwanted random variables to get the distribution\nof the desired random variable.\n\u000fThe covariance and correlation coefficient provide a way to measure any linear relation-\nship between two correlated random variables.\nA.6.3Exercises\n1.Suppose that we have the random variable with density given by \ud835\udc5d\u00b9\ud835\udc65\u00ba=1\n\ud835\udc652for\ud835\udc65\u00151\nand\ud835\udc5d\u00b9\ud835\udc65\u00ba=0otherwise. What is \ud835\udc43\u00b9\ud835\udc4b > 2\u00ba?\n2.The Laplace distribution is a random variable whose density is given by \ud835\udc5d\u00b9\ud835\udc65=1\n2\ud835\udc52\u0000j\ud835\udc65j.\nWhatisthemeanandthestandarddeviationofthisfunction? Asahint,\u00af1\n0\ud835\udc65\ud835\udc52\u0000\ud835\udc65\ud835\udc51\ud835\udc65=1\nand\u00af1\n0\ud835\udc652\ud835\udc52\u0000\ud835\udc65\ud835\udc51\ud835\udc65=2.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2069, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "63649d0c-a608-4e13-831f-9ad6be019cf1": {"__data__": {"id_": "63649d0c-a608-4e13-831f-9ad6be019cf1", "embedding": null, "metadata": {"page_label": "979", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "43291d5d-62f5-499d-8c07-ffdeee836c43", "node_type": "4", "metadata": {"page_label": "979", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bc2ab1a853b33bec4071028f50e420b17781df7e80582e2cf89ccf5a75643c1d", "class_name": "RelatedNodeInfo"}}, "text": "979 Maximum Likelihood\n2853.I walk up to you on the street and say \u201cI have a random variable with mean 1, standard\ndeviation 2, and I observed 25%of my samples taking a value larger than 9.\u201d Do you\nbelieve me? Why or why not?\n4.Supposethatyouhavetworandomvariables \ud835\udc4b,\ud835\udc4c,withjointdensitygivenby \ud835\udc5d\ud835\udc4b\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba=\n4\ud835\udc65\ud835\udc66for\ud835\udc65,\ud835\udc662\u00bb0,1\u00bcand\ud835\udc5d\ud835\udc4b\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba=0otherwise. What is the covariance of \ud835\udc4band\ud835\udc4c?\nDiscussions285.\nA.7Maximum Likelihood\nOne of the most commonly encountered way of thinking in machine learning is the maxi-\nmum likelihood point of view. This is the concept that when working with a probabilistic\nmodel with unknown parameters, the parameters which make the data have the highest\nprobability are the most likely ones.\nA.7.1The Maximum LikelihoodPrinciple\nThis has a Bayesian interpretation which can be helpful to think about. Suppose that we\nhave a model with parameters \ud835\udf3dand a collection of data examples \ud835\udc4b. For concreteness, we\ncanimaginethat \ud835\udf3disasinglevaluerepresentingtheprobabilitythatacoincomesupheads\nwhen flipped, and \ud835\udc4bis a sequence of independent coin flips. We will look at this example\nin depth later.\nIf we want to find the most likely value for the parameters of our model, that means we\nwant to find\nargmax\ud835\udc43\u00b9\ud835\udf3dj\ud835\udc4b\u00ba. (A.1)\nBy Bayes\u2019 rule, this is the same thing as\nargmax\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf3d\u00ba\ud835\udc43\u00b9\ud835\udf3d\u00ba\n\ud835\udc43\u00b9\ud835\udc4b\u00ba. (A.2)\nThe expression \ud835\udc43\u00b9\ud835\udc4b\u00ba, a parameter agnostic probability of generating the data, does not\ndepend on \ud835\udf3dat all, and so can be dropped without changing the best choice of \ud835\udf3d. Similarly,\nwe may now posit that we have no prior assumption on which set of parameters are better\nthan any others, so we may declare that \ud835\udc43\u00b9\ud835\udf3d\u00badoes not depend on theta either! This, for\ninstance,makessenseinourcoinflippingexamplewheretheprobabilityitcomesupheads\ncould be any value in \u00bb0,1\u00bcwithout any prior belief it is fair or not (often referred to as an\nuninformative prior ). Thus we see that our application of Bayes\u2019 rule shows that our best\nchoice of \ud835\udf3dis the maximum likelihood estimate for \ud835\udf3d:\n\u02c6\ud835\udf3d=argmax\n\ud835\udf3d\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf3d\u00ba. (A.3)\nAsamatterofcommonterminology,theprobabilityofthedatagiventheparameters( \ud835\udc43\u00b9\ud835\udc4bj\n\ud835\udf3d\u00ba) is referred to as the likelihood .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3e56e09-2cfb-485c-aec3-15fe76030a26": {"__data__": {"id_": "a3e56e09-2cfb-485c-aec3-15fe76030a26", "embedding": null, "metadata": {"page_label": "980", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e468b855-cc21-4d80-b909-8200bf17080b", "node_type": "4", "metadata": {"page_label": "980", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b3232eefb92bff68d7d76c13afbce4c4b1e9f71ef214f0786b393af5f12fb9ec", "class_name": "RelatedNodeInfo"}}, "text": "980 Mathematics for Deep Learning\nA ConcreteExample\nLet\u2019s see how this works in a concrete example. Suppose that we have a single parameter \ud835\udf03\nrepresenting the probability that a coin flip is heads. Then the probability of getting a tails\nis1\u0000\ud835\udf03, and so if our observed data \ud835\udc4bis a sequence with \ud835\udc5b\ud835\udc3bheads and\ud835\udc5b\ud835\udc47tails, we can\nuse the fact that independent probabilities multiply to see that\n\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf03\u00ba=\ud835\udf03\ud835\udc5b\ud835\udc3b\u00b91\u0000\ud835\udf03\u00ba\ud835\udc5b\ud835\udc47. (A.4)\nIf we flip 13coins and get the sequence \u201cHHHTHTTHHHHHT\u201d, which has \ud835\udc5b\ud835\udc3b=9and\n\ud835\udc5b\ud835\udc47=4, we see that this is\n\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf03\u00ba=\ud835\udf039\u00b91\u0000\ud835\udf03\u00ba4. (A.5)\nOne nice thing about this example will be that we know the answer going in. Indeed, if\nwe said verbally, \u201cI flipped 13 coins, and 9 came up heads, what is our best guess for the\nprobabilitythatthecoincomesusheads?,\u201deveryonewouldcorrectlyguess 9\u009d13. Whatthis\nmaximum likelihood method will give us is a way to get that number from first principals\nin a way that will generalize to vastly more complex situations.\nFor our example, the plot of \ud835\udc43\u00b9\ud835\udc4bj\ud835\udf03\u00bais as follows:\n%matplotlib inline\nimport torch\nfrom d2l import torch asd2l\ntheta =torch .arange( 0,1,0.001 )\np=theta **9*(1-theta) **4.\nd2l.plot(theta, p, 'theta ','likelihood ')\nThis has its maximum value somewhere near our expected 9\u009d13\u00190.7.... To see if it\nis exactly there, we can turn to calculus. Notice that at the maximum, the gradient of the\nfunction is flat. Thus, we could find the maximum likelihood estimate (A.1)by finding\nthe values of \ud835\udf03where the derivative is zero, and finding the one that gives the highest", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a2c1165-6a37-4719-9389-543b9c5d68e1": {"__data__": {"id_": "0a2c1165-6a37-4719-9389-543b9c5d68e1", "embedding": null, "metadata": {"page_label": "981", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83f797ec-4ce8-44b5-89eb-850eb5133e1c", "node_type": "4", "metadata": {"page_label": "981", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5bdf0763965385c8e2d01a7013b1f527120bbea126439c9874cbafaaa32e716f", "class_name": "RelatedNodeInfo"}}, "text": "981 Maximum Likelihood\nprobability. We compute:\n0=\ud835\udc51\n\ud835\udc51\ud835\udf03\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf03\u00ba\n=\ud835\udc51\n\ud835\udc51\ud835\udf03\ud835\udf039\u00b91\u0000\ud835\udf03\u00ba4\n=9\ud835\udf038\u00b91\u0000\ud835\udf03\u00ba4\u00004\ud835\udf039\u00b91\u0000\ud835\udf03\u00ba3\n=\ud835\udf038\u00b91\u0000\ud835\udf03\u00ba3\u00b99\u000013\ud835\udf03\u00ba.(A.6)\nThis has three solutions: 0,1and9\u009d13. The first two are clearly minima, not maxima as\nthey assign probability 0to our sequence. The final value does notassign zero probability\nto our sequence, and thus must be the maximum likelihood estimate \u02c6\ud835\udf03=9\u009d13.\nA.7.2NumericalOptimizationand the NegativeLog-Likelihood\nThe previous example is nice, but what if we have billions of parameters and data exam-\nples?\nFirst, notice that if we make the assumption that all the data examples are independent, we\ncan no longer practically consider the likelihood itself as it is a product of many probabili-\nties. Indeed,eachprobabilityisin \u00bb0,1\u00bc,saytypicallyofvalueabout 1\u009d2,andtheproductof\n\u00b91\u009d2\u00ba1000000000is far below machine precision. We cannot work with that directly.\nHowever, recall that the logarithm turns products to sums, in which case\nlog\u00b9\u00b91\u009d2\u00ba1000000000\u00ba=1000000000\u0001log\u00b91\u009d2\u00ba\u0019\u0000 301029995.6... (A.7)\nThis number fits perfectly within even a single precision 32-bit float. Thus, we should\nconsider the log-likelihood , which is\nlog\u00b9\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf3d\u00ba\u00ba. (A.8)\nSince the function \ud835\udc657!log\u00b9\ud835\udc65\u00bais increasing, maximizing the likelihood is the same thing\nasmaximizingthelog-likelihood. Indeedin SectionA.9 wewillseethisreasoningapplied\nwhen working with the specific example of the naive Bayes classifier.\nWe often work with loss functions, where we wish to minimize the loss. We may turn\nmaximum likelihood into the minimization of a loss by taking \u0000log\u00b9\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf3d\u00ba\u00ba, which is\nthenegativelog-likelihood .\nTo illustrate this, consider the coin flipping problem from before, and pretend that we do\nnot know the closed form solution. We may compute that\n\u0000log\u00b9\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf3d\u00ba\u00ba=\u0000log\u00b9\ud835\udf03\ud835\udc5b\ud835\udc3b\u00b91\u0000\ud835\udf03\u00ba\ud835\udc5b\ud835\udc47\u00ba=\u0000\u00b9\ud835\udc5b\ud835\udc3blog\u00b9\ud835\udf03\u00ba\u00b8\ud835\udc5b\ud835\udc47log\u00b91\u0000\ud835\udf03\u00ba\u00ba. (A.9)\nThis can be written into code, and freely optimized even for billions of coin flips.\n# Set up our data\nn_H =8675309\nn_T =256245\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1948, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6cb8db1-9369-4c0b-bc7d-9056c48f1a70": {"__data__": {"id_": "e6cb8db1-9369-4c0b-bc7d-9056c48f1a70", "embedding": null, "metadata": {"page_label": "982", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "58b60538-3f44-4246-b039-bfa7e6c89fb6", "node_type": "4", "metadata": {"page_label": "982", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4b9995f7831bae9dbaf36769ed303488df381b0afedfd6e2686d00e0d448bf11", "class_name": "RelatedNodeInfo"}}, "text": "982 Mathematics for Deep Learning\n(continued from previous page)\n# Initialize our paramteres\ntheta =torch .tensor( 0.5, requires_grad =True )\n# Perform gradient descent\nlr=1e-9\nfor iter inrange (100):\nloss =-(n_H *torch .log(theta) +n_T *torch .log( 1-theta))\nloss .backward()\nwith torch .no_grad():\ntheta -=lr*theta .grad\ntheta .grad .zero_()\n# Check output\ntheta, n_H /(n_H +n_T)\n(tensor( 0.9713 , requires_grad =True ),0.9713101437890875 )\nNumericalconvenienceisnottheonlyreasonwhypeopleliketousenegativelog-likelihoods.\nThere are several other reasons why it is preferable.\nThe second reason we consider the log-likelihood is the simplified application of calcu-\nlus rules. As discussed above, due to independence assumptions, most probabilities we\nencounter in machine learning are products of individual probabilities.\n\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf3d\u00ba=\ud835\udc5d\u00b9\ud835\udc651j\ud835\udf3d\u00ba\u0001\ud835\udc5d\u00b9\ud835\udc652j\ud835\udf3d\u00ba\u0001\u0001\u0001\ud835\udc5d\u00b9\ud835\udc65\ud835\udc5bj\ud835\udf3d\u00ba. (A.10)\nThismeansthatifwedirectlyapplytheproductruletocomputeaderivativeweget\n\ud835\udf15\n\ud835\udf15\ud835\udf3d\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf3d\u00ba=\u0012\ud835\udf15\n\ud835\udf15\ud835\udf3d\ud835\udc43\u00b9\ud835\udc651j\ud835\udf3d\u00ba\u0013\n\u0001\ud835\udc43\u00b9\ud835\udc652j\ud835\udf3d\u00ba\u0001\u0001\u0001\ud835\udc43\u00b9\ud835\udc65\ud835\udc5bj\ud835\udf3d\u00ba\n\u00b8\ud835\udc43\u00b9\ud835\udc651j\ud835\udf3d\u00ba\u0001\u0012\ud835\udf15\n\ud835\udf15\ud835\udf3d\ud835\udc43\u00b9\ud835\udc652j\ud835\udf3d\u00ba\u0013\n\u0001\u0001\u0001\ud835\udc43\u00b9\ud835\udc65\ud835\udc5bj\ud835\udf3d\u00ba\n...\n\u00b8\ud835\udc43\u00b9\ud835\udc651j\ud835\udf3d\u00ba\u0001\ud835\udc43\u00b9\ud835\udc652j\ud835\udf3d\u00ba\u0001\u0001\u0001\u0012\ud835\udf15\n\ud835\udf15\ud835\udf3d\ud835\udc43\u00b9\ud835\udc65\ud835\udc5bj\ud835\udf3d\u00ba\u0013\n.(A.11)\nThis requires \ud835\udc5b\u00b9\ud835\udc5b\u00001\u00bamultiplications, along with \u00b9\ud835\udc5b\u00001\u00baadditions, so it is proportional\nto quadratic time in the inputs! Sufficient cleverness in grouping terms will reduce this\nto linear time, but it requires some thought. For the negative log-likelihood we have in-\nstead\n\u0000log\u00b9\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf3d\u00ba\u00ba=\u0000log\u00b9\ud835\udc43\u00b9\ud835\udc651j\ud835\udf3d\u00ba\u00ba\u0000 log\u00b9\ud835\udc43\u00b9\ud835\udc652j\ud835\udf3d\u00ba\u00ba\u0001\u0001\u0001\u0000 log\u00b9\ud835\udc43\u00b9\ud835\udc65\ud835\udc5bj\ud835\udf3d\u00ba\u00ba,(A.12)\nwhich then gives\n\u0000\ud835\udf15\n\ud835\udf15\ud835\udf3dlog\u00b9\ud835\udc43\u00b9\ud835\udc4bj\ud835\udf3d\u00ba\u00ba=1\n\ud835\udc43\u00b9\ud835\udc651j\ud835\udf3d\u00ba\u0012\ud835\udf15\n\ud835\udf15\ud835\udf3d\ud835\udc43\u00b9\ud835\udc651j\ud835\udf3d\u00ba\u0013\n\u00b8\u0001\u0001\u0001\u00b81\n\ud835\udc43\u00b9\ud835\udc65\ud835\udc5bj\ud835\udf3d\u00ba\u0012\ud835\udf15\n\ud835\udf15\ud835\udf3d\ud835\udc43\u00b9\ud835\udc65\ud835\udc5bj\ud835\udf3d\u00ba\u0013\n.\n(A.13)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0547c61b-deae-493d-a650-abc29af755a1": {"__data__": {"id_": "0547c61b-deae-493d-a650-abc29af755a1", "embedding": null, "metadata": {"page_label": "983", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5be7398b-a424-4fe8-b370-8b3f29de85f7", "node_type": "4", "metadata": {"page_label": "983", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3902df11775474e652080cc6c95c1a06e0753d528c4d4792d75560271de480ab", "class_name": "RelatedNodeInfo"}}, "text": "983 Maximum Likelihood\nThis requires only \ud835\udc5bdivides and\ud835\udc5b\u00001sums, and thus is linear time in the inputs.\nThe third and final reason to consider the negative log-likelihood is the relationship to\ninformation theory, which we will discuss in detail in Section A.11 . This is a rigorous\nmathematicaltheorywhichgivesawaytomeasurethedegreeofinformationorrandomness\nin a random variable. The key object of study in that field is the entropy which is\n\ud835\udc3b\u00b9\ud835\udc5d\u00ba=\u0000\u00d5\n\ud835\udc56\ud835\udc5d\ud835\udc56log2\u00b9\ud835\udc5d\ud835\udc56\u00ba,(A.14)\nwhich measures the randomness of a source. Notice that this is nothing more than the av-\nerage\u0000logprobability, and thus if we take our negative log-likelihood and divide by the\nnumberofdataexamples,wegetarelativeofentropyknownascross-entropy. Thistheoret-\nicalinterpretationalonewouldbesufficientlycompellingtomotivatereportingtheaverage\nnegative log-likelihood over the dataset as a way of measuring model performance.\nA.7.3MaximumLikelihoodforContinuous Variables\nEverything that we have done so far assumes we are working with discrete random vari-\nables, but what if we want to work with continuous ones?\nThe short summary is that nothing at all changes, except we replace all the instances of the\nprobability with the probability density. Recalling that we write densities with lower case\n\ud835\udc5d, this means that for example we now say\n\u0000log\u00b9\ud835\udc5d\u00b9\ud835\udc4bj\ud835\udf3d\u00ba\u00ba=\u0000log\u00b9\ud835\udc5d\u00b9\ud835\udc651j\ud835\udf3d\u00ba\u00ba\u0000 log\u00b9\ud835\udc5d\u00b9\ud835\udc652j\ud835\udf3d\u00ba\u00ba\u0001\u0001\u0001\u0000 log\u00b9\ud835\udc5d\u00b9\ud835\udc65\ud835\udc5bj\ud835\udf3d\u00ba\u00ba=\u0000\u00d5\n\ud835\udc56log\u00b9\ud835\udc5d\u00b9\ud835\udc65\ud835\udc56j\ud835\udf03\u00ba\u00ba.\n(A.15)\nThequestionbecomes,\u201cWhyisthisOK?\u201dAfterall,thereasonweintroduceddensitieswas\nbecause probabilities of getting specific outcomes themselves was zero, and thus is not the\nprobability of generating our data for any set of parameters zero?\nIndeed, this is the case, and understanding why we can shift to densities is an exercise in\ntracing what happens to the epsilons.\nLet\u2019s first re-define our goal. Suppose that for continuous random variables we no longer\nwant to compute the probability of getting exactly the right value, but instead matching to\nwithinsomerange \ud835\udf16. Forsimplicity,weassumeourdataisrepeatedobservations \ud835\udc651,...,\ud835\udc65\ud835\udc41\nof identically distributed random variables \ud835\udc4b1,...,\ud835\udc4b\ud835\udc41. As we have seen previously, this\ncan be written as\n\ud835\udc43\u00b9\ud835\udc4b12\u00bb\ud835\udc651,\ud835\udc651\u00b8\ud835\udf16\u00bc,\ud835\udc4b22\u00bb\ud835\udc652,\ud835\udc652\u00b8\ud835\udf16\u00bc,...,\ud835\udc4b\ud835\udc412\u00bb\ud835\udc65\ud835\udc41,\ud835\udc65\ud835\udc41\u00b8\ud835\udf16\u00bcj\ud835\udf3d\u00ba\n\u0019\ud835\udf16\ud835\udc41\ud835\udc5d\u00b9\ud835\udc651j\ud835\udf3d\u00ba\u0001\ud835\udc5d\u00b9\ud835\udc652j\ud835\udf3d\u00ba\u0001\u0001\u0001\ud835\udc5d\u00b9\ud835\udc65\ud835\udc5bj\ud835\udf3d\u00ba.(A.16)\nThus, if we take negative logarithms of this we obtain\n\u0000log\u00b9\ud835\udc43\u00b9\ud835\udc4b12\u00bb\ud835\udc651,\ud835\udc651\u00b8\ud835\udf16\u00bc,\ud835\udc4b22\u00bb\ud835\udc652,\ud835\udc652\u00b8\ud835\udf16\u00bc,...,\ud835\udc4b\ud835\udc412\u00bb\ud835\udc65\ud835\udc41,\ud835\udc65\ud835\udc41\u00b8\ud835\udf16\u00bcj\ud835\udf3d\u00ba\u00ba\n\u0019\u0000\ud835\udc41log\u00b9\ud835\udf16\u00ba\u0000\u00d5\n\ud835\udc56log\u00b9\ud835\udc5d\u00b9\ud835\udc65\ud835\udc56j\ud835\udf3d\u00ba\u00ba.(A.17)\nIf we examine this expression, the only place that the \ud835\udf16occurs is in the additive constant", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fba56e42-afc4-44ce-9a92-8e8beed521c7": {"__data__": {"id_": "fba56e42-afc4-44ce-9a92-8e8beed521c7", "embedding": null, "metadata": {"page_label": "984", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4a264752-044e-40fe-b85f-31e60aad4b47", "node_type": "4", "metadata": {"page_label": "984", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8d8aff2e9d7ed8176c8b5dd329b0b7ce2ebb119857fba47fe886c50d0606b136", "class_name": "RelatedNodeInfo"}}, "text": "984 Mathematics for Deep Learning\n286\u0000\ud835\udc41log\u00b9\ud835\udf16\u00ba. Thisdoesnotdependontheparameters \ud835\udf3datall,sotheoptimalchoiceof \ud835\udf3ddoes\nnot depend on our choice of \ud835\udf16! If we demand four digits or four-hundred, the best choice\nof\ud835\udf3dremains the same, thus we may freely drop the epsilon to see that what we want to\noptimize is\n\u0000\u00d5\n\ud835\udc56log\u00b9\ud835\udc5d\u00b9\ud835\udc65\ud835\udc56j\ud835\udf3d\u00ba\u00ba.(A.18)\nThus, we see that the maximum likelihood point of view can operate with continuous ran-\ndomvariablesaseasilyaswithdiscreteonesbyreplacingtheprobabilitieswithprobability\ndensities.\nA.7.4Summary\n\u000fThe maximum likelihood principle tells us that the best fit model for a given dataset is\nthe one that generates the data with the highest probability.\n\u000fOften people work with the negative log-likelihood instead for a variety of reasons: nu-\nmerical stability, conversion of products to sums (and the resulting simplification of\ngradient computations), and theoretical ties to information theory.\n\u000fWhile simplest to motivate in the discrete setting, it may be freely generalized to the\ncontinuous setting as well by maximizing the probability density assigned to the dat-\napoints.\nA.7.5Exercises\n1.Supposethatyouknowthatanon-negativerandomvariablehasdensity \ud835\udefc\ud835\udc52\u0000\ud835\udefc\ud835\udc65forsome\nvalue\ud835\udefc > 0. You obtain a single observation from the random variable which is the\nnumber 3. What is the maximum likelihood estimate for \ud835\udefc?\n2.Supposethatyouhaveadatasetofsamples f\ud835\udc65\ud835\udc56g\ud835\udc41\n\ud835\udc56=1drawnfromaGaussianwithunknown\nmean, but variance 1. What is the maximum likelihood estimate for the mean?\nDiscussions286.\nA.8Distributions\nNowthat wehavelearned howto workwith probability in both the discrete and the contin-\nuous setting, let\u2019s get to know some of the common distributions encountered. Depending\non the area of machine learning, we may need to be familiar with vastly more of these, or\nfor some areas of deep learning potentially none at all. This is, however, a good basic list\nto be familiar with. Let\u2019s first import some common libraries.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dccbc241-4231-4dc0-a06b-e964458bf1c2": {"__data__": {"id_": "dccbc241-4231-4dc0-a06b-e964458bf1c2", "embedding": null, "metadata": {"page_label": "985", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4ef10fae-1db0-4059-a410-4269445e3742", "node_type": "4", "metadata": {"page_label": "985", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b7e18c6c34c6808a0dff52ba655a4bc671579600cd5e7947f6e65d1eb1e46419", "class_name": "RelatedNodeInfo"}}, "text": "985 Distributions\n%matplotlib inline\nfrom math import erf, factorial\nimport torch\nfrom IPython import display\nfrom d2l import torch asd2l\ntorch .pi=torch .acos(torch .zeros( 1))*2# Define pi in torch\nA.8.1Bernoulli\nThis is the simplest random variable usually encountered. This random variable encodes a\ncoin flip which comes up 1with probability \ud835\udc5dand0with probability 1\u0000\ud835\udc5d. If we have a\nrandom variable \ud835\udc4bwith this distribution, we will write\n\ud835\udc4b\u0018Bernoulli\u00b9\ud835\udc5d\u00ba. (A.1)\nThe cumulative distribution function is\n\ud835\udc39\u00b9\ud835\udc65\u00ba=8>>> <\n>>>:0\ud835\udc65 <0,\n1\u0000\ud835\udc5d0\u0014\ud835\udc65 <1,\n1\ud835\udc65 >=1.(A.2)\nThe probability mass function is plotted below.\np=0.3\nd2l.set_figsize()\nd2l.plt.stem([ 0,1], [ 1-p, p], use_line_collection =True )\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.show()\nNow, let\u2019s plot the cumulative distribution function (A.2).\nx=torch .arange( -1,2,0.01 )\ndef F(x):\nreturn 0ifx<0else 1ifx>1else 1-p\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 902, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6f079f40-afe0-4b65-9866-27f79a152bc1": {"__data__": {"id_": "6f079f40-afe0-4b65-9866-27f79a152bc1", "embedding": null, "metadata": {"page_label": "986", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e8ba577-26c2-482b-8626-160edc2666b8", "node_type": "4", "metadata": {"page_label": "986", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "75e9c15e6b1cc33b6d1cb602fd86a33fd6bcbb4ab5e6c69279de7800917821e9", "class_name": "RelatedNodeInfo"}}, "text": "986 Mathematics for Deep Learning\n(continued from previous page)\nd2l.plot(x, torch .tensor([F(y) for yinx]), 'x','c.d.f. ')\nIf\ud835\udc4b\u0018Bernoulli\u00b9\ud835\udc5d\u00ba, then:\n\u000f\ud835\udf07\ud835\udc4b=\ud835\udc5d,\n\u000f\ud835\udf0e2\n\ud835\udc4b=\ud835\udc5d\u00b91\u0000\ud835\udc5d\u00ba.\nWecansampleanarrayofarbitraryshapefromaBernoullirandomvariableasfollows.\n1*(torch .rand( 10,10)<p)\ntensor([[ 0,1,0,0,1,0,0,0,0,0],\n[0,1,0,0,0,0,1,0,0,0],\n[0,1,0,0,1,0,0,0,0,1],\n[1,0,0,0,0,0,0,0,0,0],\n[0,0,0,1,0,0,1,0,0,1],\n[0,0,0,0,0,0,1,1,0,0],\n[1,1,0,0,1,1,1,1,1,0],\n[1,0,0,0,1,0,1,1,0,0],\n[0,0,0,0,1,0,0,0,0,0],\n[1,0,1,1,1,1,0,1,0,0]])\nA.8.2DiscreteUniform\nThenextcommonlyencounteredrandomvariableisadiscreteuniform. Forourdiscussion\nhere, we will assume that it is supported on the integers f1,2,...,\ud835\udc5bg, however any other\nset of values can be freely chosen. The meaning of the word uniform in this context is that\nevery possible value is equally likely. The probability for each value \ud835\udc562f1,2,3,...,\ud835\udc5bgis\n\ud835\udc5d\ud835\udc56=1\n\ud835\udc5b. We will denote a random variable \ud835\udc4bwith this distribution as\n\ud835\udc4b\u0018\ud835\udc48\u00b9\ud835\udc5b\u00ba. (A.3)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 957, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "baa2fbaf-78de-4a07-8d2c-41b9fd6d144b": {"__data__": {"id_": "baa2fbaf-78de-4a07-8d2c-41b9fd6d144b", "embedding": null, "metadata": {"page_label": "987", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19737886-8029-4703-9ad1-9dbe0c77a189", "node_type": "4", "metadata": {"page_label": "987", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "45e71524d3c371acf3402cba05b6ee5053b501b102ce026f5ea5752ed41bed0b", "class_name": "RelatedNodeInfo"}}, "text": "987 Distributions\nThe cumulative distribution function is\n\ud835\udc39\u00b9\ud835\udc65\u00ba=8>>> <\n>>>:0\ud835\udc65 <1,\n\ud835\udc58\n\ud835\udc5b\ud835\udc58\u0014\ud835\udc65 < \ud835\udc58\u00b81with 1\u0014\ud835\udc58 <\ud835\udc5b,\n1\ud835\udc65 >=\ud835\udc5b.(A.4)\nLet\u2019s first plot the probability mass function.\nn=5\nd2l.plt.stem([i +1for iinrange (n)], n *[1/n], use_line_collection =True )\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.show()\nNow, let\u2019s plot the cumulative distribution function (A.4).\nx=torch .arange( -1,6,0.01 )\ndef F(x):\nreturn 0ifx<1else 1ifx>nelse torch .floor(x) /n\nd2l.plot(x, torch .tensor([F(y) for yinx]), 'x','c.d.f. ')\nIf\ud835\udc4b\u0018\ud835\udc48\u00b9\ud835\udc5b\u00ba, then:\n\u000f\ud835\udf07\ud835\udc4b=1\u00b8\ud835\udc5b\n2,\n\u000f\ud835\udf0e2\n\ud835\udc4b=\ud835\udc5b2\u00001\n12.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "00f4552c-96ed-42f0-9c38-f753ce816a45": {"__data__": {"id_": "00f4552c-96ed-42f0-9c38-f753ce816a45", "embedding": null, "metadata": {"page_label": "988", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fdd10dda-4495-45d7-b626-205c73b0e350", "node_type": "4", "metadata": {"page_label": "988", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8ce1ea7dd3a87a1ff6478a128a22b15a0c9e34aa73400ea4ab260a510b003651", "class_name": "RelatedNodeInfo"}}, "text": "988 Mathematics for Deep Learning\nWe can sample an array of arbitrary shape from a discrete uniform random variable as\nfollows.\ntorch .randint( 1, n, size =(10,10))\ntensor([[ 1,4,3,2,1,1,3,1,1,4],\n[4,1,1,4,4,1,4,3,2,4],\n[2,4,4,1,4,2,4,3,2,1],\n[1,2,3,1,1,4,2,4,1,3],\n[1,2,4,1,4,3,3,2,2,1],\n[1,2,2,4,1,3,2,4,2,3],\n[1,2,3,4,1,3,4,1,4,3],\n[3,1,1,4,4,1,3,1,1,2],\n[2,2,4,3,4,2,3,4,2,4],\n[1,4,3,3,2,3,3,4,1,3]])\nA.8.3Continuous Uniform\nNext, let\u2019s discuss the continuous uniform distribution. The idea behind this random vari-\nable is that if we increase the \ud835\udc5bin the discrete uniform distribution, and then scale it to fit\nwithin the interval \u00bb\ud835\udc4e,\ud835\udc4f\u00bc, we will approach a continuous random variable that just picks\nan arbitrary value in \u00bb\ud835\udc4e,\ud835\udc4f\u00bcall with equal probability. We will denote this distribution\nas\n\ud835\udc4b\u0018\ud835\udc48\u00b9\ud835\udc4e,\ud835\udc4f\u00ba. (A.5)\nThe probability density function is\n\ud835\udc5d\u00b9\ud835\udc65\u00ba=(\n1\n\ud835\udc4f\u0000\ud835\udc4e\ud835\udc652\u00bb\ud835\udc4e,\ud835\udc4f\u00bc,\n0\ud835\udc65\u2209\u00bb\ud835\udc4e,\ud835\udc4f\u00bc.(A.6)\nThe cumulative distribution function is\n\ud835\udc39\u00b9\ud835\udc65\u00ba=8>>> <\n>>>:0\ud835\udc65 <\ud835\udc4e,\n\ud835\udc65\u0000\ud835\udc4e\n\ud835\udc4f\u0000\ud835\udc4e\ud835\udc652\u00bb\ud835\udc4e,\ud835\udc4f\u00bc,\n1\ud835\udc65 >=\ud835\udc4f.(A.7)\nLet\u2019s first plot the probability density function (A.6).\na, b =1,3\nx=torch .arange( 0,4,0.01 )\np=(x>a).type(torch .float32) *(x<b).type(torch .float32) /(b-a)\nd2l.plot(x, p, 'x','p.d.f. ')\nNow, let\u2019s plot the cumulative distribution function (A.7).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "413a6f6b-d2b6-4654-9171-78e0a3075635": {"__data__": {"id_": "413a6f6b-d2b6-4654-9171-78e0a3075635", "embedding": null, "metadata": {"page_label": "989", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3fe14965-f01f-4843-8c32-5de48b389f24", "node_type": "4", "metadata": {"page_label": "989", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "eeeeda2d0d53611ef4ba49b5df2915cfdb57f6c20be7f241301acc2127de09aa", "class_name": "RelatedNodeInfo"}}, "text": "989 Distributions\ndef F(x):\nreturn 0ifx<aelse 1ifx>belse (x-a)/(b-a)\nd2l.plot(x, torch .tensor([F(y) for yinx]), 'x','c.d.f. ')\nIf\ud835\udc4b\u0018\ud835\udc48\u00b9\ud835\udc4e,\ud835\udc4f\u00ba, then:\n\u000f\ud835\udf07\ud835\udc4b=\ud835\udc4e\u00b8\ud835\udc4f\n2,\n\u000f\ud835\udf0e2\n\ud835\udc4b=\u00b9\ud835\udc4f\u0000\ud835\udc4e\u00ba2\n12.\nWecansampleanarrayofarbitraryshapefromauniformrandomvariableasfollows. Note\nthat it by default samples from a \ud835\udc48\u00b90,1\u00ba, so if we want a different range we need to scale\nit.\n(b-a)*torch .rand( 10,10)+a\ntensor([[ 2.4857 ,2.2461 ,1.6809 ,2.7434 ,2.7072 ,2.6190 ,1.4883 ,1.2517 ,1.\n\u21a9!3454 ,\n2.4754 ],\n[1.0974 ,1.5680 ,1.8788 ,2.8231 ,2.1695 ,2.6461 ,1.4914 ,1.4887 ,1.\n\u21a9!3860 ,\n1.9090 ],\n[1.3746 ,1.7773 ,1.2412 ,1.1950 ,2.7281 ,2.8356 ,1.2266 ,2.4724 ,2.\n\u21a9!4641 ,\n2.8991 ],\n[2.4018 ,2.6727 ,1.0308 ,1.1951 ,1.9390 ,1.6486 ,2.8314 ,1.1025 ,1.\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43a6c154-d77c-4bba-a247-a6a1aa983001": {"__data__": {"id_": "43a6c154-d77c-4bba-a247-a6a1aa983001", "embedding": null, "metadata": {"page_label": "990", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "369c3d17-e714-4d75-96df-0da9758a69c0", "node_type": "4", "metadata": {"page_label": "990", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f8cccf234465e1f3439ba38967b061ba53d7b47c96b9fdeeaf123c72e4a07ce8", "class_name": "RelatedNodeInfo"}}, "text": "990 Mathematics for Deep Learning\n(continued from previous page)\n\u21a9!3354 ,\n1.0130 ],\n[1.1281 ,1.8000 ,2.3788 ,2.6580 ,1.6750 ,2.2081 ,1.2705 ,1.0757 ,2.\n\u21a9!3311 ,\n2.6557 ],\n[2.9912 ,1.2263 ,1.8115 ,1.5940 ,1.9321 ,1.6469 ,2.2990 ,2.1473 ,1.\n\u21a9!8165 ,\n1.2806 ],\n[1.1672 ,1.1536 ,1.9649 ,2.1655 ,1.7170 ,1.0284 ,1.3305 ,2.1904 ,1.\n\u21a9!4036 ,\n2.1958 ],\n[2.5891 ,2.5840 ,2.2679 ,2.0687 ,2.9249 ,1.6741 ,1.2238 ,2.4463 ,2.\n\u21a9!2235 ,\n2.7038 ],\n[1.8697 ,2.4965 ,1.5785 ,2.7890 ,2.3319 ,2.1434 ,2.3333 ,1.0286 ,1.\n\u21a9!9245 ,\n1.7640 ],\n[1.2504 ,1.7558 ,1.4322 ,1.5226 ,1.3380 ,1.1388 ,1.8707 ,2.2330 ,2.\n\u21a9!3818 ,\n2.2087 ]])\nA.8.4Binomial\nLet\u2019s make things a little more complex and examine the binomial random variable. This\nrandomvariableoriginatesfromperformingasequenceof \ud835\udc5bindependentexperiments,each\nof which has probability \ud835\udc5dof succeeding, and asking how many successes we expect to\nsee.\nLet\u2019s express this mathematically. Each experiment is an independent random variable \ud835\udc4b\ud835\udc56\nwherewewilluse 1toencodesuccess,and 0toencodefailure. Sinceeachisanindependent\ncoin flip which is successful with probability \ud835\udc5d, we can say that \ud835\udc4b\ud835\udc56\u0018Bernoulli\u00b9\ud835\udc5d\u00ba. Then,\nthe binomial random variable is\n\ud835\udc4b=\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc4b\ud835\udc56. (A.8)\nIn this case, we will write\n\ud835\udc4b\u0018Binomial\u00b9\ud835\udc5b,\ud835\udc5d\u00ba. (A.9)\nTo get the cumulative distribution function, we need to notice that getting exactly \ud835\udc58suc-\ncesses can occur in\u0000\ud835\udc5b\n\ud835\udc58\u0001=\ud835\udc5b!\n\ud835\udc58!\u00b9\ud835\udc5b\u0000\ud835\udc58\u00ba!ways each of which has a probability of \ud835\udc5d\ud835\udc58\u00b91\u0000\ud835\udc5d\u00ba\ud835\udc5b\u0000\ud835\udc58\nof occurring. Thus the cumulative distribution function is\n\ud835\udc39\u00b9\ud835\udc65\u00ba=8>>> <\n>>>:0 \ud835\udc65 <0,\n\u00cd\n\ud835\udc5a\u0014\ud835\udc58\u0000\ud835\udc5b\n\ud835\udc5a\u0001\ud835\udc5d\ud835\udc5a\u00b91\u0000\ud835\udc5d\u00ba\ud835\udc5b\u0000\ud835\udc5a\ud835\udc58\u0014\ud835\udc65 < \ud835\udc58\u00b81with 0\u0014\ud835\udc58 <\ud835\udc5b,\n1 \ud835\udc65 >=\ud835\udc5b.(A.10)\nLet\u2019s first plot the probability mass function.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1597, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94cb6485-2b67-4601-bba9-7adb37ae7962": {"__data__": {"id_": "94cb6485-2b67-4601-bba9-7adb37ae7962", "embedding": null, "metadata": {"page_label": "991", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "455ce185-23a1-4116-8a54-50d7c7a90082", "node_type": "4", "metadata": {"page_label": "991", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "974c1b52b59d512508258dae444377c7df9c4401c7eab0991993f912fd69b153", "class_name": "RelatedNodeInfo"}}, "text": "991 Distributions\nn, p =10,0.2\n# Compute binomial coefficient\ndef binom (n, k):\ncomb =1\nfor iinrange (min(k, n -k)):\ncomb =comb *(n-i)//(i+1)\nreturn comb\npmf =torch .tensor([p **i*(1-p)**(n-i)*binom(n, i) for iinrange (n+1)])\nd2l.plt.stem([i for iinrange (n+1)], pmf, use_line_collection =True )\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.show()\nNow, let\u2019s plot the cumulative distribution function (A.10 ).\nx=torch .arange( -1,11,0.01 )\ncmf =torch .cumsum(pmf, dim =0)\ndef F(x):\nreturn 0ifx<0else 1ifx>nelse cmf[ int(x)]\nd2l.plot(x, torch .tensor([F(y) for yinx.tolist()]), 'x','c.d.f. ')\nIf\ud835\udc4b\u0018Binomial\u00b9\ud835\udc5b,\ud835\udc5d\u00ba, then:\n\u000f\ud835\udf07\ud835\udc4b=\ud835\udc5b\ud835\udc5d,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c5da40fb-97b7-4128-96ff-c568415710a3": {"__data__": {"id_": "c5da40fb-97b7-4128-96ff-c568415710a3", "embedding": null, "metadata": {"page_label": "992", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2411fb60-fa01-4b22-afeb-bfb127e8e749", "node_type": "4", "metadata": {"page_label": "992", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dab1c0f373eeca38be5ea29768fb650cb75f7e9f6ce5db544450342c16bddb42", "class_name": "RelatedNodeInfo"}}, "text": "992 Mathematics for Deep Learning\n\u000f\ud835\udf0e2\n\ud835\udc4b=\ud835\udc5b\ud835\udc5d\u00b91\u0000\ud835\udc5d\u00ba.\nThis follows from the linearity of expected value over the sum of \ud835\udc5bBernoulli random vari-\nables, and the fact that the variance of the sum of independent random variables is the sum\nof the variances. This can be sampled as follows.\nm=torch .distributions .binomial .Binomial(n, p)\nm.sample(sample_shape =(10,10))\ntensor([[ 6.,3.,4.,3.,3.,1.,3.,3.,3.,3.],\n[3.,1.,2.,2.,3.,2.,1.,3.,1.,4.],\n[6.,1.,0.,3.,0.,3.,1.,0.,1.,1.],\n[1.,2.,3.,1.,2.,2.,2.,2.,3.,2.],\n[2.,2.,5.,4.,1.,3.,4.,3.,2.,0.],\n[2.,0.,2.,2.,3.,1.,1.,4.,3.,1.],\n[1.,1.,3.,2.,4.,2.,2.,2.,1.,0.],\n[0.,3.,2.,1.,1.,3.,2.,1.,1.,3.],\n[2.,3.,2.,3.,4.,3.,1.,2.,1.,2.],\n[1.,2.,1.,1.,3.,2.,4.,3.,3.,2.]])\nA.8.5Poisson\nLet\u2019s now perform a thought experiment. We are standing at a bus stop and we want to\nknow how many buses will arrive in the next minute. Let\u2019s start by considering \ud835\udc4b\u00b91\u00ba\u0018\nBernoulli\u00b9\ud835\udc5d\u00bawhich is simply the probability that a bus arrives in the one minute window.\nForbusstopsfarfromanurbancenter, thismightbeaprettygoodapproximation. Wemay\nnever see more than one bus in a minute.\nHowever, if we are in a busy area, it is possible or even likely that two buses will arrive.\nWe can model this by splitting our random variable into two parts for the first 30 seconds,\nor the second 30 seconds. In this case we can write\n\ud835\udc4b\u00b92\u00ba\u0018\ud835\udc4b\u00b92\u00ba\n1\u00b8\ud835\udc4b\u00b92\u00ba\n2, (A.11)\nwhere\ud835\udc4b\u00b92\u00bais the total sum, and \ud835\udc4b\u00b92\u00ba\n\ud835\udc56\u0018Bernoulli\u00b9\ud835\udc5d\u009d2\u00ba. The total distribution is then\n\ud835\udc4b\u00b92\u00ba\u0018Binomial\u00b92,\ud835\udc5d\u009d2\u00ba.\nWhy stop here? Let\u2019s continue to split that minute into \ud835\udc5bparts. By the same reasoning as\nabove, we see that\n\ud835\udc4b\u00b9\ud835\udc5b\u00ba\u0018Binomial\u00b9\ud835\udc5b,\ud835\udc5d\u009d\ud835\udc5b\u00ba. (A.12)\nConsider these random variables. By the previous section, we know that (A.12 )has mean\n\ud835\udf07\ud835\udc4b\u00b9\ud835\udc5b\u00ba=\ud835\udc5b\u00b9\ud835\udc5d\u009d\ud835\udc5b\u00ba=\ud835\udc5d, and variance \ud835\udf0e2\n\ud835\udc4b\u00b9\ud835\udc5b\u00ba=\ud835\udc5b\u00b9\ud835\udc5d\u009d\ud835\udc5b\u00ba\u00b91\u0000\u00b9\ud835\udc5d\u009d\ud835\udc5b\u00ba\u00ba=\ud835\udc5d\u00b91\u0000\ud835\udc5d\u009d\ud835\udc5b\u00ba. If we take\n\ud835\udc5b!1,wecanseethatthesenumbersstabilizeto \ud835\udf07\ud835\udc4b\u00b91\u00ba=\ud835\udc5d,andvariance \ud835\udf0e2\n\ud835\udc4b\u00b91\u00ba=\ud835\udc5d. This\nindicatesthatthere couldbe somerandomvariablewecandefineinthisinfinitesubdivision\nlimit.\nThis should not come as too much of a surprise, since in the real world we can just count", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2014, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c96a7c2e-19e3-4dbb-b068-68e7c45c2c44": {"__data__": {"id_": "c96a7c2e-19e3-4dbb-b068-68e7c45c2c44", "embedding": null, "metadata": {"page_label": "993", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e7f2396-0cac-40bd-a590-95885dd01ad1", "node_type": "4", "metadata": {"page_label": "993", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ff6894d3c14b0fdc1b1dd69e882221494b2f30f30b7b592b6dc414a1b2a1140a", "class_name": "RelatedNodeInfo"}}, "text": "993 Distributions\nthe number of bus arrivals, however it is nice to see that our mathematical model is well\ndefined. This discussion can be made formal as the lawof rareevents .\nFollowing through this reasoning carefully, we can arrive at the following model. We will\nsay that\ud835\udc4b\u0018Poisson\u00b9\ud835\udf06\u00baif it is a random variable which takes the values f0,1,2,...gwith\nprobability\n\ud835\udc5d\ud835\udc58=\ud835\udf06\ud835\udc58\ud835\udc52\u0000\ud835\udf06\n\ud835\udc58!. (A.13)\nThe value\ud835\udf06 > 0is known as the rate(or theshapeparameter), and denotes the average\nnumber of arrivals we expect in one unit of time.\nWemaysumthisprobabilitymassfunctiontogetthecumulativedistributionfunction.\n\ud835\udc39\u00b9\ud835\udc65\u00ba=(\n0 \ud835\udc65 <0,\n\ud835\udc52\u0000\ud835\udf06\u00cd\ud835\udc58\n\ud835\udc5a=0\ud835\udf06\ud835\udc5a\n\ud835\udc5a!\ud835\udc58\u0014\ud835\udc65 < \ud835\udc58\u00b81with 0\u0014\ud835\udc58.(A.14)\nLet\u2019s first plot the probability mass function (A.13 ).\nlam =5.0\nxs=[ifor iinrange (20)]\npmf =torch .tensor([torch .exp(torch .tensor( -lam)) *lam**k\n/factorial(k) for kinxs])\nd2l.plt.stem(xs, pmf, use_line_collection =True )\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.show()\nNow, let\u2019s plot the cumulative distribution function (A.14 ).\nx=torch .arange( -1,21,0.01 )\ncmf =torch .cumsum(pmf, dim =0)\ndef F(x):\nreturn 0ifx<0else 1ifx>nelse cmf[ int(x)]\nd2l.plot(x, torch .tensor([F(y) for yinx.tolist()]), 'x','c.d.f. ')\nAs we saw above, the means and variances are particularly concise. If \ud835\udc4b\u0018Poisson\u00b9\ud835\udf06\u00ba,\nthen:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1269, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b90c7bf-fae1-47b2-bf1c-25c78c3999ee": {"__data__": {"id_": "9b90c7bf-fae1-47b2-bf1c-25c78c3999ee", "embedding": null, "metadata": {"page_label": "994", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24aeb13b-8341-4cb5-86a3-2367cf7d611a", "node_type": "4", "metadata": {"page_label": "994", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "aad8be9a827925ffccba733c741a6b5c706d8a40da3f390403676deab9a98e7f", "class_name": "RelatedNodeInfo"}}, "text": "994 Mathematics for Deep Learning\n\u000f\ud835\udf07\ud835\udc4b=\ud835\udf06,\n\u000f\ud835\udf0e2\n\ud835\udc4b=\ud835\udf06.\nThis can be sampled as follows.\nm=torch .distributions .poisson .Poisson(lam)\nm.sample(( 10,10))\ntensor([[ 1.,4.,6.,8.,4.,4.,4.,7.,6.,4.],\n[3.,6.,7.,7.,5.,7.,7.,3.,5.,4.],\n[4.,1.,3.,3.,10.,5.,5.,3.,7.,5.],\n[4.,3.,4.,10.,8.,6.,4.,6.,5.,5.],\n[5.,11.,1.,5.,7.,5.,2.,4.,3.,5.],\n[6.,6.,4.,4.,3.,1.,5.,8.,4.,5.],\n[2.,9.,7.,2.,6.,5.,2.,8.,6.,10.],\n[1.,4.,3.,7.,3.,1.,7.,5.,3.,6.],\n[5.,4.,6.,4.,9.,8.,3.,3.,1.,8.],\n[3.,12.,9.,13.,2.,14.,3.,2.,0.,3.]])\nA.8.6Gaussian\nNow Let\u2019s try a different, but related experiment. Let\u2019s say we again are performing \ud835\udc5b\nindependentBernoulli \u00b9\ud835\udc5d\u00bameasurements \ud835\udc4b\ud835\udc56. Thedistributionofthesumoftheseis \ud835\udc4b\u00b9\ud835\udc5b\u00ba\u0018\nBinomial\u00b9\ud835\udc5b,\ud835\udc5d\u00ba. Rather than taking a limit as \ud835\udc5bincreases and \ud835\udc5ddecreases, Let\u2019s fix \ud835\udc5d, and\nthen send\ud835\udc5b!1. In this case \ud835\udf07\ud835\udc4b\u00b9\ud835\udc5b\u00ba=\ud835\udc5b\ud835\udc5d!1and\ud835\udf0e2\n\ud835\udc4b\u00b9\ud835\udc5b\u00ba=\ud835\udc5b\ud835\udc5d\u00b91\u0000\ud835\udc5d\u00ba!1, so there is\nno reason to think this limit should be well defined.\nHowever, not all hope is lost! Let\u2019s just make the mean and variance be well behaved by\ndefining\n\ud835\udc4c\u00b9\ud835\udc5b\u00ba=\ud835\udc4b\u00b9\ud835\udc5b\u00ba\u0000\ud835\udf07\ud835\udc4b\u00b9\ud835\udc5b\u00ba\n\ud835\udf0e\ud835\udc4b\u00b9\ud835\udc5b\u00ba. (A.15)\nThis can be seen to have mean zero and variance one, and so it is plausible to believe that\nit will converge to some limiting distribution. If we plot what these distributions look like,\nwe will become even more convinced that it will work.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7ca21c7b-703b-4f03-8532-5e5f75773b4f": {"__data__": {"id_": "7ca21c7b-703b-4f03-8532-5e5f75773b4f", "embedding": null, "metadata": {"page_label": "995", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "030e0f32-38fe-48b1-8cfc-88be95ab76fa", "node_type": "4", "metadata": {"page_label": "995", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1e0411a36d2a447ca462fbeb35678d6c614b24caa1046f053334875ac5d07bc7", "class_name": "RelatedNodeInfo"}}, "text": "995 Distributions\np=0.2\nns=[1,10,100,1000 ]\nd2l.plt.figure(figsize =(10,3))\nfor iinrange (4):\nn=ns[i]\npmf =torch .tensor([p **i*(1-p)**(n-i)*binom(n, i)\nfor iinrange (n+1)])\nd2l.plt.subplot( 1,4, i +1)\nd2l.plt.stem([(i -n*p)/torch .sqrt(torch .tensor(n *p*(1-p)))\nfor iinrange (n+1)], pmf,\nuse_line_collection =True )\nd2l.plt.xlim([ -4,4])\nd2l.plt.xlabel( 'x')\nd2l.plt.ylabel( 'p.m.f. ')\nd2l.plt.title( \"n = {}\".format(n))\nd2l.plt.show()\nOne thing to note: compared to the Poisson case, we are now dividing by the standard de-\nviation which means that we are squeezing the possible outcomes into smaller and smaller\nareas. This is an indication that our limit will no longer be discrete, but rather continu-\nous.\nA derivation of what occurs is beyond the scope of this document, but the central limit\ntheorem states that as \ud835\udc5b!1, this will yield the Gaussian Distribution (or sometimes\nnormal distribution). More explicitly, for any \ud835\udc4e,\ud835\udc4f:\nlim\n\ud835\udc5b!1\ud835\udc43\u00b9\ud835\udc4c\u00b9\ud835\udc5b\u00ba2\u00bb\ud835\udc4e,\ud835\udc4f\u00bc\u00ba=\ud835\udc43\u00b9N\u00b90,1\u00ba2\u00bb\ud835\udc4e,\ud835\udc4f\u00bc\u00ba, (A.16)\nwhere we say a random variable is normally distributed with given mean \ud835\udf07and variance\n\ud835\udf0e2, written\ud835\udc4b\u0018N\u00b9\ud835\udf07,\ud835\udf0e2\u00baif\ud835\udc4bhas density\n\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba=1p\n2\ud835\udf0b\ud835\udf0e2\ud835\udc52\u0000\u00b9\ud835\udc65\u0000\ud835\udf07\u00ba2\n2\ud835\udf0e2. (A.17)\nLet\u2019s first plot the probability density function (A.17 ).\nmu, sigma =0,1\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2656542d-47c2-4360-a265-2c4a79b61074": {"__data__": {"id_": "2656542d-47c2-4360-a265-2c4a79b61074", "embedding": null, "metadata": {"page_label": "996", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2de137e7-5a91-4e95-bc4a-fc3825196fe7", "node_type": "4", "metadata": {"page_label": "996", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "559ee142e0924181bcd0f61513fcef58a7d2e48a278c266b188999721f12b7f1", "class_name": "RelatedNodeInfo"}}, "text": "996 Mathematics for Deep Learning\n(continued from previous page)\nx=torch .arange( -3,3,0.01 )\np=1/torch .sqrt( 2*torch .pi*sigma **2)*torch .exp(\n-(x-mu)**2/(2*sigma **2))\nd2l.plot(x, p, 'x','p.d.f. ')\nNow, let\u2019s plot the cumulative distribution function. It is beyond the scope of this ap-\npendix, but the Gaussian c.d.f. does not have a closed-form formula in terms of more\nelementary functions. We will use erfwhich provides a way to compute this integral nu-\nmerically.\ndef phi(x):\nreturn (1.0 +erf((x -mu) /(sigma *torch .sqrt(torch .tensor( 2.))))) /2.0\nd2l.plot(x, torch .tensor([phi(y) for yinx.tolist()]), 'x','c.d.f. ')\nKeen-eyedreaderswillrecognizesomeoftheseterms. Indeed,weencounteredthisintegral\ninSection A.5 . Indeed we need exactly that computation to see that this \ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00bahas total\narea one and is thus a valid density.\nOur choice of working with coin flips made computations shorter, but nothing about that\nchoice was fundamental. Indeed, if we take any collection of independent identically dis-\ntributed random variables \ud835\udc4b\ud835\udc56, and form\n\ud835\udc4b\u00b9\ud835\udc41\u00ba=\ud835\udc41\u00d5\n\ud835\udc56=1\ud835\udc4b\ud835\udc56. (A.18)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1074, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "312d428a-4940-4c60-917e-67ddb87cd1db": {"__data__": {"id_": "312d428a-4940-4c60-917e-67ddb87cd1db", "embedding": null, "metadata": {"page_label": "997", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31ef0ef6-9099-4b04-9d31-09356ddf80c4", "node_type": "4", "metadata": {"page_label": "997", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "750414a45f6cedb79766c3cdbd0dc149ee522105d5141b89e3f833f0e6801328", "class_name": "RelatedNodeInfo"}}, "text": "997 Distributions\nThen\n\ud835\udc4b\u00b9\ud835\udc41\u00ba\u0000\ud835\udf07\ud835\udc4b\u00b9\ud835\udc41\u00ba\n\ud835\udf0e\ud835\udc4b\u00b9\ud835\udc41\u00ba(A.19)\nwillbeapproximatelyGaussian. Thereareadditionalrequirementsneededtomakeitwork,\nmost commonly \ud835\udc38\u00bb\ud835\udc4b4\u00bc<1, but the philosophy is clear.\nThe central limit theorem is the reason why the Gaussian is fundamental to probability,\nstatistics, and machine learning. Whenever we can say that something we measured is a\nsumofmanysmallindependentcontributions,wecanassumethatthethingbeingmeasured\nwill be close to Gaussian.\nTherearemanymorefascinatingpropertiesofGaussians,andwewouldliketodiscussone\nmore here. The Gaussian is what is known as a maximum entropy distribution . We will\nget into entropy more deeply in Section A.11 , however all we need to know at this point\nis that it is a measure of randomness. In a rigorous mathematical sense, we can think of\nthe Gaussian as the mostrandom choice of random variable with fixed mean and variance.\nThus, if we know that our random variable has some mean and variance, the Gaussian is in\na sense the most conservative choice of distribution we can make.\nTo close the section, let\u2019s recall that if \ud835\udc4b\u0018N\u00b9\ud835\udf07,\ud835\udf0e2\u00ba, then:\n\u000f\ud835\udf07\ud835\udc4b=\ud835\udf07,\n\u000f\ud835\udf0e2\n\ud835\udc4b=\ud835\udf0e2.\nWecansamplefromtheGaussian(orstandardnormal)distributionasshownbelow.\ntorch .normal(mu, sigma, size =(10,10))\ntensor([[ 1.3588 ,0.0473 ,-1.5805 ,-0.0108 ,0.4253 ,0.7924 ,-0.6547 ,0.\n\u21a9!7313 ,\n-0.3038 ,1.1935 ],\n[0.0089 ,0.8951 ,1.0055 ,0.0956 ,-1.1109 ,-0.6342 ,1.6772 ,1.\n\u21a9!0314 ,\n0.3819 ,-1.7822 ],\n[-0.0604 ,-1.0318 ,0.9113 ,1.3118 ,-1.8370 ,-0.9023 ,1.0365 ,0.\n\u21a9!9052 ,\n-0.6411 ,-0.8949 ],\n[-0.1713 ,-0.2347 ,0.0767 ,-0.6375 ,-0.4612 ,-1.6875 ,-0.1570 ,1.\n\u21a9!0591 ,\n0.8377 ,0.5097 ],\n[0.2762 ,-0.6213 ,-0.3422 ,0.9449 ,-0.7544 ,-0.2150 ,1.0240 ,1.\n\u21a9!0253 ,\n-0.9182 ,1.1536 ],\n[0.0614 ,0.2758 ,-0.3610 ,-1.0577 ,-0.5513 ,-0.9158 ,0.7539 ,0.\n\u21a9!9204 ,\n-0.5908 ,0.9113 ],\n[1.6190 ,-0.9213 ,-0.7944 ,-2.2621 ,0.5826 ,-1.8287 ,1.4097 ,-0.\n\u21a9!5744 ,\n-0.0668 ,1.2074 ],\n[-0.0624 ,0.1928 ,1.3002 ,0.6756 ,1.1590 ,1.0144 ,1.1840 ,-0.\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1790231f-52d4-4bd3-a063-0cd2707dec6b": {"__data__": {"id_": "1790231f-52d4-4bd3-a063-0cd2707dec6b", "embedding": null, "metadata": {"page_label": "998", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ca373e6-9500-4c0f-bc2c-c7c376545751", "node_type": "4", "metadata": {"page_label": "998", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "38bffad8a367c358bc986dbdff9486ced628ebda75120b1afd9239b50f4654eb", "class_name": "RelatedNodeInfo"}}, "text": "998 Mathematics for Deep Learning\n(continued from previous page)\n\u21a9!5010 ,\n0.6026 ,-0.7722 ],\n[-2.0148 ,0.6958 ,0.9940 ,0.8477 ,1.0957 ,-0.5253 ,0.2353 ,-0.\n\u21a9!2663 ,\n1.2275 ,0.5993 ],\n[0.4651 ,-0.8218 ,-0.5441 ,-2.0338 ,-0.6930 ,-0.0674 ,-0.4448 ,-0.\n\u21a9!8397 ,\n0.0360 ,-0.7089 ]])\nA.8.7ExponentialFamily\nOne shared property for all the distributions listed above is that they all belong to which\nis known as the exponential family . The exponential family is a set of distributions whose\ndensity can be expressed in the following form:\n\ud835\udc5d\u00b9xj\ud835\udf3c\u00ba=\u210e\u00b9x\u00ba\u0001exp\u0000\ud835\udf3c>\u0001\ud835\udc47\u00b9x\u00ba\u0000\ud835\udc34\u00b9\ud835\udf3c\u00ba\u0001(A.20)\nAs this definition can be a little subtle, let\u2019s examine it closely.\nFirst,\u210e\u00b9x\u00bais known as the underlying measure or thebase measure . This can be viewed\nas an original choice of measure we are modifying with our exponential weight.\nSecond, we have the vector \ud835\udf3c=\u00b9\ud835\udf021,\ud835\udf022,...,\ud835\udf02\ud835\udc59\u00ba 2R\ud835\udc59called the natural parameters or\ncanonical parameters . These define how the base measure will be modified. The natural\nparametersenterintothenewmeasurebytakingthedotproductoftheseparametersagainst\nsome function \ud835\udc47\u00b9\u0001\u00baofx=\u00b9\ud835\udc651,\ud835\udc652,...,\ud835\udc65\ud835\udc5b\u00ba 2R\ud835\udc5band exponentiated. The vector \ud835\udc47\u00b9x\u00ba=\n\u00b9\ud835\udc471\u00b9x\u00ba,\ud835\udc472\u00b9x\u00ba,...,\ud835\udc47\ud835\udc59\u00b9x\u00ba\u00bais called the su\ufb00icientstatistics for\ud835\udf3c. This name is used since the\ninformation represented by \ud835\udc47\u00b9x\u00bais sufficient to calculate the probability density and no\nother information from the sample x\u2019s are required.\nThird, we have \ud835\udc34\u00b9\ud835\udf3c\u00ba, which is referred to as the cumulantfunction , which ensures that the\nabove distribution (A.20 )integrates to one, i.e.,\n\ud835\udc34\u00b9\ud835\udf3c\u00ba=log\u0014\u00b9\n\u210e\u00b9x\u00ba\u0001exp\u0000\ud835\udf3c>\u0001\ud835\udc47\u00b9x\u00ba\u0001\ud835\udc51x\u0015\n. (A.21)\nTo be concrete, let\u2019s consider the Gaussian. Assuming that xis an univariate variable, we\nsaw that it had a density of\n\ud835\udc5d\u00b9\ud835\udc65j\ud835\udf07,\ud835\udf0e\u00ba=1p\n2\ud835\udf0b\ud835\udf0e2\u0001exp\u001a\u0000\u00b9\ud835\udc65\u0000\ud835\udf07\u00ba2\n2\ud835\udf0e2\u001b\n=1p\n2\ud835\udf0b\u0001exp\u001a\ud835\udf07\n\ud835\udf0e2\ud835\udc65\u00001\n2\ud835\udf0e2\ud835\udc652\u0000\u00121\n2\ud835\udf0e2\ud835\udf072\u00b8log\u00b9\ud835\udf0e\u00ba\u0013\u001b\n.(A.22)\nThis matches the definition of the exponential family with:\n\u000funderlyingmeasure :\u210e\u00b9\ud835\udc65\u00ba=1p\n2\ud835\udf0b,\n\u000fnaturalparameters :\ud835\udf3c=\u0014\ud835\udf021\n\ud835\udf022\u0015\n=\u0014\ud835\udf07\n\ud835\udf0e2\n1\n2\ud835\udf0e2\u0015\n,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1880, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "65375221-f862-48ef-86da-ec0ef4d2f3c4": {"__data__": {"id_": "65375221-f862-48ef-86da-ec0ef4d2f3c4", "embedding": null, "metadata": {"page_label": "999", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1f14d2b8-2692-425e-a7af-117aa00dc560", "node_type": "4", "metadata": {"page_label": "999", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ff3c6bcffba9e92befe699d21b4e27af99f0a868b014a5ef4004acd532f5fc73", "class_name": "RelatedNodeInfo"}}, "text": "999 Naive Bayes\n287\u000fsu\ufb00icient statistics :\ud835\udc47\u00b9\ud835\udc65\u00ba=\u0014\ud835\udc65\n\u0000\ud835\udc652\u0015\n, and\n\u000fcumulant function :\ud835\udc34\u00b9\ud835\udf3c\u00ba=1\n2\ud835\udf0e2\ud835\udf072\u00b8log\u00b9\ud835\udf0e\u00ba=\ud835\udf022\n1\n4\ud835\udf022\u00001\n2log\u00b92\ud835\udf022\u00ba.\nItisworthnotingthattheexactchoiceofeachofabovetermsissomewhatarbitrary. Indeed,\ntheimportantfeatureisthatthedistributioncanbeexpressedinthisform,nottheexactform\nitself.\nAs we allude to in Section 4.1.2 , a widely used technique is to assume that the final output\nyfollows an exponential family distribution. The exponential family is a common and\npowerful family of distributions encountered frequently in machine learning.\nA.8.8Summary\n\u000fBernoulli random variables can be used to model events with a yes/no outcome.\n\u000fDiscrete uniform distributions model selects from a finite set of possibilities.\n\u000fContinuous uniform distributions select from an interval.\n\u000fBinomialdistributionsmodelaseriesofBernoullirandomvariables,andcountthenum-\nber of successes.\n\u000fPoisson random variables model the arrival of rare events.\n\u000fGaussian random variables model the result of adding a large number of independent\nrandom variables together.\n\u000fAll the above distributions belong to exponential family.\nA.8.9Exercises\n1.What is the standard deviation of a random variable that is the difference \ud835\udc4b\u0000\ud835\udc4cof two\nindependent binomial random variables \ud835\udc4b,\ud835\udc4c\u0018Binomial\u00b916,1\u009d2\u00ba.\n2.If we take a Poisson random variable \ud835\udc4b\u0018Poisson\u00b9\ud835\udf06\u00baand consider\u00b9\ud835\udc4b\u0000\ud835\udf06\u00ba\u009dp\n\ud835\udf06as\n\ud835\udf06!1, we can show that this becomes approximately Gaussian. Why does this make\nsense?\n3.Whatistheprobabilitymassfunctionforasumoftwodiscreteuniformrandomvariables\non\ud835\udc5belements?\nDiscussions287.\nA.9NaiveBayes\nThroughout the previous sections, we learned about the theory of probability and random\nvariables. To put this theory to work, let\u2019s introduce the naive Bayes classifier. This", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb89a527-8106-4162-9e15-0999c469b114": {"__data__": {"id_": "eb89a527-8106-4162-9e15-0999c469b114", "embedding": null, "metadata": {"page_label": "1000", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9d95332-2e83-4b70-b3ef-13d46c8da8bc", "node_type": "4", "metadata": {"page_label": "1000", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b93167433a0e43274a9622abe57243fd5ebe9720bbfb23a6b5ac642745eb6818", "class_name": "RelatedNodeInfo"}}, "text": "1000 Mathematics for Deep Learning\nuses nothing but probabilistic fundamentals to allow us to perform classification of dig-\nits.\nLearningisallaboutmakingassumptions. Ifwewanttoclassifyanewdataexamplethatwe\nhave never seen before we have to make some assumptions about which data examples are\nsimilartoeachother. ThenaiveBayesclassifier,apopularandremarkablyclearalgorithm,\nassumes all features are independent from each other to simplify the computation. In this\nsection, we will apply this model to recognize characters in images.\n%matplotlib inline\nimport math\nimport torch\nimport torchvision\nfrom d2l import torch asd2l\nd2l.use_svg_display()\nA.9.1OpticalCharacter Recognition\nMNIST ( LeCunet al., 1998) is one of widely used datasets. It contains 60,000 images for\ntraining and 10,000 images for validation. Each image contains a handwritten digit from 0\nto 9. The task is classifying each image into the corresponding digit.\nGluon provides a MNISTclass in the data.vision module to automatically retrieve the\ndatasetfromtheInternet. Subsequently,Gluonwillusethealready-downloadedlocalcopy.\nWe specify whether we are requesting the training set or the test set by setting the value of\nthe parameter traintoTrueorFalse, respectively. Each image is a grayscale image with\nboth width and height of 28with shape ( 28,28,1). We use a customized transformation to\nremove the last channel dimension. In addition, the dataset represents each pixel by an un-\nsigned 8-bitinteger. Wequantizethemintobinaryfeaturestosimplifytheproblem.\ndata_transform =torchvision .transforms .Compose([\ntorchvision .transforms .ToTensor(),\nlambda x: torch .floor(x *255 /128).squeeze(dim =0)\n])\nmnist_train =torchvision .datasets .MNIST(\nroot ='./temp ', train =True , transform =data_transform, download =True )\nmnist_test =torchvision .datasets .MNIST(\nroot ='./temp ', train =False , transform =data_transform, download =True )\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./\n\u21a9!temp/MNIST/raw/train-images-idx3-ubyte.gz\n100%|\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff| 9912422/9912422 [00:00<00:00, 115752065.81it/s]\nExtracting ./temp/MNIST/raw/train-images-idx3-ubyte.gz to ./temp/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ae78e98-da02-48b0-b925-7a69c9afc861": {"__data__": {"id_": "0ae78e98-da02-48b0-b925-7a69c9afc861", "embedding": null, "metadata": {"page_label": "1001", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f78015cb-5822-4aba-9634-dc6813c5f4f2", "node_type": "4", "metadata": {"page_label": "1001", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bccc8690f2962d0d2ac5ccccda122d686c44328d278ef670f1ee1abd67920d5f", "class_name": "RelatedNodeInfo"}}, "text": "1001 Naive Bayes\n(continued from previous page)\n\u21a9!temp/MNIST/raw/train-labels-idx1-ubyte.gz\n100%|\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff| 28881/28881 [00:00<00:00, 5234904.66it/s]\nExtracting ./temp/MNIST/raw/train-labels-idx1-ubyte.gz to ./temp/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./\n\u21a9!temp/MNIST/raw/t10k-images-idx3-ubyte.gz\n100%|\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff| 1648877/1648877 [00:00<00:00, 43715298.68it/s]Extracting ./\n\u21a9!temp/MNIST/raw/t10k-images-idx3-ubyte.gz to ./temp/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./\n\u21a9!temp/MNIST/raw/t10k-labels-idx1-ubyte.gz\n100%|\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff\uffff| 4542/4542 [00:00<00:00, 21501725.47it/s]\nExtracting ./temp/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./temp/MNIST/raw\nWe can access a particular example, which contains the image and the corresponding la-\nbel.\nimage, label =mnist_train[ 2]\nimage .shape, label\n(torch .Size([ 28,28]), 4)\nOur example, stored here in the variable image, corresponds to an image with a height and\nwidth of 28pixels.\nimage .shape, image .dtype\n(torch .Size([ 28,28]), torch .float32)\nOur code stores the label of each image as a scalar. Its type is a 32-bit integer.\nlabel, type (label)\n(4,int)\nWe can also access multiple examples at the same time.\nimages =torch .stack([mnist_train[i][ 0]for iinrange (10,38)], dim =0)\nlabels =torch .tensor([mnist_train[i][ 1]for iinrange (10,38)])\nimages .shape, labels .shape", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d9059c9-f2a9-4b96-8c99-fb7ea6fa04bb": {"__data__": {"id_": "6d9059c9-f2a9-4b96-8c99-fb7ea6fa04bb", "embedding": null, "metadata": {"page_label": "1002", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "88ace39e-f3cb-446f-9cf1-629e1172ce31", "node_type": "4", "metadata": {"page_label": "1002", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4b75ac2db30c4e603728b91e6ae6755d59ad72b651ce61185f72881996234d41", "class_name": "RelatedNodeInfo"}}, "text": "1002 Mathematics for Deep Learning\n(torch .Size([ 28,28,28]), torch .Size([ 28]))\nLet\u2019s visualize these examples.\nd2l.show_images(images, 2,9);\nA.9.2TheProbabilisticModel forClassification\nIn a classification task, we map an example into a category. Here an example is a grayscale\n28\u000228image, and a category is a digit. (Refer to Section 4.1 for a more detailed expla-\nnation.) One natural way to express the classification task is via the probabilistic question:\nwhat is the most likely label given the features (i.e., image pixels)? Denote by x2R\ud835\udc51the\nfeatures of the example and \ud835\udc662Rthe label. Here features are image pixels, where we can\nreshape a 2-dimensional image to a vector so that \ud835\udc51=282=784, and labels are digits.\nThe probability of the label given the features is \ud835\udc5d\u00b9\ud835\udc66jx\u00ba. If we are able to compute these\nprobabilities, which are \ud835\udc5d\u00b9\ud835\udc66jx\u00bafor\ud835\udc66=0,..., 9in our example, then the classifier will\noutput the prediction \u02c6\ud835\udc66given by the expression:\n\u02c6\ud835\udc66=argmax\ud835\udc5d\u00b9\ud835\udc66jx\u00ba. (A.1)\nUnfortunately, this requires that we estimate \ud835\udc5d\u00b9\ud835\udc66jx\u00bafor every value of x=\ud835\udc651,...,\ud835\udc65\ud835\udc51.\nImaginethateachfeaturecouldtakeoneof 2values. Forexample,thefeature \ud835\udc651=1might\nsignify that the word apple appears in a given document and \ud835\udc651=0would signify that it\ndoes not. If we had 30such binary features, that would mean that we need to be prepared\nto classify any of 230(over 1 billion!) possible values of the input vector x.\nMoreover,whereisthelearning? Ifweneedtoseeeverysinglepossibleexampleinorderto\npredictthecorrespondinglabelthenwearenotreallylearningapatternbutjustmemorizing\nthe dataset.\nA.9.3The NaiveBayesClassifier\nFortunately, by making some assumptions about conditional independence, we can intro-\nduce some inductive bias and build a model capable of generalizing from a comparatively\nmodest selection of training examples. To begin, let\u2019s use Bayes theorem, to express the\nclassifier as\n\u02c6\ud835\udc66=argmax\ud835\udc66\ud835\udc5d\u00b9\ud835\udc66jx\u00ba=argmax\ud835\udc66\ud835\udc5d\u00b9xj\ud835\udc66\u00ba\ud835\udc5d\u00b9\ud835\udc66\u00ba\n\ud835\udc5d\u00b9x\u00ba. (A.2)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1911, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23051ad3-4b2d-4ee6-9dca-488a3dad74f7": {"__data__": {"id_": "23051ad3-4b2d-4ee6-9dca-488a3dad74f7", "embedding": null, "metadata": {"page_label": "1003", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68b30d78-5beb-4b1b-b8c8-a3eaf7779648", "node_type": "4", "metadata": {"page_label": "1003", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "55d29c39e6ab1933a04adee0615d9c4eb533d4ca7803ff96a47f708b681f4959", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba79490e-36bf-4af9-b070-a1072bfe6837", "node_type": "1", "metadata": {}, "hash": "a839463053b0acb189b29449f45325f8d03a4f5d32f98fd228a14b2398ab69a3", "class_name": "RelatedNodeInfo"}}, "text": "1003 Naive Bayes\nNotethatthedenominatoristhenormalizingterm \ud835\udc5d\u00b9x\u00bawhichdoesnotdependonthevalue\nof the label\ud835\udc66. As a result, we only need to worry about comparing the numerator across\ndifferent values of \ud835\udc66. Even if calculating the denominator turned out to be intractable, we\ncould get away with ignoring it, so long as we could evaluate the numerator. Fortunately,\neven if we wanted to recover the normalizing constant, we could. We can always recover\nthe normalization term since\u00cd\n\ud835\udc66\ud835\udc5d\u00b9\ud835\udc66jx\u00ba=1.\nNow, let\u2019s focus on \ud835\udc5d\u00b9xj\ud835\udc66\u00ba. Using the chain rule of probability, we can express the term\n\ud835\udc5d\u00b9xj\ud835\udc66\u00baas\n\ud835\udc5d\u00b9\ud835\udc651j\ud835\udc66\u00ba\u0001\ud835\udc5d\u00b9\ud835\udc652j\ud835\udc651,\ud835\udc66\u00ba\u0001...\u0001\ud835\udc5d\u00b9\ud835\udc65\ud835\udc51j\ud835\udc651,...,\ud835\udc65\ud835\udc51\u00001,\ud835\udc66\u00ba. (A.3)\nBy itself, this expression does not get us any further. We still must estimate roughly 2\ud835\udc51\nparameters. However, if we assume that thefeaturesareconditionallyindependentofeach\nother,giventhelabel , then suddenly we are in much better shape, as this term simplifies to\u00ce\n\ud835\udc56\ud835\udc5d\u00b9\ud835\udc65\ud835\udc56j\ud835\udc66\u00ba, giving us the predictor\n\u02c6\ud835\udc66=argmax\ud835\udc66\ud835\udc51\u00d6\n\ud835\udc56=1\ud835\udc5d\u00b9\ud835\udc65\ud835\udc56j\ud835\udc66\u00ba\ud835\udc5d\u00b9\ud835\udc66\u00ba. (A.4)\nIf we can estimate \ud835\udc5d\u00b9\ud835\udc65\ud835\udc56=1j\ud835\udc66\u00bafor every\ud835\udc56and\ud835\udc66, and save its value in \ud835\udc43\ud835\udc65\ud835\udc66\u00bb\ud835\udc56,\ud835\udc66\u00bc, here\ud835\udc43\ud835\udc65\ud835\udc66\nis a\ud835\udc51\u0002\ud835\udc5bmatrix with\ud835\udc5bbeing the number of classes and \ud835\udc662f1,...,\ud835\udc5bg, then we can also\nuse this to estimate \ud835\udc5d\u00b9\ud835\udc65\ud835\udc56=0j\ud835\udc66\u00ba, i.e.,\n\ud835\udc5d\u00b9\ud835\udc65\ud835\udc56=\ud835\udc61\ud835\udc56j\ud835\udc66\u00ba=(\n\ud835\udc43\ud835\udc65\ud835\udc66\u00bb\ud835\udc56,\ud835\udc66\u00bcfor\ud835\udc61\ud835\udc56=1;\n1\u0000\ud835\udc43\ud835\udc65\ud835\udc66\u00bb\ud835\udc56,\ud835\udc66\u00bcfor\ud835\udc61\ud835\udc56=0.(A.5)\nIn addition, we estimate \ud835\udc5d\u00b9\ud835\udc66\u00bafor every\ud835\udc66and save it in \ud835\udc43\ud835\udc66\u00bb\ud835\udc66\u00bc, with\ud835\udc43\ud835\udc66a\ud835\udc5b-length vector.\nThen, for any new example t=\u00b9\ud835\udc611,\ud835\udc612,...,\ud835\udc61\ud835\udc51\u00ba, we could compute\n\u02c6\ud835\udc66=argmax\ud835\udc66\ud835\udc5d\u00b9\ud835\udc66\u00ba\ud835\udc51\u00d6\n\ud835\udc56=1\ud835\udc5d\u00b9\ud835\udc65\ud835\udc61=\ud835\udc61\ud835\udc56j\ud835\udc66\u00ba\n=argmax\ud835\udc66\ud835\udc43\ud835\udc66\u00bb\ud835\udc66\u00bc\ud835\udc51\u00d6\n\ud835\udc56=1\ud835\udc43\ud835\udc65\ud835\udc66\u00bb\ud835\udc56,\ud835\udc66\u00bc\ud835\udc61\ud835\udc56\u00001\u0000\ud835\udc43\ud835\udc65\ud835\udc66\u00bb\ud835\udc56,\ud835\udc66\u00bc\u00011\u0000\ud835\udc61\ud835\udc56(A.6)\nfor any\ud835\udc66. So our assumption of conditional independence has taken the complexity of\nour model from an exponential dependence on the number of features O\u00b92\ud835\udc51\ud835\udc5b\u00bato a linear\ndependence, which is O\u00b9\ud835\udc51\ud835\udc5b\u00ba.\nA.9.4Training\nThe problem now is that we do not know \ud835\udc43\ud835\udc65\ud835\udc66and\ud835\udc43\ud835\udc66. So we need to estimate their values\ngiven some training data first. This is training the model. Estimating \ud835\udc43\ud835\udc66is not too hard.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba79490e-36bf-4af9-b070-a1072bfe6837": {"__data__": {"id_": "ba79490e-36bf-4af9-b070-a1072bfe6837", "embedding": null, "metadata": {"page_label": "1003", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68b30d78-5beb-4b1b-b8c8-a3eaf7779648", "node_type": "4", "metadata": {"page_label": "1003", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "55d29c39e6ab1933a04adee0615d9c4eb533d4ca7803ff96a47f708b681f4959", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "23051ad3-4b2d-4ee6-9dca-488a3dad74f7", "node_type": "1", "metadata": {"page_label": "1003", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ec2ef04db479bb9e80f3731d63eea8390a164837009e623072bacae549fb3c97", "class_name": "RelatedNodeInfo"}}, "text": "So our assumption of conditional independence has taken the complexity of\nour model from an exponential dependence on the number of features O\u00b92\ud835\udc51\ud835\udc5b\u00bato a linear\ndependence, which is O\u00b9\ud835\udc51\ud835\udc5b\u00ba.\nA.9.4Training\nThe problem now is that we do not know \ud835\udc43\ud835\udc65\ud835\udc66and\ud835\udc43\ud835\udc66. So we need to estimate their values\ngiven some training data first. This is training the model. Estimating \ud835\udc43\ud835\udc66is not too hard.\nSince we are only dealing with 10classes, we may count the number of occurrences \ud835\udc5b\ud835\udc66for\neach of the digits and divide it by the total amount of data \ud835\udc5b. For instance, if digit 8 occurs\n\ud835\udc5b8=5,800times and we have a total of \ud835\udc5b=60,000images, the probability estimate is\n\ud835\udc5d\u00b9\ud835\udc66=8\u00ba=0.0967.", "mimetype": "text/plain", "start_char_idx": 1469, "end_char_idx": 2123, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb056a27-3fe8-4236-a5e8-5dd2d7712844": {"__data__": {"id_": "fb056a27-3fe8-4236-a5e8-5dd2d7712844", "embedding": null, "metadata": {"page_label": "1004", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d26d6f2-7afc-468b-b9c5-2bf5394a9a98", "node_type": "4", "metadata": {"page_label": "1004", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "86dfaaf2dc6ff8af17ca937933aa2b7770d8f350b0ab29f9cd2e715ba5bcc07b", "class_name": "RelatedNodeInfo"}}, "text": "1004 Mathematics for Deep Learning\nX=torch .stack([mnist_train[i][ 0]for iinrange (len(mnist_train))], dim =0)\nY=torch .tensor([mnist_train[i][ 1]for iinrange (len(mnist_train))])\nn_y =torch .zeros( 10)\nfor yinrange (10):\nn_y[y] =(Y==y).sum()\nP_y =n_y /n_y.sum()\nP_y\ntensor([ 0.0987 ,0.1124 ,0.0993 ,0.1022 ,0.0974 ,0.0904 ,0.0986 ,0.1044 ,0.0975 ,\n0.0992 ])\nNow on to slightly more difficult things \ud835\udc43\ud835\udc65\ud835\udc66. Since we picked black and white images,\n\ud835\udc5d\u00b9\ud835\udc65\ud835\udc56j\ud835\udc66\u00badenotes the probability that pixel \ud835\udc56is switched on for class \ud835\udc66. Just like before we\ncan go and count the number of times \ud835\udc5b\ud835\udc56\ud835\udc66such that an event occurs and divide it by the\ntotalnumberofoccurrencesof \ud835\udc66,i.e.,\ud835\udc5b\ud835\udc66. Butthereissomethingslightlytroubling: certain\npixels may never be black (e.g., for well cropped images the corner pixels might always be\nwhite). Aconvenientwayforstatisticianstodealwiththisproblemistoaddpseudocounts\nto all occurrences. Hence, rather than \ud835\udc5b\ud835\udc56\ud835\udc66we use\ud835\udc5b\ud835\udc56\ud835\udc66\u00b81and instead of \ud835\udc5b\ud835\udc66we use\ud835\udc5b\ud835\udc66\u00b82\n(since there are two possible values pixel \ud835\udc56can take - it can either be black or white). This\nis also called LaplaceSmoothing . It may seem ad-hoc, however it can be motivated from a\nBayesian point-of-view by a Beta-binomial model.\nn_x =torch .zeros(( 10,28,28))\nfor yinrange (10):\nn_x[y] =torch .tensor(X .numpy()[Y .numpy() ==y].sum(axis =0))\nP_xy =(n_x +1)/(n_y +2).reshape( 10,1,1)\nd2l.show_images(P_xy, 2,5);\nBy visualizing these 10\u000228\u000228probabilities (for each pixel for each class) we could get\nsome mean looking digits.\nNow we can use (A.6)to predict a new image. Given x, the following functions computes\n\ud835\udc5d\u00b9xj\ud835\udc66\u00ba\ud835\udc5d\u00b9\ud835\udc66\u00bafor every\ud835\udc66.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1593, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6080442-3796-4724-a4a8-5c1749a540ac": {"__data__": {"id_": "d6080442-3796-4724-a4a8-5c1749a540ac", "embedding": null, "metadata": {"page_label": "1005", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6e8c0b4b-529c-44de-8d1d-061d4eb83c4c", "node_type": "4", "metadata": {"page_label": "1005", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4ac69e63b95b39227d3904f39616702eb055dbf7bbd3d57bfde65ee7fd537492", "class_name": "RelatedNodeInfo"}}, "text": "1005 Naive Bayes\ndef bayes_pred (x):\nx=x.unsqueeze( 0)# (28, 28) -> (1, 28, 28)\np_xy =P_xy *x+(1-P_xy) *(1-x)\np_xy =p_xy .reshape( 10,-1).prod(dim =1)# p(x|y)\nreturn p_xy *P_y\nimage, label =mnist_test[ 0]\nbayes_pred(image)\ntensor([ 0.,0.,0.,0.,0.,0.,0.,0.,0.,0.])\nThis went horribly wrong! To find out why, let\u2019s look at the per pixel probabilities. They\nare typically numbers between 0.001and1. We are multiplying 784of them. At this point\nit is worth mentioning that we are calculating these numbers on a computer, hence with a\nfixedrangefortheexponent. Whathappensisthatweexperience numericalunderflow ,i.e.,\nmultiplying all the small numbers leads to something even smaller until it is rounded down\nto zero. We discussed this as a theoretical issue in Section A.7 , but we see the phenomena\nclearly here in practice.\nAs discussed in that section, we fix this by use the fact that log\ud835\udc4e\ud835\udc4f=log\ud835\udc4e\u00b8log\ud835\udc4f, i.e.,\nwe switch to summing logarithms. Even if both \ud835\udc4eand\ud835\udc4fare small numbers, the logarithm\nvalues should be in a proper range.\na=0.1\nprint ('underflow: ', a**784)\nprint ('logarithm is normal: ',784*math .log(a))\nunderflow: 0.0\nlogarithm isnormal: -1805.2267129073316\nSince the logarithm is an increasing function, we can rewrite (A.6)as\n\u02c6\ud835\udc66=argmax\ud835\udc66log\ud835\udc43\ud835\udc66\u00bb\ud835\udc66\u00bc\u00b8\ud835\udc51\u00d5\n\ud835\udc56=1h\n\ud835\udc61\ud835\udc56log\ud835\udc43\ud835\udc65\ud835\udc66\u00bb\ud835\udc65\ud835\udc56,\ud835\udc66\u00bc\u00b8\u00b9 1\u0000\ud835\udc61\ud835\udc56\u00balog\u00b91\u0000\ud835\udc43\ud835\udc65\ud835\udc66\u00bb\ud835\udc65\ud835\udc56,\ud835\udc66\u00bc\u00bai\n.(A.7)\nWe can implement the following stable version:\nlog_P_xy =torch .log(P_xy)\nlog_P_xy_neg =torch .log( 1-P_xy)\nlog_P_y =torch .log(P_y)\ndef bayes_pred_stable (x):\nx=x.unsqueeze( 0)# (28, 28) -> (1, 28, 28)\np_xy =log_P_xy *x+log_P_xy_neg *(1-x)\np_xy =p_xy .reshape( 10,-1).sum(axis =1)# p(x|y)\nreturn p_xy +log_P_y\npy=bayes_pred_stable(image)\npy", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1652, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6c731c5c-a58d-4383-8566-cb809b0e3a87": {"__data__": {"id_": "6c731c5c-a58d-4383-8566-cb809b0e3a87", "embedding": null, "metadata": {"page_label": "1006", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9f3e69c2-0fbc-4c49-999a-c1f32ee6b5bb", "node_type": "4", "metadata": {"page_label": "1006", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "216c93afa98e1f94ccad1bcf434f53cb4327a625ef94a39c47329070a4f164dd", "class_name": "RelatedNodeInfo"}}, "text": "1006 Mathematics for Deep Learning\ntensor([ -268.9725 ,-301.7044 ,-245.1951 ,-218.8738 ,-193.4570 ,-206.0909 ,\n-292.5226 ,-114.6257 ,-220.3313 ,-163.1784 ])\nWe may now check if the prediction is correct.\npy.argmax(dim =0)==label\ntensor( True )\nIf we now predict a few validation examples, we can see the Bayes classifier works pretty\nwell.\ndef predict (X):\nreturn [bayes_pred_stable(x) .argmax(dim =0).type(torch .int32) .item()\nfor xinX]\nX=torch .stack([mnist_test[i][ 0]for iinrange (18)], dim =0)\ny=torch .tensor([mnist_test[i][ 1]for iinrange (18)])\npreds =predict(X)\nd2l.show_images(X, 2,9, titles =[str(d) for dinpreds]);\nFinally, let\u2019s compute the overall accuracy of the classifier.\nX=torch .stack([mnist_test[i][ 0]for iinrange (len(mnist_test))], dim =0)\ny=torch .tensor([mnist_test[i][ 1]for iinrange (len(mnist_test))])\npreds =torch .tensor(predict(X), dtype =torch .int32)\nfloat ((preds ==y).sum()) /len(y) # Validation accuracy\n0.8427\nModern deep networks achieve error rates of less than 0.01. The relatively poor perfor-\nmance is due to the incorrect statistical assumptions that we made in our model: we as-\nsumed that each and every pixel are independently generated, depending only on the label.\nThis is clearly not how humans write digits, and this wrong assumption led to the downfall\nof our overly naive (Bayes) classifier.\nA.9.5Summary", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "837717b5-85f6-4c5f-9406-030490cadde1": {"__data__": {"id_": "837717b5-85f6-4c5f-9406-030490cadde1", "embedding": null, "metadata": {"page_label": "1007", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae9e144a-b356-4fd1-a5f2-3cfca09816e7", "node_type": "4", "metadata": {"page_label": "1007", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7a002eec7220044779f9cdd112939defa047622e88a69db2897f028ddfb0acc2", "class_name": "RelatedNodeInfo"}}, "text": "1007 Statistics\n288\u000fUsing Bayes\u2019 rule, a classifier can be made by assuming all observed features are inde-\npendent.\n\u000fThis classifier can be trained on a dataset by counting the number of occurrences of\ncombinations of labels and pixel values.\n\u000fThis classifier was the gold standard for decades for tasks such as spam detection.\nA.9.6Exercises\n1.Consider the dataset \u00bb\u00bb0,0\u00bc,\u00bb0,1\u00bc,\u00bb1,0\u00bc,\u00bb1,1\u00bc\u00bcwith labels given by the XOR of the\ntwo elements\u00bb0,1,1,0\u00bc. What are the probabilities for a Naive Bayes classifier built\non this dataset. Does it successfully classify our points? If not, what assumptions are\nviolated?\n2.Suppose that we did not use Laplace smoothing when estimating probabilities and a\ndataexamplearrivedattestingtimewhichcontainedavalueneverobservedintraining.\nWhat would the model output?\n3.The naive Bayes classifier is a specific example of a Bayesian network, where the de-\npendenceofrandomvariablesareencodedwithagraphstructure. Whilethefulltheory\nis beyond the scope of this section (see Koller and Friedman ( 2009) for full details),\nexplain why allowing explicit dependence between the two input variables in the XOR\nmodel allows for the creation of a successful classifier.\nDiscussions288.\nA.10Statistics\nUndoubtedly, to be a top deep learning practitioner, the ability to train the state-of-the-art\nand high accurate models is crucial. However, it is often unclear when improvements are\nsignificant, or only the result of random fluctuations in the training process. To be able to\ndiscuss uncertainty in estimated values, we must learn some statistics.\nTheearliestreferenceof statistics canbetracedbacktoanArabscholarAl-Kindiinthe 9th-\ncentury, who gave a detailed description of how to use statistics and frequency analysis to\ndecipher encrypted messages. After 800 years, the modern statistics arose from Germany\nin 1700s, when the researchers focused on the demographic and economic data collection\nandanalysis. Today,statisticsisthesciencesubjectthatconcernsthecollection,processing,\nanalysis,interpretationandvisualizationofdata. Whatismore,thecoretheoryofstatistics\nhas been widely used in the research within academia, industry, and government.\nMorespecifically, statisticscan be divided to descriptivestatistics andstatisticalinference .\nThe former focus on summarizing and illustrating the features of a collection of observed\ndata, which is referred to as a sample. The sample is drawn from a population , denotes", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2443, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c01e35c-fc3d-426c-81c5-3daf593f820d": {"__data__": {"id_": "2c01e35c-fc3d-426c-81c5-3daf593f820d", "embedding": null, "metadata": {"page_label": "1008", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8f59441c-7148-4bb4-82e4-00d48a4b9bac", "node_type": "4", "metadata": {"page_label": "1008", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "94c04c0db3d933faa9e0f937b6af61b72ed15c254aaac8c23a082ef295fe9581", "class_name": "RelatedNodeInfo"}}, "text": "1008 Mathematics for Deep Learning\nthetotalsetofsimilarindividuals, items, oreventsofourexperimentinterests. Contraryto\ndescriptivestatistics, statisticalinference furtherdeducesthecharacteristicsofapopulation\nfromthegiven samples,basedontheassumptionsthatthesampledistributioncanreplicate\nthe population distribution at some degree.\nYou may wonder: \u201cWhat is the essential difference between machine learning and statis-\ntics?\u201d Fundamentally speaking, statistics focuses on the inference problem. This type of\nproblems includes modeling the relationship between the variables, such as causal infer-\nence, and testing the statistically significance of model parameters, such as A/B testing. In\ncontrast, machine learning emphasizes on making accurate predictions, without explicitly\nprogramming and understanding each parameter\u2019s functionality.\nInthissection,wewillintroducethreetypesofstatisticsinferencemethods: evaluatingand\ncomparing estimators, conducting hypothesis tests, and constructing confidence intervals.\nThese methods can help us infer the characteristics of a given population, i.e., the true\nparameter\ud835\udf03. For brevity, we assume that the true parameter \ud835\udf03of a given population is a\nscalar value. It is straightforward to extend to the case where \ud835\udf03is a vector or a tensor, thus\nwe omit it in our discussion.\nA.10.1Evaluatingand ComparingEstimators\nInstatistics, an estimator isafunctionofgivensamplesusedtoestimatethetrueparameter\n\ud835\udf03. We will write \u02c6\ud835\udf03\ud835\udc5b=\u02c6\ud835\udc53\u00b9\ud835\udc651,...,\ud835\udc65\ud835\udc5b\u00bafor the estimate of \ud835\udf03after observing the samples\n{\ud835\udc651,\ud835\udc652,...,\ud835\udc65\ud835\udc5b}.\nWe have seen simple examples of estimators before in section Section A.7 . If you have a\nnumber of samples from a Bernoulli random variable, then the maximum likelihood esti-\nmatefortheprobabilitytherandomvariableisonecanbeobtainedbycountingthenumber\nof ones observed and dividing by the total number of samples. Similarly, an exercise asked\nyouto showthat the maximum likelihoodestimateof the mean of a Gaussian givena num-\nberofsamplesisgivenbytheaveragevalueofallthesamples. Theseestimatorswillalmost\nnever give the true value of the parameter, but ideally for a large number of samples the\nestimate will be close.\nAs an example, we show below the true density of a Gaussian random variable with mean\nzeroandvarianceone,alongwithacollectionsamplesfromthatGaussian. Weconstructed\nthe\ud835\udc66coordinate so every point is visible and the relationship to the original density is\nclearer.\nimport torch\nfrom d2l import torch asd2l\ntorch .pi=torch .acos(torch .zeros( 1))*2#define pi in torch\n# Sample datapoints and create y coordinate\nepsilon =0.1\ntorch .manual_seed( 8675309 )\nxs=torch .randn(size =(300,))\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2664, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b89a0101-81f4-42cc-9573-e9b1041461f3": {"__data__": {"id_": "b89a0101-81f4-42cc-9573-e9b1041461f3", "embedding": null, "metadata": {"page_label": "1009", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4787e665-1654-4fba-9717-88d6e6213c24", "node_type": "4", "metadata": {"page_label": "1009", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "34e219f88d7258d6928f02c9ad006f2f29f1818a0e7007a5ff79bd97c49a6a92", "class_name": "RelatedNodeInfo"}}, "text": "1009 Statistics\n(continued from previous page)\nys=torch .tensor(\n[torch .sum(torch .exp( -(xs[:i] -xs[i]) **2/(2*epsilon **2))\\\n/torch .sqrt( 2*torch .pi*epsilon **2))/len(xs)\\\nfor iinrange (len(xs))])\n# Compute true density\nxd=torch .arange(torch .min(xs), torch .max(xs), 0.01 )\nyd=torch .exp( -xd**2/2)/torch .sqrt( 2*torch .pi)\n# Plot the results\nd2l.plot(xd, yd, 'x','density ')\nd2l.plt.scatter(xs, ys)\nd2l.plt.axvline(x =0)\nd2l.plt.axvline(x =torch .mean(xs), linestyle ='--', color ='purple ')\nd2l.plt.title( f'sample mean: {float (torch .mean(xs) .item()) :.2f}')\nd2l.plt.show()\nThere can be many ways to compute an estimator of a parameter \u02c6\ud835\udf03\ud835\udc5b. In this section, we\nintroduce three common methods to evaluate and compare estimators: the mean squared\nerror, the standard deviation, and statistical bias.\nMean SquaredError\nPerhaps the simplest metric used to evaluate estimators is the mean squared error (MSE)\n(or\ud835\udc592loss) estimator which can be defined as\nMSE\u00b9\u02c6\ud835\udf03\ud835\udc5b,\ud835\udf03\u00ba=\ud835\udc38\u00bb\u00b9\u02c6\ud835\udf03\ud835\udc5b\u0000\ud835\udf03\u00ba2\u00bc. (A.1)\nThisallowsustoquantifytheaveragesquareddeviationfromthetruevalue. MSEisalways\nnon-negative. Ifyouhaveread Section3.1 ,youwillrecognizeitasthemostcommonlyused\nregressionlossfunction. Asameasuretoevaluateanestimator, thecloseritsvaluetozero,\nthe closer the estimator is close to the true parameter \ud835\udf03.\nStatistical Bias\nThe MSE provides a natural metric, but we can easily imagine multiple different phenom-\nena that might make it large. Two fundamentally important are fluctuation in the estimator", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5d17c222-989c-40b5-97cf-326ddf0127af": {"__data__": {"id_": "5d17c222-989c-40b5-97cf-326ddf0127af", "embedding": null, "metadata": {"page_label": "1010", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6098143c-9292-4f57-a499-bd881f4210f5", "node_type": "4", "metadata": {"page_label": "1010", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "34e16248b2dab7ea31e5633e4cbb242fb89ebe513dde270506e0bec06b21820a", "class_name": "RelatedNodeInfo"}}, "text": "1010 Mathematics for Deep Learning\nduetorandomnessinthedataset,andsystematicerrorintheestimatorduetotheestimation\nprocedure.\nFirst, let\u2019s measure the systematic error. For an estimator \u02c6\ud835\udf03\ud835\udc5b, the mathematical illustration\nofstatisticalbias can be defined as\nbias\u00b9\u02c6\ud835\udf03\ud835\udc5b\u00ba=\ud835\udc38\u00b9\u02c6\ud835\udf03\ud835\udc5b\u0000\ud835\udf03\u00ba=\ud835\udc38\u00b9\u02c6\ud835\udf03\ud835\udc5b\u00ba\u0000\ud835\udf03. (A.2)\nNote that when bias \u00b9\u02c6\ud835\udf03\ud835\udc5b\u00ba=0, the expectation of the estimator \u02c6\ud835\udf03\ud835\udc5bis equal to the true value\nof parameter. In this case, we say \u02c6\ud835\udf03\ud835\udc5bis an unbiased estimator. In general, an unbiased\nestimator is better than a biased estimator since its expectation is the same as the true pa-\nrameter.\nIt is worth being aware, however, that biased estimators are frequently used in practice.\nThere are cases where unbiased estimators do not exist without further assumptions, or\nare intractable to compute. This may seem like a significant flaw in an estimator, however\nthe majority of estimators encountered in practice are at least asymptotically unbiased in\nthe sense that the bias tends to zero as the number of available samples tends to infinity:\nlim\ud835\udc5b!1bias\u00b9\u02c6\ud835\udf03\ud835\udc5b\u00ba=0.\nVarianceand StandardDeviation\nSecond, let\u2019s measure the randomness in the estimator. Recall from Section A.6 , thestan-\ndard deviation (orstandard error ) is defined as the squared root of the variance. We may\nmeasure the degree of fluctuation of an estimator by measuring the standard deviation or\nvariance of that estimator.\n\ud835\udf0e\u02c6\ud835\udf03\ud835\udc5b=q\nVar\u00b9\u02c6\ud835\udf03\ud835\udc5b\u00ba=q\n\ud835\udc38\u00bb\u00b9\u02c6\ud835\udf03\ud835\udc5b\u0000\ud835\udc38\u00b9\u02c6\ud835\udf03\ud835\udc5b\u00ba\u00ba2\u00bc. (A.3)\nIt is important to compare (A.3)to(A.1). In this equation we do not compare to the true\npopulation value \ud835\udf03, but instead to \ud835\udc38\u00b9\u02c6\ud835\udf03\ud835\udc5b\u00ba, the expected sample mean. Thus we are not\nmeasuring how far the estimator tends to be from the true value, but instead we measuring\nthe fluctuation of the estimator itself.\nTheBias-VarianceTrade-off\nIt is intuitively clear that these two main components contribute to the mean squared error.\nWhat is somewhat shocking is that we can show that this is actually a decomposition of\nthe mean squared error into these two contributions plus a third one. That is to say that we\ncan write the mean squared error as the sum of the square of the bias, the variance and the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2107, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71361730-f0d0-4ff9-b186-3854d8baf7fa": {"__data__": {"id_": "71361730-f0d0-4ff9-b186-3854d8baf7fa", "embedding": null, "metadata": {"page_label": "1011", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "174a8d1b-05e5-410d-b995-1b5f7f446daa", "node_type": "4", "metadata": {"page_label": "1011", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6360967f76da8cdf1019b0e97db6906637b7454f38cc8ec12db469c816057ef7", "class_name": "RelatedNodeInfo"}}, "text": "1011 Statistics\nirreducible error.\nMSE\u00b9\u02c6\ud835\udf03\ud835\udc5b,\ud835\udf03\u00ba=\ud835\udc38\u00bb\u00b9\u02c6\ud835\udf03\ud835\udc5b\u0000\ud835\udf03\u00ba2\u00bc\n=\ud835\udc38\u00bb\u00b9\u02c6\ud835\udf03\ud835\udc5b\u00ba2\u00bc\u00b8\ud835\udc38\u00bb\ud835\udf032\u00bc\u00002\ud835\udc38\u00bb\u02c6\ud835\udf03\ud835\udc5b\ud835\udf03\u00bc\n=Var\u00bb\u02c6\ud835\udf03\ud835\udc5b\u00bc\u00b8\ud835\udc38\u00bb\u02c6\ud835\udf03\ud835\udc5b\u00bc2\u00b8Var\u00bb\ud835\udf03\u00bc\u00b8\ud835\udc38\u00bb\ud835\udf03\u00bc2\u00002\ud835\udc38\u00bb\u02c6\ud835\udf03\ud835\udc5b\u00bc\ud835\udc38\u00bb\ud835\udf03\u00bc\n=\u00b9\ud835\udc38\u00bb\u02c6\ud835\udf03\ud835\udc5b\u00bc\u0000\ud835\udc38\u00bb\ud835\udf03\u00bc\u00ba2\u00b8Var\u00bb\u02c6\ud835\udf03\ud835\udc5b\u00bc\u00b8Var\u00bb\ud835\udf03\u00bc\n=\u00b9\ud835\udc38\u00bb\u02c6\ud835\udf03\ud835\udc5b\u0000\ud835\udf03\u00bc\u00ba2\u00b8Var\u00bb\u02c6\ud835\udf03\ud835\udc5b\u00bc\u00b8Var\u00bb\ud835\udf03\u00bc\n=\u00b9bias\u00bb\u02c6\ud835\udf03\ud835\udc5b\u00bc\u00ba2\u00b8Var\u00b9\u02c6\ud835\udf03\ud835\udc5b\u00ba\u00b8Var\u00bb\ud835\udf03\u00bc.(A.4)\nWe refer the above formula as bias-variance trade-off . The mean squared error can be di-\nvidedintothreesourcesoferror: theerrorfromhighbias,theerrorfromhighvarianceand\nthe irreducible error. The bias error is commonly seen in a simple model (such as a linear\nregression model), which cannot extract high dimensional relations between the features\nand the outputs. If a model suffers from high bias error, we often say it is underfitting or\nlack offlexibilty as introduced in ( Section 3.6 ). The high variance usually results from a\ntoo complex model, which overfits the training data. As a result, an overfitting model is\nsensitive to small fluctuations in the data. If a model suffers from high variance, we often\nsayitisoverfitting andlackof generalization asintroducedin( Section3.6 ). Theirreducible\nerror is the result from noise in the \ud835\udf03itself.\nEvaluatingEstimators in Code\nSince the standard deviation of an estimator has been implementing by simply calling a.\nstd()foratensor a,wewillskipitbutimplementthestatisticalbiasandthemeansquared\nerror.\n# Statistical bias\ndef stat_bias (true_theta, est_theta):\nreturn (torch .mean(est_theta) -true_theta)\n# Mean squared error\ndef mse(data, true_theta):\nreturn (torch .mean(torch .square(data -true_theta)))\nToillustratetheequationofthebias-variancetrade-off,let\u2019ssimulateofnormaldistribution\nN\u00b9\ud835\udf03,\ud835\udf0e2\u00bawith 10,000samples. Here, we use a \ud835\udf03=1and\ud835\udf0e=4. As the estimator is a\nfunction of the given samples, here we use the mean of the samples as an estimator for true\n\ud835\udf03in this normal distribution N\u00b9\ud835\udf03,\ud835\udf0e2\u00ba.\ntheta_true =1\nsigma =4\nsample_len =10000\nsamples =torch .normal(theta_true, sigma, size =(sample_len, 1))\ntheta_est =torch .mean(samples)\ntheta_est", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d46e4021-3dc8-4bd8-bced-fc4fabda7c3f": {"__data__": {"id_": "d46e4021-3dc8-4bd8-bced-fc4fabda7c3f", "embedding": null, "metadata": {"page_label": "1012", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5aaef3ed-c7a0-4b55-b679-5fad8959b283", "node_type": "4", "metadata": {"page_label": "1012", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "67282deda1230d30b4afaf5196282265843c646241a1508c8dc9934095441925", "class_name": "RelatedNodeInfo"}}, "text": "1012 Mathematics for Deep Learning\ntensor( 1.0170 )\nLet\u2019s validate the trade-off equation by calculating the summation of the squared bias and\nthe variance of our estimator. First, calculate the MSE of our estimator.\nmse(samples, theta_true)\ntensor( 16.0298 )\nNext, we calculate Var \u00b9\u02c6\ud835\udf03\ud835\udc5b\u00ba\u00b8\u00bbbias\u00b9\u02c6\ud835\udf03\ud835\udc5b\u00ba\u00bc2as below. As you can see, the two values agree to\nnumerical precision.\nbias =stat_bias(theta_true, theta_est)\ntorch .square(samples .std(unbiased =False ))+torch .square(bias)\ntensor( 16.0298 )\nA.10.2Conducting HypothesisTests\nThe most commonly encountered topic in statistical inference is hypothesis testing. While\nhypothesis testing was popularized in the early 20th century, the first use can be traced\nback to John Arbuthnot in the 1700s. John tracked 80-year birth records in London and\nconcluded that more men were born than women each year. Following that, the modern\nsignificance testing is the intelligence heritage by Karl Pearson who invented \ud835\udc5d-value and\nPearson\u2019schi-squaredtest,WilliamGossetwhoisthefatherofStudent\u2019st-distribution,and\nRonald Fisher who initialed the null hypothesis and the significance test.\nAhypothesistest isawayofevaluatingsomeevidenceagainstthedefaultstatementabouta\npopulation. We refer the default statement as the nullhypothesis \ud835\udc3b0, which we try to reject\nusing the observed data. Here, we use \ud835\udc3b0as a starting point for the statistical significance\ntesting. The alternative hypothesis \ud835\udc3b\ud835\udc34(or\ud835\udc3b1) is a statement that is contrary to the null\nhypothesis. A null hypothesis is often stated in a declarative form which posits a relation-\nship between variables. It should reflect the brief as explicit as possible, and be testable by\nstatistics theory.\nImagine you are a chemist. After spending thousands of hours in the lab, you develop a\nnew medicine which can dramatically improve one\u2019s ability to understand math. To show\nits magic power, you need to test it. Naturally, you may need some volunteers to take\nthe medicine and see whether it can help them learn mathematics better. How do you get\nstarted?\nFirst, you will need carefully random selected two groups of volunteers, so that there is no\ndifference between their mathematical understanding ability measured by some metrics.\nThe two groups are commonly referred to as the test group and the control group. The\ntestgroup (ortreatmentgroup )isagroupofindividualswhowillexperiencethemedicine,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4023bfd4-7339-4e80-9557-7544492e1bc5": {"__data__": {"id_": "4023bfd4-7339-4e80-9557-7544492e1bc5", "embedding": null, "metadata": {"page_label": "1013", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0f20ab2-cf4e-43f6-bb0d-a998c38e63f6", "node_type": "4", "metadata": {"page_label": "1013", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "14d960fa3b2d1a54905892df98dab4a40fc7dffb502d7b6c46ab0c8481d48824", "class_name": "RelatedNodeInfo"}}, "text": "1013 Statistics\nwhile the control group represents the group of users who are set aside as a benchmark,\ni.e., identical environment setups except taking this medicine. In this way, the influence\nof all the variables are minimized, except the impact of the independent variable in the\ntreatment.\nSecond, after a period of taking the medicine, you will need to measure the two groups\u2019\nmathematicalunderstandingbythesamemetrics,suchaslettingthevolunteersdothesame\ntests after learning a new mathematical formula. Then, you can collect their performance\nand compare the results. In this case, our null hypothesis will be that there is no difference\nbetween the two groups, and our alternate will be that there is.\nThis is still not fully formal. There are many details you have to think of carefully. For\nexample,whatisthesuitablemetricstotesttheirmathematicalunderstandingability? How\nmany volunteers for your test so you can be confident to claim the effectiveness of your\nmedicine? How long should you run the test? How do you decide if there is a difference\nbetweenthetwogroups? Doyoucareabouttheaverageperformanceonly,oralsotherange\nof variation of the scores? And so on.\nInthisway,hypothesistestingprovidesaframeworkforexperimentaldesignandreasoning\nabout certainty in observed results. If we can now show that the null hypothesis is very\nunlikely to be true, we may reject it with confidence.\nTo complete the story of how to work with hypothesis testing, we need to now introduce\nsome additional terminology and make some of our concepts above formal.\nStatistical Significance\nThestatistical significance measures the probability of erroneously rejecting the null hy-\npothesis,\ud835\udc3b0, when it should not be rejected, i.e.,\nstatistical significance =1\u0000\ud835\udefc=1\u0000\ud835\udc43\u00b9reject\ud835\udc3b0j\ud835\udc3b0is true\u00ba. (A.5)\nIt is also referred to as the typeIerror orfalsepositive . The\ud835\udefc, is called as the significance\nleveland its commonly used value is 5%, i.e., 1\u0000\ud835\udefc=95%. The significance level can\nbe explained as the level of risk that we are willing to take, when we reject a true null\nhypothesis.\nFig. A.1 shows the observations\u2019 values and probability of a given normal distribution\nin a two-sample hypothesis test. If the observation data example is located outsides the\n95%threshold, it will be a very unlikely observation under the null hypothesis assump-\ntion. Hence, there might be something wrong with the null hypothesis and we will reject\nit.\nStatistical Power\nThestatisticalpower (orsensitivity ) measures the probability of reject the null hypothesis,\n\ud835\udc3b0, when it should be rejected, i.e.,\nstatistical power =1\u0000\ud835\udefd=1\u0000\ud835\udc43\u00b9fail to reject \ud835\udc3b0j\ud835\udc3b0is false\u00ba. (A.6)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc19044d-be9d-4f8c-a193-beebd29aeb75": {"__data__": {"id_": "cc19044d-be9d-4f8c-a193-beebd29aeb75", "embedding": null, "metadata": {"page_label": "1014", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b4be816-716c-487b-8cb8-1fa3eaf70060", "node_type": "4", "metadata": {"page_label": "1014", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "61612158bcdf4de2dfb2c80ceba981f2a30aa07b88f2f303d89a8b45cfa7ae81", "class_name": "RelatedNodeInfo"}}, "text": "1014 Mathematics for Deep Learning\ntFig. A.1 Statistical signi\ufb01cance.\nRecall that a type I error is error caused by rejecting the null hypothesis when it is true,\nwhereas a typeIIerror is resulted from failing to reject the null hypothesis when it is false.\nA type II error is usually denoted as \ud835\udefd, and hence the corresponding statistical power is\n1\u0000\ud835\udefd.\nIntuitively, statistical power can be interpreted as how likely our test will detect a real dis-\ncrepancy of some minimum magnitude at a desired statistical significance level. 80%is\na commonly used statistical power threshold. The higher the statistical power, the more\nlikely we are to detect true differences.\nOne of the most common uses of statistical power is in determining the number of samples\nneeded. Theprobabilityyourejectthenullhypothesiswhenitisfalsedependsonthedegree\nto which it is false (known as the effect size ) and the number of samples you have. As you\nmightexpect,smalleffectsizeswillrequireaverylargenumberofsamplestobedetectable\nwith high probability. While beyond the scope of this brief appendix to derive in detail, as\nan example, want to be able to reject a null hypothesis that our sample came from a mean\nzerovarianceoneGaussian, andwebelievethatoursample\u2019smeanisactuallyclosetoone,\nwe can do so with acceptable error rates with a sample size of only 8. However, if we think\nour sample population true mean is close to 0.01, then we\u2019d need a sample size of nearly\n80000to detect the difference.\nWe can imagine the power as a water filter. In this analogy, a high power hypothesis test is\nlike a high quality water filtration system that will reduce harmful substances in the water\nas much as possible. On the other hand, a smaller discrepancy is like a low quality water\nfilter, where some relative small substances may easily escape from the gaps. Similarly, if\nthe statistical power is not of enough high power, then the test may not catch the smaller\ndiscrepancy.\nTestStatistic\nAtest statistic\ud835\udc47\u00b9\ud835\udc65\u00bais a scalar which summarizes some characteristic of the sample data.\nThegoalofdefiningsuchastatisticisthatitshouldallowustodistinguishbetweendifferent\ndistributions and conduct our hypothesis test. Thinking back to our chemist example, if we\nwish to show that one population performs better than the other, it could be reasonable to", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ce5ee54-71a2-4630-8537-9bee700c8cd1": {"__data__": {"id_": "5ce5ee54-71a2-4630-8537-9bee700c8cd1", "embedding": null, "metadata": {"page_label": "1015", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eab24963-c5f1-4db8-b5e4-9a9dcc78bbeb", "node_type": "4", "metadata": {"page_label": "1015", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cd47079844ad944ea5ce3d03c1d96ee64c320cbd38ac1183c684d1db1035b1bd", "class_name": "RelatedNodeInfo"}}, "text": "1015 Statistics\ntake the mean as the test statistic. Different choices of test statistic can lead to statistical\ntest with drastically different statistical power.\nOften,\ud835\udc47\u00b9\ud835\udc4b\u00ba(the distribution of the test statistic under our null hypothesis) will follow, at\nleast approximately, a common probability distribution such as a normal distribution when\nconsidered under the null hypothesis. If we can derive explicitly such a distribution, and\nthen measure our test statistic on our dataset, we can safely reject the null hypothesis if our\nstatistic is far outside the range that we would expect. Making this quantitative leads us to\nthe notion of \ud835\udc5d-values.\n\ud835\udc5d-value\nThe\ud835\udc5d-value (or the probabilityvalue ) is the probability that \ud835\udc47\u00b9\ud835\udc4b\u00bais at least as extreme as\nthe observed test statistic \ud835\udc47\u00b9\ud835\udc65\u00baassuming that the null hypothesis is true, i.e.,\n\ud835\udc5d-value =\ud835\udc43\ud835\udc3b0\u00b9\ud835\udc47\u00b9\ud835\udc4b\u00ba\u0015\ud835\udc47\u00b9\ud835\udc65\u00ba\u00ba. (A.7)\nIf the\ud835\udc5d-valueis smaller than or equal to a predefined and fixed statistical significance level\n\ud835\udefc, we may reject the null hypothesis. Otherwise, we will conclude that we are lack of\nevidence to reject the null hypothesis. For a given population distribution, the region of\nrejection will be the interval contained of all the points which has a \ud835\udc5d-value smaller than\nthe statistical significance level \ud835\udefc.\nOne-side Testand Two-sidedTest\nNormally there are two kinds of significance test: the one-sided test and the two-sided\ntest. The one-sided test (orone-tailed test ) is applicable when the null hypothesis and the\nalternative hypothesis only have one direction. For example, the null hypothesis may state\nthat the true parameter \ud835\udf03is less than or equal to a value \ud835\udc50. The alternative hypothesis\nwould be that \ud835\udf03is greater than \ud835\udc50. That is, the region of rejection is on only one side of the\nsampling distribution. Contrary to the one-sided test, the two-sidedtest (ortwo-tailedtest )\nis applicable when the region of rejection is on both sides of the sampling distribution. An\nexample in this case may have a null hypothesis state that the true parameter \ud835\udf03is equal to a\nvalue\ud835\udc50. The alternative hypothesis would be that \ud835\udf03is not equal to \ud835\udc50.\nGeneral Steps of HypothesisTesting\nAftergettingfamiliarwiththeaboveconcepts, let\u2019sgothroughthegeneralstepsofhypoth-\nesis testing.\n1.State the question and establish a null hypotheses \ud835\udc3b0.\n2.Set the statistical significance level \ud835\udefcand a statistical power ( 1\u0000\ud835\udefd).\n3.Obtain samples through experiments. The number of samples needed will depend on\nthe statistical power, and the expected effect size.\n4.Calculate the test statistic and the \ud835\udc5d-value.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ba3b272-8108-4e27-ad1e-f299a4decb16": {"__data__": {"id_": "0ba3b272-8108-4e27-ad1e-f299a4decb16", "embedding": null, "metadata": {"page_label": "1016", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd558d3a-bbbd-4469-a5d9-37407de68fbf", "node_type": "4", "metadata": {"page_label": "1016", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e175d57bea188593921a5ef12a744fc44ca077f70d58b813ad06d98bb97949dc", "class_name": "RelatedNodeInfo"}}, "text": "1016 Mathematics for Deep Learning\n5.Make the decision to keep or reject the null hypothesis based on the \ud835\udc5d-value and the\nstatistical significance level \ud835\udefc.\nTo conduct a hypothesis test, we start by defining a null hypothesis and a level of risk that\nwe are willing to take. Then we calculate the test statistic of the sample, taking an extreme\nvalue of the test statistic as evidence against the null hypothesis. If the test statistic falls\nwithintherejectregion,wemayrejectthenullhypothesisinfavorofthealternative.\nHypothesis testing is applicable in a variety of scenarios such as the clinical trails and A/B\ntesting.\nA.10.3Constructing Confidence Intervals\nWhen estimating the value of a parameter \ud835\udf03, point estimators like \u02c6\ud835\udf03are of limited utility\nsince they contain no notion of uncertainty. Rather, it would be far better if we could\nproduce an interval that would contain the true parameter \ud835\udf03with high probability. If you\nwere interested in such ideas a century ago, then you would have been excited to read\n\u201cOutlineofaTheoryofStatisticalEstimationBasedontheClassicalTheoryofProbability\u201d\nby Jerzy Neyman ( Neyman, 1937 ), who first introduced the concept of confidence interval\nin 1937.\nTo be useful, a confidence interval should be as small as possible for a given degree of\ncertainty. Let\u2019s see how to derive it.\nDefinition\nMathematically, a confidence interval for the true parameter \ud835\udf03is an interval \ud835\udc36\ud835\udc5bthat com-\nputed from the sample data such that\n\ud835\udc43\ud835\udf03\u00b9\ud835\udc36\ud835\udc5b3\ud835\udf03\u00ba\u00151\u0000\ud835\udefc,8\ud835\udf03. (A.8)\nHere\ud835\udefc2\u00b90,1\u00ba, and 1\u0000\ud835\udefcis called the confidencelevel orcoverage of the interval. This is\nthe same\ud835\udefcas the significance level as we discussed about above.\nNote that (A.8)is about variable \ud835\udc36\ud835\udc5b, not about the fixed \ud835\udf03. To emphasize this, we write\n\ud835\udc43\ud835\udf03\u00b9\ud835\udc36\ud835\udc5b3\ud835\udf03\u00barather than\ud835\udc43\ud835\udf03\u00b9\ud835\udf032\ud835\udc36\ud835\udc5b\u00ba.\nInterpretation\nIt is very tempting to interpret a 95%confidence interval as an interval where you can be\n95%surethetrueparameterlies,howeverthisissadlynottrue. Thetrueparameterisfixed,\nand it is the interval that is random. Thus a better interpretation would be to say that if you\ngenerated a large number of confidence intervals by this procedure, 95%of the generated\nintervals would contain the true parameter.\nThismayseempedantic,butitcanhaverealimplicationsfortheinterpretationoftheresults.\nIn particular, we may satisfy (A.8)by constructing intervals that we are almost certain do\nnot contain the true value, as long as we only do so rarely enough. We close this section by", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "989507b8-d4f6-474b-a2b5-eb7115184462": {"__data__": {"id_": "989507b8-d4f6-474b-a2b5-eb7115184462", "embedding": null, "metadata": {"page_label": "1017", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1e1692a1-aab8-46fd-870b-ca0de5950f7a", "node_type": "4", "metadata": {"page_label": "1017", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ba95756cb7345dd339577086c12d1abcb11c4f7f2b291fee83dd18783d20d2be", "class_name": "RelatedNodeInfo"}}, "text": "1017 Statistics\nproviding three tempting but false statements. An in-depth discussion of these points can\nbe found in Morey etal.(2016).\n\u000fFallacy1 . Narrow confidence intervals mean we can estimate the parameter precisely.\n\u000fFallacy2 . The values inside the confidence interval are more likely to be the true value\nthan those outside the interval.\n\u000fFallacy 3 . The probability that a particular observed 95%confidence interval contains\nthe true value is 95%.\nSufficed to say, confidence intervals are subtle objects. However, if you keep the interpre-\ntation clear, they can be powerful tools.\nA Gaussian Example\nLet\u2019sdiscussthemostclassicalexample,theconfidenceintervalforthemeanofaGaussian\nof unknown mean and variance. Suppose we collect \ud835\udc5bsamplesf\ud835\udc65\ud835\udc56g\ud835\udc5b\n\ud835\udc56=1from our Gaussian\nN\u00b9\ud835\udf07,\ud835\udf0e2\u00ba. We can compute estimators for the mean and variance by taking\n\u02c6\ud835\udf07\ud835\udc5b=1\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc65\ud835\udc56and \u02c6\ud835\udf0e2\n\ud835\udc5b=1\n\ud835\udc5b\u00001\ud835\udc5b\u00d5\n\ud835\udc56=1\u00b9\ud835\udc65\ud835\udc56\u0000\u02c6\ud835\udf07\u00ba2. (A.9)\nIf we now consider the random variable\n\ud835\udc47=\u02c6\ud835\udf07\ud835\udc5b\u0000\ud835\udf07\n\u02c6\ud835\udf0e\ud835\udc5b\u009dp\ud835\udc5b, (A.10)\nwe obtain a random variable following a well-known distribution called the Student\u2019s t-\ndistributionon \ud835\udc5b\u00001degreesof freedom .\nThis distribution is very well studied, and it is known, for instance, that as \ud835\udc5b!1, it is\napproximately a standard Gaussian, and thus by looking up values of the Gaussian c.d.f. in\na table, we may conclude that the value of \ud835\udc47is in the interval\u00bb\u00001.96,1.96\u00bcat least 95%\nof the time. For finite values of \ud835\udc5b, the interval needs to be somewhat larger, but are well\nknown and precomputed in tables.\nThus, we may conclude that for large \ud835\udc5b,\n\ud835\udc43\u0012\u02c6\ud835\udf07\ud835\udc5b\u0000\ud835\udf07\n\u02c6\ud835\udf0e\ud835\udc5b\u009dp\ud835\udc5b2\u00bb\u0000 1.96,1.96\u00bc\u0013\n\u00150.95. (A.11)\nRearrangingthisbymultiplyingbothsidesby \u02c6\ud835\udf0e\ud835\udc5b\u009dp\ud835\udc5bandthenadding \u02c6\ud835\udf07\ud835\udc5b,weobtain\n\ud835\udc43\u0012\n\ud835\udf072\u0014\n\u02c6\ud835\udf07\ud835\udc5b\u00001.96\u02c6\ud835\udf0e\ud835\udc5bp\ud835\udc5b,\u02c6\ud835\udf07\ud835\udc5b\u00b81.96\u02c6\ud835\udf0e\ud835\udc5bp\ud835\udc5b\u0015\u0013\n\u00150.95. (A.12)\nThus we know that we have found our 95%confidence interval:\n\u0014\n\u02c6\ud835\udf07\ud835\udc5b\u00001.96\u02c6\ud835\udf0e\ud835\udc5bp\ud835\udc5b,\u02c6\ud835\udf07\ud835\udc5b\u00b81.96\u02c6\ud835\udf0e\ud835\udc5bp\ud835\udc5b\u0015\n. (A.13)\nIt is safe to say that (A.13 )is one of the most used formula in statistics. Let\u2019s close our", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1879, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fa54a59-ede9-41ec-a26a-dcf5fde958b7": {"__data__": {"id_": "6fa54a59-ede9-41ec-a26a-dcf5fde958b7", "embedding": null, "metadata": {"page_label": "1018", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "492f26e7-34b3-45ba-801a-531824a6f307", "node_type": "4", "metadata": {"page_label": "1018", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bc461bf7c45f69ba3e2e16656f25737ff40d79a6a1be103e7ca751a7b1ea073c", "class_name": "RelatedNodeInfo"}}, "text": "1018 Mathematics for Deep Learning\ndiscussion of statisticsbyimplementing it. Forsimplicity, weassume we are in the asymp-\ntotic regime. Small values of \ud835\udc41should include the correct value of t_starobtained either\nprogrammatically or from a \ud835\udc61-table.\n# PyTorch uses Bessel's correction by default, which means the use of ddof=1\n# instead of default ddof=0 in numpy. We can use unbiased=False to imitate\n# ddof=0.\n# Number of samples\nN=1000\n# Sample dataset\nsamples =torch .normal( 0,1, size =(N,))\n# Lookup Students's t-distribution c.d.f.\nt_star =1.96\n# Construct interval\nmu_hat =torch .mean(samples)\nsigma_hat =samples .std(unbiased =True )\n(mu_hat -t_star *sigma_hat /torch .sqrt(torch .tensor(N, dtype =torch .float32)),\\\nmu_hat +t_star *sigma_hat /torch .sqrt(torch .tensor(N, dtype =torch .float32)))\n(tensor( -0.0568 ), tensor( 0.0704 ))\nA.10.4Summary\n\u000fStatistics focuses on inference problems, whereas deep learning emphasizes on making\naccurate predictions without explicitly programming and understanding.\n\u000fTherearethreecommonstatisticsinferencemethods: evaluatingandcomparingestima-\ntors, conducting hypothesis tests, and constructing confidence intervals.\n\u000fThere are three most common estimators: statistical bias, standard deviation, and mean\nsquare error.\n\u000fA confidence interval is an estimated range of a true population parameter that we can\nconstruct by given the samples.\n\u000fHypothesis testing is a way of evaluating some evidence against the default statement\nabout a population.\nA.10.5Exercises\n1.Let\ud835\udc4b1,\ud835\udc4b2,...,\ud835\udc4b\ud835\udc5biid\u0018Unif\u00b90,\ud835\udf03\u00ba, where \u201ciid\u201d stands for independent and identically\ndistributed . Consider the following estimators of \ud835\udf03:\n\u02c6\ud835\udf03=maxf\ud835\udc4b1,\ud835\udc4b2,...,\ud835\udc4b\ud835\udc5bg; (A.14)\n\u02dc\ud835\udf03=2\u00af\ud835\udc4b\ud835\udc5b=2\n\ud835\udc5b\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc4b\ud835\udc56. (A.15)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1704, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d442b39d-325b-4fcc-b611-1451523f279a": {"__data__": {"id_": "d442b39d-325b-4fcc-b611-1451523f279a", "embedding": null, "metadata": {"page_label": "1019", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "717eefd6-75f1-44ff-8817-5aa70f56b554", "node_type": "4", "metadata": {"page_label": "1019", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8c5bff2473a32ebc39d71c9a7e617675133e67f244f215a648318c311e7bade4", "class_name": "RelatedNodeInfo"}}, "text": "1019 Information Theory\n289\u000fFind the statistical bias, standard deviation, and mean square error of \u02c6\ud835\udf03.\n\u000fFind the statistical bias, standard deviation, and mean square error of \u02dc\ud835\udf03.\n\u000fWhich estimator is better?\n2.For our chemist example in introduction, can you derive the 5 steps to conduct a two-\nsided hypothesis testing? Given the statistical significance level \ud835\udefc=0.05and the sta-\ntistical power 1\u0000\ud835\udefd=0.8.\n3.Run the confidence interval code with \ud835\udc41=2and\ud835\udefc=0.5for100independently gener-\nated dataset, and plot the resulting intervals (in this case t_star = 1.0 ). You will see\nseveral very short intervals which are very far from containing the true mean 0. Does\nthis contradict the interpretation of the confidence interval? Do you feel comfortable\nusing short intervals to indicate high precision estimates?\nDiscussions289.\nA.11InformationTheory\nThe universe is overflowing with information. Information provides a common language\nacrossdisciplinaryrifts: fromShakespeare\u2019sSonnettoresearchers\u2019paperonCornellArXiv,\nfrom Van Gogh\u2019s printing Starry Night to Beethoven\u2019s music Symphony No. 5, from the\nfirstprogramminglanguagePlankalk\u00fcltothestate-of-the-artmachinelearningalgorithms.\nEverything must follow the rules of information theory, no matter the format. With infor-\nmation theory, we can measure and compare how much information is present in different\nsignals. Inthissection, wewillinvestigatethefundamentalconceptsofinformationtheory\nand applications of information theory in machine learning.\nBefore we get started, let\u2019s outline the relationship between machine learning and informa-\ntiontheory. Machinelearningaimstoextractinterestingsignalsfromdataandmakecritical\npredictions. On the other hand, information theory studies encoding, decoding, transmit-\nting, and manipulating information. As a result, information theory provides fundamental\nlanguagefordiscussingtheinformationprocessinginmachinelearnedsystems. Forexam-\nple, many machine learning applications use the cross-entropy loss as described in Section\n4.1. This loss can be directly derived from information theoretic considerations.\nA.11.1Information\nLet\u2019s start with the \u201csoul\u201d of information theory: information. Information can be encoded\nin anything with a particular sequence of one or more encoding formats. Suppose that we\ntask ourselves with trying to define a notion of information. What could be our starting\npoint?\nConsider the following thought experiment. We have a friend with a deck of cards. They", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2483, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc40fca0-96c3-4e69-a1d2-a09372e7aa11": {"__data__": {"id_": "dc40fca0-96c3-4e69-a1d2-a09372e7aa11", "embedding": null, "metadata": {"page_label": "1020", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e0cd431e-ad0c-45f0-a6e5-207c087fe494", "node_type": "4", "metadata": {"page_label": "1020", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "81ce9a22903e77e640843efb0b7d6bd86e37c85cfe74e7508ea00ed6a7d61777", "class_name": "RelatedNodeInfo"}}, "text": "1020 Mathematics for Deep Learning\nwill shuffle the deck, flip over some cards, and tell us statements about the cards. We will\ntry to assess the information content of each statement.\nFirst, they flip over a card and tell us, \u201cI see a card.\u201d This provides us with no information\nat all. We were already certain that this was the case so we hope the information should be\nzero.\nNext, they flip over a card and say, \u201cI see a heart.\u201d This provides us some information,\nbut in reality there are only 4different suits that were possible, each equally likely, so we\nare not surprised by this outcome. We hope that whatever the measure of information, this\nevent should have low information content.\nNext, they flip over a card and say, \u201cThis is the 3of spades.\u201d This is more information.\nIndeed there were 52equally likely possible outcomes, and our friend told us which one it\nwas. This should be a medium amount of information.\nLet\u2019stakethistothelogicalextreme. Supposethatfinallytheyflipovereverycardfromthe\ndeck and read off the entire sequence of the shuffled deck. There are 52!different orders\nto the deck, again all equally likely, so we need a lot of information to know which one it\nis.\nAny notion of information we develop must conform to this intuition. Indeed, in the next\nsections we will learn how to compute that these events have 0bits, 2bits, 5.7bits, and\n225.6bits of information respectively.\nIf we read through these thought experiments, we see a natural idea. As a starting point,\nrather than caring about the knowledge, we may build off the idea that information repre-\nsentsthedegreeofsurpriseortheabstractpossibilityoftheevent. Forexample, ifwewant\nto describe an unusual event, we need a lot information. For a common event, we may not\nneed much information.\nIn 1948, Claude E. Shannon published A Mathematical Theory of Communication (Shan-\nnon, 1948 ) establishing the theory of information. In his article, Shannon introduced the\nconcept of information entropy for the first time. We will begin our journey here.\nSelf-information\nSince information embodies the abstract possibility of an event, how do we map the pos-\nsibility to the number of bits? Shannon introduced the terminology bitas the unit of in-\nformation, which was originally created by John Tukey. So what is a \u201cbit\u201d and why do we\nuse it to measure information? Historically, an antique transmitter can onlysend or receive\ntwo types of code: 0and1. Indeed, binary encoding is still in common use on all modern\ndigital computers. In this way, any information is encoded by a series of 0and 1. And\nhence, a series of binary digits of length \ud835\udc5bcontains\ud835\udc5bbits of information.\nNow, suppose that for any series of codes, each 0or1occurs with a probability of1\n2.\nHence, an event \ud835\udc4bwith a series of codes of length \ud835\udc5b, occurs with a probability of1\n2\ud835\udc5b. At\nthe same time, as we mentioned before, this series contains \ud835\udc5bbits of information. So, can", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2922, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55c43a4d-85d5-4aac-a9cf-726606322bbe": {"__data__": {"id_": "55c43a4d-85d5-4aac-a9cf-726606322bbe", "embedding": null, "metadata": {"page_label": "1021", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "983ef592-fc28-44b2-a9a8-2ef46879f1ce", "node_type": "4", "metadata": {"page_label": "1021", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "180cadfd33648924deb402eb5e45f06cb7ef2fef3af9444c92eabe01a78b607f", "class_name": "RelatedNodeInfo"}}, "text": "1021 Information Theory\nwegeneralizetoamathematicalfunctionwhichcantransfertheprobability \ud835\udc5dtothenumber\nof bits? Shannon gave the answer by defining self-information\n\ud835\udc3c\u00b9\ud835\udc4b\u00ba=\u0000log2\u00b9\ud835\udc5d\u00ba, (A.1)\nas thebitsof information we have received for this event \ud835\udc4b. Note that we will always use\nbase-2logarithmsinthissection. Forthesakeofsimplicity,therestofthissectionwillomit\nthe subscript 2 in the logarithm notation, i.e., log\u00b9.\u00baalways refers to log2\u00b9.\u00ba. For example,\nthe code \u201c0010\u201d has a self-information\n\ud835\udc3c\u00b9\u201d0010\u201d\u00ba=\u0000log\u00b9\ud835\udc5d\u00b9\u201d0010\u201d\u00ba\u00ba=\u0000log\u00121\n24\u0013\n=4bits. (A.2)\nWe can calculate self information as shown below. Before that, let\u2019s first import all the\nnecessary packages in this section.\nimport torch\nfrom torch .nnimport NLLLoss\ndef nansum (x):\n# Define nansum, as pytorch does not offer it inbuilt.\nreturn x[~torch .isnan(x)] .sum()\ndef self_information (p):\nreturn -torch .log2(torch .tensor(p)) .item()\nself_information( 1/64)\n6.0\nA.11.2Entropy\nAs self-information only measures the information of a single discrete event, we need a\nmore generalized measure for any random variable of either discrete or continuous distri-\nbution.\nMotivatingEntropy\nLet\u2019strytogetspecificaboutwhatwewant. Thiswillbeaninformalstatementofwhatare\nknown as the axioms of Shannon entropy . It will turn out that the following collection of\ncommon-sense statements force us to a unique definition of information. A formal version\nof these axioms, along with several others may be found in Csisz\u00e1r ( 2008).\n1.The information we gain by observing a random variable does not depend on what we\ncall the elements, or the presence of additional elements which have probability zero.\n2.The information we gain by observing two random variables is no more than the sum\nof the information we gain by observing them separately. If they are independent, then\nit is exactly the sum.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1830, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8d9475c-207f-409d-ad51-efb1dc40ac29": {"__data__": {"id_": "d8d9475c-207f-409d-ad51-efb1dc40ac29", "embedding": null, "metadata": {"page_label": "1022", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86b3dac2-9179-40d3-b464-e76178b124bf", "node_type": "4", "metadata": {"page_label": "1022", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e11e18e14a2bfa01ffd68ddbaadf32173f817a629b115f13fea2d5df9809bca9", "class_name": "RelatedNodeInfo"}}, "text": "1022 Mathematics for Deep Learning\n3.The information gained when observing (nearly) certain events is (nearly) zero.\nWhile proving this fact is beyond the scope of our text, it is important to know that this\nuniquely determines the form that entropy must take. The only ambiguity that these allow\nisinthechoiceoffundamentalunits, whichismostoftennormalizedbymakingthechoice\nwe saw before that the information provided by a single fair coin flip is one bit.\nDefinition\nFor any random variable \ud835\udc4bthat follows a probability distribution \ud835\udc43with a probability den-\nsityfunction(p.d.f.) oraprobabilitymassfunction(p.m.f.) \ud835\udc5d\u00b9\ud835\udc65\u00ba,wemeasuretheexpected\namount of information through entropy(orShannonentropy )\n\ud835\udc3b\u00b9\ud835\udc4b\u00ba=\u0000\ud835\udc38\ud835\udc65\u0018\ud835\udc43\u00bblog\ud835\udc5d\u00b9\ud835\udc65\u00ba\u00bc. (A.3)\nTo be specific, if \ud835\udc4bis discrete,\n\ud835\udc3b\u00b9\ud835\udc4b\u00ba=\u0000\u00d5\n\ud835\udc56\ud835\udc5d\ud835\udc56log\ud835\udc5d\ud835\udc56, where\ud835\udc5d\ud835\udc56=\ud835\udc43\u00b9\ud835\udc4b\ud835\udc56\u00ba.(A.4)\nOtherwise, if \ud835\udc4bis continuous, we also refer entropy as differentialentropy\n\ud835\udc3b\u00b9\ud835\udc4b\u00ba=\u0000\u00b9\n\ud835\udc65\ud835\udc5d\u00b9\ud835\udc65\u00balog\ud835\udc5d\u00b9\ud835\udc65\u00ba\ud835\udc51\ud835\udc65. (A.5)\nWe can define entropy as below.\ndef entropy (p):\nentropy =-p*torch .log2(p)\n# Operator `nansum` will sum up the non-nan number\nout =nansum(entropy)\nreturn out\nentropy(torch .tensor([ 0.1,0.5,0.1,0.3]))\ntensor( 1.6855 )\nInterpretations\nYou may be curious: in the entropy definition (A.3), why do we use an expectation of a\nnegative logarithm? Here are some intuitions.\nFirst, why do we use a logarithm function log? Suppose that \ud835\udc5d\u00b9\ud835\udc65\u00ba=\ud835\udc531\u00b9\ud835\udc65\u00ba\ud835\udc532\u00b9\ud835\udc65\u00ba..., \ud835\udc53\ud835\udc5b\u00b9\ud835\udc65\u00ba,\nwhereeachcomponentfunction \ud835\udc53\ud835\udc56\u00b9\ud835\udc65\u00baisindependentfromeachother. Thismeansthateach\n\ud835\udc53\ud835\udc56\u00b9\ud835\udc65\u00bacontributes independently to the total information obtained from \ud835\udc5d\u00b9\ud835\udc65\u00ba. As discussed\nabove, we want the entropy formula to be additive over independent random variables.\nLuckily, logcan naturally turn a product of probability distributions to a summation of the\nindividual terms.\nNext, why do we use a negative log? Intuitively, more frequent events should contain less", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8f2f9b6-7855-430a-aa2b-16bcdc5a8527": {"__data__": {"id_": "d8f2f9b6-7855-430a-aa2b-16bcdc5a8527", "embedding": null, "metadata": {"page_label": "1023", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f08388fc-5680-40a0-9504-66e18baaaada", "node_type": "4", "metadata": {"page_label": "1023", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f980abf2308a039bc716e6c0dbb08bc650c53539b8e754135b0a6ad3a87c0baa", "class_name": "RelatedNodeInfo"}}, "text": "1023 Information Theory\ninformation than less common events, since we often gain more information from an un-\nusual case than from an ordinary one. However, logis monotonically increasing with the\nprobabilities, and indeed negative for all values in \u00bb0,1\u00bc. We need to construct a monoton-\nically decreasing relationship between the probability of events and their entropy, which\nwill ideally be always positive (for nothing we observe should force us to forget what we\nhave known). Hence, we add a negative sign in front of logfunction.\nLast, where does the expectation function come from? Consider a random variable \ud835\udc4b. We\ncan interpret the self-information ( \u0000log\u00b9\ud835\udc5d\u00ba) as the amount of surprise we have at seeing\na particular outcome. Indeed, as the probability approaches zero, the surprise becomes\ninfinite. Similarly, we can interpret the entropy as the average amount of surprise from\nobserving\ud835\udc4b. For example, imagine that a slot machine system emits statistical indepen-\ndently symbols \ud835\udc601,...,\ud835\udc60\ud835\udc58with probabilities \ud835\udc5d1,...,\ud835\udc5d\ud835\udc58respectively. Then the entropy of\nthis system equals to the average self-information from observing each output, i.e.,\n\ud835\udc3b\u00b9\ud835\udc46\u00ba=\u00d5\n\ud835\udc56\ud835\udc5d\ud835\udc56\u0001\ud835\udc3c\u00b9\ud835\udc60\ud835\udc56\u00ba=\u0000\u00d5\n\ud835\udc56\ud835\udc5d\ud835\udc56\u0001log\ud835\udc5d\ud835\udc56.(A.6)\nPropertiesof Entropy\nBy the above examples and interpretations, we can derive the following properties of en-\ntropy (A.3). Here, we refer to \ud835\udc4bas an event and \ud835\udc43as the probability distribution of\n\ud835\udc4b.\n\u000f\ud835\udc3b\u00b9\ud835\udc4b\u00ba\u00150for all discrete \ud835\udc4b(entropy can be negative for continuous \ud835\udc4b).\n\u000fIf\ud835\udc4b\u0018\ud835\udc43with a p.d.f. or a p.m.f. \ud835\udc5d\u00b9\ud835\udc65\u00ba, and we try to estimate \ud835\udc43by a new probability\ndistribution\ud835\udc44with a p.d.f. or a p.m.f. \ud835\udc5e\u00b9\ud835\udc65\u00ba, then\n\ud835\udc3b\u00b9\ud835\udc4b\u00ba=\u0000\ud835\udc38\ud835\udc65\u0018\ud835\udc43\u00bblog\ud835\udc5d\u00b9\ud835\udc65\u00ba\u00bc\u0014\u0000\ud835\udc38\ud835\udc65\u0018\ud835\udc43\u00bblog\ud835\udc5e\u00b9\ud835\udc65\u00ba\u00bc,with equality if and only if \ud835\udc43=\ud835\udc44.\n(A.7)\nAlternatively, \ud835\udc3b\u00b9\ud835\udc4b\u00bagives a lower bound of the average number of bits needed to\nencode symbols drawn from \ud835\udc43.\n\u000fIf\ud835\udc4b\u0018\ud835\udc43,then\ud835\udc65conveysthemaximumamountofinformationifitspreadsevenlyamong\nall possible outcomes. Specifically, if the probability distribution \ud835\udc43is discrete with\n\ud835\udc58-classf\ud835\udc5d1,...,\ud835\udc5d\ud835\udc58g, then\n\ud835\udc3b\u00b9\ud835\udc4b\u00ba\u0014log\u00b9\ud835\udc58\u00ba,with equality if and only if \ud835\udc5d\ud835\udc56=1\n\ud835\udc58,8\ud835\udc56. (A.8)\nIf\ud835\udc43isacontinuousrandomvariable,thenthestorybecomesmuchmorecomplicated.\nHowever, if we additionally impose that \ud835\udc43is supported on a finite interval (with all\nvaluesbetween 0and1),then\ud835\udc43hasthehighestentropyifitistheuniformdistribution\non that interval.\nA.11.3MutualInformation\nPreviously we defined entropy of a single random variable \ud835\udc4b, how about the entropy of a\npair random variables \u00b9\ud835\udc4b,\ud835\udc4c\u00ba? We can think of these techniques as trying to answer the", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6fa8f98d-d005-4856-9539-dbd1693c2a49": {"__data__": {"id_": "6fa8f98d-d005-4856-9539-dbd1693c2a49", "embedding": null, "metadata": {"page_label": "1024", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "126b16cd-8866-4519-979a-fbb79c99a932", "node_type": "4", "metadata": {"page_label": "1024", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f7bab55cd3bd034027e97cf05f778a3dbd80650890d019ab2fe6cbe5d9e06e50", "class_name": "RelatedNodeInfo"}}, "text": "1024 Mathematics for Deep Learning\nfollowing type of question, \u201cWhat information is contained in \ud835\udc4band\ud835\udc4ctogether compared\nto each separately? Is there redundant information, or is it all unique?\u201d\nForthefollowingdiscussion,wealwaysuse \u00b9\ud835\udc4b,\ud835\udc4c\u00baasapairofrandomvariablesthatfollows\na joint probability distribution \ud835\udc43with a p.d.f. or a p.m.f. \ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba, while\ud835\udc4band\ud835\udc4cfollow\nprobability distribution \ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00baand\ud835\udc5d\ud835\udc4c\u00b9\ud835\udc66\u00ba, respectively.\nJointEntropy\nSimilar to entropy of a single random variable (A.3), we define the joint entropy \ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba\nof a pair random variables \u00b9\ud835\udc4b,\ud835\udc4c\u00baas\n\ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\u0000\ud835\udc38\u00b9\ud835\udc65,\ud835\udc66\u00ba\u0018\ud835\udc43\u00bblog\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba\u00bc. (A.9)\nPrecisely, on the one hand, if \u00b9\ud835\udc4b,\ud835\udc4c\u00bais a pair of discrete random variables, then\n\ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\u0000\u00d5\n\ud835\udc65\u00d5\n\ud835\udc66\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00balog\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba.(A.10)\nOn the other hand, if \u00b9\ud835\udc4b,\ud835\udc4c\u00bais a pair of continuous random variables, then we define the\ndifferentialjoint entropy as\n\ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\u0000\u00b9\n\ud835\udc65,\ud835\udc66\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00balog\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66. (A.11)\nWe can think of (A.9)as telling us the total randomness in the pair of random variables.\nAs a pair of extremes, if \ud835\udc4b=\ud835\udc4care two identical random variables, then the information in\nthe pair is exactly the information in one and we have \ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\ud835\udc3b\u00b9\ud835\udc4b\u00ba=\ud835\udc3b\u00b9\ud835\udc4c\u00ba. On the\nother extreme, if \ud835\udc4band\ud835\udc4care independent then \ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\ud835\udc3b\u00b9\ud835\udc4b\u00ba\u00b8\ud835\udc3b\u00b9\ud835\udc4c\u00ba. Indeed we will\nalways have that the information contained in a pair of random variables is no smaller than\nthe entropy of either random variable and no more than the sum of both.\n\ud835\udc3b\u00b9\ud835\udc4b\u00ba,\ud835\udc3b\u00b9\ud835\udc4c\u00ba\u0014\ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba\u0014\ud835\udc3b\u00b9\ud835\udc4b\u00ba\u00b8\ud835\udc3b\u00b9\ud835\udc4c\u00ba. (A.12)\nLet\u2019s implement joint entropy from scratch.\ndef joint_entropy (p_xy):\njoint_ent =-p_xy *torch .log2(p_xy)\n# Operator `nansum` will sum up the non-nan number\nout =nansum(joint_ent)\nreturn out\njoint_entropy(torch .tensor([[ 0.1,0.5], [ 0.1,0.3]]))\ntensor( 1.6855 )\nNotice that this is the same codeas before, but now we interpret it differently as working\non the joint distribution of the two random variables.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ada0f6ff-086d-41d1-a52d-0e458e57050f": {"__data__": {"id_": "ada0f6ff-086d-41d1-a52d-0e458e57050f", "embedding": null, "metadata": {"page_label": "1025", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61895bb2-23d8-47cc-b4b7-0c05bcfaeecd", "node_type": "4", "metadata": {"page_label": "1025", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "9631d36492aeaa2509e3ed5eaef6422f5e91b5f5f3c8806c5dd2a5db5ba21d54", "class_name": "RelatedNodeInfo"}}, "text": "1025 Information Theory\nConditionalEntropy\nThe joint entropy defined above the amount of information contained in a pair of random\nvariables. This is useful, but oftentimes it is not what we care about. Consider the setting\nofmachinelearning. Let\u2019stake \ud835\udc4btobetherandomvariable(orvectorofrandomvariables)\nthat describes the pixel values of an image, and \ud835\udc4cto be the random variable which is the\nclass label.\ud835\udc4bshould contain substantial information\u2014a natural image is a complex thing.\nHowever, the information contained in \ud835\udc4conce the image has been show should be low.\nIndeed, the image of a digit should already contain the information about what digit it is\nunless the digit is illegible. Thus, to continue to extend our vocabulary of information\ntheory, we need to be able to reason about the information content in a random variable\nconditional on another.\nIn the probability theory, we saw the definition of the conditional probability to measure\nthe relationship between variables. We now want to analogously define the conditional\nentropy\ud835\udc3b\u00b9\ud835\udc4cj\ud835\udc4b\u00ba. We can write this as\n\ud835\udc3b\u00b9\ud835\udc4cj\ud835\udc4b\u00ba=\u0000\ud835\udc38\u00b9\ud835\udc65,\ud835\udc66\u00ba\u0018\ud835\udc43\u00bblog\ud835\udc5d\u00b9\ud835\udc66j\ud835\udc65\u00ba\u00bc, (A.13)\nwhere\ud835\udc5d\u00b9\ud835\udc66j\ud835\udc65\u00ba=\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba\n\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00bais the conditional probability. Specifically, if \u00b9\ud835\udc4b,\ud835\udc4c\u00bais a pair of\ndiscrete random variables, then\n\ud835\udc3b\u00b9\ud835\udc4cj\ud835\udc4b\u00ba=\u0000\u00d5\n\ud835\udc65\u00d5\n\ud835\udc66\ud835\udc5d\u00b9\ud835\udc65,\ud835\udc66\u00balog\ud835\udc5d\u00b9\ud835\udc66j\ud835\udc65\u00ba.(A.14)\nIf\u00b9\ud835\udc4b,\ud835\udc4c\u00bais a pair of continuous random variables, then the differentialconditionalentropy\nis similarly defined as\n\ud835\udc3b\u00b9\ud835\udc4cj\ud835\udc4b\u00ba=\u0000\u00b9\n\ud835\udc65\u00b9\n\ud835\udc66\ud835\udc5d\u00b9\ud835\udc65,\ud835\udc66\u00balog\ud835\udc5d\u00b9\ud835\udc66j\ud835\udc65\u00ba\ud835\udc51\ud835\udc65 \ud835\udc51\ud835\udc66. (A.15)\nIt is now natural to ask, how does the conditional entropy \ud835\udc3b\u00b9\ud835\udc4cj\ud835\udc4b\u00barelate to the entropy\n\ud835\udc3b\u00b9\ud835\udc4b\u00baand the joint entropy \ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba? Using the definitions above, we can express this\ncleanly:\n\ud835\udc3b\u00b9\ud835\udc4cj\ud835\udc4b\u00ba=\ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba\u0000\ud835\udc3b\u00b9\ud835\udc4b\u00ba. (A.16)\nThis has an intuitive interpretation: the information in \ud835\udc4cgiven\ud835\udc4b(\ud835\udc3b\u00b9\ud835\udc4cj\ud835\udc4b\u00ba) is the same\nas the information in both \ud835\udc4band\ud835\udc4ctogether (\ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba) minus the information already con-\ntained in\ud835\udc4b. This gives us the information in \ud835\udc4cwhich is not also represented in \ud835\udc4b.\nNow, let\u2019s implement conditional entropy (A.13 )from scratch.\ndef conditional_entropy (p_xy, p_x):\np_y_given_x =p_xy /p_x\ncond_ent =-p_xy *torch .log2(p_y_given_x)\n# Operator `nansum` will sum up the non-nan number\nout =nansum(cond_ent)\nreturn out\nconditional_entropy(torch .tensor([[ 0.1,0.5], [ 0.2,0.3]]),\ntorch .tensor([ 0.2,0.8]))", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9b5bb047-bb25-4edd-90f9-ec19cf4a04de": {"__data__": {"id_": "9b5bb047-bb25-4edd-90f9-ec19cf4a04de", "embedding": null, "metadata": {"page_label": "1026", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5da67f7-cb8a-4822-9aed-5886d8404d15", "node_type": "4", "metadata": {"page_label": "1026", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c2b2acdc808bd07c7735d5aea133cf5517e7b3312f5d5a24994a77f23503a158", "class_name": "RelatedNodeInfo"}}, "text": "1026 Mathematics for Deep Learning\ntensor( 0.8635 )\nMutualInformation\nGiven the previous setting of random variables \u00b9\ud835\udc4b,\ud835\udc4c\u00ba, you may wonder: \u201cNow that we\nknowhowmuchinformationiscontainedin \ud835\udc4cbutnotin\ud835\udc4b,canwesimilarlyaskhowmuch\ninformation is shared between \ud835\udc4band\ud835\udc4c?\u201d The answer will be the mutual information of\n\u00b9\ud835\udc4b,\ud835\udc4c\u00ba, which we will write as \ud835\udc3c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba.\nRather than diving straight into the formal definition, let\u2019s practice our intuition by first\ntrying to derive an expression for the mutual information entirely based on terms we have\nconstructed before. Wewish to find the information shared betweentwo random variables.\nOne way we could try to do this is to start with all the information contained in both \ud835\udc4band\n\ud835\udc4ctogether, and then we take off the parts that are not shared. The information contained in\nboth\ud835\udc4band\ud835\udc4ctogetheriswrittenas \ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba. Wewanttosubtractfromthistheinformation\ncontained in \ud835\udc4bbut not in\ud835\udc4c, and the information contained in \ud835\udc4cbut not in\ud835\udc4b. As we saw in\nthe previous section, this is given by \ud835\udc3b\u00b9\ud835\udc4bj\ud835\udc4c\u00baand\ud835\udc3b\u00b9\ud835\udc4cj\ud835\udc4b\u00barespectively. Thus, we have\nthat the mutual information should be\n\ud835\udc3c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba\u0000\ud835\udc3b\u00b9\ud835\udc4cj\ud835\udc4b\u00ba\u0000\ud835\udc3b\u00b9\ud835\udc4bj\ud835\udc4c\u00ba. (A.17)\nIndeed,thisisavaliddefinitionforthemutualinformation. Ifweexpandoutthedefinitions\nof these terms and combine them, a little algebra shows that this is the same as\n\ud835\udc3c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\ud835\udc38\ud835\udc65\ud835\udc38\ud835\udc66\u001a\n\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00balog\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba\n\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba\ud835\udc5d\ud835\udc4c\u00b9\ud835\udc66\u00ba\u001b\n. (A.18)\nWe can summarize all of these relationships in image Fig. A.1 . It is an excellent test of\nintuition to see why the following statements are all also equivalent to \ud835\udc3c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba.\n\u000f\ud835\udc3b\u00b9\ud835\udc4b\u00ba\u0000\ud835\udc3b\u00b9\ud835\udc4bj\ud835\udc4c\u00ba\n\u000f\ud835\udc3b\u00b9\ud835\udc4c\u00ba\u0000\ud835\udc3b\u00b9\ud835\udc4cj\ud835\udc4b\u00ba\n\u000f\ud835\udc3b\u00b9\ud835\udc4b\u00ba\u00b8\ud835\udc3b\u00b9\ud835\udc4c\u00ba\u0000\ud835\udc3b\u00b9\ud835\udc4b,\ud835\udc4c\u00ba\ntFig. A.1 Mutual information\u2019s relationship with joint entropy and conditional entropy.\nIn many ways we can think of the mutual information (A.18 )as principled extension of\ncorrelation coefficient we saw in Section A.6 . This allows us to ask not only for linear", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6aaf7fa0-f7f4-45b8-9c86-fdd98956ff61": {"__data__": {"id_": "6aaf7fa0-f7f4-45b8-9c86-fdd98956ff61", "embedding": null, "metadata": {"page_label": "1027", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bd41028f-590a-49f1-88df-bd062f17fd4a", "node_type": "4", "metadata": {"page_label": "1027", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "39a23a1c3dfb45f32bf777154c2c3da047f7851196abc2fe9820fb1f67e8ca5a", "class_name": "RelatedNodeInfo"}}, "text": "1027 Information Theory\nrelationships between variables, but for the maximum information shared between the two\nrandom variables of any kind.\nNow, let\u2019s implement mutual information from scratch.\ndef mutual_information (p_xy, p_x, p_y):\np=p_xy /(p_x *p_y)\nmutual =p_xy *torch .log2(p)\n# Operator `nansum` will sum up the non-nan number\nout =nansum(mutual)\nreturn out\nmutual_information(torch .tensor([[ 0.1,0.5], [ 0.1,0.3]]),\ntorch .tensor([ 0.2,0.8]), torch .tensor([[ 0.75 ,0.25 ]]))\ntensor( 0.7195 )\nProperties of Mutual Information\nRatherthanmemorizingthedefinitionofmutualinformation (A.18 ),youonlyneedtokeep\nin mind its notable properties:\n\u000fMutual information is symmetric, i.e., \ud835\udc3c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\ud835\udc3c\u00b9\ud835\udc4c,\ud835\udc4b\u00ba.\n\u000fMutual information is non-negative, i.e., \ud835\udc3c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba\u00150.\n\u000f\ud835\udc3c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=0if and only if \ud835\udc4band\ud835\udc4care independent. For example, if \ud835\udc4band\ud835\udc4care in-\ndependent, then knowing \ud835\udc4cdoes not give any information about \ud835\udc4band vice versa, so\ntheir mutual information is zero.\n\u000fAlternatively, if \ud835\udc4bis an invertible function of \ud835\udc4c, then\ud835\udc4cand\ud835\udc4bshare all information and\n\ud835\udc3c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\ud835\udc3b\u00b9\ud835\udc4c\u00ba=\ud835\udc3b\u00b9\ud835\udc4b\u00ba. (A.19)\nPointwiseMutual Information\nWhen we worked with entropy at the beginning of this chapter, we were able to provide an\ninterpretation of\u0000log\u00b9\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba\u00baas howsurprised we were with the particular outcome. We\nmay give a similar interpretation to the logarithmic term in the mutual information, which\nis often referred to as the pointwise mutual information :\npmi\u00b9\ud835\udc65,\ud835\udc66\u00ba=log\ud835\udc5d\ud835\udc4b,\ud835\udc4c\u00b9\ud835\udc65,\ud835\udc66\u00ba\n\ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba\ud835\udc5d\ud835\udc4c\u00b9\ud835\udc66\u00ba. (A.20)\nWe can think of (A.20 )as measuring how much more or less likely the specific combina-\ntion of outcomes \ud835\udc65and\ud835\udc66are compared to what we would expect for independent random\noutcomes. Ifitislargeandpositive,thenthesetwospecificoutcomesoccurmuchmorefre-\nquentlythantheywouldcomparedtorandomchance( note: thedenominatoris \ud835\udc5d\ud835\udc4b\u00b9\ud835\udc65\u00ba\ud835\udc5d\ud835\udc4c\u00b9\ud835\udc66\u00ba\nwhich is the probability of the two outcomes were independent), whereas if it is large and", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1866, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1917c322-83e2-4c3b-b51e-3c3b91d28e63": {"__data__": {"id_": "1917c322-83e2-4c3b-b51e-3c3b91d28e63", "embedding": null, "metadata": {"page_label": "1028", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f7dc9c37-f551-4a6e-a623-4c2e336a84a7", "node_type": "4", "metadata": {"page_label": "1028", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e3fbec89f67fe1c6fe2e3efccf52b0f05205e85ac50a6b3cfdc6893475fbb067", "class_name": "RelatedNodeInfo"}}, "text": "1028 Mathematics for Deep Learning\nnegativeitrepresentsthetwooutcomeshappeningfarlessthanwewouldexpectbyrandom\nchance.\nThis allows us to interpret the mutual information (A.18 )as the average amount that we\nwere surprised to see two outcomes occurring together compared to what we would expect\nif they were independent.\nApplicationsof Mutual Information\nMutual information may be a little abstract in it pure definition, so how does it related to\nmachinelearning? Innaturallanguageprocessing, oneofthemostdifficultproblemsisthe\nambiguity resolution , or the issue of the meaning of a word being unclear from context.\nFor example, recently a headline in the news reported that \u201cAmazon is on fire\u201d. You may\nwonder whether the company Amazon has a building on fire, or the Amazon rain forest is\non fire.\nIn this case, mutual information can help us resolve this ambiguity. We first find the group\nof words that each has a relatively large mutual information with the company Amazon,\nsuch as e-commerce, technology, and online. Second, we find another group of words that\neach has a relatively large mutual information with the Amazon rain forest, such as rain,\nforest, and tropical. When we need to disambiguate \u201cAmazon\u201d, we can compare which\ngroup has more occurrence in the context of the word Amazon. In this case the article\nwould go on to describe the forest, and make the context clear.\nA.11.4Kullback\u2013LeiblerDivergence\nAs what we have discussed in Section 2.3 , we can use norms to measure distance between\ntwo points in space of any dimensionality. We would like to be able to do a similar task\nwithprobabilitydistributions. Therearemanywaystogoaboutthis,butinformationtheory\nprovides one of the nicest. We now explore the Kullback\u2013Leibler (KL) divergence , which\nprovides a way to measure if two distributions are close together or not.\nDefinition\nGiven a random variable \ud835\udc4bthat follows the probability distribution \ud835\udc43with a p.d.f. or a\np.m.f.\ud835\udc5d\u00b9\ud835\udc65\u00ba, and we estimate \ud835\udc43by another probability distribution \ud835\udc44with a p.d.f. or a\np.m.f.\ud835\udc5e\u00b9\ud835\udc65\u00ba. Then the Kullback\u2013Leibler (KL) divergence (orrelative entropy ) between\ud835\udc43\nand\ud835\udc44is\n\ud835\udc37KL\u00b9\ud835\udc43k\ud835\udc44\u00ba=\ud835\udc38\ud835\udc65\u0018\ud835\udc43\u0014\nlog\ud835\udc5d\u00b9\ud835\udc65\u00ba\n\ud835\udc5e\u00b9\ud835\udc65\u00ba\u0015\n. (A.21)\nAswiththepointwisemutualinformation (A.20 ),wecanagainprovideaninterpretationof\nthe logarithmic term: \u0000log\ud835\udc5e\u00b9\ud835\udc65\u00ba\n\ud835\udc5d\u00b9\ud835\udc65\u00ba=\u0000log\u00b9\ud835\udc5e\u00b9\ud835\udc65\u00ba\u00ba\u0000\u00b9\u0000 log\u00b9\ud835\udc5d\u00b9\ud835\udc65\u00ba\u00ba\u00bawill be large and positive\nif we see\ud835\udc65far more often under \ud835\udc43than we would expect for \ud835\udc44, and large and negative if\nwe see the outcome far less than expected. In this way, we can interpret it as our relative\nsurprise at observing the outcome compared to how surprised we would be observing it\nfrom our reference distribution.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da513750-5241-4602-be80-3b7edd0f3824": {"__data__": {"id_": "da513750-5241-4602-be80-3b7edd0f3824", "embedding": null, "metadata": {"page_label": "1029", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5276800f-96e3-452e-aa87-d9c96a332856", "node_type": "4", "metadata": {"page_label": "1029", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "85b2fea6f9d476762b737539ca3101fa234fe43ca3214ca2282cd22d88242ca1", "class_name": "RelatedNodeInfo"}}, "text": "1029 Information Theory\nLet\u2019s implement the KL divergence from Scratch.\ndef kl_divergence (p, q):\nkl=p*torch .log2(p /q)\nout =nansum(kl)\nreturn out.abs() .item()\nKL DivergenceProperties\nLet\u2019s take a look at some properties of the KL divergence (A.21 ).\n\u000fKL divergence is non-symmetric, i.e., there are \ud835\udc43,\ud835\udc44such that\n\ud835\udc37KL\u00b9\ud835\udc43k\ud835\udc44\u00ba\u2260\ud835\udc37KL\u00b9\ud835\udc44k\ud835\udc43\u00ba. (A.22)\n\u000fKL divergence is non-negative, i.e.,\n\ud835\udc37KL\u00b9\ud835\udc43k\ud835\udc44\u00ba\u00150. (A.23)\nNote that the equality holds only when \ud835\udc43=\ud835\udc44.\n\u000fIf there exists an \ud835\udc65such that\ud835\udc5d\u00b9\ud835\udc65\u00ba>0and\ud835\udc5e\u00b9\ud835\udc65\u00ba=0, then\ud835\udc37KL\u00b9\ud835\udc43k\ud835\udc44\u00ba=1.\n\u000fThere is a close relationship between KL divergence and mutual information. Besides\nthe relationship shown in Fig. A.1 ,\ud835\udc3c\u00b9\ud835\udc4b,\ud835\udc4c\u00bais also numerically equivalent with the\nfollowing terms:\n1.\ud835\udc37KL\u00b9\ud835\udc43\u00b9\ud835\udc4b,\ud835\udc4c\u00bak\ud835\udc43\u00b9\ud835\udc4b\u00ba\ud835\udc43\u00b9\ud835\udc4c\u00ba\u00ba;\n2.\ud835\udc38\ud835\udc4cf\ud835\udc37KL\u00b9\ud835\udc43\u00b9\ud835\udc4bj\ud835\udc4c\u00bak\ud835\udc43\u00b9\ud835\udc4b\u00ba\u00bag;\n3.\ud835\udc38\ud835\udc4bf\ud835\udc37KL\u00b9\ud835\udc43\u00b9\ud835\udc4cj\ud835\udc4b\u00bak\ud835\udc43\u00b9\ud835\udc4c\u00ba\u00bag.\nFor the first term, we interpret mutual information as the KL divergence between\n\ud835\udc43\u00b9\ud835\udc4b,\ud835\udc4c\u00baand the product of \ud835\udc43\u00b9\ud835\udc4b\u00baand\ud835\udc43\u00b9\ud835\udc4c\u00ba, and thus is a measure of how differ-\nent the joint distribution is from the distribution if they were independent. For the\nsecond term, mutual information tells us the average reduction in uncertainty about \ud835\udc4c\nthatresultsfromlearningthevalueofthe \ud835\udc4b\u2019sdistribution. Similarlytothethirdterm.\nExample\nLet\u2019s go through a toy example to see the non-symmetry explicitly.\nFirst, let\u2019s generate and sort three tensors of length 10,000: an objective tensor \ud835\udc5dwhich\nfollows a normal distribution \ud835\udc41\u00b90,1\u00ba, and two candidate tensors \ud835\udc5e1and\ud835\udc5e2which follow\nnormal distributions \ud835\udc41\u00b9\u00001,1\u00baand\ud835\udc41\u00b91,1\u00barespectively.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9e0a11ee-c4f7-41bd-8c2b-e0c2d39a5067": {"__data__": {"id_": "9e0a11ee-c4f7-41bd-8c2b-e0c2d39a5067", "embedding": null, "metadata": {"page_label": "1030", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "02b4f4f3-3929-4107-bcc9-dccc2f6fde59", "node_type": "4", "metadata": {"page_label": "1030", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e74c53c702e28ff8301cccff1df0cf054efd0569de31ce46bd9d12da916680a1", "class_name": "RelatedNodeInfo"}}, "text": "1030 Mathematics for Deep Learning\ntorch .manual_seed( 1)\ntensor_len =10000\np=torch .normal( 0,1, (tensor_len, ))\nq1=torch .normal( -1,1, (tensor_len, ))\nq2=torch .normal( 1,1, (tensor_len, ))\np=torch .sort(p)[ 0]\nq1=torch .sort(q1)[ 0]\nq2=torch .sort(q2)[ 0]\nSince\ud835\udc5e1and\ud835\udc5e2are symmetric with respect to the y-axis (i.e., \ud835\udc65=0), we expect a similar\nvalue of KL divergencebetween \ud835\udc37KL\u00b9\ud835\udc5dk\ud835\udc5e1\u00baand\ud835\udc37KL\u00b9\ud835\udc5dk\ud835\udc5e2\u00ba. As youcan see below, there\nis only a less than 3% off between \ud835\udc37KL\u00b9\ud835\udc5dk\ud835\udc5e1\u00baand\ud835\udc37KL\u00b9\ud835\udc5dk\ud835\udc5e2\u00ba.\nkl_pq1 =kl_divergence(p, q1)\nkl_pq2 =kl_divergence(p, q2)\nsimilar_percentage =abs(kl_pq1 -kl_pq2) /((kl_pq1 +kl_pq2) /2)*100\nkl_pq1, kl_pq2, similar_percentage\n(8582.0341796875 ,8828.3095703125 ,2.8290698237936858 )\nIn contrast, you may find that \ud835\udc37KL\u00b9\ud835\udc5e2k\ud835\udc5d\u00baand\ud835\udc37KL\u00b9\ud835\udc5dk\ud835\udc5e2\u00baare off a lot, with around 40%\noff as shown below.\nkl_q2p =kl_divergence(q2, p)\ndiffer_percentage =abs(kl_q2p -kl_pq2) /((kl_q2p +kl_pq2) /2)*100\nkl_q2p, differ_percentage\n(14130.125 ,46.18621024399691 )\nA.11.5Cross-Entropy\nIfyouarecuriousaboutapplicationsofinformationtheoryindeeplearning,hereisaquick\nexample. We define the true distribution \ud835\udc43with probability distribution \ud835\udc5d\u00b9\ud835\udc65\u00ba, and the\nestimated distribution \ud835\udc44with probability distribution \ud835\udc5e\u00b9\ud835\udc65\u00ba, and we will use them in the\nrest of this section.\nSay we need to solve a binary classification problem based on given \ud835\udc5bdata examples\n{\ud835\udc651,...,\ud835\udc65\ud835\udc5b}. Assume that we encode 1and 0as the positive and negative class label \ud835\udc66\ud835\udc56\nrespectively, and our neural network is parametrized by \ud835\udf03. If we aim to find a best \ud835\udf03so\nthat \u02c6\ud835\udc66\ud835\udc56=\ud835\udc5d\ud835\udf03\u00b9\ud835\udc66\ud835\udc56j\ud835\udc65\ud835\udc56\u00ba, it is natural to apply the maximum log-likelihood approach as was\nseen inSection A.7 . To be specific, for true labels \ud835\udc66\ud835\udc56and predictions \u02c6\ud835\udc66\ud835\udc56=\ud835\udc5d\ud835\udf03\u00b9\ud835\udc66\ud835\udc56j\ud835\udc65\ud835\udc56\u00ba, the\nprobability to be classified as positive is \ud835\udf0b\ud835\udc56=\ud835\udc5d\ud835\udf03\u00b9\ud835\udc66\ud835\udc56=1j\ud835\udc65\ud835\udc56\u00ba. Hence, the log-likelihood", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fa2eae93-a930-4735-98a0-f208e329207c": {"__data__": {"id_": "fa2eae93-a930-4735-98a0-f208e329207c", "embedding": null, "metadata": {"page_label": "1031", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "304a7d0f-3c3e-4362-9359-c3d5b5777dd3", "node_type": "4", "metadata": {"page_label": "1031", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d83f4b61e413f96c7dabb2eb3400a26464593285cc24916a107ead88ce2a34b3", "class_name": "RelatedNodeInfo"}}, "text": "1031 Information Theory\nfunction would be\n\ud835\udc59\u00b9\ud835\udf03\u00ba=log\ud835\udc3f\u00b9\ud835\udf03\u00ba\n=log\ud835\udc5b\u00d6\n\ud835\udc56=1\ud835\udf0b\ud835\udc66\ud835\udc56\n\ud835\udc56\u00b91\u0000\ud835\udf0b\ud835\udc56\u00ba1\u0000\ud835\udc66\ud835\udc56\n=\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc66\ud835\udc56log\u00b9\ud835\udf0b\ud835\udc56\u00ba\u00b8\u00b9 1\u0000\ud835\udc66\ud835\udc56\u00balog\u00b91\u0000\ud835\udf0b\ud835\udc56\u00ba.(A.24)\nMaximizing the log-likelihood function \ud835\udc59\u00b9\ud835\udf03\u00bais identical to minimizing \u0000\ud835\udc59\u00b9\ud835\udf03\u00ba, and hence\nwe can find the best \ud835\udf03from here. To generalize the above loss to any distributions, we also\ncalled\u0000\ud835\udc59\u00b9\ud835\udf03\u00bathecross-entropyloss CE\u00b9\ud835\udc66,\u02c6\ud835\udc66\u00ba, where\ud835\udc66follows the true distribution \ud835\udc43and \u02c6\ud835\udc66\nfollows the estimated distribution \ud835\udc44.\nThis was all derived by working from the maximum likelihood point of view. However, if\nwelookcloselywecanseethattermslike log\u00b9\ud835\udf0b\ud835\udc56\u00bahaveenteredintoourcomputationwhich\nis a solid indication that we can understand the expression from an information theoretic\npoint of view.\nFormalDefinition\nLikeKLdivergence, forarandomvariable \ud835\udc4b, wecanalsomeasurethedivergencebetween\nthe estimating distribution \ud835\udc44and the true distribution \ud835\udc43viacross-entropy ,\nCE\u00b9\ud835\udc43,\ud835\udc44\u00ba=\u0000\ud835\udc38\ud835\udc65\u0018\ud835\udc43\u00bblog\u00b9\ud835\udc5e\u00b9\ud835\udc65\u00ba\u00ba\u00bc. (A.25)\nBy using properties of entropy discussed above, we can also interpret it as the summation\nof the entropy \ud835\udc3b\u00b9\ud835\udc43\u00baand the KL divergence between \ud835\udc43and\ud835\udc44, i.e.,\nCE\u00b9\ud835\udc43,\ud835\udc44\u00ba=\ud835\udc3b\u00b9\ud835\udc43\u00ba\u00b8\ud835\udc37KL\u00b9\ud835\udc43k\ud835\udc44\u00ba. (A.26)\nWe can implement the cross-entropy loss as below.\ndef cross_entropy (y_hat, y):\nce=-torch .log(y_hat[ range (len(y_hat)), y])\nreturn ce.mean()\nNow define two tensors for the labels and predictions, and calculate the cross-entropy loss\nof them.\nlabels =torch .tensor([ 0,2])\npreds =torch .tensor([[ 0.3,0.6,0.1], [ 0.2,0.3,0.5]])\ncross_entropy(preds, labels)\ntensor( 0.9486 )", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1452, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ccff6b56-5230-4e30-a58f-203513ee8223": {"__data__": {"id_": "ccff6b56-5230-4e30-a58f-203513ee8223", "embedding": null, "metadata": {"page_label": "1032", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8537139b-95f0-4aa1-82e6-e6c0686c7b93", "node_type": "4", "metadata": {"page_label": "1032", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b4b0a7a62cbe5b36c1395cd2d6fa4df001cd84d7f82dc9ea8c28a9a98fc6c5ca", "class_name": "RelatedNodeInfo"}}, "text": "1032 Mathematics for Deep Learning\nProperties\nAsalludedinthebeginningofthissection,cross-entropy (A.25 )canbeusedtodefinealoss\nfunction in the optimization problem. It turns out that the following are equivalent:\n1.Maximizing predictive probability of \ud835\udc44for distribution \ud835\udc43, (i.e.,\ud835\udc38\ud835\udc65\u0018\ud835\udc43\u00bblog\u00b9\ud835\udc5e\u00b9\ud835\udc65\u00ba\u00ba\u00bc);\n2.Minimizing cross-entropy CE \u00b9\ud835\udc43,\ud835\udc44\u00ba;\n3.Minimizing the KL divergence \ud835\udc37KL\u00b9\ud835\udc43k\ud835\udc44\u00ba.\nThe definition of cross-entropy indirectly proves the equivalent relationship between ob-\njective 2 and objective 3, as long as the entropy of true data \ud835\udc3b\u00b9\ud835\udc43\u00bais constant.\nCross-Entropyas An ObjectiveFunction of Multi-class Classification\nIf we dive deep into the classification objective function with cross-entropy loss CE, we\nwillfind minimizing CE is equivalentto maximizing the log-likelihoodfunction \ud835\udc3f.\nTo begin with, suppose that we are given a dataset with \ud835\udc5bexamples, and it can be classified\ninto\ud835\udc58-classes. For each data example \ud835\udc56, we represent any \ud835\udc58-class label y\ud835\udc56=\u00b9\ud835\udc66\ud835\udc561,...,\ud835\udc66\ud835\udc56\ud835\udc58\u00ba\nbyone-hot encoding . To be specific, if the example \ud835\udc56belongs to class \ud835\udc57, then we set the\n\ud835\udc57-th entry to 1, and all other components to 0, i.e.,\n\ud835\udc66\ud835\udc56\ud835\udc57=(\n1\ud835\udc572\ud835\udc3d;\n0otherwise.(A.27)\nForinstance,ifamulti-classclassificationproblemcontainsthreeclasses \ud835\udc34,\ud835\udc35,and\ud835\udc36,then\nthe labels y\ud835\udc56can be encoded in { \ud835\udc34:\u00b91,0,0\u00ba;\ud835\udc35:\u00b90,1,0\u00ba;\ud835\udc36:\u00b90,0,1\u00ba}.\nAssume that our neural network is parametrized by \ud835\udf03. For true label vectors y\ud835\udc56and predic-\ntions\n\u02c6y\ud835\udc56=\ud835\udc5d\ud835\udf03\u00b9y\ud835\udc56jx\ud835\udc56\u00ba=\ud835\udc58\u00d5\n\ud835\udc57=1\ud835\udc66\ud835\udc56\ud835\udc57\ud835\udc5d\ud835\udf03\u00b9\ud835\udc66\ud835\udc56\ud835\udc57jx\ud835\udc56\u00ba. (A.28)\nHence, the cross-entropyloss would be\nCE\u00b9y,\u02c6y\u00ba=\u0000\ud835\udc5b\u00d5\n\ud835\udc56=1y\ud835\udc56log \u02c6y\ud835\udc56=\u0000\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc58\u00d5\n\ud835\udc57=1\ud835\udc66\ud835\udc56\ud835\udc57log\ud835\udc5d\ud835\udf03\u00b9\ud835\udc66\ud835\udc56\ud835\udc57jx\ud835\udc56\u00ba. (A.29)\nOn the other side, we can also approach the problem through maximum likelihood es-\ntimation. To begin with, let\u2019s quickly introduce a \ud835\udc58-class multinoulli distribution. It is\nan extension of the Bernoulli distribution from binary class to multi-class. If a random\nvariable z=\u00b9\ud835\udc671,...,\ud835\udc67\ud835\udc58\u00bafollows a\ud835\udc58-classmultinoulli distribution with probabilities p=\n(\ud835\udc5d1,...,\ud835\udc5d\ud835\udc58), i.e.,\n\ud835\udc5d\u00b9z\u00ba=\ud835\udc5d\u00b9\ud835\udc671,...,\ud835\udc67\ud835\udc58\u00ba=Multi\u00b9\ud835\udc5d1,...,\ud835\udc5d\ud835\udc58\u00ba,where\ud835\udc58\u00d5\n\ud835\udc56=1\ud835\udc5d\ud835\udc56=1, (A.30)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9e3092c-942e-4b5b-9b34-178dc694e178": {"__data__": {"id_": "c9e3092c-942e-4b5b-9b34-178dc694e178", "embedding": null, "metadata": {"page_label": "1033", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d2e232e-c67b-4d67-818a-ca81529e7f92", "node_type": "4", "metadata": {"page_label": "1033", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ce3d85a3c612b2a25c46525ca2b61ff59cb4f18d7f9b65d4bec4e15fb59aa328", "class_name": "RelatedNodeInfo"}}, "text": "1033 Information Theory\nthen the joint probability mass function(p.m.f.) of zis\npz=\ud835\udc58\u00d6\n\ud835\udc57=1\ud835\udc5d\ud835\udc67\ud835\udc57\n\ud835\udc57. (A.31)\nIt can be seen that the label of each data example, y\ud835\udc56, is following a \ud835\udc58-class multinoulli\ndistribution with probabilities \ud835\udf45=(\ud835\udf0b1,...,\ud835\udf0b\ud835\udc58). Therefore, the joint p.m.f. of each data\nexample y\ud835\udc56is\u00dfy\ud835\udc56=\u00ce\ud835\udc58\n\ud835\udc57=1\ud835\udf0b\ud835\udc66\ud835\udc56 \ud835\udc57\n\ud835\udc57.Hence, the log-likelihood function would be\n\ud835\udc59\u00b9\ud835\udf03\u00ba=log\ud835\udc3f\u00b9\ud835\udf03\u00ba=log\ud835\udc5b\u00d6\n\ud835\udc56=1\ud835\udf45y\ud835\udc56=log\ud835\udc5b\u00d6\n\ud835\udc56=1\ud835\udc58\u00d6\n\ud835\udc57=1\ud835\udf0b\ud835\udc66\ud835\udc56 \ud835\udc57\n\ud835\udc57=\ud835\udc5b\u00d5\n\ud835\udc56=1\ud835\udc58\u00d5\n\ud835\udc57=1\ud835\udc66\ud835\udc56\ud835\udc57log\ud835\udf0b\ud835\udc57. (A.32)\nSince in maximum likelihood estimation, we maximizing the objective function \ud835\udc59\u00b9\ud835\udf03\u00baby\nhaving\ud835\udf0b\ud835\udc57=\ud835\udc5d\ud835\udf03\u00b9\ud835\udc66\ud835\udc56\ud835\udc57jx\ud835\udc56\u00ba. Therefore, for any multi-class classification, maximizing the\nabovelog-likelihoodfunction \ud835\udc59\u00b9\ud835\udf03\u00baisequivalenttominimizingtheCElossCE \u00b9\ud835\udc66,\u02c6\ud835\udc66\u00ba.\nTo test the above proof, let\u2019s apply the built-in measure NegativeLogLikelihood . Using\nthe same labelsandpredsas in the earlier example, we will get the same numerical loss\nas the previous example up to the 5 decimal place.\n# Implementation of cross-entropy loss in PyTorch combines `nn.LogSoftmax()`\n# and `nn.NLLLoss()`\nnll_loss =NLLLoss()\nloss =nll_loss(torch .log(preds), labels)\nloss\ntensor( 0.9486 )\nA.11.6Summary\n\u000fInformation theory is a field of study about encoding, decoding, transmitting, and ma-\nnipulating information.\n\u000fEntropy is the unit to measure how much information is presented in different signals.\n\u000fKL divergence can also measure the divergence between two distributions.\n\u000fCross-entropy can be viewed as an objective function of multi-class classification. Min-\nimizing cross-entropy loss is equivalent to maximizing the log-likelihood function.\nA.11.7Exercises\n1.Verify that the card examples from the first section indeed have the claimed entropy.\n2.ShowthattheKLdivergence \ud835\udc37\u00b9\ud835\udc5dk\ud835\udc5e\u00baisnonnegativeforalldistributions \ud835\udc5dand\ud835\udc5e. Hint:\nuse Jensen\u2019s inequality, i.e., use the fact that \u0000log\ud835\udc65is a convex function.\n3.Let\u2019s compute the entropy from a few data sources:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7137ccbf-b9f7-4a9c-ae4c-12fba3c78c38": {"__data__": {"id_": "7137ccbf-b9f7-4a9c-ae4c-12fba3c78c38", "embedding": null, "metadata": {"page_label": "1034", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ad734571-2657-4907-8fc1-22e12393e69b", "node_type": "4", "metadata": {"page_label": "1034", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c3ac5092bb334e4d4911239a4ec5ef73b22f78f97ecdaa9e823f7be863f0d017", "class_name": "RelatedNodeInfo"}}, "text": "1034 Mathematics for Deep Learning\n290\u000fAssume that you are watching the output generated by a monkey at a typewriter. The\nmonkey presses any of the 44keys of the typewriter at random (you can assume\nthat it has not discovered any special keys or the shift key yet). How many bits of\nrandomness per character do you observe?\n\u000fBeing unhappy with the monkey, you replaced it by a drunk typesetter. It is able\nto generate words, albeit not coherently. Instead, it picks a random word out of\na vocabulary of 2,000words. Let\u2019s assume that the average length of a word is\n4.5letters in English. How many bits of randomness per character do you observe\nnow?\n\u000fStill being unhappy with the result, you replace the typesetter by a high quality lan-\nguage model. The language model can currently obtain a perplexity as low as 15\npoints per word. The character perplexity of a language model is defined as the\ninverse of the geometric mean of a set of probabilities, each probability is corre-\nsponding to a character in the word. To be specific, if the length of a given word is\n\ud835\udc59, then PPL\u00b9word\u00ba=\u00bb\u00ce\n\ud835\udc56\ud835\udc5d\u00b9character\ud835\udc56\u00ba\u00bc\u00001\n\ud835\udc59=exp\u0002\n\u00001\n\ud835\udc59\u00cd\n\ud835\udc56log\ud835\udc5d\u00b9character\ud835\udc56\u00ba\u0003\n.As-\nsume that the test word has 4.5 letters, how many bits of randomness per character\ndo you observe now?\n4.Explain intuitively why \ud835\udc3c\u00b9\ud835\udc4b,\ud835\udc4c\u00ba=\ud835\udc3b\u00b9\ud835\udc4b\u00ba\u0000\ud835\udc3b\u00b9\ud835\udc4bj\ud835\udc4c\u00ba. Then, show this is true by\nexpressing both sides as an expectation with respect to the joint distribution.\n5.What is the KL Divergence between the two Gaussian distributions N\u00b9\ud835\udf071,\ud835\udf0e2\n1\u00baand\nN\u00b9\ud835\udf072,\ud835\udf0e2\n2\u00ba?\nDiscussions290.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0a40b30-b1bf-4bde-ac31-56f5d404c874": {"__data__": {"id_": "b0a40b30-b1bf-4bde-ac31-56f5d404c874", "embedding": null, "metadata": {"page_label": "1035", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a9c17bef-cfe1-4da0-b79a-9043fa967d4f", "node_type": "4", "metadata": {"page_label": "1035", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e94697bb69d35a7ac46a1be84dbb3460e22b6952351bb7c853cbff41393cb9b4", "class_name": "RelatedNodeInfo"}}, "text": "291\nB Tools for Deep Learning\nTo get the most out of DiveintoDeepLearning , we will talk you through different tools in\nthisappendix,suchasforrunningandcontributingtothisinteractiveopen-sourcebook.\nB.1UsingJupyterNotebooks\nThis section describes how to edit and run the code in each section of this book using\nthe Jupyter Notebook. Make sure you have installed Jupyter and downloaded the code as\ndescribed in Installation (page xxxiv). If you want to know more about Jupyter see the\nexcellent tutorial in their documentation291.\nB.1.1Editingand Runningthe Code Locally\nSuppose that the local path of the book\u2019s code is xx/yy/d2l-en/ . Use the shell to change\nthe directory to this path ( cd xx/yy/d2l-en ) and run the command jupyter notebook .\nIf your browser does not do this automatically, open http://localhost:8888 and you will see\ntheinterfaceofJupyterandallthefolderscontainingthecodeofthebook, asshownin Fig.\nB.1.\ntFig. B.1 The folders containing the code of this book.\nYoucanaccessthenotebookfilesbyclickingonthefolderdisplayedonthewebpage. They\nusuallyhavethesuffix\u201c.ipynb\u201d. Forthesakeofbrevity,wecreateatemporary\u201ctest.ipynb\u201d\nfile. The content displayed after you click it is shown in Fig. B.2. This notebook includes a\n1035", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1234, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c60d27eb-6bb5-4363-a8a6-8a5fd3fa3bed": {"__data__": {"id_": "c60d27eb-6bb5-4363-a8a6-8a5fd3fa3bed", "embedding": null, "metadata": {"page_label": "1036", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b72fad3-1ac5-426c-8a67-0b062ac2aa24", "node_type": "4", "metadata": {"page_label": "1036", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c67a689a09b348a9f25d63ad45facccfe63cf102ec8e2a1b90f9501eb05f9ffc", "class_name": "RelatedNodeInfo"}}, "text": "1036 Tools for Deep Learning\nmarkdowncellandacodecell. Thecontentinthemarkdowncellincludes\u201cThisIsaTitle\u201d\nand \u201cThis is text.\u201d. The code cell contains two lines of Python code.\ntFig. B.2 Markdown and code cells in the \u201ctext.ipynb\u201d \ufb01le.\nDoubleclickonthemarkdowncelltoentereditmode. Addanewtextstring\u201cHelloworld.\u201d\nat the end of the cell, as shown in Fig. B.3.\ntFig. B.3 Edit the markdown cell.\nAs demonstrated in Fig. B.4, click \u201cCell\u201d!\u201cRun Cells\u201d in the menu bar to run the edited\ncell.\nAfter running, the markdown cell is shown in Fig. B.5.\nNext, clickonthecodecell. Multiplytheelementsby2afterthelastlineofcode, asshown\ninFig. B.6.\nYou can also run the cell with a shortcut (\u201cCtrl + Enter\u201d by default) and obtain the output\nresult from Fig. B.7.\nWhenanotebookcontainsmorecells,wecanclick\u201cKernel\u201d !\u201cRestart&RunAll\u201dinthe\nmenu bar to run all the cells in the entire notebook. By clicking \u201cHelp\u201d !\u201cEdit Keyboard\nShortcuts\u201dinthemenubar,youcanedittheshortcutsaccordingtoyourpreferences.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "699c7f56-58d2-4f9f-b095-5543ad619a8e": {"__data__": {"id_": "699c7f56-58d2-4f9f-b095-5543ad619a8e", "embedding": null, "metadata": {"page_label": "1037", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42d8241e-c9a5-4cae-9ebd-7c5168d2703f", "node_type": "4", "metadata": {"page_label": "1037", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5eb67d7548a92cf602506c39ae37d274dab8ad0a692074f32064d2f537703299", "class_name": "RelatedNodeInfo"}}, "text": "1037 B.1 Using Jupyter Notebooks\ntFig. B.4 Run the cell.\ntFig. B.5 The markdown cell after running.\ntFig. B.6 Edit the code cell.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 129, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "018101bc-3d92-4323-8f7c-793c858bba83": {"__data__": {"id_": "018101bc-3d92-4323-8f7c-793c858bba83", "embedding": null, "metadata": {"page_label": "1038", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6de9d35f-0e37-457b-bf6e-b621a00458c9", "node_type": "4", "metadata": {"page_label": "1038", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8650e9f6a927333136c959061b12a4bb1e1e1b306d5a18bf0d243a67108fc47c", "class_name": "RelatedNodeInfo"}}, "text": "1038 Tools for Deep Learning\ntFig. B.7 Run the code cell to obtain the output.\nB.1.2AdvancedOptions\nBeyondlocaleditingtwothingsarequiteimportant: editingthenotebooksinthemarkdown\nformatandrunningJupyterremotely. Thelattermatterswhenwewanttorunthecodeona\nfasterserver. TheformermatterssinceJupyter\u2019snativeipynbformatstoresalotofauxiliary\ndata that is irrelevant to the content, mostly related to how and where the code is run. This\nis confusing for Git, making reviewing contributions very difficult. Fortunately there is an\nalternative\u2014native editing in the markdown format.\nMarkdownFiles in Jupyter\nIfyouwishtocontributetothecontentofthisbook,youneedtomodifythesourcefile(md\nfile, notipynbfile)onGitHub. Usingthenotedownpluginwecanmodifynotebooksinthe\nmd format directly in Jupyter.\nFirst, install the notedown plugin, run the Jupyter Notebook, and load the plugin:\npip install d2l -notedown # You may need to uninstall the original notedown.\njupyter notebook --NotebookApp .contents_manager_class ='notedown.\n\u21a9!NotedownContentsManager '\nYou may also turn on the notedown plugin by default whenever you run the Jupyter Note-\nbook. First,generateaJupyterNotebookconfigurationfile(ifithasalreadybeengenerated,\nyou can skip this step).\njupyter notebook --generate -config\nThen,addthefollowinglinetotheendoftheJupyterNotebookconfigurationfile(forLinux\nor macOS, usually in the path ~/.jupyter/jupyter_notebook_config.py ):", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1419, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc4d6639-6fb6-49a1-85bf-670b572da010": {"__data__": {"id_": "dc4d6639-6fb6-49a1-85bf-670b572da010", "embedding": null, "metadata": {"page_label": "1039", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5fb4be06-fa68-46d2-bd2a-60c3500df870", "node_type": "4", "metadata": {"page_label": "1039", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c2e7ab14ed4235967c879365d0e032b8df99ead3d1eab8e0c6ca45c75900a55b", "class_name": "RelatedNodeInfo"}}, "text": "1039 Using Jupyter Notebooks\n292c.NotebookApp .contents_manager_class ='notedown.NotedownContentsManager '\nAfter that, you only need to run the jupyter notebook command to turn on the notedown\nplugin by default.\nRunningJupyterNotebookson a RemoteServer\nSometimes,youmaywanttorunJupyternotebooksonaremoteserverandaccessitthrough\na browser on your local computer. If Linux or macOS is installed on your local machine\n(Windowscanalsosupportthisfunctionthroughthird-partysoftwaresuchasPuTTY),you\ncan use port forwarding:\nssh myserver -L8888 :localhost: 8888\nThe above string myserver is the address of the remote server. Then we can use http:\n//localhost:8888 to access the remote server myserver that runs Jupyter notebooks. We\nwilldetailonhowtorunJupyternotebooksonAWSinstanceslaterinthisappendix.\nTiming\nWe can use the ExecuteTime plugin to time the execution of each code cell in Jupyter\nnotebooks. Use the following commands to install the plugin:\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable execute_time /ExecuteTime\nB.1.3Summary\n\u000fUsing the Jupyter Notebook tool, we can edit, run, and contribute to each section of the\nbook.\n\u000fWe can run Jupyter notebooks on remote servers using port forwarding.\nB.1.4Exercises\n1.Edit and run the code in this book with the Jupyter Notebook on your local machine.\n2.EditandrunthecodeinthisbookwiththeJupyterNotebook remotely viaportforward-\ning.\n3.Compare the running time of the operations A>BandABfor two square matrices in\nR1024\u00021024. Which one is faster?\nDiscussions292.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1577, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69778296-dfa3-44b1-b2ae-549b954a6a8f": {"__data__": {"id_": "69778296-dfa3-44b1-b2ae-549b954a6a8f", "embedding": null, "metadata": {"page_label": "1040", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "097a64d0-a747-4251-8a56-663b7c99f7b9", "node_type": "4", "metadata": {"page_label": "1040", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "872e59bc6c09d785aecbe6c50d67fccadff01de7d9ccc241ea08e8d60157bc96", "class_name": "RelatedNodeInfo"}}, "text": "1040 Tools for Deep Learning\n293B.2Using Amazon SageMaker\nDeep learning applications may demand so much computational resource that easily goes\nbeyond what your local machine can offer. Cloud computing services allow you to run\nGPU-intensivecodeofthisbookmoreeasilyusingmorepowerfulcomputers. Thissection\nwill introduce how to use Amazon SageMaker to run the code of this book.\nB.2.1SigningUp\nFirst, we need to sign up an account at https://aws.amazon.com/ . For additional security,\nusingtwo-factorauthenticationisencouraged. Itisalsoagoodideatosetupdetailedbilling\nand spending alerts to avoid any surprise, e.g., when forgetting to stop running instances.\nAfter logging into your AWS account, go to your console293and search for \u201cAmazon\nSageMaker\u201d (see Fig. B.1), then click it to open the SageMaker panel.\ntFig. B.1 Search for and open the SageMaker panel.\nB.2.2Creatinga SageMakerInstance\nNext, let\u2019s create a notebook instance as described in Fig. B.2.\ntFig. B.2 Create a SageMaker instance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b449a9b1-836b-4ffe-a656-fa1dcdcfc3c6": {"__data__": {"id_": "b449a9b1-836b-4ffe-a656-fa1dcdcfc3c6", "embedding": null, "metadata": {"page_label": "1041", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f1adb9f6-d60d-4cf8-9866-cbedc1ef30d1", "node_type": "4", "metadata": {"page_label": "1041", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cb6721817aa7be9fa612ab7c34dab15afda88949c4f2fb9bad3bfc11c6fc63b2", "class_name": "RelatedNodeInfo"}}, "text": "1041 Using Amazon SageMaker\n294SageMaker provides multiple instance types294with varying computational power and\nprices. When creating a notebook instance, we can specify its name and type. In Fig. B.3,\nwe choose ml.p3.2xlarge : with one Tesla V100 GPU and an 8-core CPU, this instance is\npowerful enough for most of the book.\ntFig. B.3 Choose the instance type.\nThe entire book in the ipynb format for running with SageMaker is available at https://\ngithub.com/d2l-ai/d2l-pytorch-sagemaker . We can specify this GitHub repository URL\n(Fig. B.4) to allow SageMaker to clone it when creating the instance.\ntFig. B.4 Specify the GitHub repository.\nB.2.3Runningand Stopping an Instance\nCreatinganinstancemaytakeafewminutes. Whenitisready,clickonthe\u201cOpenJupyter\u201d\nlink next to it ( Fig. B.5) so you can edit and run all the Jupyter notebooks of this book on\nthis instance (similar to steps in Section B.1 ).\ntFig. B.5 Open Jupyter on the created SageMaker instance.\nAfter finishing your work, do not forget to stop the instance to avoid being charged further\n(Fig. B.6).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1065, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73b08989-3669-4118-9293-d457c2f58d02": {"__data__": {"id_": "73b08989-3669-4118-9293-d457c2f58d02", "embedding": null, "metadata": {"page_label": "1042", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "15f67db8-59ae-4e03-8fa8-a32d43fb4897", "node_type": "4", "metadata": {"page_label": "1042", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1962c4b51f887d5be754056fb67dd6653fd1e2d7d8957efd626fcddc4d72cc38", "class_name": "RelatedNodeInfo"}}, "text": "1042 Tools for Deep Learning\ntFig. B.6 Stop a SageMaker instance.\n295B.2.4UpdatingNotebooks\nNotebooks of this open-source book will be regularly updated in the d2l-ai/d2l-pytorch-\nsagemaker295repository on GitHub. To update to the latest version, you may open a\nterminal on the SageMaker instance ( Fig. B.7).\ntFig. B.7 Open a terminal on the SageMaker instance.\nYoumaywish to commit yourlocal changesbeforepulling updates fromthe remote repos-\nitory. Otherwise, simply discard all your local changes with the following commands in\nthe terminal:\ncdSageMaker/d2l-pytorch-sagemaker/\ngit reset --hard\ngit pull\nB.2.5Summary\n\u000fWecancreateanotebookinstanceusingAmazonSageMakertorunGPU-intensivecode\nof this book.\n\u000fWe can update notebooks via the terminal on the Amazon SageMaker instance.\nB.2.6Exercises\n1.Edit and run any section that requires a GPU using Amazon SageMaker.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 867, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df5138fc-f800-4612-b670-3d169b5b52fe": {"__data__": {"id_": "df5138fc-f800-4612-b670-3d169b5b52fe", "embedding": null, "metadata": {"page_label": "1043", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f6a8165-b518-43f9-98f7-cbae8632931d", "node_type": "4", "metadata": {"page_label": "1043", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a282aaac9317a4327bca58a3550123acf19a038810f51ef02d8be85a4d42599d", "class_name": "RelatedNodeInfo"}}, "text": "1043 Using AWS EC2 Instances\n2962.Open a terminal to access the local directory that hosts all the notebooks of this book.\nDiscussions296.\nB.3Using AWSEC2 Instances\nInthissection,wewillshowyouhowtoinstallalllibrariesonarawLinuxmachine. Recall\nthatinSectionB.2 wediscussedhowtouseAmazonSageMaker,whilebuildinganinstance\nby yourself costs less on AWS. The walkthrough includes three steps:\n1.Request for a GPU Linux instance from AWS EC2.\n2.Install CUDA (or use an Amazon Machine Image with preinstalled CUDA).\n3.Installthedeeplearningframeworkandotherlibrariesforrunningthecodeofthebook.\nThis process applies to other instances (and other clouds), too, albeit with some minor\nmodifications. Before going forward, you need to create an AWS account, see Section B.2\nfor more details.\nB.3.1Creating and Runningan EC2 Instance\nAfterloggingintoyourAWSaccount,click\u201cEC2\u201d( Fig.B.1)togototheEC2panel.\ntFig. B.1 Open the EC2 console.\nFig. B.2 shows the EC2 panel.\nPresettingLocation\nSelect a nearby data center to reduce latency, e.g., \u201cOregon\u201d (marked by the red box in the\ntop-right of Fig. B.2). If you are located in China, you can select a nearby Asia Pacific", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "175a053d-79e9-4cab-aecd-b33743b00019": {"__data__": {"id_": "175a053d-79e9-4cab-aecd-b33743b00019", "embedding": null, "metadata": {"page_label": "1044", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "48fc486b-3e45-4eb8-9cd3-7abf312194f1", "node_type": "4", "metadata": {"page_label": "1044", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1a95685545f0a549970197b9d350a92027947e72f4bc1fdfc20249f8986419cd", "class_name": "RelatedNodeInfo"}}, "text": "1044 Tools for Deep Learning\ntFig. B.2 The EC2 panel.\nregion, such as Seoul or Tokyo. Please note that some data centers may not have GPU\ninstances.\nIncreasingLimits\nBefore choosing an instance, check if there are quantity restrictions by clicking the \u201cLim-\nits\u201d label in the bar on the left as shown in Fig. B.2.Fig. B.3 shows an example of such\na limitation. The account currently cannot open \u201cp2.xlarge\u201d instances according to the re-\ngion. If you need to open one or more instances, click on the \u201cRequest limit increase\u201d link\nto apply for a higher instance quota. Generally, it takes one business day to process an\napplication.\ntFig. B.3 Instance quantity restrictions.\nLaunchingan Instance\nNext, click the \u201cLaunch Instance\u201d button marked by the red box in Fig. B.2 to launch your\ninstance.\nWebeginbyselectingasuitableAmazonMachineImage(AMI).SelectanUbuntuinstance\n(Fig. B.4).\nEC2 provides many different instance configurations to choose from. This can sometimes\nfeel overwhelming to a beginner. tab_ec2 lists different suitable machines.\n:Different EC2 instance types", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d47acd47-9612-4069-820d-c9f1a8a4305f": {"__data__": {"id_": "d47acd47-9612-4069-820d-c9f1a8a4305f", "embedding": null, "metadata": {"page_label": "1045", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ce3d2ef9-72fa-4372-bd47-e4c270b094c6", "node_type": "4", "metadata": {"page_label": "1045", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "95b39f3e98df56b4956433342f33f0c800a8956f3544e0008bb215de47ba9fed", "class_name": "RelatedNodeInfo"}}, "text": "1045 Using AWS EC2 Instances\ntFig. B.4 Choose an AMI.\n297\n298Table B.1: label: tab_ec2\nName GPU Notes\ng2 Grid K520 ancient\np2 Kepler K80 old but often cheap as spot\ng3 Maxwell M60 good trade-off\np3 Volta V100 high performance for FP16\np4 Ampere A100 high performance for large-scale training\ng4 Turing T4 inference optimized FP16/INT8\nAll these servers come in multiple flavors indicating the number of GPUs used. For exam-\nple, a p2.xlarge has 1 GPU and a p2.16xlarge has 16 GPUs and more memory. For more\ndetails, see the AWS EC2 documentation297or asummary page298. For the purpose of\nillustration, a p2.xlarge will suffice (marked in the red box of Fig. B.5).\ntFig. B.5 Choose an instance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 693, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab8098c1-8ef6-4b5e-9929-1411d322665b": {"__data__": {"id_": "ab8098c1-8ef6-4b5e-9929-1411d322665b", "embedding": null, "metadata": {"page_label": "1046", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "80fd7197-223c-41be-92ac-82cff791983c", "node_type": "4", "metadata": {"page_label": "1046", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5aff8f791de302963dbc49b1da74e45d2ee9c358bbea9e5d4f540e13287f7cb6", "class_name": "RelatedNodeInfo"}}, "text": "1046 Tools for Deep Learning\nNote that youshould use a GPU-enabled instancewith suitable driversand a GPU-enabled\ndeeplearningframework. OtherwiseyouwillnotseeanybenefitfromusingGPUs.\nWe go on to select the key pair used to access the instance. If you do not have a key pair,\nclick \u201cCreate new key pair\u201d in Fig. B.6 to generate a key pair. Subsequently, you can select\nthe previously generated key pair. Make sure that you download the key pair and store\nit in a safe location if you generated a new one. This is your only way to SSH into the\nserver.\ntFig. B.6 Select a key pair.\nIn this example, we will keep the default configurations for \u201cNetwork settings\u201d (click the\n\u201cEdit\u201d button to configure items such as the subnet and security groups). We just increase\nthe default hard disk size to 64 GB ( Fig. B.7). Note that CUDA by itself already takes up\n4 GB.\ntFig. B.7 Modify the hard disk size.\nClick\u201cLaunchInstance\u201dtolaunchthecreatedinstance. ClicktheinstanceIDshownin Fig.\nB.8to view the status of this instance.\nConnecting to the Instance\nAsshownin Fig.B.9,aftertheinstancestateturnsgreen,right-clicktheinstanceandselect\nConnect to view the instance access method.\nIfthisisanewkey,itmustnotbepubliclyviewableforSSHtowork. Gotothefolderwhere", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1244, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0ae79d6b-9679-4e8a-89f3-0d81a200131c": {"__data__": {"id_": "0ae79d6b-9679-4e8a-89f3-0d81a200131c", "embedding": null, "metadata": {"page_label": "1047", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "94629437-77de-469e-b198-a264957172ba", "node_type": "4", "metadata": {"page_label": "1047", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "20f91eb075bec1d916d2b1f3174813a602b1a48a5b260178e93aea00e7642e2a", "class_name": "RelatedNodeInfo"}}, "text": "1047 Using AWS EC2 Instances\ntFig. B.8 Click the instance ID.\ntFig. B.9 View the instance access method.\nyou store D2L_key.pem and execute the following command to make the key not publicly\nviewable:\nchmod 400 D2L_key.pem\ntFig. B.10 View instance access and startup method.\nNow,copytheSSHcommandinthelowerredboxof Fig.B.10 andpasteontothecommand\nline:", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "23b15c56-56d3-4322-8136-070545946120": {"__data__": {"id_": "23b15c56-56d3-4322-8136-070545946120", "embedding": null, "metadata": {"page_label": "1048", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9710fcf5-be7b-4307-995a-a044827c7eae", "node_type": "4", "metadata": {"page_label": "1048", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f185038db90a3503f0340ea4de55e04b5c175902b8efafeff990f62ecef2c49a", "class_name": "RelatedNodeInfo"}}, "text": "1048 Tools for Deep Learning\n299ssh -i\"D2L_key.pem\" ubuntu@ec2-xx-xxx-xxx-xxx.y.compute.amazonaws.com\nWhenthecommandlineprompts\u201cAreyousureyouwanttocontinueconnecting(yes/no)\u201d,\nenter \u201cyes\u201d and press Enter to log into the instance.\nYour server is ready now.\nB.3.2Installing CUDA\nBefore installing CUDA, be sure to update the instance with the latest drivers.\nsudo apt-get update &&sudo apt-get install -ybuild-essential git libgfortran3\nHerewedownloadCUDA12.1. VisitNVIDIA\u2019s officialrepository299tofindthedownload\nlink as shown in Fig. B.11 .\ntFig. B.11 Find the CUDA 12.1 download address.\nCopy the instructions and paste them onto the terminal to install CUDA 12.1.\n# The link and file name are subject to changes\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_\n\u21a9!64/cuda-ubuntu2204.pin\nsudo mvcuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_\n\u21a9!installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo dpkg -icuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1132, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ebfc489f-02b1-49aa-ae19-ef351bd59a80": {"__data__": {"id_": "ebfc489f-02b1-49aa-ae19-ef351bd59a80", "embedding": null, "metadata": {"page_label": "1049", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9a26dbff-c0b5-4f07-8e19-057c9c37ab10", "node_type": "4", "metadata": {"page_label": "1049", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "daebeed5c3604438408fe556ff04951875d8362a2ed6fd8e0281e4770fb9756c", "class_name": "RelatedNodeInfo"}}, "text": "1049 Using AWS EC2 Instances\n(continued from previous page)\nsudo cp/var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/\n\u21a9!keyrings/\nsudo apt-get update\nsudo apt-get -yinstall cuda\nAfter installing the program, run the following command to view the GPUs:\nnvidia-smi\nFinally, add CUDA to the library path to help other libraries find it, such as appending the\nfollowing lines to the end of ~/.bashrc .\nexport PATH =\"/usr/local/cuda-12.1/bin: $PATH \"\nexport LD_LIBRARY_PATH =${LD_LIBRARY_PATH }:/usr/local/cuda-12.1/lib64\nB.3.3Installing Libraries forRunningthe Code\nTorunthecodeofthisbook,justfollowstepsin Installation (pagexxxiv)forLinuxuserson\nthe EC2 instance and use the following tips for working on a remote Linux server:\n\u000fTodownloadthebashscriptontheMinicondainstallationpage,rightclickthedownload\nlink and select \u201cCopy Link Address\u201d, then execute wget [copied link address] .\n\u000fAfter running ~/miniconda3/bin/conda init , you may execute source ~/.bashrc\ninstead of closing and reopening your current shell.\nB.3.4Runningthe JupyterNotebookremotely\nTo run the Jupyter Notebook remotely you need to use SSH port forwarding. After all, the\nserver in the cloud does not havea monitor or keyboard. Forthis, log into your serverfrom\nyour desktop (or laptop) as follows:\n# This command must be run in the local command line\nssh -i\"/path/to/key.pem \"ubuntu @ec2 -xx-xxx-xxx-xxx.y.compute .amazonaws .com -L\u2423\n\u21a9!8889 :localhost: 8888\nNext, go to the location of the downloaded code of this book on the EC2 instance, then\nrun:\nconda activate d2l\njupyter notebook\nFig.B.12 showsthepossibleoutputafteryouruntheJupyterNotebook. Thelastrowisthe\nURL for port 8888.\nSince you used port forwarding to port 8889, copy the last row in the red box of Fig. B.12 ,\nreplace \u201c8888\u201d with \u201c8889\u201d in the URL, and open it in your local browser.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1836, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d817af21-a84b-4bd8-bc0e-530fa27921bd": {"__data__": {"id_": "d817af21-a84b-4bd8-bc0e-530fa27921bd", "embedding": null, "metadata": {"page_label": "1050", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "effc38b6-67cc-4a2d-8120-0c0df34121d4", "node_type": "4", "metadata": {"page_label": "1050", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c8ca12db44f0f1c4479c4be6828af6a225c3e8e8e2e96cccaf4ec3cfec579eb8", "class_name": "RelatedNodeInfo"}}, "text": "1050 Tools for Deep Learning\ntFig. B.12 Output after running the Jupyter Notebook. The last row is the URL for port 8888.\n300B.3.5Closing UnusedInstances\nAscloudservicesarebilledbythetimeofuse,youshouldcloseinstancesthatarenotbeing\nused. Note that there are alternatives:\n\u000f\u201cStopping\u201d an instance means that you will be able to start it again. This is akin to\nswitching off the power for your regular server. However, stopped instances will still\nbe billed a small amount for the hard disk space retained.\n\u000f\u201cTerminating\u201d an instance will delete all data associated with it. This includes the disk,\nhence you cannot start it again. Only do this if you know that you will not need it in\nthe future.\nIf you want to use the instance as a template for many more instances, right-click on the\nexamplein Fig.B.9 andselect\u201cImage\u201d!\u201cCreate\u201dtocreateanimageoftheinstance. Once\nthisiscomplete,select\u201cInstanceState\u201d !\u201cTerminate\u201dtoterminatetheinstance. Thenext\ntime you want to use this instance, you can follow the steps in this section to create an\ninstance based on the saved image. The only difference is that, in \u201c1. Choose AMI\u201d shown\ninFig.B.4,youmustusethe\u201cMyAMIs\u201doptiononthelefttoselectyoursavedimage. The\ncreated instance will retain the information stored on the image hard disk. For example,\nyou will not have to reinstall CUDA and other runtime environments.\nB.3.6Summary\n\u000fWe can launch and stop instances on demand without having to buy and build our own\ncomputer.\n\u000fWe need to install CUDA before using the GPU-enabled deep learning framework.\n\u000fWe can use port forwarding to run the Jupyter Notebook on a remote server.\nB.3.7Exercises\n1.The cloud offers convenience, but it does not come cheap. Find out how to launch spot\ninstances300to see how to reduce costs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca3122e4-a77e-4c27-9702-50700e9c16c6": {"__data__": {"id_": "ca3122e4-a77e-4c27-9702-50700e9c16c6", "embedding": null, "metadata": {"page_label": "1051", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05a6157c-cb63-4311-8c4b-5842260982e4", "node_type": "4", "metadata": {"page_label": "1051", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8a1aa155d20660bc9b67ca0ca6aff68f500f003be6806470323fb49ba7f1c517", "class_name": "RelatedNodeInfo"}}, "text": "1051 Using Google Colab\n301\n3022.Experiment with different GPU servers. How fast are they?\n3.Experiment with multi-GPU servers. How well can you scale things up?\nDiscussions301.\nB.4UsingGoogle Colab\nWe introduced how to run this book on AWS in Section B.2 andSection B.3 . Another\noption is running this book on Google Colab302if you have a Google account.\nTo run the code of a section on Colab, simply click the Colabbutton as shown in Fig.\nB.1.\ntFig. B.1 Run the code of a section on Colab\nIf it is your first time to run a code cell, you will receive a warning message as shown in\nFig. B.2. Just click \u201cRUN ANYWAY\u201d to ignore it.\ntFig. B.2 Ignore the warning message by clicking \u201cRUN ANYWAY\u201d.\nNext, Colab will connect you to an instance to run the code of this section. Specifically,\nif a GPU is needed, Colab will be automatically requested for connecting to a GPU in-\nstance.\nB.4.1Summary\n\u000fYou can use Google Colab to run each section\u2019s code in this book.\n\u000fColab will be requested to connect to a GPU instance if a GPU is needed in any section\nof this book.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1061, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "009141f8-67f7-452a-a37d-3b98f4d36440": {"__data__": {"id_": "009141f8-67f7-452a-a37d-3b98f4d36440", "embedding": null, "metadata": {"page_label": "1052", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "560d8476-0eb4-4264-b22a-3fa5bfb876a5", "node_type": "4", "metadata": {"page_label": "1052", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "96022139b37b12f33683474f10a15578a7b01496081533295dfd449f2f7c5557", "class_name": "RelatedNodeInfo"}}, "text": "1052 Tools for Deep Learning\n303\n304\n305B.4.2Exercises\n1.Open any section of this book using Google Colab.\n2.Edit and run any section that requires a GPU using Google Colab.\nDiscussions303.\nB.5SelectingServersand GPUs\nDeep learning training generally requires large amounts of computation. At present GPUs\narethemostcost-effectivehardwareacceleratorsfordeeplearning. Inparticular,compared\nwith CPUs, GPUs are cheaper and offer higher performance, often by over an order of\nmagnitude. Furthermore, a single server can support multiple GPUs, up to 8 for high end\nservers. More typical numbers are up to 4 GPUs for an engineering workstation, since\nheat, cooling, and power requirements escalate quickly beyond what an office building can\nsupport. For larger deployments, cloud computing (e.g., Amazon\u2019s P3304andG4305\ninstances) is a much more practical solution.\nB.5.1Selecting Servers\nThere is typically no need to purchase high-end CPUs with many threads since much of\nthe computation occurs on the GPUs. That said, due to the global interpreter lock (GIL)\nin Python single-thread performance of a CPU can matter in situations where we have 4\u20138\nGPUs. AllthingsequalthissuggeststhatCPUswithasmallernumberofcoresbutahigher\nclockfrequencymightbeamoreeconomicalchoice. Forexample,whenchoosingbetween\na 6-core 4 GHz and an 8-core 3.5 GHz CPU, the former is much preferable, even though\nits aggregate speed is less. An important consideration is that GPUs use lots of power and\nthus dissipate lots of heat. This requires very good cooling and a large enough chassis to\nuse the GPUs. Follow the guidelines below if possible:\n1.Power Supply . GPUs use significant amounts of power. Budget with up to 350W per\ndevice(checkforthe peakdemand ofthegraphicscardratherthantypicaldemand,since\nefficient code can use lots of energy). If your power supply is not up to the demand you\nwill find that your system becomes unstable.\n2.ChassisSize . GPUsarelargeandtheauxiliarypowerconnectorsoftenneedextraspace.\nAlso, large chassis are easier to cool.\n3.GPU Cooling . If you have a large number of GPUs you might want to invest in water\ncooling. Also, aim for reference designs even if they have fewer fans, since they are\nthin enough to allow for air intake between the devices. If you buy a multi-fan GPU it\nmight be too thick to get enough air when installing multiple GPUs and you will run\ninto thermal throttling.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b786500-41b7-4103-abf4-16fe61447225": {"__data__": {"id_": "3b786500-41b7-4103-abf4-16fe61447225", "embedding": null, "metadata": {"page_label": "1053", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5277ad7-283f-41e5-9995-d49586d4cb92", "node_type": "4", "metadata": {"page_label": "1053", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4d827a267ba9e8a0cbab0eef03962ccd10463f74197a93cc9d36787ec5ce29f7", "class_name": "RelatedNodeInfo"}}, "text": "1053 Selecting Servers and GPUs\n4.PCIe Slots . Moving data to and from the GPU (and exchanging it between GPUs)\nrequires lots of bandwidth. We recommend PCIe 3.0 slots with 16 lanes. If you mount\nmultipleGPUs,besuretocarefullyreadthemotherboarddescriptiontoensurethat16 \u0002\nbandwidth is still available when multiple GPUs are used at the same time and that you\naregettingPCIe3.0asopposedtoPCIe2.0fortheadditionalslots. Somemotherboards\ndowngradeto8\u0002oreven4\u0002bandwidthwithmultipleGPUsinstalled. Thisispartlydue\nto the number of PCIe lanes that the CPU offers.\nIn short, here are some recommendations for building a deep learning server:\n\u000fBeginner . Buy a low end GPU with low power consumption (cheap gaming GPUs suit-\nablefordeeplearninguse150\u2013200W).Ifyouareluckyyourcurrentcomputersupports\nit.\n\u000f1 GPU. A low-end CPU with 4 cores will be sufficient and most motherboards suffice.\nAim for at least 32 GB DRAM and invest into an SSD for local data access. A power\nsupply with 600W should be sufficient. Buy a GPU with lots of fans.\n\u000f2GPUs. A low-end CPU with 4-6 cores will suffice. Aim for 64 GB DRAM and invest\ninto an SSD. You will need in the order of 1000W for two high-end GPUs. In terms\nof mainboards, make sure that they have twoPCIe 3.0 x16 slots. If you can, get a\nmainboard that has two free spaces (60mm spacing) between the PCIe 3.0 x16 slots\nfor extra air. In this case, buy two GPUs with lots of fans.\n\u000f4GPUs. MakesurethatyoubuyaCPUwithrelativelyfastsingle-threadspeed(i.e.,high\nclock frequency). You will probably need a CPU with a larger number of PCIe lanes,\nsuch as an AMD Threadripper. You will likely need relatively expensive mainboards\ntoget4PCIe3.0x16slotssincetheyprobablyneedaPLXtomultiplexthePCIelanes.\nBuyGPUswithreferencedesignthatarenarrowandletairinbetweentheGPUs. You\nneeda1600\u20132000Wpowersupplyandtheoutletinyourofficemightnotsupportthat.\nThis server will probably run loud and hot . You do not want it under your desk. 128\nGB of DRAM is recommended. Get an SSD (1\u20132 TB NVMe) for local storage and a\nbunch of hard disks in RAID configuration to store your data.\n\u000f8GPUs. Youneedtobuyadedicatedmulti-GPUserverchassiswithmultipleredundant\npower supplies (e.g., 2+1 for 1600W per power supply). This will require dual socket\nserver CPUs, 256 GB ECC DRAM, a fast network card (10 GBE recommended),\nand you will need to check whether the servers support the physical form factor of\nthe GPUs. Airflow and wiring placement differ significantly between consumer and\nserver GPUs (e.g., RTX 2080 vs. Tesla V100). This means that you might not be able\ntoinstalltheconsumerGPUinaserverduetoinsufficientclearanceforthepowercable\nor lack of a suitable wiring harness (as one of the coauthors painfully discovered).\nB.5.2SelectingGPUs\nAtpresent,AMDandNVIDIAarethetwomainmanufacturersofdedicatedGPUs. NVIDIA\nwas the first to enter the deep learning field and provides better support for deep learning\nframeworks via CUDA. Therefore, most buyers choose NVIDIA GPUs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2974, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d0433cd4-9744-4ff2-a556-48e2aaf450e7": {"__data__": {"id_": "d0433cd4-9744-4ff2-a556-48e2aaf450e7", "embedding": null, "metadata": {"page_label": "1054", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77c7181f-a596-4142-a757-d270eaccf56b", "node_type": "4", "metadata": {"page_label": "1054", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4eb3164083e8348f5a61e1c12398ff98d94d719f923e1ece4061c38739b2623f", "class_name": "RelatedNodeInfo"}}, "text": "1054 Tools for Deep Learning\nNVIDIA provides two types of GPUs, targeting individual users (e.g., via the GTX and\nRTX series) and enterprise users (via its Tesla series). The two types of GPUs provide\ncomparable compute power. However, the enterprise user GPUs generally use (passive)\nforced cooling, more memory, and ECC (error correcting) memory. These GPUs are more\nsuitable for data centers and usually cost ten times more than consumer GPUs.\nIfyouarealargecompanywith100+serversyoushouldconsidertheNVIDIATeslaseries\noralternativelyuseGPUserversinthecloud. Foralaborasmalltomediumcompanywith\n10+ servers the NVIDIA RTX series is likely most cost effective. You can buy preconfig-\nured servers with Supermicro or Asus chassis that hold 4\u20138 GPUs efficiently.\nGPU vendors typically release a new generation every one to two years, such as the GTX\n1000 (Pascal) series released in 2017 and the RTX 2000 (Turing) series released in 2019.\nEach series offers several different models that provide different performance levels. GPU\nperformance is primarily a combination of the following three parameters:\n1.Compute Power . Generally we look for 32-bit floating-point compute power. 16-bit\nfloatingpointtraining(FP16)isalsoenteringthemainstream. Ifyouareonlyinterested\ninprediction,youcanalsouse8-bitinteger. ThelatestgenerationofTuringGPUsoffers\n4-bit acceleration. Unfortunately at the time of writing the algorithms for training low-\nprecision networks are not yet widespread.\n2.MemorySize . As your models become larger or the batches used during training grow\nbigger,youwillneedmoreGPUmemory. CheckforHBM2(HighBandwidthMemory)\nvs. GDDR6 (Graphics DDR) memory. HBM2 is faster but much more expensive.\n3.MemoryBandwidth . You can only get the most out of your compute power when you\nhave sufficient memory bandwidth. Look for wide memory buses if using GDDR6.\nFormostusers,itisenoughtolookatcomputepower. NotethatmanyGPUsofferdifferent\ntypes of acceleration. For example, NVIDIA\u2019s TensorCores accelerate a subset of opera-\ntors by 5\u0002. Ensure that your libraries support this. The GPU memory should be no less\nthan 4 GB (8 GB is much better). Try to avoid using the GPU also for displaying a GUI\n(use the built-in graphics instead). If you cannot avoid it, add an extra 2 GB of RAM for\nsafety.\nFig. B.1 compares the 32-bit floating-point compute power and price of the various GTX\n900, GTX 1000 and RTX 2000 series models. The prices suggested are those found on\nWikipedia at the time of writing.\nWe can see a number of things:\n1.Withineachseries,priceandperformanceareroughlyproportional. Titanmodelscom-\nmand a significant premium for the benefit of larger amounts of GPU memory. How-\never, the newer models offer better cost effectiveness, as can be seen by comparing the\n980Tiand1080Ti. ThepricedoesnotappeartoimprovemuchfortheRTX2000series.\nHowever, this is due to the fact that they offer far superior low precision performance\n(FP16, INT8, and INT4).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2955, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ab6d3e51-1b31-4bd6-a5d0-ffeb262805ad": {"__data__": {"id_": "ab6d3e51-1b31-4bd6-a5d0-ffeb262805ad", "embedding": null, "metadata": {"page_label": "1055", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c767897f-b55a-4b94-bd50-cd45f5603ca8", "node_type": "4", "metadata": {"page_label": "1055", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2acf72419621e91385d95791ee2daf08f9f19fdb8f16f4af8046a15810b1a39f", "class_name": "RelatedNodeInfo"}}, "text": "1055 Selecting Servers and GPUs\ntFig. B.1 Floating-point compute power and price comparison.\n2.The performance-to-cost ratio of the GTX 1000 series is about two times greater than\nthe 900 series.\n3.FortheRTX2000seriestheperformance(inGFLOPs)isan a\ufb00inefunctionoftheprice.\ntFig. B.2 Floating-point compute power and energy consumption.\nFig. B.2 shows how energy consumption scales mostly linearly with the amount of com-", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eb76921b-bb2d-47ea-83f0-b2d39ff1a5f2": {"__data__": {"id_": "eb76921b-bb2d-47ea-83f0-b2d39ff1a5f2", "embedding": null, "metadata": {"page_label": "1056", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bfad893a-dda3-472a-bfd9-5e154c9dc397", "node_type": "4", "metadata": {"page_label": "1056", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "c88207180bb11b546d38049ba650aa028d69a8b7dad6659671803b3e8fc1d1e3", "class_name": "RelatedNodeInfo"}}, "text": "1056 Tools for Deep Learning\n306\n307\n308\n309\n310putation. Second, later generations are more efficient. This seems to be contradicted by\nthe graph corresponding to the RTX 2000 series. However, this is a consequence of the\nTensorCores that draw disproportionately much energy.\nB.5.3Summary\n\u000fWatch out for power, PCIe bus lanes, CPU single thread speed, and cooling when build-\ning a server.\n\u000fYou should purchase the latest GPU generation if possible.\n\u000fUse the cloud for large deployments.\n\u000fHigh density servers may not be compatible with all GPUs. Check the mechanical and\ncooling specifications before you buy.\n\u000fUse FP16 or lower precision for high efficiency.\nDiscussions306.\nB.6Contributing to This Book\nContributionsby readers307helpusimprovethisbook. Ifyoufindatypo,anoutdatedlink,\nsomething where you think we missed a citation, where the code does not look elegant or\nwhereanexplanationisunclear,pleasecontributebackandhelpushelpourreaders. While\nin regular books the delay between print runs (and thus between typo corrections) can be\nmeasured in years, it typically takes hours to days to incorporate an improvement in this\nbook. Thisisallpossibleduetoversioncontrolandcontinuousintegration(CI)testing. To\ndosoyouneedtosubmita pullrequest308totheGitHubrepository. Whenyourpullrequest\nis merged into the code repository by the authors, you will become a contributor.\nB.6.1Submitting Minor Changes\nThe most common contributions are editing one sentence or fixing typos. We recommend\nthat you find the source file in the GitHub repository309and edit the file directly. For\nexample, you can search the file through the Find file310button (Fig. B.1) to locate the\nsource file (a markdown file). Then you click the \u201cEdit this file\u201d button on the upper-right\ncorner to make your changes in the markdown file.\nAfter you are done, fill in your change descriptions in the \u201cPropose file change\u201d panel on\nthe page bottom and then click the \u201cPropose file change\u201d button. It will redirect you to a\nnew page to review your changes ( Fig. B.7). If everything is good, you can submit a pull\nrequest by clicking the \u201cCreate pull request\u201d button.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c3e8cfa-02f3-4276-b531-df4013d1734e": {"__data__": {"id_": "4c3e8cfa-02f3-4276-b531-df4013d1734e", "embedding": null, "metadata": {"page_label": "1057", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cce5e56-9480-46e8-9baf-d6d0e1f634f3", "node_type": "4", "metadata": {"page_label": "1057", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ed0126187ba01314a5c14c39c0471bd5a1fdbfc015e20950c56f175702b6bd9a", "class_name": "RelatedNodeInfo"}}, "text": "1057 Contributing to This Book\ntFig. B.1 Edit the \ufb01le on Github.\n311\n312B.6.2ProposingMajorChanges\nIf you plan to update a large portion of text or code, then you need to know a little bit more\nabout the format this book is using. The source file is based on the markdown format311\nwith a set of extensions through the D2L-Book312package such as referring to equations,\nimages, chapters, and citations. You can use any markdown editors to open these files and\nmake your changes.\nIf you would like to change the code, we recommend that you use the Jupyter Notebook to\nopen these markdown files as described in Section B.1 , so that you can run and test your\nchanges. Please remember to clear all outputs before submitting your changes since our CI\nsystem will execute the sections you updated to generate outputs.\nSome sections may support multiple framework implementations. If you add a new code\nblock, please use %%tabto mark this block on the beginning line. For example, %%tab\npytorch for a PyTorch code block, %%tab tensorflow for a TensorFlow code block, or\n%%tab all a shared code block for all implementations. You may refer to the d2lbook\npackage for more information.\nB.6.3Submitting MajorChanges\nWe suggest you to use the standard Git process to submit a major change. In a nutshell the\nprocess works as described in Fig. B.2.\ntFig. B.2 Contributing to the book.\nWe will walk you through the steps in detail. If you are already familiar with Git you\ncan skip this section. For concreteness we assume that the contributor\u2019s user name is \u201cas-\ntonzhang\u201d.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "738cc6bf-32c8-4d6c-826b-86c31e6f9e6d": {"__data__": {"id_": "738cc6bf-32c8-4d6c-826b-86c31e6f9e6d", "embedding": null, "metadata": {"page_label": "1058", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6836bd8c-15dc-4239-864a-2d93e1461e3b", "node_type": "4", "metadata": {"page_label": "1058", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a33dfaa21de4590aa47cb799383987570640ab3b138cfcd68822af2b22900d60", "class_name": "RelatedNodeInfo"}}, "text": "1058 Tools for Deep Learning\n313\n314\n315Installing Git\nThe Git open-source book describes how to install Git313. This typically works via apt\ninstall git on Ubuntu Linux, by installing the Xcode developer tools on macOS, or by\nusing GitHub\u2019s desktop client314. If you do not have a GitHub account, you need to sign\nup for one.\nLogging in to GitHub\nEnter the address315of the book\u2019s code repository in your browser. Click on the Fork\nbutton in the red box at the upper-right of Fig. B.3, to make a copy of the repository of this\nbook. This is now yourcopy and you can change it any way you want.\ntFig. B.3 The code repository page.\nNow, the code repository of this book will be forked (i.e., copied) to your username, such\nasastonzhang/d2l-en shown at the upper-left of Fig. B.4.\ntFig. B.4 The forked code repository.\nCloningthe Repository\nTo clone the repository (i.e., to make a local copy) we need to get its repository address.\nThe green button in Fig. B.5 displays this. Make sure that your local copy is up to date\nwith the main repository if you decide to keep this fork around for longer. For now simply\nfollow the instructions in Installation (page xxxiv) to get started. The main difference is\nthat you are now downloading yourownfork of the repository.\ntFig. B.5 Cloning the repository.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78c3b75b-7a60-4e63-8645-5404d79cd004": {"__data__": {"id_": "78c3b75b-7a60-4e63-8645-5404d79cd004", "embedding": null, "metadata": {"page_label": "1059", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3d432e47-783f-4545-b1bd-5139c3c15059", "node_type": "4", "metadata": {"page_label": "1059", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8bc6b27998b0df400aaf460262c0730e226ee837005717c70e934cf0eaf0a584", "class_name": "RelatedNodeInfo"}}, "text": "1059 Contributing to This Book\n# Replace your_github_username with your GitHub username\ngit clone https: //github .com/your_github_username /d2l-en.git\nEditing and Pushing\nNowitistimetoeditthebook. ItisbesttoedititintheJupyterNotebookfollowinginstruc-\ntions inSection B.1 . Make the changes and check that they are OK. Assume that we have\nmodified a typo in the file ~/d2l-en/chapter_appendix-tools-for-deep-learning/\ncontributing.md . You can then check which files you have changed.\nAt this point Git will prompt that the chapter_appendix-tools-for-deep-learning/\ncontributing.md file has been modified.\nmylaptop:d2l-en me$ git status\nOn branch master\nYour branch is up-to-date with 'origin/master'.\nChanges not staged for commit:\n(use \"git add <file>...\" to update what will be committed)\n(use \"git checkout -- <file>...\" to discard changes in working directory)\nmodified: chapter_appendix-tools-for-deep-learning/contributing.md\nAfter confirming that this is what you want, execute the following command:\ngit add chapter_appendix -tools -for-deep -learning /contributing .md\ngit commit -m'Fix a typo in git documentation '\ngit push\nThe changed code will then be in your personal fork of the repository. To request the\naddition of your change, you have to create a pull request for the official repository of the\nbook.\nSubmittingPull Requests\nAs shown in Fig. B.6, go to your fork of the repository on GitHub and select \u201cNew pull\nrequest\u201d. This will open up a screen that shows you the changes between your edits and\nwhat is current in the main repository of the book.\ntFig. B.6 New pull request.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1599, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a43bb76-f705-4e83-8df6-77fe05367747": {"__data__": {"id_": "6a43bb76-f705-4e83-8df6-77fe05367747", "embedding": null, "metadata": {"page_label": "1060", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "853908f1-595d-4280-882e-879d1228ca47", "node_type": "4", "metadata": {"page_label": "1060", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a36b0f08c79d6986577a79155b7d92fdcae4521b3c40b5533749237d4176e373", "class_name": "RelatedNodeInfo"}}, "text": "1060 Tools for Deep Learning\n316\n317Finally, submit a pull request by clicking the button as shown in Fig. B.7. Make sure to\ndescribe the changes you have made in the pull request. This will make it easier for the\nauthors to review it and to merge it with the book. Depending on the changes, this might\ngetacceptedrightaway,rejected,ormorelikely,youwillgetsomefeedbackonthechanges.\nOnce you have incorporated them, you are good to go.\ntFig. B.7 Create pull request.\nB.6.4Summary\n\u000fYou can use GitHub to contribute to this book.\n\u000fYou can edit the file on GitHub directly for minor changes.\n\u000fFor a major change, please fork the repository, edit things locally, and only contribute\nback once you are ready.\n\u000fPull requests are how contributions are being bundled up. Try not to submit huge pull\nrequestssincethismakesthemhardtounderstandandincorporate. Bettersendseveral\nsmaller ones.\nB.6.5Exercises\n1.Star and fork the d2l-ai/d2l-en repository.\n2.If you spot anything that needs improvement (e.g., missing a reference), submit a pull\nrequest.\n3.It is usually a better practice to create a pull request using a new branch. Learn how to\ndo it with Git branching316.\nDiscussions317.\nB.7UtilityFunctions and Classes\nThis section contains the implementations of utility functions and classes used in this\nbook.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1301, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a940d782-a78b-4fd3-9270-c198e3281230": {"__data__": {"id_": "a940d782-a78b-4fd3-9270-c198e3281230", "embedding": null, "metadata": {"page_label": "1061", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c5877935-558a-4b3d-90f8-5066af04aed0", "node_type": "4", "metadata": {"page_label": "1061", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "bf6674b93570289947cb73bfdbab40bb7f585ff5015db477469d9afd51c7eaaa", "class_name": "RelatedNodeInfo"}}, "text": "1061 Utility Functions and Classes\nimport collections\nimport inspect\nfrom IPython import display\nfrom torch import nn\nfrom d2l import torch asd2l\nHyperparameters.\n@d2l .add_to_class(d2l .HyperParameters) #@save\ndef save_hyperparameters (self , ignore =[]):\n\"\"\"Save function arguments into class attributes.\"\"\"\nframe =inspect .currentframe() .f_back\n_, _, _, local_vars =inspect .getargvalues(frame)\nself .hparams ={k:v for k, v inlocal_vars .items()\nifknot inset(ignore +['self '])and not k.startswith( '_')}\nfor k, v inself .hparams .items():\nsetattr (self , k, v)\nProgress bar.\n@d2l .add_to_class(d2l .ProgressBoard) #@save\ndef draw (self , x, y, label, every_n =1):\nPoint =collections .namedtuple( 'Point ', ['x','y'])\nifnot hasattr (self ,'raw_points '):\nself .raw_points =collections .OrderedDict()\nself .data =collections .OrderedDict()\niflabel not inself .raw_points:\nself .raw_points[label] =[]\nself .data[label] =[]\npoints =self .raw_points[label]\nline =self .data[label]\npoints .append(Point(x, y))\niflen(points) !=every_n:\nreturn\nmean =lambda x:sum(x) /len(x)\nline .append(Point(mean([p .xfor pinpoints]),\nmean([p .yfor pinpoints])))\npoints .clear()\nifnot self .display:\nreturn\nd2l.use_svg_display()\nifself .fig isNone :\nself .fig =d2l.plt.figure(figsize =self .figsize)\nplt_lines, labels =[], []\nfor (k, v), ls, color inzip(self .data .items(), self .ls, self .colors):\nplt_lines .append(d2l .plt.plot([p .xfor pinv], [p .yfor pinv],\nlinestyle =ls, color =color)[ 0])\nlabels .append(k)\naxes =self .axes ifself .axes else d2l.plt.gca()\nifself .xlim: axes .set_xlim( self .xlim)\nifself .ylim: axes .set_ylim( self .ylim)\nifnot self .xlabel: self .xlabel =self .x\naxes .set_xlabel( self .xlabel)\naxes .set_ylabel( self .ylabel)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1761, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04a6eded-c1ba-4f6b-9353-d37268b84945": {"__data__": {"id_": "04a6eded-c1ba-4f6b-9353-d37268b84945", "embedding": null, "metadata": {"page_label": "1062", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c06221d7-0038-4909-a32a-10beb9601cc5", "node_type": "4", "metadata": {"page_label": "1062", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1b347e867c646890b66609b33f82d48a8fd509f1ccc875ed31e78dfe735c20a5", "class_name": "RelatedNodeInfo"}}, "text": "1062 Tools for Deep Learning\n(continued from previous page)\naxes .set_xscale( self .xscale)\naxes .set_yscale( self .yscale)\naxes .legend(plt_lines, labels)\ndisplay .display( self .fig)\ndisplay .clear_output(wait =True )\nAdd FrozenLake enviroment\ndef frozen_lake (seed): #@save\n# See https://www.gymlibrary.dev/environments/toy_text/frozen_lake/ to \u2423\n\u21a9!learn more about this env\n# How to process env.P.items is adpated from https://sites.google.com/view/\n\u21a9!deep-rl-bootcamp/labs\nimport gym\nenv =gym.make( 'FrozenLake-v1 ', is_slippery =False )\nenv.seed(seed)\nenv.action_space .np_random .seed(seed)\nenv.action_space .seed(seed)\nenv_info ={}\nenv_info[ 'desc ']=env.desc # 2D array specifying what each grid item \u2423\n\u21a9!means\nenv_info[ 'num_states ']=env.nS # Number of observations/states or obs/\n\u21a9!state dim\nenv_info[ 'num_actions ']=env.nA # Number of actions or action dim\n# Define indices for (transition probability, nextstate, reward, done) \u2423\n\u21a9!tuple\nenv_info[ 'trans_prob_idx ']=0# Index of transition probability entry\nenv_info[ 'nextstate_idx ']=1# Index of next state entry\nenv_info[ 'reward_idx ']=2# Index of reward entry\nenv_info[ 'done_idx ']=3# Index of done entry\nenv_info[ 'mdp']={}\nenv_info[ 'env']=env\nfor (s, others) inenv.P.items():\n# others(s) = {a0: [ (p(s'|s,a0), s', reward, done),...], a1:[...], ...\n\u21a9!}\nfor (a, pxrds) inothers .items():\n# pxrds is [(p1,next1,r1,d1),(p2,next2,r2,d2),..].\n# e.g. [(0.3, 0, 0, False), (0.3, 0, 0, False), (0.3, 4, 1, False)]\nenv_info[ 'mdp'][(s,a)] =pxrds\nreturn env_info\nCreate enviroment\ndef make_env (name ='', seed =0):#@save\n# Input parameters:\n# name: specifies a gym environment.\n# For Value iteration, only FrozenLake-v1 is supported.\nifname =='FrozenLake-v1 ':\nreturn frozen_lake(seed)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1772, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40e0c668-5f4b-40c4-b3ff-090f74aee5f9": {"__data__": {"id_": "40e0c668-5f4b-40c4-b3ff-090f74aee5f9", "embedding": null, "metadata": {"page_label": "1063", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee500273-a3d5-49d8-9acc-091f39ada3a5", "node_type": "4", "metadata": {"page_label": "1063", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "19862c81aeb6f0c9b5b41791ec40b64445a6e3eed167aadff0bda35c1c628628", "class_name": "RelatedNodeInfo"}}, "text": "1063 Utility Functions and Classes\n(continued from previous page)\nelse :\nraise ValueError (\"%senv is not supported in this Notebook \")\nShow value function\ndef show_value_function_progress (env_desc, V, pi): #@save\n# This function visualizes how value and policy changes over time.\n# V: [num_iters, num_states]\n# pi: [num_iters, num_states]\n# How to visualize value function is adapted (but changed) from: https://\n\u21a9!sites.google.com/view/deep-rl-bootcamp/labs\nnum_iters =V.shape[ 0]\nfig, ax =plt.subplots(figsize =(15,15))\nfor kinrange (V.shape[ 0]):\nplt.subplot( 4,4, k +1)\nplt.imshow(V[k] .reshape( 4,4), cmap =\"bone \")\nax=plt.gca()\nax.set_xticks(np .arange( 0,5)-.5, minor =True )\nax.set_yticks(np .arange( 0,5)-.5, minor =True )\nax.grid(which =\"minor \", color =\"w\", linestyle ='-', linewidth =3)\nax.tick_params(which =\"minor \", bottom =False , left =False )\nax.set_xticks([])\nax.set_yticks([])\n# LEFT action: 0, DOWN action: 1\n# RIGHT action: 2, UP action: 3\naction2dxdy ={0:(-.25,0),1: (0,.25),\n2:(0.25 ,0),3: (-.25,0)}\nfor yinrange (4):\nfor xinrange (4):\naction =pi[k] .reshape( 4,4)[y, x]\ndx, dy =action2dxdy[action]\nifenv_desc[y,x] .decode() =='H':\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"y\",\nsize =20, fontweight ='bold ')\nelif env_desc[y,x] .decode() =='G':\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"w\",\nsize =20, fontweight ='bold ')\nelse :\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"g\",\nsize =15, fontweight ='bold ')\n# No arrow for cells with G and H labels\nifenv_desc[y,x] .decode() !='G'and env_desc[y,x] .decode() !=\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2837d0ff-25fc-44a3-b8a7-a313f811c646": {"__data__": {"id_": "2837d0ff-25fc-44a3-b8a7-a313f811c646", "embedding": null, "metadata": {"page_label": "1064", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac812169-2dbe-4ae8-8f5e-4db944318afa", "node_type": "4", "metadata": {"page_label": "1064", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "caac5c01a759b6357daaf2f2261ac982af5bcf9237066927956a0f6e8edb7605", "class_name": "RelatedNodeInfo"}}, "text": "1064 Tools for Deep Learning\n(continued from previous page)\n\u21a9!'H':\nax.arrow(x, y, dx, dy, color ='r', head_width =0.2, head_\n\u21a9!length =0.15 )\nax.set_title( \"Step = \"+str(k+1), fontsize =20)\nfig.tight_layout()\nplt.show()\nShow Q function\ndef show_Q_function_progress (env_desc, V_all, pi_all): #@save\n# This function visualizes how value and policy changes over time.\n# V: [num_iters, num_states]\n# pi: [num_iters, num_states]\n# We want to only shows few values\nnum_iters_all =V_all .shape[ 0]\nnum_iters =num_iters_all //10\nvis_indx =np.arange( 0, num_iters_all, num_iters) .tolist()\nvis_indx .append(num_iters_all -1)\nV=np.zeros(( len(vis_indx), V_all .shape[ 1]))\npi=np.zeros(( len(vis_indx), V_all .shape[ 1]))\nfor c, i inenumerate (vis_indx):\nV[c] =V_all[i]\npi[c] =pi_all[i]\nnum_iters =V.shape[ 0]\nfig, ax =plt.subplots(figsize =(15,15))\nfor kinrange (V.shape[ 0]):\nplt.subplot( 4,4, k +1)\nplt.imshow(V[k] .reshape( 4,4), cmap =\"bone \")\nax=plt.gca()\nax.set_xticks(np .arange( 0,5)-.5, minor =True )\nax.set_yticks(np .arange( 0,5)-.5, minor =True )\nax.grid(which =\"minor \", color =\"w\", linestyle ='-', linewidth =3)\nax.tick_params(which =\"minor \", bottom =False , left =False )\nax.set_xticks([])\nax.set_yticks([])\n# LEFT action: 0, DOWN action: 1\n# RIGHT action: 2, UP action: 3\naction2dxdy ={0:(-.25,0),1:(0,.25),\n2:(0.25 ,0),3:(-.25,0)}\nfor yinrange (4):\nfor xinrange (4):\naction =pi[k] .reshape( 4,4)[y, x]\ndx, dy =action2dxdy[action]\nifenv_desc[y,x] .decode() =='H':\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1496, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f120881f-f240-4591-b8ed-812bd92e1ce4": {"__data__": {"id_": "f120881f-f240-4591-b8ed-812bd92e1ce4", "embedding": null, "metadata": {"page_label": "1065", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "928cc9ae-35f5-4bf8-a84b-6147914d4e8a", "node_type": "4", "metadata": {"page_label": "1065", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d8a0a71a4ad58f7f1126161c385a7690a827e9e46f84a1dd35a4c945c3a7faf0", "class_name": "RelatedNodeInfo"}}, "text": "1065 Utility Functions and Classes\n(continued from previous page)\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"y\",\nsize =20, fontweight ='bold ')\nelif env_desc[y,x] .decode() =='G':\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"w\",\nsize =20, fontweight ='bold ')\nelse :\nax.text(x, y, str(env_desc[y,x] .decode()),\nha=\"center \", va =\"center \", color =\"g\",\nsize =15, fontweight ='bold ')\n# No arrow for cells with G and H labels\nifenv_desc[y,x] .decode() !='G'and env_desc[y,x] .decode() !=\n\u21a9!'H':\nax.arrow(x, y, dx, dy, color ='r', head_width =0.2, head_\n\u21a9!length =0.15 )\nax.set_title( \"Step = \"+str(vis_indx[k] +1), fontsize =20)\nfig.tight_layout()\nplt.show()\nTrainer\nA bunch of functions that will be deprecated:\ndef load_array (data_arrays, batch_size, is_train =True ): #@save\n\"\"\"Construct a PyTorch data iterator.\"\"\"\ndataset =torch .utils .data .TensorDataset( *data_arrays)\nreturn torch .utils .data .DataLoader(dataset, batch_size, shuffle =is_train)\ndef synthetic_data (w, b, num_examples): #@save\n\"\"\"Generate y = Xw + b + noise.\"\"\"\nX=torch .normal( 0,1, (num_examples, len(w)))\ny=torch .matmul(X, w) +b\ny+=torch .normal( 0,0.01 , y.shape)\nreturn X, y .reshape(( -1,1))\ndef sgd(params, lr, batch_size): #@save\n\"\"\"Minibatch stochastic gradient descent.\"\"\"\nwith torch .no_grad():\nfor param inparams:\nparam -=lr*param .grad /batch_size\nparam .grad .zero_()\ndef get_dataloader_workers (): #@save\n\"\"\"Use 4 processes to read the data.\"\"\"\nreturn 4\ndef load_data_fashion_mnist (batch_size, resize =None ): #@save\n\"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d143041-3a1f-49c1-8b12-9e5982acfcba": {"__data__": {"id_": "2d143041-3a1f-49c1-8b12-9e5982acfcba", "embedding": null, "metadata": {"page_label": "1066", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8d16451d-7b2d-4127-a41f-d1430e17378a", "node_type": "4", "metadata": {"page_label": "1066", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "7506fe549f0a8c1220b979102738ef2cb1e6c494e6d89266f7608e18cc2e1b70", "class_name": "RelatedNodeInfo"}}, "text": "1066 Tools for Deep Learning\n(continued from previous page)\ntrans =[transforms .ToTensor()]\nifresize:\ntrans .insert( 0, transforms .Resize(resize))\ntrans =transforms .Compose(trans)\nmnist_train =torchvision .datasets .FashionMNIST(\nroot =\"../data \", train =True , transform =trans, download =True )\nmnist_test =torchvision .datasets .FashionMNIST(\nroot =\"../data \", train =False , transform =trans, download =True )\nreturn (torch .utils .data .DataLoader(mnist_train, batch_size, shuffle =True ,\nnum_workers =get_dataloader_workers()),\ntorch .utils .data .DataLoader(mnist_test, batch_size, shuffle =False ,\nnum_workers =get_dataloader_workers()))\ndef evaluate_accuracy_gpu (net, data_iter, device =None ):#@save\n\"\"\"Compute the accuracy for a model on a dataset using a GPU.\"\"\"\nifisinstance (net, nn .Module):\nnet.eval() # Set the model to evaluation mode\nifnot device:\ndevice =next (iter (net .parameters())) .device\n# No. of correct predictions, no. of predictions\nmetric =d2l.Accumulator( 2)\nwith torch .no_grad():\nfor X, y indata_iter:\nifisinstance (X, list ):\n# Required for BERT Fine-tuning (to be covered later)\nX=[x.to(device) for xinX]\nelse :\nX=X.to(device)\ny=y.to(device)\nmetric .add(d2l .accuracy(net(X), y), y .numel())\nreturn metric[ 0]/metric[ 1]\n#@save\ndef train_ch6 (net, train_iter, test_iter, num_epochs, lr, device):\n\"\"\"Train a model with a GPU (defined in Chapter 6).\"\"\"\ndef init_weights (m):\niftype (m) ==nn.Linear ortype (m) ==nn.Conv2d:\nnn.init .xavier_uniform_(m .weight)\nnet.apply(init_weights)\nprint ('training on ', device)\nnet.to(device)\noptimizer =torch .optim .SGD(net .parameters(), lr =lr)\nloss =nn.CrossEntropyLoss()\nanimator =d2l.Animator(xlabel ='epoch ', xlim =[1, num_epochs],\nlegend =['train loss ','train acc ','test acc '])\ntimer, num_batches =d2l.Timer(), len(train_iter)\nfor epoch inrange (num_epochs):\n# Sum of training loss, sum of training accuracy, no. of examples\nmetric =d2l.Accumulator( 3)\nnet.train()\nfor i, (X, y) inenumerate (train_iter):\ntimer .start()\noptimizer .zero_grad()\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13b12716-f2b1-46d3-923b-7857a3ebde7b": {"__data__": {"id_": "13b12716-f2b1-46d3-923b-7857a3ebde7b", "embedding": null, "metadata": {"page_label": "1067", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e2a745d1-c965-4707-b4ab-55713cec3b46", "node_type": "4", "metadata": {"page_label": "1067", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "15762110ace6a9e3ffe68b215b1b6efc87c77ba9a728ea882d61818ed9c6301d", "class_name": "RelatedNodeInfo"}}, "text": "1067 Utility Functions and Classes\n(continued from previous page)\nX, y =X.to(device), y .to(device)\ny_hat =net(X)\nl=loss(y_hat, y)\nl.backward()\noptimizer .step()\nwith torch .no_grad():\nmetric .add(l *X.shape[ 0], d2l .accuracy(y_hat, y), X .shape[ 0])\ntimer .stop()\ntrain_l =metric[ 0]/metric[ 2]\ntrain_acc =metric[ 1]/metric[ 2]\nif(i+1)%(num_batches //5)==0ori==num_batches -1:\nanimator .add(epoch +(i+1)/num_batches,\n(train_l, train_acc, None ))\ntest_acc =evaluate_accuracy_gpu(net, test_iter)\nanimator .add(epoch +1, (None ,None , test_acc))\nprint (f'loss {train_l :.3f}, train acc {train_acc :.3f},'\nf'test acc {test_acc :.3f}')\nprint (f'{metric[ 2]*num_epochs /timer .sum() :.1f}examples/sec '\nf'on{str(device) }')\ndef show_images (imgs, num_rows, num_cols, titles =None , scale =1.5): #@save\n\"\"\"Plot a list of images.\"\"\"\nfigsize =(num_cols *scale, num_rows *scale)\n_, axes =d2l.plt.subplots(num_rows, num_cols, figsize =figsize)\naxes =axes .flatten()\nfor i, (ax, img) inenumerate (zip(axes, imgs)):\ntry:\nimg =img.detach() .numpy()\nexcept :\npass\nax.imshow(img)\nax.axes .get_xaxis() .set_visible( False )\nax.axes .get_yaxis() .set_visible( False )\niftitles:\nax.set_title(titles[i])\nreturn axes\ndef linreg (X, w, b): #@save\n\"\"\"The linear regression model.\"\"\"\nreturn torch .matmul(X, w) +b\ndef squared_loss (y_hat, y): #@save\n\"\"\"Squared loss.\"\"\"\nreturn (y_hat -y.reshape(y_hat .shape)) **2/2\ndef get_fashion_mnist_labels (labels): #@save\n\"\"\"Return text labels for the Fashion-MNIST dataset.\"\"\"\ntext_labels =['t-shirt ','trouser ','pullover ','dress ','coat ',\n'sandal ','shirt ','sneaker ','bag','ankle boot ']\nreturn [text_labels[ int(i)] for iinlabels]\nclass Animator :#@save\n\"\"\"For plotting data in animation.\"\"\"\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1742, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39f38b70-3d51-4914-8f57-cb0073e3948e": {"__data__": {"id_": "39f38b70-3d51-4914-8f57-cb0073e3948e", "embedding": null, "metadata": {"page_label": "1068", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e822d38c-1118-46b7-89ed-6fa36726c5d6", "node_type": "4", "metadata": {"page_label": "1068", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "67fc01042763abdc3c1c8c8165f6575b319c4a248abca6158c6d54a66ceb5c76", "class_name": "RelatedNodeInfo"}}, "text": "1068 Tools for Deep Learning\n(continued from previous page)\ndef __init__ (self , xlabel =None , ylabel =None , legend =None , xlim =None ,\nylim =None , xscale ='linear ', yscale ='linear ',\nfmts =('-','m--','g-.','r:'), nrows =1, ncols =1,\nfigsize =(3.5,2.5)):\n# Incrementally plot multiple lines\niflegend isNone :\nlegend =[]\nd2l.use_svg_display()\nself .fig, self .axes =d2l.plt.subplots(nrows, ncols, figsize =figsize)\nifnrows *ncols ==1:\nself .axes =[self .axes, ]\n# Use a lambda function to capture arguments\nself .config_axes =lambda : d2l .set_axes(\nself .axes[ 0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\nself .X,self .Y,self .fmts =None ,None , fmts\ndef add(self , x, y):\n# Add multiple data points into the figure\nifnot hasattr (y, \"__len__ \"):\ny=[y]\nn=len(y)\nifnot hasattr (x, \"__len__ \"):\nx=[x] *n\nifnot self .X:\nself .X=[[] for _inrange (n)]\nifnot self .Y:\nself .Y=[[] for _inrange (n)]\nfor i, (a, b) inenumerate (zip(x, y)):\nifaisnot None and bisnot None :\nself .X[i] .append(a)\nself .Y[i] .append(b)\nself .axes[ 0].cla()\nfor x, y, fmt inzip(self .X,self .Y,self .fmts):\nself .axes[ 0].plot(x, y, fmt)\nself .config_axes()\ndisplay .display( self .fig)\ndisplay .clear_output(wait =True )\nclass Accumulator :#@save\n\"\"\"For accumulating sums over `n` variables.\"\"\"\ndef __init__ (self , n):\nself .data =[0.0]*n\ndef add(self ,*args):\nself .data =[a+float (b) for a, b inzip(self .data, args)]\ndef reset (self ):\nself .data =[0.0]*len(self .data)\ndef __getitem__ (self , idx):\nreturn self .data[idx]\ndef accuracy (y_hat, y): #@save\n\"\"\"Compute the number of correct predictions.\"\"\"\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1621, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d4e39efd-ef3d-4f5b-b895-8234df8e531a": {"__data__": {"id_": "d4e39efd-ef3d-4f5b-b895-8234df8e531a", "embedding": null, "metadata": {"page_label": "1069", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0f92aa44-848f-46db-ace6-2f909b451715", "node_type": "4", "metadata": {"page_label": "1069", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f607480237c010b4a9ada644895eb6f8d7a34dac8bf18ef7d7922b7cf8d52eb9", "class_name": "RelatedNodeInfo"}}, "text": "1069 Utility Functions and Classes\n(continued from previous page)\niflen(y_hat .shape) >1and y_hat .shape[ 1]>1:\ny_hat =y_hat .argmax(axis =1)\ncmp =y_hat .type(y .dtype) ==y\nreturn float (cmp .type(y .dtype) .sum())\nimport hashlib\nimport os\nimport tarfile\nimport zipfile\nimport requests\ndef download (url, folder ='../data ', sha1_hash =None ): #@save\n\"\"\"Download a file to folder and return the local filepath.\"\"\"\nifnot url.startswith( 'http '):\n# For back compatability\nurl, sha1_hash =DATA_HUB[url]\nos.makedirs(folder, exist_ok =True )\nfname =os.path .join(folder, url .split( '/')[-1])\n# Check if hit cache\nifos.path .exists(fname) and sha1_hash:\nsha1 =hashlib .sha1()\nwith open (fname, 'rb')asf:\nwhile True :\ndata =f.read( 1048576 )\nifnot data:\nbreak\nsha1 .update(data)\nifsha1 .hexdigest() ==sha1_hash:\nreturn fname\n# Download\nprint (f'Downloading {fname }from {url}...')\nr=requests .get(url, stream =True , verify =True )\nwith open (fname, 'wb')asf:\nf.write(r .content)\nreturn fname\ndef extract (filename, folder =None ): #@save\n\"\"\"Extract a zip/tar file into folder.\"\"\"\nbase_dir =os.path .dirname(filename)\n_, ext =os.path .splitext(filename)\nassert ext in('.zip ','.tar ','.gz'),'Only support zip/tar files. '\nifext =='.zip ':\nfp=zipfile .ZipFile(filename, 'r')\nelse :\nfp=tarfile .open(filename, 'r')\niffolder isNone :\nfolder =base_dir\nfp.extractall(folder)\ndef download_extract (name, folder =None ): #@save\n\"\"\"Download and extract a zip/tar file.\"\"\"\nfname =download(name)\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e16f0be8-ee70-42b7-9c87-b08f5ccffbee": {"__data__": {"id_": "e16f0be8-ee70-42b7-9c87-b08f5ccffbee", "embedding": null, "metadata": {"page_label": "1070", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "33c3afa9-7d09-49dc-b043-fd7549e02697", "node_type": "4", "metadata": {"page_label": "1070", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "79a591a4b256e2c8d4132ff90bfaed2609ff8c25b2e3352d405f55f08d489f34", "class_name": "RelatedNodeInfo"}}, "text": "1070 Tools for Deep Learning\n(continued from previous page)\nbase_dir =os.path .dirname(fname)\ndata_dir, ext =os.path .splitext(fname)\nifext =='.zip ':\nfp=zipfile .ZipFile(fname, 'r')\nelif ext in('.tar ','.gz'):\nfp=tarfile .open(fname, 'r')\nelse :\nassert False ,'Only zip/tar files can be extracted. '\nfp.extractall(base_dir)\nreturn os.path .join(base_dir, folder) iffolder else data_dir\ndef tokenize (lines, token ='word '): #@save\n\"\"\"Split text lines into word or character tokens.\"\"\"\nassert token in('word ','char '),'Unknown token type: '+token\nreturn [line .split() iftoken =='word 'else list (line) for line inlines]\ndef evaluate_loss (net, data_iter, loss): #@save\n\"\"\"Evaluate the loss of a model on the given dataset.\"\"\"\nmetric =d2l.Accumulator( 2)# Sum of losses, no. of examples\nfor X, y indata_iter:\nout =net(X)\ny=y.reshape(out .shape)\nl=loss(out, y)\nmetric .add(l .sum(), l .numel())\nreturn metric[ 0]/metric[ 1]\ndef grad_clipping (net, theta): #@save\n\"\"\"Clip the gradient.\"\"\"\nifisinstance (net, nn .Module):\nparams =[pfor pinnet.parameters() ifp.requires_grad]\nelse :\nparams =net.params\nnorm =torch .sqrt( sum(torch .sum((p .grad **2))for pinparams))\nifnorm >theta:\nfor param inparams:\nparam .grad[:] *=theta /norm\nMore for the attention chapter.\n#@save\nd2l.DATA_HUB[ 'fra-eng ']=(d2l .DATA_URL +'fra-eng.zip ',\n'94646ad1522d915e7b0f9296181140edcf86a4f5 ')\n#@save\ndef read_data_nmt ():\n\"\"\"Load the English-French dataset.\"\"\"\ndata_dir =d2l.download_extract( 'fra-eng ')\nwith open (os.path .join(data_dir, 'fra.txt '),'r', encoding ='utf-8 ')asf:\nreturn f.read()\n#@save\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "43d85eab-56aa-404d-8a6c-5f569f0be93f": {"__data__": {"id_": "43d85eab-56aa-404d-8a6c-5f569f0be93f", "embedding": null, "metadata": {"page_label": "1071", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ffc8974e-cefa-4a8b-8532-60a43124722f", "node_type": "4", "metadata": {"page_label": "1071", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "81aed450a867b6484a370f5913ef7fb016c449e45fdd1d5950b41edf300853c3", "class_name": "RelatedNodeInfo"}}, "text": "1071 Utility Functions and Classes\n(continued from previous page)\ndef preprocess_nmt (text):\n\"\"\"Preprocess the English-French dataset.\"\"\"\ndef no_space (char, prev_char):\nreturn char inset(',.!? ')and prev_char !=''\n# Replace non-breaking space with space, and convert uppercase letters to\n# lowercase ones\ntext =text .replace( '\\u202f ','').replace( '\\xa0 ','').lower()\n# Insert space between words and punctuation marks\nout =[''+char ifi>0and no_space(char, text[i -1])else char\nfor i, char inenumerate (text)]\nreturn ''.join(out)\n#@save\ndef tokenize_nmt (text, num_examples =None ):\n\"\"\"Tokenize the English-French dataset.\"\"\"\nsource, target =[], []\nfor i, line inenumerate (text .split( '\\n')):\nifnum_examples and i>num_examples:\nbreak\nparts =line .split( '\\t')\niflen(parts) ==2:\nsource .append(parts[ 0].split( ''))\ntarget .append(parts[ 1].split( ''))\nreturn source, target\n#@save\ndef truncate_pad (line, num_steps, padding_token):\n\"\"\"Truncate or pad sequences.\"\"\"\niflen(line) >num_steps:\nreturn line[:num_steps] # Truncate\nreturn line +[padding_token] *(num_steps -len(line)) # Pad\n#@save\ndef build_array_nmt (lines, vocab, num_steps):\n\"\"\"Transform text sequences of machine translation into minibatches.\"\"\"\nlines =[vocab[l] for linlines]\nlines =[l+[vocab[ '<eos> ']]for linlines]\narray =torch .tensor([truncate_pad(\nl, num_steps, vocab[ '<pad> '])for linlines])\nvalid_len =(array !=vocab[ '<pad> ']).type(torch .int32) .sum( 1)\nreturn array, valid_len\n#@save\ndef load_data_nmt (batch_size, num_steps, num_examples =600):\n\"\"\"Return the iterator and the vocabularies of the translation dataset.\"\"\"\ntext =preprocess_nmt(read_data_nmt())\nsource, target =tokenize_nmt(text, num_examples)\nsrc_vocab =d2l.Vocab(source, min_freq =2,\nreserved_tokens =['<pad> ','<bos> ','<eos> '])\ntgt_vocab =d2l.Vocab(target, min_freq =2,\nreserved_tokens =['<pad> ','<bos> ','<eos> '])\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1891, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1b37d06-9524-4968-bff0-c018b96c690f": {"__data__": {"id_": "e1b37d06-9524-4968-bff0-c018b96c690f", "embedding": null, "metadata": {"page_label": "1072", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5fbf7d35-020c-4161-bbd9-282f30ff0783", "node_type": "4", "metadata": {"page_label": "1072", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cf6a35c9a0681c6037bcd530df0ea919064abb8c374897c2eef2de383bfa667b", "class_name": "RelatedNodeInfo"}}, "text": "1072 Tools for Deep Learning\n(continued from previous page)\nsrc_array, src_valid_len =build_array_nmt(source, src_vocab, num_steps)\ntgt_array, tgt_valid_len =build_array_nmt(target, tgt_vocab, num_steps)\ndata_arrays =(src_array, src_valid_len, tgt_array, tgt_valid_len)\ndata_iter =d2l.load_array(data_arrays, batch_size)\nreturn data_iter, src_vocab, tgt_vocab\n#@save\ndef sequence_mask (X, valid_len, value =0):\n\"\"\"Mask irrelevant entries in sequences.\"\"\"\nmaxlen =X.size( 1)\nmask =torch .arange((maxlen), dtype =torch .float32,\ndevice =X.device)[ None , :] <valid_len[:, None ]\nX[~mask] =value\nreturn X\n#@save\nclass MaskedSoftmaxCELoss (nn.CrossEntropyLoss):\n\"\"\"The softmax cross-entropy loss with masks.\"\"\"\n# `pred` shape: (`batch_size`, `num_steps`, `vocab_size`)\n# `label` shape: (`batch_size`, `num_steps`)\n# `valid_len` shape: (`batch_size`,)\ndef forward (self , pred, label, valid_len):\nweights =torch .ones_like(label)\nweights =sequence_mask(weights, valid_len)\nself .reduction ='none '\nunweighted_loss =super (MaskedSoftmaxCELoss, self ).forward(\npred .permute( 0,2,1), label)\nweighted_loss =(unweighted_loss *weights) .mean(dim =1)\nreturn weighted_loss\n#@save\ndef train_seq2seq (net, data_iter, lr, num_epochs, tgt_vocab, device):\n\"\"\"Train a model for sequence to sequence.\"\"\"\ndef xavier_init_weights (m):\niftype (m) ==nn.Linear:\nnn.init .xavier_uniform_(m .weight)\niftype (m) ==nn.GRU:\nfor param inm._flat_weights_names:\nif\"weight \"inparam:\nnn.init .xavier_uniform_(m ._parameters[param])\nnet.apply(xavier_init_weights)\nnet.to(device)\noptimizer =torch .optim .Adam(net .parameters(), lr =lr)\nloss =MaskedSoftmaxCELoss()\nnet.train()\nanimator =d2l.Animator(xlabel ='epoch ', ylabel ='loss ',\nxlim =[10, num_epochs])\nfor epoch inrange (num_epochs):\ntimer =d2l.Timer()\nmetric =d2l.Accumulator( 2)# Sum of training loss, no. of tokens\nfor batch indata_iter:\noptimizer .zero_grad()\nX, X_valid_len, Y, Y_valid_len =[x.to(device) for xinbatch]\n(continues on next page)", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81f0d7cc-e154-458c-8fe0-1ab14de17ab3": {"__data__": {"id_": "81f0d7cc-e154-458c-8fe0-1ab14de17ab3", "embedding": null, "metadata": {"page_label": "1073", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3d05ef5-22b1-4476-9f6b-29d23d41a99c", "node_type": "4", "metadata": {"page_label": "1073", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e957b5d33dae9f834455330b6e0af3716fbef9413a7956b8789f39df3676b7e7", "class_name": "RelatedNodeInfo"}}, "text": "1073 Utility Functions and Classes\n(continued from previous page)\nbos =torch .tensor([tgt_vocab[ '<bos> ']]*Y.shape[ 0],\ndevice =device) .reshape( -1,1)\ndec_input =torch .cat([bos, Y[:, : -1]], 1)# Teacher forcing\nY_hat, _ =net(X, dec_input, X_valid_len)\nl=loss(Y_hat, Y, Y_valid_len)\nl.sum() .backward() # Make the loss scalar for `backward`\nd2l.grad_clipping(net, 1)\nnum_tokens =Y_valid_len .sum()\noptimizer .step()\nwith torch .no_grad():\nmetric .add(l .sum(), num_tokens)\nif(epoch +1)%10==0:\nanimator .add(epoch +1, (metric[ 0]/metric[ 1],))\nprint (f'loss {metric[ 0]/metric[ 1]:.3f},{metric[ 1]/timer .stop() :.1f}'\nf'tokens/sec on {str(device) }')\n#@save\ndef predict_seq2seq (net, src_sentence, src_vocab, tgt_vocab, num_steps,\ndevice, save_attention_weights =False ):\n\"\"\"Predict for sequence to sequence.\"\"\"\n# Set `net` to eval mode for inference\nnet.eval()\nsrc_tokens =src_vocab[src_sentence .lower() .split( '')]+[\nsrc_vocab[ '<eos> ']]\nenc_valid_len =torch .tensor([ len(src_tokens)], device =device)\nsrc_tokens =d2l.truncate_pad(src_tokens, num_steps, src_vocab[ '<pad> '])\n# Add the batch axis\nenc_X =torch .unsqueeze(\ntorch .tensor(src_tokens, dtype =torch .long, device =device), dim =0)\nenc_outputs =net.encoder(enc_X, enc_valid_len)\ndec_state =net.decoder .init_state(enc_outputs, enc_valid_len)\n# Add the batch axis\ndec_X =torch .unsqueeze(torch .tensor(\n[tgt_vocab[ '<bos> ']], dtype =torch .long, device =device), dim =0)\noutput_seq, attention_weight_seq =[], []\nfor _inrange (num_steps):\nY, dec_state =net.decoder(dec_X, dec_state)\n# We use the token with the highest prediction likelihood as input\n# of the decoder at the next time step\ndec_X =Y.argmax(dim =2)\npred =dec_X .squeeze(dim =0).type(torch .int32) .item()\n# Save attention weights (to be covered later)\nifsave_attention_weights:\nattention_weight_seq .append(net .decoder .attention_weights)\n# Once the end-of-sequence token is predicted, the generation of the\n# output sequence is complete\nifpred ==tgt_vocab[ '<eos> ']:\nbreak\noutput_seq .append(pred)\nreturn ''.join(tgt_vocab .to_tokens(output_seq)), attention_weight_seq", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a653b7b8-1908-4ae7-8c5b-fadb23b96c1d": {"__data__": {"id_": "a653b7b8-1908-4ae7-8c5b-fadb23b96c1d", "embedding": null, "metadata": {"page_label": "1074", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c385ae98-d182-49d1-af2d-babe1a090f6f", "node_type": "4", "metadata": {"page_label": "1074", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "00466577c290820f6bbb1af8c1eebe643405be9eaaed9133a853c05d1c64426a", "class_name": "RelatedNodeInfo"}}, "text": "1074 Tools for Deep Learning\n318B.8The d2lAPI Document\nThis section displays classes and functions (sorted alphabetically) in the d2lpackage,\nshowing where they are defined in the book so you can find more detailed implementa-\ntions and explanations. See also the source code on the GitHub repository318.\nB.8.1Classes\nclass d2l.torch.AdditiveAttention( num_hiddens ,dropout,**kwargs )\nBases: Module\nAdditive attention.\nDefined in Section 11.3.2\nforward( queries,keys,values,valid_lens )\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nclass d2l.torch.AddNorm( norm_shape ,dropout )\nBases: Module\nThe residual connection followed by layer normalization.\nDefined in Section 11.7.2\nforward( X,Y)\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nclass d2l.torch.AttentionDecoder\nBases: Decoder (page 1075)\nThe base attention-based decoder interface.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "509576d7-b2f8-440b-9dd2-23fe8a5fa0c1": {"__data__": {"id_": "509576d7-b2f8-440b-9dd2-23fe8a5fa0c1", "embedding": null, "metadata": {"page_label": "1075", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "32211d49-2073-48b4-8ff7-1f734f10a6b3", "node_type": "4", "metadata": {"page_label": "1075", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "83291c231d6b65b58857b885caaddf4979941aefa879dfd05b64b1faeb840be7", "class_name": "RelatedNodeInfo"}}, "text": "1075 Thed2lAPI Document\nDefined in Section 11.4\nproperty attention_weights\nclass d2l.torch.Classifier( plot_train_per_epoch=2 ,plot_valid_per_epoch=1 )\nBases: Module(page 1078)\nThe base class of classification models.\nDefined in Section 4.3\naccuracy( Y_hat,Y,averaged=True )\nCompute the number of correct predictions.\nDefined in Section 4.3\nlayer_summary( X_shape )\nDefined in Section 7.6\nloss(Y_hat,Y,averaged=True )\nDefined in Section 4.5\nvalidation_step( batch )\nclass d2l.torch.DataModule( root=\u2019../data\u2019 ,num_workers=4 )\nBases: HyperParameters (page 1077)\nThe base class of data.\nDefined in Section 3.2.2\nget_dataloader( train )\nget_tensorloader( tensors,train,indices=slice(0,None,None) )\nDefined in Section 3.3\ntrain_dataloader()\nval_dataloader()\nclass d2l.torch.Decoder\nBases: Module\nThe base decoder interface for the encoder\u2013decoder architecture.\nDefined in Section 10.6\nforward( X,state)\nDefines the computation performed at every call.\nShould be overridden by all subclasses.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4de06e6a-06ef-4d12-83ba-b9c7253cb552": {"__data__": {"id_": "4de06e6a-06ef-4d12-83ba-b9c7253cb552", "embedding": null, "metadata": {"page_label": "1076", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3e62bcc8-9676-48c7-b5e7-23c67f8ca9cb", "node_type": "4", "metadata": {"page_label": "1076", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "802dcd4ec88df1aaf5c290ed328a1ea4a49b95a68a80c89cb0729ab49328eaff", "class_name": "RelatedNodeInfo"}}, "text": "1076 Tools for Deep Learning\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ninit_state( enc_all_outputs ,*args )\nclass d2l.torch.DotProductAttention( dropout )\nBases: Module\nScaled dot product attention.\nDefined in Section 11.3.2\nforward( queries,keys,values,valid_lens=None )\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nclass d2l.torch.Encoder\nBases: Module\nThe base encoder interface for the encoder\u2013decoder architecture.\nDefined in Section 10.6\nforward( X,*args )\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nclass d2l.torch.EncoderDecoder( encoder,decoder )\nBases: Classifier (page 1075)\nThe base class for the encoder\u2013decoder architecture.\nDefined in Section 10.6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1445, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a507e63b-8fe6-4935-b765-a29166829d47": {"__data__": {"id_": "a507e63b-8fe6-4935-b765-a29166829d47", "embedding": null, "metadata": {"page_label": "1077", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "07fd9c5c-57e0-4e10-bed5-2745789d0e70", "node_type": "4", "metadata": {"page_label": "1077", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "0ac65e6d249bd77ffa3dd5c5833656c2798b30f15bc1cc7626df852dcb8b4ea1", "class_name": "RelatedNodeInfo"}}, "text": "1077 Thed2lAPI Document\nforward( enc_X,dec_X,*args )\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\npredict_step( batch,device,num_steps ,save_attention_weights=False )\nDefined in Section 10.7.6\nclass d2l.torch.FashionMNIST( batch_size=64 ,resize=(28, 28) )\nBases: DataModule (page 1075)\nThe Fashion-MNIST dataset.\nDefined in Section 4.2\nget_dataloader( train )\nDefined in Section 4.2\ntext_labels( indices )\nReturn text labels.\nDefined in Section 4.2\nvisualize( batch,nrows=1 ,ncols=8,labels=[] )\nDefined in Section 4.2\nclass d2l.torch.GRU( num_inputs ,num_hiddens ,num_layers ,dropout=0 )\nBases: RNN(page 1081)\nThe multilayer GRU model.\nDefined in Section 10.3\nclass d2l.torch.HyperParameters\nBases: object\nThe base class of hyperparameters.\nsave_hyperparameters( ignore=[] )\nSave function arguments into class attributes.\nDefined in Section B.7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1133, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "062c43cc-3bf7-4a67-a318-11330492a136": {"__data__": {"id_": "062c43cc-3bf7-4a67-a318-11330492a136", "embedding": null, "metadata": {"page_label": "1078", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c9f70ba-b429-4425-a7b0-faf45f8f87be", "node_type": "4", "metadata": {"page_label": "1078", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "af4aca06489239cf57cf3221fe591db7937a9870cdbc54f0c6b830b56adf28e5", "class_name": "RelatedNodeInfo"}}, "text": "1078 Tools for Deep Learning\nclass d2l.torch.LeNet( lr=0.1,num_classes=10 )\nBases: Classifier (page 1075)\nThe LeNet-5 model.\nDefined in Section 7.6\nclass d2l.torch.LinearRegression( lr)\nBases: Module(page 1078)\nThe linear regression model implemented with high-level APIs.\nDefined in Section 3.5\nconfigure_optimizers()\nDefined in Section 3.5\nforward( X)\nDefined in Section 3.5\nget_w_b()\nDefined in Section 3.5\nloss(y_hat,y)\nDefined in Section 3.5\nclass d2l.torch.LinearRegressionScratch( num_inputs ,lr,sigma=0.01 )\nBases: Module(page 1078)\nThe linear regression model implemented from scratch.\nDefined in Section 3.4\nconfigure_optimizers()\nDefined in Section 3.4\nforward( X)\nDefined in Section 3.4\nloss(y_hat,y)\nDefined in Section 3.4\nclass d2l.torch.Module( plot_train_per_epoch=2 ,plot_valid_per_epoch=1 )\nBases: Module,HyperParameters (page 1077)\nThe base class of models.\nDefined in Section 3.2\napply_init( inputs,init=None )\nDefined in Section 6.4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 953, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3b5f6a5-5ab2-4eeb-9995-51dbc62e6026": {"__data__": {"id_": "f3b5f6a5-5ab2-4eeb-9995-51dbc62e6026", "embedding": null, "metadata": {"page_label": "1079", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18ef0941-93ff-42dc-a734-660d9b930f7c", "node_type": "4", "metadata": {"page_label": "1079", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b4b4441ba9ce1035ede5d907ab5d69bf14bdd63a0bc5d33b5f527593f158aff1", "class_name": "RelatedNodeInfo"}}, "text": "1079 Thed2lAPI Document\nconfigure_optimizers()\nDefined in Section 4.3\nforward( X)\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nloss(y_hat,y)\nplot(key,value,train )\nPlot a point in animation.\ntraining_step( batch )\nvalidation_step( batch )\nclass d2l.torch.MTFraEng( batch_size ,num_steps=9 ,num_train=512 ,\nnum_val=128 )\nBases: DataModule (page 1075)\nThe English-French dataset.\nDefined in Section 10.5\nbuild(src_sentences ,tgt_sentences )\nDefined in Section 10.5.3\nget_dataloader( train )\nDefined in Section 10.5.3\nclass d2l.torch.MultiHeadAttention( num_hiddens ,num_heads ,dropout,\nbias=False ,**kwargs )\nBases: Module(page 1078)\nMulti-head attention.\nDefined in Section 11.5\nforward( queries,keys,values,valid_lens )\nDefines the computation performed at every call.\nShould be overridden by all subclasses.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1097, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5372f5d7-2582-47ef-8b71-f1e410b9c3c4": {"__data__": {"id_": "5372f5d7-2582-47ef-8b71-f1e410b9c3c4", "embedding": null, "metadata": {"page_label": "1080", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "656daad1-1fe0-4de6-84cf-c83fc5bb6f24", "node_type": "4", "metadata": {"page_label": "1080", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "161c9d9a211d95175333225e02f1b25e6defd2b510ea7f8f7880716e07650755", "class_name": "RelatedNodeInfo"}}, "text": "1080 Tools for Deep Learning\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\ntranspose_output( X)\nReverse the operation of transpose_qkv.\nDefined in Section 11.5\ntranspose_qkv( X)\nTransposition for parallel computation of multiple attention heads.\nDefined in Section 11.5\nclass d2l.torch.PositionalEncoding( num_hiddens ,dropout,max_len=1000 )\nBases: Module\nPositional encoding.\nDefined in Section 11.6\nforward( X)\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nclass d2l.torch.PositionWiseFFN( ffn_num_hiddens ,ffn_num_outputs )\nBases: Module\nThe positionwise feed-forward network.\nDefined in Section 11.7\nforward( X)\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84319c2f-37eb-49ac-8024-ef05b77288db": {"__data__": {"id_": "84319c2f-37eb-49ac-8024-ef05b77288db", "embedding": null, "metadata": {"page_label": "1081", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8955b9b8-b58a-41a4-bcfc-55eb3bfd1b93", "node_type": "4", "metadata": {"page_label": "1081", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "ec04ab60eec8f7256056ba573f6b7c36e28352571a8f7f9fd8eeea7a51f87bcd", "class_name": "RelatedNodeInfo"}}, "text": "1081 Thed2lAPI Document\nclass d2l.torch.ProgressBoard( xlabel=None ,ylabel=None ,xlim=None ,\nylim=None ,xscale=\u2019linear\u2019 ,yscale=\u2019linear\u2019 ,\nls=[\u2019-\u2019,\u2019--\u2019,\u2019-.\u2019,\u2019:\u2019] ,colors=[\u2019C0\u2019,\u2019C1\u2019,\u2019C2\u2019,\n\u2019C3\u2019],fig=None ,axes=None ,figsize=(3.5, 2.5) ,\ndisplay=True )\nBases: HyperParameters (page 1077)\nThe board that plots data points in animation.\nDefined in Section 3.2\ndraw(x,y,label,every_n=1 )\nDefined in Section B.7\nclass d2l.torch.Residual( num_channels ,use_1x1conv=False ,strides=1 )\nBases: Module\nThe Residual block of ResNet models.\nDefined in Section 8.6\nforward( X)\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nclass d2l.torch.ResNeXtBlock( num_channels ,groups,bot_mul,\nuse_1x1conv=False ,strides=1 )\nBases: Module\nThe ResNeXt block.\nDefined in Section 8.6.2\nforward( X)\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e29898ed-9490-41ad-96da-6895c9f657be": {"__data__": {"id_": "e29898ed-9490-41ad-96da-6895c9f657be", "embedding": null, "metadata": {"page_label": "1082", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "397457e8-308a-4713-94ab-883b715a2a67", "node_type": "4", "metadata": {"page_label": "1082", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "389b21b2401786972d63add03cef083c33e5fc97eb05ceffca3eeaf60f9a5ce6", "class_name": "RelatedNodeInfo"}}, "text": "1082 Tools for Deep Learning\nclass d2l.torch.RNN( num_inputs ,num_hiddens )\nBases: Module(page 1078)\nThe RNN model implemented with high-level APIs.\nDefined in Section 9.6\nforward( inputs,H=None )\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nclass d2l.torch.RNNLM( rnn,vocab_size ,lr=0.01 )\nBases: RNNLMScratch (page 1082)\nThe RNN-based language model implemented with high-level APIs.\nDefined in Section 9.6\ninit_params()\noutput_layer( hiddens )\nDefined in Section 9.5\nclass d2l.torch.RNNLMScratch( rnn,vocab_size ,lr=0.01 )\nBases: Classifier (page 1075)\nThe RNN-based language model implemented from scratch.\nDefined in Section 9.5\nforward( X,state=None )\nDefined in Section 9.5\ninit_params()\none_hot( X)\nDefined in Section 9.5\noutput_layer( rnn_outputs )\nDefined in Section 9.5\npredict( prefix,num_preds ,vocab,device=None )\nDefined in Section 9.5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1139, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a002ac36-61a7-44ca-9dde-c6db7b6d6097": {"__data__": {"id_": "a002ac36-61a7-44ca-9dde-c6db7b6d6097", "embedding": null, "metadata": {"page_label": "1083", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab540cf0-82ba-494d-ab73-f5c4e3a97bd9", "node_type": "4", "metadata": {"page_label": "1083", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "eb0f636261a00d9d245c3386963f3ae294f9379fcf428efeae322361874057bb", "class_name": "RelatedNodeInfo"}}, "text": "1083 Thed2lAPI Document\ntraining_step( batch )\nvalidation_step( batch )\nclass d2l.torch.RNNScratch( num_inputs ,num_hiddens ,sigma=0.01 )\nBases: Module(page 1078)\nThe RNN model implemented from scratch.\nDefined in Section 9.5\nforward( inputs,state=None )\nDefined in Section 9.5\nclass d2l.torch.Seq2Seq( encoder,decoder,tgt_pad,lr)\nBases: EncoderDecoder (page 1076)\nThe RNN encoder\u2013decoder for sequence to sequence learning.\nDefined in Section 10.7.3\nconfigure_optimizers()\nDefined in Section 4.3\nvalidation_step( batch )\nclass d2l.torch.Seq2SeqEncoder( vocab_size ,embed_size ,num_hiddens ,\nnum_layers ,dropout=0 )\nBases: Encoder (page 1076)\nThe RNN encoder for sequence-to-sequence learning.\nDefined in Section 10.7\nforward( X,*args )\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nclass d2l.torch.SGD( params,lr)\nBases: HyperParameters (page 1077)\nMinibatch stochastic gradient descent.\nDefined in Section 3.4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "40c0cba8-4942-4aa4-b683-db27a40dd5a9": {"__data__": {"id_": "40c0cba8-4942-4aa4-b683-db27a40dd5a9", "embedding": null, "metadata": {"page_label": "1084", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b9b9a2f5-a75e-492a-90dd-6617e54b0b80", "node_type": "4", "metadata": {"page_label": "1084", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4d9fd8526e0399352c0e2307687a80f18a8e7f7b30c940f9c4b13d2985e450d9", "class_name": "RelatedNodeInfo"}}, "text": "1084 Tools for Deep Learning\nstep()\nzero_grad()\nclass d2l.torch.SoftmaxRegression( num_outputs ,lr)\nBases: Classifier (page 1075)\nThe softmax regression model.\nDefined in Section 4.5\nforward( X)\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nclass d2l.torch.SyntheticRegressionData( w,b,noise=0.01 ,num_train=1000 ,\nnum_val=1000 ,batch_size=32 )\nBases: DataModule (page 1075)\nSynthetic data for linear regression.\nDefined in Section 3.3\nget_dataloader( train )\nDefined in Section 3.3\nclass d2l.torch.TimeMachine( batch_size ,num_steps ,num_train=10000 ,\nnum_val=5000 )\nBases: DataModule (page 1075)\nThe Time Machine dataset.\nDefined in Section 9.2\nbuild(raw_text ,vocab=None )\nDefined in Section 9.2\nget_dataloader( train )\nDefined in Section 9.3.3\nclass d2l.torch.Trainer( max_epochs ,num_gpus=0 ,gradient_clip_val=0 )\nBases: HyperParameters (page 1077)\nThe base class for training models with data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1187, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e897a7b9-9bcc-4806-b322-a71bde74d230": {"__data__": {"id_": "e897a7b9-9bcc-4806-b322-a71bde74d230", "embedding": null, "metadata": {"page_label": "1085", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a51a6fa6-02c9-4e5d-859f-1021e93cabbe", "node_type": "4", "metadata": {"page_label": "1085", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8b50045fba70743300f1d53f8fb91e17bfb78b79ca4d13c9cd27ac85a9fc3286", "class_name": "RelatedNodeInfo"}}, "text": "1085 Thed2lAPI Document\nDefined in Section 3.2.2\nclip_gradients( grad_clip_val ,model )\nDefined in Section 9.5\nfit(model,data)\nfit_epoch()\nDefined in Section 3.4\nprepare_batch( batch )\nDefined in Section 6.7\nprepare_data( data)\nprepare_model( model )\nDefined in Section 6.7\nclass d2l.torch.TransformerEncoder( vocab_size ,num_hiddens ,ffn_num_hiddens ,\nnum_heads ,num_blks ,dropout,\nuse_bias=False )\nBases: Encoder (page 1076)\nThe Transformer encoder.\nDefined in Section 11.7.4\nforward( X,valid_lens )\nDefines the computation performed at every call.\nShould be overridden by all subclasses.\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nclass d2l.torch.TransformerEncoderBlock( num_hiddens ,ffn_num_hiddens ,\nnum_heads ,dropout,use_bias=False )\nBases: Module\nThe Transformer encoder block.\nDefined in Section 11.7.2\nforward( X,valid_lens )\nDefines the computation performed at every call.\nShould be overridden by all subclasses.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1127, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8020bdcd-949e-479c-a4cb-4feb516a0d9f": {"__data__": {"id_": "8020bdcd-949e-479c-a4cb-4feb516a0d9f", "embedding": null, "metadata": {"page_label": "1086", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f62047b8-29c1-4507-aff8-886f2f84387b", "node_type": "4", "metadata": {"page_label": "1086", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "424e3fe94a2906db67de6b6464d1440c271cd2f29fa9072c1691db4cdc82e966", "class_name": "RelatedNodeInfo"}}, "text": "1086 Tools for Deep Learning\nNote:Although the recipe for forward pass needs to be defined within this function,\none should call the Module(page 1078) instance afterwards instead of this since the\nformertakescareofrunningtheregisteredhookswhilethelattersilentlyignoresthem.\nclass d2l.torch.Vocab( tokens=[] ,min_freq=0 ,reserved_tokens=[] )\nBases: object\nVocabulary for text.\nto_tokens( indices )\nproperty unk\nB.8.2Functions\nd2l.torch.add_to_class( Class )\nRegister functions as methods in created class.\nDefined in Section 3.2\nd2l.torch.bleu( pred_seq ,label_seq ,k)\nCompute the BLEU.\nDefined in Section 10.7.6\nd2l.torch.check_len( a,n)\nCheck the length of a list.\nDefined in Section 9.5\nd2l.torch.check_shape( a,shape )\nCheck the shape of a tensor.\nDefined in Section 9.5\nd2l.torch.corr2d( X,K)\nCompute 2D cross-correlation.\nDefined in Section 7.2\nd2l.torch.cpu()\nGet the CPU device.\nDefined in Section 6.7\nd2l.torch.gpu( i=0)\nGet a GPU device.\nDefined in Section 6.7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f00dc602-1d44-45d0-938d-f91f95dc3f09": {"__data__": {"id_": "f00dc602-1d44-45d0-938d-f91f95dc3f09", "embedding": null, "metadata": {"page_label": "1087", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c9250d3-ade2-46d3-b651-687b499fad8f", "node_type": "4", "metadata": {"page_label": "1087", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "3aace01c6714f6c702ff8bb0f5cdd46d16e2409dfb39ac55842e3ff620dfe8f5", "class_name": "RelatedNodeInfo"}}, "text": "1087 Thed2lAPI Document\nd2l.torch.init_cnn( module )\nInitialize weights for CNNs.\nDefined in Section 7.6\nd2l.torch.init_seq2seq( module )\nInitialize weights for sequence-to-sequence learning.\nDefined in Section 10.7\nd2l.torch.masked_softmax( X,valid_lens )\nPerform softmax operation by masking elements on the last axis.\nDefined in Section 11.3\nd2l.torch.num_gpus()\nGet the number of available GPUs.\nDefined in Section 6.7\nd2l.torch.plot( X,Y=None,xlabel=None ,ylabel=None ,legend=[] ,xlim=None ,\nylim=None ,xscale=\u2019linear\u2019 ,yscale=\u2019linear\u2019 ,fmts=(\u2019-\u2019,\u2019m--\u2019,\u2019g-.\u2019,\n\u2019r:\u2019),figsize=(3.5,2.5) ,axes=None )\nPlot data points.\nDefined in Section 2.4\nd2l.torch.set_axes( axes,xlabel,ylabel,xlim,ylim,xscale,yscale,legend )\nSet the axes for matplotlib.\nDefined in Section 2.4\nd2l.torch.set_figsize( figsize=(3.5,2.5) )\nSet the figure size for matplotlib.\nDefined in Section 2.4\nd2l.torch.show_heatmaps( matrices ,xlabel,ylabel,titles=None ,figsize=(2.5, 2.5) ,\ncmap=\u2019Reds\u2019 )\nShow heatmaps of matrices.\nDefined in Section 11.1\nd2l.torch.show_list_len_pair_hist( legend,xlabel,ylabel,xlist,ylist)\nPlot the histogram for list length pairs.\nDefined in Section 10.5\nd2l.torch.try_all_gpus()\nReturn all available GPUs, or [cpu(),] if no GPU exists.\nDefined in Section 6.7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1256, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b856c401-a290-43ab-bf1f-1509000bb591": {"__data__": {"id_": "b856c401-a290-43ab-bf1f-1509000bb591", "embedding": null, "metadata": {"page_label": "1088", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "613e48f1-6a4e-45a9-bd81-6a4cf41dbcb7", "node_type": "4", "metadata": {"page_label": "1088", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "21d1cfae5e660bc56fc759bb48918a6282f069bb06d693910b8d4b3382c05c32", "class_name": "RelatedNodeInfo"}}, "text": "1088 Tools for Deep Learning\nd2l.torch.try_gpu( i=0)\nReturn gpu(i) if exists, otherwise return cpu().\nDefined in Section 6.7\nd2l.torch.use_svg_display()\nUse the svg format to display a plot in Jupyter.\nDefined in Section 2.4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3539a9a9-7781-4e12-a8a8-d4941c031bef": {"__data__": {"id_": "3539a9a9-7781-4e12-a8a8-d4941c031bef", "embedding": null, "metadata": {"page_label": "1089", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d10b4bba-644b-4f1e-8905-26ff299c52c4", "node_type": "4", "metadata": {"page_label": "1089", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "22a45228bb85255af912584389f369868288b642913d0ed6dadf339f860d2426", "class_name": "RelatedNodeInfo"}}, "text": "References\nAbadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., \u2026 et al. (2016). Tensor-\nFlow: asystemforlarge-scalemachinelearning. 12thUSENIXSymposiumonOperating\nSystemsDesign and Implementation (OSDI 16) (pp. 265\u2013283).\nAbdel-Hamid, O., Mohamed, A.-R., Jiang, H., Deng, L., Penn, G., & Yu, D. (2014). Con-\nvolutional neural networks for speech recognition. IEEE/ACM Transactions on Audio,\nSpeech,and LanguageProcessing ,22(10), 1533\u20131545.\nAhmed, A., Aly, M., Gonzalez, J., Narayanamurthy, S., & Smola, A. J. (2012). Scalable\ninference in latent variable models. ProceedingsoftheFifthACMInternationalConfer-\nenceon WebSearchand Data Mining (pp. 123\u2013132).\nAkiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: a next-generation\nhyperparameter optimization framework. Proceedings of the 25th ACM SIGKDD Inter-\nnationalConferenceon KnowledgeDiscovery & Data Mining .\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., \u2026 et al. (2022).\nFlamingo: a visual language model for few-shot learning. ArXiv:2204.14198 .\nAlsallakh, B., Kokhlikyan, N., Miglani, V., Yuan, J., & Reblitz-Richardson, O. (2020).\nMind the PAD \u2013 CNNs can develop blind spots. ArXiv:2010.02178 .\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., \u2026 et al. (2023).\nPaLM 2 Technical Report. ArXiv:2305.10403 .\nAnil, R., Gupta, V., Koren, T., Regan, K., & Singer, Y. (2020). Scalable second-order\noptimization for deep learning. ArXiv:2002.09018 .\nAronszajn, N. (1950). Theory of reproducing kernels. TransactionsoftheAmericanMath-\nematicalSociety ,68(3), 337\u2013404.\nBa, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. ArXiv:1607.06450 .\nBaevski, A., & Auli, M. (2018). Adaptive input representations for neural language mod-\neling.International Conferenceon Learning Representations .\nBahdanau,D.,Cho,K.,&Bengio,Y.(2014).Neuralmachinetranslationbyjointlylearning\nto align and translate. ArXiv:1409.0473 .\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., \u2026 et al. (2022). Con-\nstitutional AI: harmlessness from AI feedback. ArXiv:2212.08073 .\nBaptista, R., & Poloczek, M. (2018). Bayesian optimization of combinatorial structures.\nProceedingsof the35thInternational Conferenceon MachineLearning .\nBardenet, R., Brendel, M., K\u00e9gl, B., & Sebag, M. (2013). Collaborative hyperparam-\neter tuning. Proceedings of the 30th International Conference on Machine Learning\n(ICML\u201913) .\nBay, H., Tuytelaars, T., & Van Gool, L. (2006). SURF: Speeded up robust features. Euro-\npeanConferenceon ComputerVision (pp. 404\u2013417).\n1089", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1984ab8a-5c18-423e-9d40-a1a3d9793bee": {"__data__": {"id_": "1984ab8a-5c18-423e-9d40-a1a3d9793bee", "embedding": null, "metadata": {"page_label": "1090", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e4e1e00a-dc67-4746-9699-3add70bf247c", "node_type": "4", "metadata": {"page_label": "1090", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "6cc3b9f0beb7c0e25844d02fa710da40b0d3c99ade196a8a129afa0fff7575ed", "class_name": "RelatedNodeInfo"}}, "text": "1090 REFERENCES\nBellman, R. (1966). Dynamic programming. Science,153, 34\u201337.\nBellman, R. (1952). On the theory of dynamic programming. ProceedingsoftheNational\nAcademyof Sciences ,38(8), 716\u2013719.\nBellman, R. (1957). A Markovian decision process. JournalofMathematicsandMechan-\nics,6(5), 679\u2013684. URL: http://www.jstor.org/stable/24900506\nBellman, R. (1957). Dynamic Programming . Dover Publications.\nBeltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: the long-document trans-\nformer.ArXiv:2004.05150 .\nBengio,Y.,Ducharme,R.,Vincent,P.,&Jauvin,C.(2003).Aneuralprobabilisticlanguage\nmodel.Journal of MachineLearning Research ,3(Feb), 1137\u20131155.\nBengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies with gra-\ndient descent is difficult. IEEE Transactionson NeuralNetworks ,5(2), 157\u2013166.\nBergstra, J., Bardenet, R., Bengio, Y., & K\u00e9gl, B. (2011). Algorithms for hyper-parameter\noptimization. Advancesin NeuralInformation ProcessingSystems ,24.\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., \u2026 Ben-\ngio, Y. (2010). Theano: a CPU and GPU math compiler in Python. Proc. 9th Python in\nScienceConference (pp. 3\u201310).\nBeutel, A., Murray, K., Faloutsos, C., & Smola, A. J. (2014). CoBaFi: collaborative\nBayesianfiltering. Proceedingsofthe23rdInternationalConferenceonWorldWideWeb\n(pp. 97\u2013108).\nBishop, C.M.(1995).TrainingwithnoiseisequivalenttoTikhonovregularization. Neural\nComputation ,7(1), 108\u2013116.\nBishop, C. M. (2006). Pattern Recognitionand MachineLearning . Springer.\nBlack, F., & Scholes, M. (1973). The pricing of options and corporate liabilities. Journal\nofPoliticalEconomy ,81, 637\u2013654.\nBodla, N., Singh, B., Chellappa, R., & Davis, L. S. (2017). Soft-NMS-improving object\ndetection with one line of code. Proceedings of the IEEE International Conference on\nComputerVision (pp. 5561\u20135569).\nBojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with\nsubword information. Transactions of the Association for Computational Linguistics ,5,\n135\u2013146.\nBollob\u00e1s, B. (1999). Linear Analysis . Cambridge University Press.\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., \u2026 et al.\n(2021). On the opportunities and risks of foundation models. ArXiv:2108.07258 .\nBottou,L.(2010).Large-scalemachinelearningwithstochasticgradientdescent. Proceed-\ningsof COMPSTAT\u20192010 (pp. 177\u2013186). Springer.\nBottou, L., & Le Cun, Y. (1988). SN: a simulator for connectionist models. Proceedings\nof NeuroNimes 88 (pp. 371\u2013382). Nimes, France. URL: http://leon.bottou.org/papers/\nbottou-lecun-88\nBoucheron, S., Bousquet, O., & Lugosi, G. (2005). Theory of classification: a survey of\nsome recent advances. ESAIM: Probabilityand Statistics ,9, 323\u2013375.\nBowman, S. R., Angeli, G., Potts, C., & Manning, C. D. (2015). A large annotated corpus\nfor learning natural language inference. ArXiv:1508.05326 .\nBoyd, S., & Vandenberghe, L. (2004). Convex Optimization . Cambridge, England:\nCambridge University Press.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6545d33f-26c0-43f0-ad39-5b044e97d1c6": {"__data__": {"id_": "6545d33f-26c0-43f0-ad39-5b044e97d1c6", "embedding": null, "metadata": {"page_label": "1091", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68d3b583-4b4b-4cf9-9661-d40e639c7d84", "node_type": "4", "metadata": {"page_label": "1091", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dc55251eba7072f27054878f8e5acfb3cf8892cee4e37581758939e5848f5ccc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de1c3eb5-6100-4c2e-88b5-54d9553743d7", "node_type": "1", "metadata": {}, "hash": "44d6964ada4322e72c9cfa2ca497a64e92f50cf64424b69badf581f4ef22c950", "class_name": "RelatedNodeInfo"}}, "text": "1091 REFERENCES\nBradley, R. A., & Terry, M. E. (1952). Rank analysis of incomplete block designs: I. The\nmethod of paired comparisons. Biometrika ,39(3/4), 324\u2013345.\nBrown, N., & Sandholm, T. (2017). Libratus: the superhuman AI for no-limit poker. IJCAI\n(pp. 5226\u20135228).\nBrown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Lafferty, J.,\n\u2026 Roossin, P. S. (1990). A statistical approach to machine translation. Computational\nLinguistics ,16(2), 79\u201385.\nBrown, P. F., Cocke, J., Della Pietra, S. A., Della Pietra, V. J., Jelinek, F., Mercer, R. L.,\n& Roossin, P. (1988). A statistical approach to language translation. COLING Budapest\n1988Volume1: International Conferenceon Computational Linguistics .\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., \u2026 et al. (2020).\nLanguagemodelsarefew-shotlearners. AdvancesinNeuralInformationProcessingSys-\ntems,33, 1877\u20131901.\nBuslaev, A., Iglovikov, V. I., Khvedchenya, E., Parinov, A., Druzhinin, M., & Kalinin,\nA. A. (2020). Albumentations: Fast and flexible image augmentations. Information ,\n11(2), 125.\nCampbell, M., Hoane Jr, A. J., & Hsu, F.-h. (2002). Deep blue. Artificial Intelligence ,\n134(1-2), 57\u201383.\nCanny, J. (1987). A computational approach to edge detection. Readings in Computer Vi-\nsion(pp. 184\u2013203). Elsevier.\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., & Specia, L. (2017). SemEval-2017 Task\n1: semantictextualsimilaritymultilingualandcrosslingualfocusedevaluation. Proceed-\ningsofthe11thInternationalWorkshoponSemanticEvaluation(SemEval-2017) (pp.1\u2013\n14).\nChan, W., Jaitly, N., Le, Q. V., & Vinyals, O. (2015). Listen, attend and spell.\nArXiv:1508.01211 .\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., \u2026 Mordatch, I. (2021).\nDecision transformer: reinforcement learning via sequence modeling. AdvancesinNeu-\nralInformation ProcessingSystems ,34, 15084\u201315097.\nChen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., \u2026 Zhang, Z. (2015). MXNET:\na flexible and efficient machine learning library for heterogeneous distributed systems.\nArXiv:1512.01274 .\nCheng, J., Dong, L., &Lapata, M.(2016).Longshort-termmemory-networksformachine\nreading.Proceedingsofthe2016ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing (pp. 551\u2013561).\nChetlur,S.,Woolley,C.,Vandermersch,P.,Cohen,J.,Tran,J.,Catanzaro,B.,&Shelhamer,\nE. (2014). CuDNN: Efficient primitives for deep learning. ArXiv:1410.0759 .\nCho, K., Van Merri\u00ebnboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of\nneural machine translation: Encoder\u2013decoder approaches. ArXiv:1409.1259 .\nCho, K., Van Merri\u00ebnboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H.,\n& Bengio, Y. (2014). Learning phrase representations using RNN encoder\u2013decoder for\nstatistical machine translation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2779, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de1c3eb5-6100-4c2e-88b5-54d9553743d7": {"__data__": {"id_": "de1c3eb5-6100-4c2e-88b5-54d9553743d7", "embedding": null, "metadata": {"page_label": "1091", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68d3b583-4b4b-4cf9-9661-d40e639c7d84", "node_type": "4", "metadata": {"page_label": "1091", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "dc55251eba7072f27054878f8e5acfb3cf8892cee4e37581758939e5848f5ccc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6545d33f-26c0-43f0-ad39-5b044e97d1c6", "node_type": "1", "metadata": {"page_label": "1091", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "fe8799f3b7e4832a4852b198db1f905db424a68044115a55fbdc348d093eda8c", "class_name": "RelatedNodeInfo"}}, "text": "551\u2013561).\nChetlur,S.,Woolley,C.,Vandermersch,P.,Cohen,J.,Tran,J.,Catanzaro,B.,&Shelhamer,\nE. (2014). CuDNN: Efficient primitives for deep learning. ArXiv:1410.0759 .\nCho, K., Van Merri\u00ebnboer, B., Bahdanau, D., & Bengio, Y. (2014). On the properties of\nneural machine translation: Encoder\u2013decoder approaches. ArXiv:1409.1259 .\nCho, K., Van Merri\u00ebnboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H.,\n& Bengio, Y. (2014). Learning phrase representations using RNN encoder\u2013decoder for\nstatistical machine translation. ArXiv:1406.1078 .\nChowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,Mishra,G.,Roberts,A.,\u2026etal.(2022).\nPaLM: scaling language modeling with pathways. ArXiv:2204.02311 .", "mimetype": "text/plain", "start_char_idx": 2253, "end_char_idx": 2940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "889a3660-0620-4518-a83c-b8f692658d97": {"__data__": {"id_": "889a3660-0620-4518-a83c-b8f692658d97", "embedding": null, "metadata": {"page_label": "1092", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fefecc41-9720-411d-9e18-8b2087abd80c", "node_type": "4", "metadata": {"page_label": "1092", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "452f8708d463e462cf2e6389c6847aa33d132ba52ee925a23980ec5906d1a33e", "class_name": "RelatedNodeInfo"}}, "text": "1092 REFERENCES\nChung, J., Gulcehre, C., Cho, K., & Bengio, Y. (2014). Empirical evaluation of gated re-\ncurrent neural networks on sequence modeling. ArXiv:1412.3555 .\nClark,K.,Luong,M.-T.,Le, Q.V.,&Manning,C.D.(2020).ELECTRA:pre-trainingtext\nencodersasdiscriminatorsratherthangenerators. InternationalConferenceonLearning\nRepresentations .\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. (2011).\nNatural language processing (almost) from scratch. Journal of Machine Learning Re-\nsearch,12, 2493\u20132537.\nCordonnier, J.-B., Loukas, A., & Jaggi, M. (2020). On the relationship between self-\nattention and convolutional layers. International Conference on Learning Representa-\ntions.\nCover, T., & Thomas, J. (1999). Elementsof Information Theory . John Wiley & Sons.\nCsisz\u00e1r, I. (2008). Axiomatic characterizations of information measures. Entropy,10(3),\n261\u2013273.\nCybenko,G.(1989).Approximationbysuperpositionsofasigmoidalfunction. Mathemat-\nicsof Control,Signals and Systems ,2(4), 303\u2013314.\nDalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection.\n2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR\u201905) (pp. 886\u2013893).\nDe Cock, D. (2011). Ames, Iowa: alternative to the Boston housing data as an end of\nsemester regression project. Journal of StatisticsEducation ,19(3).\nDean,J.,Corrado,G.S.,Monga,R.,Chen,K.,Devin,M.,Le,Q.V.,\u2026etal.(2012).Large\nscale distributed deep networks. Proceedings of the 25th International Conference on\nNeuralInformation ProcessingSystems,Volume1 (pp. 1223\u20131231).\nDeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A., \u2026\nVogels, W. (2007). Dynamo: Amazon\u2019s highly available key-value store. ACM SIGOPS\nOperatingSystemsReview (pp. 205\u2013220).\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-Fei, L. (2009). Imagenet: a large-\nscale hierarchical image database. 2009IEEEConferenceonComputerVisionandPat-\ntern Recognition (pp. 248\u2013255).\nDer Kiureghian, A., & Ditlevsen, O. (2009). Aleatory or epistemic? does it matter? Struc-\nturalSafety ,31(2), 105\u2013112.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep\nbidirectional transformers for language understanding. ArXiv:1810.04805 .\nDinh, L., Krueger, D., & Bengio, Y. (2014). NICE: non-linear independent components\nestimation. ArXiv:1410.8516 .\nDinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). Density estimation using real NVP. In-\nternational Conferenceon Learning Representations .\nDoersch,C.,Gupta,A.,&Efros,A.A.(2015).Unsupervisedvisualrepresentationlearning\nby context prediction. Proceedings of the IEEE International Conference on Computer\nVision(pp. 1422\u20131430).\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., \u2026\net al. (2021). An image is worth 16 x 16 words: transformers for image recognition at\nscale.International Conferenceon Learning Representations .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77f0f40c-d5f4-4334-ad1c-4c269c4a5382": {"__data__": {"id_": "77f0f40c-d5f4-4334-ad1c-4c269c4a5382", "embedding": null, "metadata": {"page_label": "1093", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e2552ba-c063-48e5-9d9d-03148eff3553", "node_type": "4", "metadata": {"page_label": "1093", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "5d58554dc590b29c36a7eab63ddf3742dfa00d5ef1734621060e9872a3e0444f", "class_name": "RelatedNodeInfo"}}, "text": "1093 REFERENCES\nDuchi,J.,Hazan,E.,&Singer,Y.(2011).Adaptivesubgradientmethodsforonlinelearning\nand stochastic optimization. Journal of MachineLearning Research ,12, 2121\u20132159.\nDumoulin, V., & Visin, F. (2016). A guide to convolution arithmetic for deep learning.\nArXiv:1603.07285 .\nDwivedi, V. P., & Bresson, X. (2020). A generalization of transformer networks to graphs.\nArXiv:2012.09699 .\nDwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., & Roth, A. L. (2015). Pre-\nservingstatisticalvalidityinadaptivedataanalysis. Proceedingsofthe47thAnnualACM\nSymposiumon Theory of Computing (pp. 117\u2013126).\nElman, J. L. (1990). Finding structure in time. CognitiveScience ,14(2), 179\u2013211.\nElsken, T., Metzen, J. H., & Hutter, F. (2018). Neural architecture search: a ssurvey.\nArXiv:1808.05377[stat.ML] .\nFechner, G. T. (1860). Elementeder Psychophysik . Vol. 2. Breitkopf u. H\u00e4rtel.\nFedus, W., Zoph, B., & Shazeer, N. (2022). Switch transformers: scaling to trillion param-\neter models with simple and efficient sparsity. Journal of Machine Learning Research ,\n23(120), 1\u201339.\nFernando,R.(2004). GPUGems: ProgrammingTechniques,Tips,andTricksforReal-Time\nGraphics . Addison-Wesley.\nFeurer, M., & Hutter, F. (2018). Hyperparameter ptimization. Automatic Machine Learn-\ning: Methods,Systems,Challenges . Springer.\nFeurer, M., Letham, B., Hutter, F., & Bakshy, E. (2022). Practical transfer learning for\nBayesian optimization. ArXiv:1802.02219[stat.ML] .\nField, D. J. (1987). Relations between the statistics of natural images and the response\nproperties of cortical cells. JOSA A,4(12), 2379\u20132394.\nFisher, R. A. (1925). Statistical MethodsforResearchWorkers. Oliver & Boyd.\nFlammarion, N., & Bach, F. (2015). From averaging to acceleration, there is only a step-\nsize.Conferenceon Learning Theory (pp. 658\u2013695).\nForrester, A. I., S\u00f3bester, A., & Keane, A. J. (2007). Multi-fidelity optimization via surro-\ngate modelling. Proceedings of the Royal Society A: Mathematical, Physical and Engi-\nneeringSciences ,463(2088), 3251\u20133269.\nFranceschi,L.,Donini,M.,Frasconi,P.,&Pontil,M.(2017).Forwardandreversegradient-\nbased hyperparameter optimization. Proceedings of the 34th International Conference\nonMachineLearning (ICML\u201917) .\nFrankle, J., & Carbin, M. (2018). The lottery ticket hypothesis: finding sparse, trainable\nneural networks. ArXiv:1803.03635 .\nFrazier, P. I. (2018). A tutorial on Bayesian optimization. ArXiv:1807.02811 .\nFreund, Y., & Schapire, R. E. (1996). Experiments with a new boosting algorithm. Pro-\nceedingsof theInternational Conferenceon MachineLearning (pp. 148\u2013156).\nFriedman, J.H.(1987).Exploratoryprojectionpursuit. JournaloftheAmericanStatistical\nAssociation ,82(397), 249\u2013266.\nFrostig, R., Johnson, M. J., & Leary, C. (2018). Compiling machine learning programs via\nhigh-level tracing. Proceedingsof SystemsforMachineLearning .\nFukushima, K. (1982). Neocognitron: a self-organizing neural network model for a\nmechanism of visual pattern recognition. Competition and Cooperation in Neural Nets\n(pp. 267\u2013285). Springer.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3038, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df25089f-6287-4466-9eee-01a4ac93b621": {"__data__": {"id_": "df25089f-6287-4466-9eee-01a4ac93b621", "embedding": null, "metadata": {"page_label": "1094", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3b4af78f-dfbb-4a17-ba98-4c9db7e74835", "node_type": "4", "metadata": {"page_label": "1094", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a910154b4623e0968274e33ae83675a4c7322aecbc9cfe9524733454263b9792", "class_name": "RelatedNodeInfo"}}, "text": "1094 REFERENCES\nGardner, J., Pleiss, G., Weinberger, K. Q., Bindel, D., & Wilson, A. G. (2018). GPyTorch:\nblackbox matrix\u2013matrix Gaussian process inference with GPU acceleration. Advances\ninNeuralInformation ProcessingSystems .\nGarg, S., Balakrishnan, S., Kolter, Z., & Lipton, Z. (2021). RATT: leveraging unla-\nbeled data to guarantee generalization. International Conference on Machine Learning\n(pp. 3598\u20133609).\nGatys, L. A., Ecker, A. S., & Bethge, M. (2016). Image style transfer using convolutional\nneural networks. Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (pp. 2414\u20132423).\nGauss, C. F. (1809). Theoria motus corporum coelestum. Werke. K\u00f6niglich Preussische\nAkademie der Wissenschaften.\nGibbs, J. W. (1902). Elementary Principles of StatisticalMhanics . Scribner\u2019s.\nGinibre,J.(1965).Statisticalensemblesofcomplex,quaternion,andrealmatrices. Journal\nofMathematicalPhysics ,6(3), 440\u2013449.\nGirshick, R. (2015). Fast R-CNN. Proceedings of the IEEE International Conference on\nComputerVision (pp. 1440\u20131448).\nGirshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature hierarchies for ac-\ncurateobjectdetectionandsemanticsegmentation. ProceedingsoftheIEEEConference\nonComputerVisionand Pattern Recognition (pp. 580\u2013587).\nGlorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward\nneural networks. Proceedings of the 13th International Conference on Artificial Intelli-\ngenceand Statistics (pp. 249\u2013256).\nGoh, G. (2017). Why momentum really works. Distill. URL: http://distill.pub/2017/\nmomentum\nGoldberg, D., Nichols, D., Oki, B. M., & Terry, D. (1992). Using collaborative filtering to\nweave an information tapestry. Communicationsof theACM ,35(12), 61\u201371.\nGolub, G. H., & Van Loan, C. F. (1996). Matrix Computations . Johns Hopkins University\nPress.\nGoodfellow,I.,Bengio,Y.,&Courville,A.(2016). DeepLearning .MITPress. http://www.\ndeeplearningbook.org .\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., \u2026 Ben-\ngio, Y. (2014). Generative adversarial nets. Advances in Neural Information Processing\nSystems (pp. 2672\u20132680).\nGotmare, A., Keskar, N. S., Xiong, C., & Socher, R. (2018). A closer look at deep learning\nheuristics: learning rate restarts, warmup and distillation. ArXiv:1810.13243 .\nGoyal, A., Bochkovskiy, A., Deng, J., & Koltun, V. (2021). Non-deep networks.\nArXiv:2110.07641 .\nGraham, B. (2014). Fractional max-pooling. ArXiv:1412.6071 .\nGraves,A.(2013).Generatingsequenceswithrecurrentneuralnetworks. ArXiv:1308.0850 .\nGraves, A., Liwicki, M., Fern\u00e1ndez, S., Bertolami, R., Bunke, H., & Schmidhuber, J.\n(2008). A novel connectionist system for unconstrained handwriting recognition. IEEE\nTransactionson Pattern Analysisand MachineIntelligence ,31(5), 855\u2013868.\nGraves,A.,&Schmidhuber,J.(2005).Framewisephonemeclassificationwithbidirectional\nLSTM and other neural network architectures. Neural Networks ,18(5-6), 602\u2013610.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2951, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d7bca40-c464-4262-a036-8b5b0fe2bfa6": {"__data__": {"id_": "4d7bca40-c464-4262-a036-8b5b0fe2bfa6", "embedding": null, "metadata": {"page_label": "1095", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5b66a0c3-0d2b-4040-b753-4ca068f46d45", "node_type": "4", "metadata": {"page_label": "1095", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "60ecf0bd845b695d04ca75db3e22ddca88d26b4575b207765d464da7afb05534", "class_name": "RelatedNodeInfo"}}, "text": "1095 REFERENCES\nGriewank, A. (1989). On automatic differentiation. Mathematical Programming: Recent\nDevelopmentsand Applications (pp. 83\u2013107). Kluwer.\nGulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., \u2026 et al. (2020). Con-\nformer: convolution-augmented transformer for speech recognition. Proc. Interspeech\n2020, pp. 5036\u20135040.\nGuyon, I., Gunn, S., Nikravesh, M., & Zadeh, L. A. (2008). Feature Extraction: Founda-\ntionsand Applications . Springer.\nHadjis, S., Zhang, C., Mitliagkas, I., Iter, D., & R\u00e9, C. (2016). Omnivore: an optimizer for\nmulti-device deep learning on CPUs and GPUs. ArXiv:1606.04487 .\nHartley, R., & Zisserman, A. (2000). Multiple View Geometry in Computer Vision .\nCambridge University Press.\nHartley, R. I., & Kahl, F. (2009). Global optimization through rotation space search. Inter-\nnationalJournal of ComputerVision ,82(1), 64\u201379.\nHe,K.,Chen,X.,Xie,S.,Li,Y.,Doll\u00e1r,P.,&Girshick,R.(2022).Maskedautoencodersare\nscalable vision learners. ProceedingsoftheIEEE/CVFConferenceonComputerVision\nandPattern Recognition (pp. 16000\u201316009).\nHe, K., Gkioxari, G., Doll\u00e1r, P., & Girshick, R. (2017). Mask R-CNN. Proceedings of the\nIEEEInternational Conferenceon ComputerVision (pp. 2961\u20132969).\nHe, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: surpassing\nhuman-level performance on ImageNet classification. ProceedingsoftheIEEEInterna-\ntionalConferenceon ComputerVision (pp. 1026\u20131034).\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recogni-\ntion.ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition\n(pp. 770\u2013778).\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Identity mappings in deep residual networks.\nEuropeanConferenceon ComputerVision (pp. 630\u2013645).\nHebb, D. O. (1949). The Organizationof Behavior . Wiley.\nHendrycks, D., & Gimpel, K. (2016). Gaussian error linear units (GELUs).\nArXiv:1606.08415 .\nHennessy, J. L., & Patterson, D. A. (2011). Computer Architecture: A Quantitative Ap-\nproach. Elsevier.\nHo, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances\ninNeuralInformation ProcessingSystems ,33, 6840\u20136851.\nHochreiter, S., Bengio, Y., Frasconi, P., & Schmidhuber, J. (2001). Gradient flow in recur-\nrentnets: thedifficultyoflearninglong-termdependencies. AFieldGuidetoDynamical\nRecurrentNeuralNetworks . IEEE Press.\nHochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. NeuralComputation ,\n9(8), 1735\u20131780.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., \u2026 et al.\n(2022). Training compute-optimal large language models. ArXiv:2203.15556 .\nHoward, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., \u2026 Adam, H. (2019).\nSearchingforMobileNetV3. ProceedingsoftheIEEE/CVFInternationalConferenceon\nComputerVision (pp. 1314\u20131324).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ce74b24f-9b5c-4a0f-8800-df3ee01374fa": {"__data__": {"id_": "ce74b24f-9b5c-4a0f-8800-df3ee01374fa", "embedding": null, "metadata": {"page_label": "1096", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b955caf2-228a-4001-ace2-365da4d78617", "node_type": "4", "metadata": {"page_label": "1096", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "e89d25ad03ef86e5cfd27e1046d9dec6fcf0439cc69d1ab71157f4f051cfd963", "class_name": "RelatedNodeInfo"}}, "text": "1096 REFERENCES\nHoyer,P.O.,Janzing,D.,Mooij,J.M.,Peters,J.,&Sch\u00f6lkopf,B.(2009).Nonlinearcausal\ndiscovery with additive noise models. Advances in Neural Information Processing Sys-\ntems(pp. 689\u2013696).\nHu, J., Shen, L., & Sun, G. (2018). Squeeze-and-excitation networks. Proceedings of the\nIEEEConferenceon ComputerVisionand Pattern Recognition (pp. 7132\u20137141).\nHu, Y., Koren, Y., & Volinsky, C. (2008). Collaborative filtering for implicit feedback\ndatasets.20088thIEEE International Conferenceon Data Mining (pp. 263\u2013272).\nHu, Z., Lee, R. K.-W., Aggarwal, C. C., & Zhang, A. (2022). Text style transfer: a review\nand experimental evaluation. SIGKDD Explor. Newsl. ,24(1). URL: https://doi.org/10.\n1145/3544903.3544906\nHuang,C.-Z.A.,Vaswani,A.,Uszkoreit,J.,Simon,I.,Hawthorne,C.,Shazeer,N.,\u2026Eck,\nD. (2018). Music transformer: generating music with long-term structure. International\nConferenceon Learning Representations .\nHuang, G., Liu, Z., Van Der Maaten, L., & Weinberger, K. Q. (2017). Densely connected\nconvolutional networks. Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (pp. 4700\u20134708).\nHuang, Z., Xu, W., & Yu, K. (2015). Bidirectional LSTM\u2013CRF models for sequence tag-\nging.ArXiv:1508.01991 .\nHubel,D.H.,&Wiesel,T.N.(1959).Receptivefieldsofsingleneuronesinthecat\u2019sstriate\ncortex.Journal of Physiology ,148(3), 574\u2013591.\nHubel,D.H.,&Wiesel,T.N.(1962).Receptivefields,binocularinteractionandfunctional\narchitecture in the cat\u2019s visual cortex. Journal of Physiology ,160(1), 106\u2013154.\nHubel, D. H., & Wiesel, T. N. (1968). Receptive fields and functional architecture of mon-\nkey striate cortex. Journal of Physiology ,195(1), 215\u2013243.\nHutter,F.,Hoos,H.,&Leyton-Brown,K.(2011).Sequentialmodel-basedoptimizationfor\ngeneral algorithm configuration. Proceedings of the Fifth International Conference on\nLearning and IntelligentOptimization(LION\u201911) .\nHutter, F., Kotthoff, L., & Vanschoren, J. (Eds.) (2019). Automated Machine Learning:\nMethods,Systems,Challenges . Springer.\nIoffe, S. (2017). Batch renormalization: towards reducing minibatch dependence in batch-\nnormalized models. Advances in Neural Information Processing Systems (pp. 1945\u2013\n1953).\nIoffe, S., & Szegedy, C. (2015). Batch normalization: accelerating deep network training\nby reducing internal covariate shift. ArXiv:1502.03167 .\nIzmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., & Wilson, A. G. (2018). Averaging\nweights leads to wider optima and better generalization. ArXiv:1803.05407 .\nJacot, A., Gabriel, F., & Hongler, C. (2018). Neural tangent kernel: convergence and gen-\neralization in neural networks. Advancesin NeuralInformation ProcessingSystems .\nJaeger, H. (2002). Tutorial on training recurrent neural networks, covering BPPT, RTRL,\nEKF and the \u201cecho state network\u201d approach . GMD-Forschungszentrum Information-\nstechnik Bonn.\nJamieson, K., & Talwalkar, A. (2016). Non-stochastic best arm identification and hyper-\nparameter optimization. Proceedings of the 17th International Conference on Artificial\nIntelligenceand Statistics .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3053, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd33374a-a801-4cdb-9718-bca9893063ad": {"__data__": {"id_": "fd33374a-a801-4cdb-9718-bca9893063ad", "embedding": null, "metadata": {"page_label": "1097", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "063e8cf1-f44a-4f0a-a7d6-da2b1b7fdca8", "node_type": "4", "metadata": {"page_label": "1097", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b12ce2dcf7cad2508fa35ce79d5ca23f6196d8de951ef5e88282801b9acc88cf", "class_name": "RelatedNodeInfo"}}, "text": "1097 REFERENCES\nJenatton, R., Archambeau, C., Gonz\u00e1lez, J., & Seeger, M. (2017). Bayesian optimization\nwith tree-structured dependencies. Proceedingsofthe34thInternationalConferenceon\nMachineLearning (ICML\u201917) .\nJia, X., Song, S., He, W., Wang, Y., Rong, H., Zhou, F., \u2026 et al. (2018). Highly scalable\ndeep learning training system with mixed-precision: training ImageNet in four minutes.\nArXiv:1807.11205 .\nJia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., \u2026 Darrell, T.\n(2014). Caffe: convolutional architecture for fast feature embedding. Proceedingsofthe\n22ndACMInternational Conferenceon Multimedia (pp. 675\u2013678).\nJoshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, L., & Levy, O. (2020). SpanBERT:\nimproving pre-training by representing and predicting spans. Transactions of the Asso-\nciationforComputational Linguistics ,8, 64\u201377.\nJouppi, N. P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., \u2026 et al. (2017).\nIn-datacenter performance analysis of a tensor processing unit. 2017 ACM/IEEE 44th\nAnnualInternational Symposium on ComputerArchitecture(ISCA) (pp. 1\u201312).\nKalchbrenner, N., Grefenstette, E., & Blunsom, P. (2014). A convolutional neural network\nfor modelling sentences. ArXiv:1404.2188 .\nKalman, B. L., & Kwasny, S. C. (1992). Why tanh: choosing a sigmoidal function. Pro-\nceedings of the International Joint Conference on Neural Networks (IJCNN) (pp. 578\u2013\n581).\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., \u2026 Amodei,\nD. (2020). Scaling laws for neural language models. ArXiv:2001.08361 .\nKarnin, Z., Koren, T., & Somekh, O. (2013). Almost optimal exploration in multi-\narmed bandits. Proceedings of the 30th International Conference on Machine Learning\n(ICML\u201913) .\nKarras, T., Aila, T., Laine, S., & Lehtinen, J. (2017). Progressive growing of GANs for\nimproved quality, stability, and variation. ArXiv:1710.10196 .\nKim, J., El-Khamy, M., & Lee, J. (2017). Residual LSTM: design of a deep recurrent\narchitecture for distant speech recognition. ArXiv:1701.03360 .\nKim, Y. (2014). Convolutional neural networks for sentence classification.\nArXiv:1408.5882 .\nKimeldorf, G. S., & Wahba, G. (1971). Some results on Tchebycheffian spline functions.\nJ.Math.Anal. Appl. ,33, 82\u201395.\nKingma, D. P., & Ba, J. (2014). Adam: a method for stochastic optimization.\nArXiv:1412.6980 .\nKingma,D.P.,&Welling,M.(2014).Auto-encodingvariationalBayes. InternationalCon-\nferenceon Learning Representations(ICLR) .\nKipf,T.N.,&Welling,M.(2016).Semi-supervisedclassificationwithgraphconvolutional\nnetworks. ArXiv:1609.02907 .\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models\nare zero-shot reasoners. arxiv.org/abs/2205.11916 .\nKoller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Tech-\nniques. MIT Press.\nKolmogorov, A. (1933). Sulla determinazione empirica di una legge di distribuzione. Inst.\nItal.Attuari, Giorn. ,4, 83\u201391.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2960, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2578d28-073c-4da0-80ae-37f1e17077a4": {"__data__": {"id_": "d2578d28-073c-4da0-80ae-37f1e17077a4", "embedding": null, "metadata": {"page_label": "1098", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4aaf6edd-5198-4cd2-964a-7b0955556f5d", "node_type": "4", "metadata": {"page_label": "1098", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "336217e241d253b8cd4df3b26a40ec7fcc7a79d952be095e867d060418887d48", "class_name": "RelatedNodeInfo"}}, "text": "1098 REFERENCES\nKolter, Z. (2008). Linear algebra review and reference. Available online:\nhttp://cs229.stanford.edu/section/cs229-linalg.pdf .\nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classification with deep\nconvolutional neural networks. Advances in Neural Information Processing Systems\n(pp. 1097\u20131105).\nKung, S. Y. (1988). VLSI Array Processors. PrenticeHall .\nKuzovkin, I., Vicente, R., Petton, M., Lachaux, J.-P., Baciu, M., Kahane, P., \u2026 Aru, J.\n(2018).Activationsofdeepconvolutionalneuralnetworksarealignedwithgammaband\nactivity of human visual cortex. CommunicationsBiology ,1(1), 1\u201312.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). ALBERT:\naliteBERTforself-supervisedlearningoflanguagerepresentations. ArXiv:1909.11942 .\nLavin, A., & Gray, S. (2016). Fast algorithms for convolutional neural networks. Proceed-\nings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4013\u2013\n4021).\nLe,Q.V.(2013).Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning. Pro-\nceedingsoftheIEEEInternationalConferenceonAcoustics,SpeechandSignalProcess-\ning(pp. 8595\u20138598).\nLeCun, Y., Bengio, Y., & et al. (1995). Convolutional networks for images, speech, and\ntime series. TheHandbook of BrainTheory and NeuralNetworks (p. 3361). MIT Press.\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., & Jackel,\nL.D.(1989).Backpropagationappliedtohandwrittenzipcoderecognition. NeuralCom-\nputation ,1(4), 541\u2013551.\nLeCun, Y., Bottou, L., Orr, G., & Muller, K.-R. (1998). Efficient backprop. Neural Net-\nworks: Tricksof theTrade . Springer.\nLeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied\nto document recognition. Proceedingsof theIEEE ,86(11), 2278\u20132324.\nLeCun, Y., Jackel, L., Bottou, L., Brunot, A., Cortes, C., Denker, J., \u2026 et al. (1995). Com-\nparison of learning algorithms for handwritten digit recognition. International Confer-\nenceon Artificial NeuralNetworks (pp. 53\u201360).\nLegendre, A.M.(1805). M\u00e9moiresurlesOp\u00e9rationsTrigonom\u00e9triques: dontlesR\u00e9sultats\nD\u00e9pendentde la Figurede la Terre . F. Didot.\nLewis,M.,Liu,Y.,Goyal,N.,Ghazvininejad,M.,Mohamed,A.,Levy,O.,\u2026Zettlemoyer,\nL.(2019).BART:denoisingsequence-to-sequencepre-trainingfornaturallanguagegen-\neration, translation, and comprehension. ArXiv:1910.13461 .\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh,\nV., \u2026 et al. (2022). Solving quantitative reasoning problems with language models.\nArXiv:2206.14858 .\nLi, L., Jamieson, K., Rostamizadeh, A., Gonina, K., Hardt, M., Recht, B., & Talwalkar, A.\n(2018). Massively parallel hyperparameter tuning. ArXiv:1810.05934 .\nLi,M.(2017). ScalingDistributedMachineLearningwithSystemandAlgorithmCo-design\n(Doctoral dissertation). PhD Thesis, CMU.\nLi, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., \u2026 Su, B.-Y.\n(2014).Scalingdistributedmachinelearningwiththeparameterserver. 11thSymposium\nonOperatingSystemsDesign and Implementation (OSDI 14) (pp. 583\u2013598).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "606325b1-1772-4deb-95a6-6da8f9fd3438": {"__data__": {"id_": "606325b1-1772-4deb-95a6-6da8f9fd3438", "embedding": null, "metadata": {"page_label": "1099", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64608d2b-9958-42f8-b7d1-d31f57b2a7f6", "node_type": "4", "metadata": {"page_label": "1099", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "687e71a4841a81dd208882a0125d8bf50128e5ce3aa45b66d6d9eeab23144973", "class_name": "RelatedNodeInfo"}}, "text": "1099 REFERENCES\nLi, M., Zhang, T., Chen, Y., & Smola, A. J. (2014). Efficient mini-batch training for\nstochastic optimization. Proceedings of the 20th ACM SIGKDD International Confer-\nenceon KnowledgeDiscovery and Data Mining (pp. 661\u2013670).\nLiaw, R., Liang, E., Nishihara, R., Moritz, P., Gonzalez, J., & Stoica, I. (2018). Tune: a\nresearch platform for distributed model selection and training. ArXiv:1807.05118 .\nLin, M., Chen, Q., & Yan, S. (2013). Network in network. ArXiv:1312.4400 .\nLin, T.-Y., Goyal, P., Girshick, R., He, K., & Doll\u00e1r, P. (2017). Focal loss for dense ob-\nject detection. Proceedings of the IEEE International Conference on Computer Vision\n(pp. 2980\u20132988).\nLin, Y., Lv, F., Zhu, S., Yang, M., Cour, T., Yu, K., \u2026 others. (2010). ImageNet classifi-\ncation: fast descriptor coding and large-scale SVM training. Large Scale Visual Recog-\nnitionChallenge .\nLin, Z., Feng, M., Santos, C. N. d., Yu, M., Xiang, B., Zhou, B., & Bengio, Y. (2017). A\nstructured self-attentive sentence embedding. ArXiv:1703.03130 .\nLipton, Z. C., Berkowitz, J., & Elkan, C. (2015). A critical review of recurrent neural\nnetworks for sequence learning. ArXiv:1506.00019 .\nLipton, Z. C., Kale, D. C., Elkan, C., & Wetzel, R. (2016). Learning to diagnose with\nLSTM recurrent neural networks. International Conference on Learning Representa-\ntions(ICLR) .\nLipton, Z. C., & Steinhardt, J. (2018). Troubling trends in machine learning scholarship.\nCommunicationsof theACM ,17, 45\u201377.\nLiu, D. C., & Nocedal, J. (1989). On the limited memory BFGS method for large scale\noptimization. MathematicalProgramming ,45(1), 503\u2013528.\nLiu, H., Simonyan, K., & Yang, Y. (2018). DARTS: differentiable architecture search.\nArXiv:1806.09055 .\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., & Berg, A. C. (2016).\nSSD: single shot multibox detector. EuropeanConferenceonComputerVision (pp. 21\u2013\n37).\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., \u2026 Stoyanov, V. (2019). RoBERTa:\na robustly optimized BERT pretraining approach. ArXiv:1907.11692 .\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., \u2026 Guo, B. (2021). Swin transformer:\nhierarchical vision transformer using shifted windows. Proceedings of the IEEE/CVF\nInternational Conferenceon ComputerVision (pp. 10012\u201310022).\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., & Xie, S. (2022). A convNet\nfor the 2020s. ArXiv:2201.03545 .\nLong, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic\nsegmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (pp. 3431\u20133440).\nLoshchilov, I., & Hutter, F. (2016). SGDR: stochastic gradient descent with warm restarts.\nArXiv:1608.03983 .\nLowe, D. G. (2004). Distinctive image features from scale-invariant keypoints. Interna-\ntionalJournal of ComputerVision ,60(2), 91\u2013110.\nLuo, P., Wang, X., Shao, W., & Peng, Z. (2018). Towards understanding regularization in\nbatch normalization. ArXiv:1809.00846 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2982, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae9c5ebe-ccf6-40ef-b4bc-7b1d3979081d": {"__data__": {"id_": "ae9c5ebe-ccf6-40ef-b4bc-7b1d3979081d", "embedding": null, "metadata": {"page_label": "1100", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55645ccd-a764-4c82-8a5b-d5c1bdb4977f", "node_type": "4", "metadata": {"page_label": "1100", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b08505d4cffb9acf5a39f8cce903f04a9cb6c3b637722d63e84734f96a70369e", "class_name": "RelatedNodeInfo"}}, "text": "1100 REFERENCES\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011). Learn-\ning word vectors for sentiment analysis. Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1\n(pp. 142\u2013150).\nMack, Y.-P., & Silverman, B. W. (1982). Weak and strong uniform consistency of kernel\nregression estimates. Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und verwandte Gebiete ,\n61(3), 405\u2013415.\nMacKay,D.J.(2003). InformationTheory,InferenceandLearningAlgorithms .Cambridge\nUniversity Press.\nMaclaurin, D., Duvenaud, D., & Adams, R. (2015). Gradient-based hyperparameter opti-\nmization through reversible learning. Proceedingsofthe32ndInternationalConference\nonMachineLearning (ICML\u201915) .\nMangasarian, O. L. (1965). Linear and nonlinear separation of patterns by linear program-\nming.Oper.Res. ,13, 444-452.\nMangram,M.E.(2013).AsimplifiedperspectiveoftheMarkowitzportfoliotheory. Global\nJournal of Business Research ,7(1), 59\u201370.\nMatthews, A. G. d. G., Rowland, M., Hron, J., Turner, R. E., & Ghahramani, Z. (2018).\nGaussian process behaviour in wide deep neural networks. ArXiv:1804.11271 .\nMcCann,B.,Bradbury,J.,Xiong,C.,&Socher,R.(2017).Learnedintranslation: Contex-\ntualized word vectors. Advances in Neural Information Processing Systems (pp. 6294\u2013\n6305).\nMcCulloch, W. S., & Pitts, W. (1943). A logical calculus of the ideas immanent in nervous\nactivity.Bulletinof MathematicalBiophysics ,5(4), 115\u2013133.\nMead, C. (1980). Introduction to VLSI systems. IEE Proceedings I-Solid-State and Elec-\ntronDevices ,128(1), 18.\nMerity, S., Xiong, C., Bradbury, J., & Socher, R. (2016). Pointer sentinel mixture models.\nArXiv:1609.07843 .\nMicchelli,C.A.(1984).Interpolationofscattereddata: distancematricesandconditionally\npositive definite functions. Approximation Theory and Spline Functions (pp. 143\u2013145).\nSpringer.\nMikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word repre-\nsentations in vector space. ArXiv:1301.3781 .\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed rep-\nresentations of words and phrases and their compositionality. AdvancesinNeuralInfor-\nmationProcessingSystems (pp. 3111\u20133119).\nMiller,G.A.(1995).WordNet: alexicaldatabaseforEnglish. CommunicationsoftheACM ,\n38(11), 39\u201341.\nMirhoseini, A., Pham, H., Le, Q. V., Steiner, B., Larsen, R., Zhou, Y., \u2026 Dean, J. (2017).\nDevice placement optimization with reinforcement learning. Proceedings of the 34th\nInternational Conferenceon MachineLearning (pp. 2430\u20132439).\nMnih, V., Heess, N., Graves, A., & others. (2014). Recurrent models of visual attention.\nAdvancesin NeuralInformation ProcessingSystems (pp. 2204\u20132212).\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Ried-\nmiller, M. (2013). Playing Atari with deep reinforcement learning. ArXiv:1312.5602 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc24e162-302d-4645-a4e5-2c5a38efda88": {"__data__": {"id_": "bc24e162-302d-4645-a4e5-2c5a38efda88", "embedding": null, "metadata": {"page_label": "1101", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6c9c85b2-4210-453f-ae49-4804109196de", "node_type": "4", "metadata": {"page_label": "1101", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2e19fbdf93a8b6e2745164186d2e50cfad5241cd6675aea89f9e426aac417f18", "class_name": "RelatedNodeInfo"}}, "text": "1101 REFERENCES\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., \u2026 et al.\n(2015). Human-level control through deep reinforcement learning. Nature,518(7540),\n529\u2013533.\nMoon,T.,Smola,A.,Chang,Y.,&Zheng,Z.(2010).Intervalrank: isotonicregressionwith\nlistwise and pairwise constraints. Proceedingsofthe3rdACMInternationalConference\nonWebSearchand Data Mining (pp. 151\u2013160).\nMorey, R. D., Hoekstra, R., Rouder, J. N., Lee, M. D., & Wagenmakers, E.-J. (2016). The\nfallacy of placing confidence in confidence intervals. Psychonomic Bulletin & Review ,\n23(1), 103\u2013123.\nMorozov, V. A. (1984). MethodsforSolvingIncorrectlyPosedProblems . Springer.\nNadaraya,E.A.(1964).Onestimatingregression. TheoryofProbability&itsApplications ,\n9(1), 141\u2013142.\nNair, V., & Hinton, G. E. (2010). Rectified linear units improve restricted Boltzmann ma-\nchines.ICML.\nNakkiran,P.,Kaplun,G.,Bansal,Y.,Yang,T.,Barak,B.,&Sutskever,I.(2021).Deepdou-\nble descent: where bigger models and more data hurt. JournalofStatisticalMechanics:\nTheory and Experiment ,2021(12), 124003.\nNaor, M., & Reingold, O. (1999). On the construction of pseudorandom permutations:\nLuby\u2013Rackoff revisited. Journal of Cryptology ,12(1), 29\u201366.\nNeal, R. M. (1996). BayesianLearning forNeuralNetworks . Springer.\nNesterov, Y. (2018). Lectureson ConvexOptimization . Springer.\nNesterov, Y., & Vial, J.-P. (2000). Confidence level solutions for stochastic programming.\nAutomatica ,44(6), 1559\u20131568.\nNeyman, J. (1937). Outline of a theory of statistical estimation based on the classical the-\nory of probability. PhilosophicalTransactionsoftheRoyalSocietyofLondon.SeriesA,\nMathematicaland PhysicalSciences ,236(767), 333\u2013380.\nNorelli, A., Fumero, M., Maiorca, V., Moschella, L., Rodol\u00e0, E., & Locatello, F.\n(2022). ASIF: coupled data turns unimodal models to multimodal without training.\nArXiv:2210.01738 .\nNovak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., \u2026 Sohl-Dickstein, J. (2018).\nBayesian deep convolutional networks with many channels are Gaussian processes.\nArXiv:1810.05148 .\nNovikoff, A. B. J. (1962). On convergence proofs on perceptrons. ProceedingsoftheSym-\nposiumon theMathematicalTheory of Automata (pp. 615\u2013622).\nOlshausen,B.A.,&Field,D.J.(1996).Emergenceofsimple-cellreceptivefieldproperties\nby learning a sparse code for natural images. Nature,381(6583), 607\u2013609.\nOng, C. S., Smola, A., & Williamson, R. (2005). Learning the kernel with hyperkernels.\nJournal of MachineLearning Research ,6, 1043\u20131071.\nOpenAI. (2023). GPT-4 Technical Report. ArXiv:2303.08774 .\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., \u2026\net al. (2022). Training language models to follow instructions with human feedback.\nArXiv:2203.02155 .\nPapineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: a method for automatic\nevaluation of machine translation. Proceedings of the 40th Annual Meeting of the Asso-\nciationforComputational Linguistics (pp. 311\u2013318).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2961, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a6a4194a-90d4-4c1c-bc96-ff6dfd17ee81": {"__data__": {"id_": "a6a4194a-90d4-4c1c-bc96-ff6dfd17ee81", "embedding": null, "metadata": {"page_label": "1102", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8f083ac-d9bc-4dff-8791-3514d3f1221e", "node_type": "4", "metadata": {"page_label": "1102", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cdd294f65a1a422970bac3f85e48631410bd17065f7f4d92c7ceef2feda58d5f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf445afd-d4cc-49e4-973b-2e95bb6d81f3", "node_type": "1", "metadata": {}, "hash": "fda761cbf10a68a9b5dae0aa126d7557998b4f2008cf111332e34f0ba1b9f534", "class_name": "RelatedNodeInfo"}}, "text": "1102 REFERENCES\nParikh, A. P., T\u00e4ckstr\u00f6m, O., Das, D., & Uszkoreit, J. (2016). A decomposable attention\nmodel for natural language inference. ArXiv:1606.01933 .\nPark, T., Liu, M.-Y., Wang, T.-C., & Zhu, J.-Y. (2019). Semantic image synthesis with\nspatially-adaptive normalization. ProceedingsoftheIEEEConferenceonComputerVi-\nsionand Pattern Recognition (pp. 2337\u20132346).\nParzen, E. (1957). On consistent estimates of the spectrum of a stationary time series. An-\nnalsof MathematicalStatistics ,28, 329\u2013348.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., \u2026 et al. (2019). Py-\nTorch: an imperative style, high-performance deep learning library. AdvancesinNeural\nInformation ProcessingSystems ,32, 8026\u20138037.\nPaulus, R., Xiong, C., & Socher, R. (2017). A deep reinforced model for abstractive sum-\nmarization. ArXiv:1705.04304 .\nPenedo,G.,Malartic,Q.,Hesslow,D.,Cojocaru,R.,Cappelli,A.,Alobeidli,H.,\u2026Launay,\nJ.(2023).TheRefinedWebdatasetforFalconLLM:outperformingcuratedcorporawith\nweb data, and web data only. ArXiv:2306.01116 .\nPennington, J., Schoenholz, S., & Ganguli, S. (2017). Resurrecting the sigmoid in deep\nlearningthroughdynamicalisometry: theoryandpractice. AdvancesinNeuralInforma-\ntionProcessingSystems (pp. 4785\u20134795).\nPennington,J.,Socher,R.,&Manning,C.(2014).GloVe: globalvectorsforwordrepresen-\ntation.Proceedingsofthe2014ConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing(EMNLP) (pp. 1532\u20131543).\nPeters,J.,Janzing,D.,&Sch\u00f6lkopf,B.(2017). ElementsofCausalInference: Foundations\nandLearning Algorithms . MIT Press.\nPeters, M., Ammar, W., Bhagavatula, C., & Power, R. (2017). Semi-supervised sequence\ntagging with bidirectional language models. Proceedingsofthe55thAnnualMeetingof\ntheAssociation forComputational Linguistics,Volume1 (pp. 1756\u20131765).\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & Zettlemoyer, L.\n(2018). Deep contextualized word representations. Proceedingsofthe2018Conference\noftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: Human\nLanguageTechnologies,Volume1 (pp. 2227\u20132237).\nPetersen, K. B., & Pedersen, M. S. (2008). The Matrix Cookbook . Technical University of\nDenmark.\nPleiss, G., Chen, D., Huang, G., Li, T., Van Der Maaten, L., & Weinberger, K. Q. (2017).\nMemory-efficient implementation of densenets. ArXiv:1707.06990 .\nPolyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods.\nUSSRComputational Mathematicsand MathematicalPhysics ,4(5), 1\u201317.\nPrakash, A., Hasan, S. A., Lee, K., Datla, V., Qadir, A., Liu, J., & Farri, O. (2016). Neural\nparaphrase generation with stacked residual LSTM networks. ArXiv:1610.03098 .\nQin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., & Yang, D. (2023). Is ChatGPT a\ngeneral-purpose natural language processing task solver? ArXiv:2302.06476 .\nQuadrana, M., Cremonesi, P., & Jannach, D. (2018).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2873, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf445afd-d4cc-49e4-973b-2e95bb6d81f3": {"__data__": {"id_": "cf445afd-d4cc-49e4-973b-2e95bb6d81f3", "embedding": null, "metadata": {"page_label": "1102", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e8f083ac-d9bc-4dff-8791-3514d3f1221e", "node_type": "4", "metadata": {"page_label": "1102", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cdd294f65a1a422970bac3f85e48631410bd17065f7f4d92c7ceef2feda58d5f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a6a4194a-90d4-4c1c-bc96-ff6dfd17ee81", "node_type": "1", "metadata": {"page_label": "1102", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "f55984444e55025b32e3ba3c6979d19cbba11553729ba922190f9d69731747f1", "class_name": "RelatedNodeInfo"}}, "text": "Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods.\nUSSRComputational Mathematicsand MathematicalPhysics ,4(5), 1\u201317.\nPrakash, A., Hasan, S. A., Lee, K., Datla, V., Qadir, A., Liu, J., & Farri, O. (2016). Neural\nparaphrase generation with stacked residual LSTM networks. ArXiv:1610.03098 .\nQin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., & Yang, D. (2023). Is ChatGPT a\ngeneral-purpose natural language processing task solver? ArXiv:2302.06476 .\nQuadrana, M., Cremonesi, P., & Jannach, D. (2018). Sequence-aware recommender sys-\ntems.ACMComputing Surveys ,51(4), 66.\nQuinlan, J. R. (1993). C4.5: ProgramsforMachineLearning . Elsevier.\nRabiner, L., & Juang, B.-H. (1993). Fundamentalsof SpeechRecognition . Prentice-Hall.", "mimetype": "text/plain", "start_char_idx": 2334, "end_char_idx": 3097, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9d83e7d1-c2a4-4c7b-9656-b5760bfe2787": {"__data__": {"id_": "9d83e7d1-c2a4-4c7b-9656-b5760bfe2787", "embedding": null, "metadata": {"page_label": "1103", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57a0300a-28b0-4de2-bf3d-482f16b49a6d", "node_type": "4", "metadata": {"page_label": "1103", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "8efe150eefeeba082bd9ab056ae5086d46fa29c3f4fbb736c50ca945dfa67129", "class_name": "RelatedNodeInfo"}}, "text": "1103 REFERENCES\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., \u2026 et al. (2021).\nLearning transferable visual models from natural language supervision. International\nConferenceon MachineLearning (pp. 8748\u20138763).\nRadford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with\ndeep convolutional generative adversarial networks. ArXiv:1511.06434 .\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language\nunderstanding by generative pre-training. OpenAI.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language\nmodels are unsupervised multitask learners. OpenAI Blog ,1(8), 9.\nRadosavovic, I., Johnson, J., Xie, S., Lo, W.-Y., & Doll\u00e1r, P. (2019). On network design\nspaces for visual recognition. Proceedings of the IEEE/CVF International Conference\nonComputerVision (pp. 1882\u20131890).\nRadosavovic, I., Kosaraju, R. P., Girshick, R., He, K., & Doll\u00e1r, P. (2020). Designing net-\nwork design spaces. ProceedingsoftheIEEE/CVFConferenceonComputerVisionand\nPattern Recognition (pp. 10428\u201310436).\nRae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., \u2026 et al.\n(2021). Scaling language models: methods, analysis & insights from training gopher.\nArXiv:2112.11446 .\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., \u2026 Liu, P. J. (2020).\nExploring the limits of transfer learning with a unified text-to-text transformer. Journal\nofMachineLearning Research ,21, 1\u201367.\nRajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). SQuAD: 100,000+ questions for\nmachine comprehension of text. ArXiv:1606.05250 .\nRamachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., & Shlens, J. (2019).\nStand-alone self-attention in vision models. AdvancesinNeuralInformationProcessing\nSystems,32.\nRamachandran, P., Zoph, B., & Le, Q. V. (2017). Searching for activation functions.\nArXiv:1710.05941 .\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical text-\nconditional image generation with clip latents. ArXiv:2204.06125 .\nRam\u00f3n y Cajal, Santiago, & Azoulay, L. (1894). Les Nouvelles Id\u00e9es sur la Structure du\nSyst\u00e8meNerveuxchezl\u2019Homme etchezles Vert\u00e9br\u00e9s . Paris, C. Reinwald & Cie.\nRanzato, M.-A., Boureau, Y.-L., Chopra, S., & LeCun, Y. (2007). A unified energy-based\nframeworkforunsupervisedlearning. ArtificialIntelligenceandStatistics (pp.371\u2013379).\nRasmussen, C. E., & Williams, C. K. (2006). Gaussian Processes for Machine Learning .\nMIT Press.\nReddi, S. J., Kale, S., & Kumar, S. (2019). On the convergence of Adam and beyond.\nArXiv:1904.09237 .\nRedmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only look once: unified,\nreal-timeobjectdetection. ProceedingsoftheIEEEConferenceonComputerVisionand\nPattern Recognition (pp. 779\u2013788).\nRedmon, J., & Farhadi, A. (2018). YOLOv3: an incremental improvement.\nArXiv:1804.02767 .\nReed, S., & De Freitas, N. (2015). Neural programmer-interpreters. ArXiv:1511.06279 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2986, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd851b36-0457-457b-8c7e-47a2b4f3e81d": {"__data__": {"id_": "fd851b36-0457-457b-8c7e-47a2b4f3e81d", "embedding": null, "metadata": {"page_label": "1104", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fec5a5df-e972-44a8-9721-4a6a5bd71f4c", "node_type": "4", "metadata": {"page_label": "1104", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "1730dedf34b2ec2bb422501f4404a236a512732c2ed794f971482db39723e667", "class_name": "RelatedNodeInfo"}}, "text": "1104 REFERENCES\nReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., \u2026\net al. (2022). A generalist agent. ArXiv:2205.06175 .\nRen, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: towards real-time object\ndetection with region proposal networks. Advances in Neural Information Processing\nSystems (pp. 91\u201399).\nRevels, J., Lubin, M., & Papamarkou, T. (2016). Forward-mode automatic differentiation\nin Julia.ArXiv:1607.07892 .\nRezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic backpropagation and ap-\nproximate inference in deep generative models. International Conference on Machine\nLearning (pp. 1278\u20131286).\nRiesenhuber, M., &Poggio, T.(1999).Hierarchicalmodelsofobjectrecognitionincortex.\nNatureNeuroscience ,2(11), 1019\u20131025.\nRockafellar, R. T. (1970). ConvexAnalysis . Princeton University Press.\nRolnick, D., Veit, A., Belongie, S., &Shavit, N.(2017).Deeplearningisrobusttomassive\nlabel noise. ArXiv:1705.10694 .\nRudin, W. (1973). FunctionalAnalysis . McGraw-Hill.\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1988). Learning representations by\nback-propagating errors. CognitiveModeling ,5(3), 1.\nRussakovsky, O., Deng, J., Huang, Z., Berg, A. C., & Fei-Fei, L. (2013). Detecting avoca-\ndostozucchinis: whathavewedone,andwherearewegoing? InternationalConference\nonComputerVision(ICCV) .\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., \u2026 et al. (2015). Ima-\ngeNetlargescalevisualrecognitionchallenge. InternationalJournalofComputerVision ,\n115(3), 211\u2013252.\nRussell, S. J., & Norvig, P. (2016). Artificial Intelligence: A Modern Approach . Pearson\nEducation Limited.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., \u2026 et al. (2022).\nPhotorealistic text-to-image diffusion models with deep language understanding.\nArXiv:2205.11487 .\nSalinas, D., Seeger, M., Klein, A., Perrone, V., Wistuba, M., & Archambeau, C. (2022).\nSyne Tune: a library for large scale hyperparameter tuning and reproducible research.\nFirstConferenceon AutomatedMachineLearning .\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of\nBERT: smaller, faster, cheaper and lighter. ArXiv:1910.01108 .\nSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., \u2026 et al. (2021).\nMultitask prompted training enables zero-shot task generalization. ArXiv:2110.08207 .\nSanturkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How does batch normaliza-\ntion help optimization? AdvancesinNeuralInformationProcessingSystems (pp. 2483\u2013\n2493).\nSarwar, B. M., Karypis, G., Konstan, J. A., & Riedl, J. (2001). Item-based collaborative\nfiltering recommendation algorithms. Proceedings of 10th International Conference on\nWorldWideWeb (pp. 285\u2013295).\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u0107, S., Hesslow, D., \u2026 et al. (2022). BLOOM:\na 176B-parameter open-access multilingual language model. ArXiv:2211.05100 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2924, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a3866778-7d08-48de-9558-b255ef53cde5": {"__data__": {"id_": "a3866778-7d08-48de-9558-b255ef53cde5", "embedding": null, "metadata": {"page_label": "1105", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "512837ee-867d-4121-8de8-ba4ac5dd4ddb", "node_type": "4", "metadata": {"page_label": "1105", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "15669350a121c93ff0dfbb8eaa666c4c922fa78410d718fec145daf14acb8f79", "class_name": "RelatedNodeInfo"}}, "text": "1105 REFERENCES\nSchein,A.I.,Popescul,A.,Ungar,L.H.,&Pennock,D.M.(2002).Methodsandmetricsfor\ncold-start recommendations. Proceedings of the 25th Annual International ACM SIGIR\nConferenceon Researchand Developmentin Information Retrieval (pp. 253\u2013260).\nSchuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., \u2026 et al.\n(2022). LAION-5B: an open large-scale dataset for training next generation image-text\nmodels.ArXiv:2210.08402 .\nSchuster,M.,&Paliwal,K.K.(1997).Bidirectionalrecurrentneuralnetworks. IEEETrans-\nactionson Signal Processing ,45(11), 2673\u20132681.\nSch\u00f6lkopf, B., Herbrich, R., & Smola, A. J. (2001). Helmbold, D. P., & Williamson, B.\n(Eds.). A generalized representer theorem. Proceedings of the Annual Conference on\nComputationalLearning Theory (pp. 416\u2013426). Springer-Verlag.\nSch\u00f6lkopf,B.,Burges,C.,&Vapnik,V.(1996).Incorporatinginvariancesinsupportvector\nlearning machines. InternationalConferenceonArtificialNeuralNetworks (pp. 47\u201352).\nSch\u00f6lkopf, B., & Smola, A. J. (2002). Learning with Kernels: Support Vector Machines,\nRegularization,Optimization,and Beyond . MIT Press.\nSennrich, R., Haddow, B., & Birch, A. (2015). Neural machine translation of rare words\nwith subword units. ArXiv:1508.07909 .\nSergeev, A., & Del Balso, M. (2018). Horovod: fast and easy distributed deep learning in\nTensorFlow. ArXiv:1802.05799 .\nShannon, C. E. (1948). A mathematical theory of communication. TheBellSystemTechni-\ncalJournal ,27(3), 379\u2013423.\nShao, H., Yao, S., Sun, D., Zhang, A., Liu, S., Liu, D., \u2026 Abdelzaher, T. (2020). Con-\ntrolVAE: controllable variational autoencoder. Proceedings of the 37th International\nConferenceon MachineLearning .\nShaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-attention with relative position repre-\nsentations. ArXiv:1803.02155 .\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., & Catanzaro, B. (2019).\nMegatron-LM: training multi-billion parameter language models using model paral-\nlelism.ArXiv:1909.08053 .\nSilver,D.,Huang,A.,Maddison,C.J.,Guez,A.,Sifre,L.,VanDenDriessche,G.,\u2026etal.\n(2016). Mastering the game of Go with deep neural networks and tree search. Nature,\n529(7587), 484.\nSilverman, B. W. (1986). Density Estimation for Statistical and Data Analysis . Chapman\nand Hall.\nSimard, P. Y., LeCun, Y. A., Denker, J. S., & Victorri, B. (1998). Transformation invari-\nanceinpatternrecognition\u2013tangentdistanceandtangentpropagation. NeuralNetworks:\nTricksof theTrade (pp. 239\u2013274). Springer.\nSimonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale\nimage recognition. ArXiv:1409.1556 .\nSindhwani, V., Sainath, T. N., & Kumar, S. (2015). Structured transforms for small-\nfootprint deep learning. ArXiv:1510.01722 .\nSivic,J.,&Zisserman,A.(2003).VideoGoogle: atextretrievalapproachtoobjectmatch-\ning in videos. Proceedings of the IEEE International Conference on Computer Vision\n(pp. 1470\u20131470).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2903, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8bf41e91-3dba-4515-888d-6edaabfbbe14": {"__data__": {"id_": "8bf41e91-3dba-4515-888d-6edaabfbbe14", "embedding": null, "metadata": {"page_label": "1106", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "afd9f82c-3eca-4281-b9aa-3d0696d81217", "node_type": "4", "metadata": {"page_label": "1106", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "826d8ca86009ce75fe962b403a0072bd681f9db4590e752f82294c19324e328b", "class_name": "RelatedNodeInfo"}}, "text": "1106 REFERENCES\nSmith, S., Patwary, M., Norick, B., LeGresley, P., Rajbhandari, S., Casper, J., \u2026 et al.\n(2022). Using DeepSpeed and Megatron to train Megatron-Turing NLG 530B, a large-\nscale generative language model. ArXiv:2201.11990 .\nSmola, A., & Narayanamurthy, S. (2010). An architecture for parallel topic models. Pro-\nceedingsof theVLDB Endowment ,3(1-2), 703\u2013710.\nSnoek,J.,Larochelle,H.,&Adams,R.(2012).PracticalBayesianoptimizationofmachine\nlearning algorithms. AdvancesinNeuralInformationProcessingSystems25 (pp. 2951\u2013\n2959).\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., & Ganguli, S. (2015). Deep unsu-\npervised learning using nonequilibrium thermodynamics. International Conference on\nMachineLearning (pp. 2256\u20132265).\nSong, Y., & Ermon, S. (2019). Generative modeling by estimating gradients of the data\ndistribution. Advancesin NeuralInformation ProcessingSystems ,32.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., & Poole, B. (2021).\nScore-basedgenerativemodelingthroughstochasticdifferentialequations. International\nConferenceon Learning Representations .\nSpeelpenning, B. (1980). Compiling fast partial derivatives of functions given by algo-\nrithms(Doctoral dissertation). University of Illinois at Urbana-Champaign.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., \u2026 et al. (2022).\nBeyond the imitation game: quantifying and extrapolating the capabilities of language\nmodels.ArXiv:2206.04615 .\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014).\nDropout: a simple way to prevent neural networks from overfitting. JournalofMachine\nLearning Research ,15(1), 1929\u20131958.\nSrivastava, R. K., Greff, K., & Schmidhuber, J. (2015). Highway networks.\nArXiv:1505.00387 .\nStrang, G. (1993). IntroductiontoLinear Algebra . Wellesley\u2013Cambridge Press.\nSu, X., & Khoshgoftaar, T. M. (2009). A survey of collaborative filtering techniques. Ad-\nvancesin Artificial Intelligence ,2009.\nSukhbaatar, S., Weston, J., & Fergus, R. (2015). End-to-end memory networks. Advances\ninNeuralInformation ProcessingSystems (pp. 2440\u20132448).\nSutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013). On the importance of initializa-\ntion and momentum in deep learning. International Conference on Machine Learning\n(pp. 1139\u20131147).\nSutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural\nnetworks. Advancesin NeuralInformation ProcessingSystems (pp. 3104\u20133112).\nSzegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017). Inception-v4, Inception-\nResNet and the impact of residual connections on learning. 31st AAAI Conference on\nArtificial Intelligence .\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., \u2026 Rabinovich, A.\n(2015). Going deeper with convolutions. Proceedings of the IEEE Conference on Com-\nputerVisionand Pattern Recognition (pp. 1\u20139).\nSzegedy,C.,Vanhoucke,V.,Ioffe,S.,Shlens,J.,&Wojna,Z.(2016).RethinkingtheIncep-\ntionarchitectureforcomputervision. ProceedingsoftheIEEEConferenceonComputer\nVisionand Pattern Recognition (pp. 2818\u20132826).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81d02376-5daa-4180-99b4-310e9610b631": {"__data__": {"id_": "81d02376-5daa-4180-99b4-310e9610b631", "embedding": null, "metadata": {"page_label": "1107", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7536d56a-0f50-4193-99b2-5211a824a7af", "node_type": "4", "metadata": {"page_label": "1107", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "90d089f638924a9499637480b8f22d7b9c8785c1b040f0e25108e3c6e93b88b5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a8f5e2fd-0bff-42eb-8742-7c43413a3ee8", "node_type": "1", "metadata": {}, "hash": "b7ac483fad4f69aa8d42067872a6c49051083738c3a4e0dcd1f9539f2e2bf6c0", "class_name": "RelatedNodeInfo"}}, "text": "1107 REFERENCES\nTallec, C., & Ollivier, Y. (2017). Unbiasing truncated backpropagation through time.\nArXiv:1705.08209 .\nTan, M., & Le, Q. (2019). EfficientNet: rethinking model scaling for convolutional neural\nnetworks. International Conferenceon MachineLearning (pp. 6105\u20136114).\nTaskar, B., Guestrin, C., & Koller, D. (2004). Max-margin Markov networks. Advances in\nNeuralInformation ProcessingSystems ,16, 25.\nTay, Y., Dehghani, M., Bahri, D., & Metzler, D. (2020). Efficient transformers: a survey.\nArXiv:2009.06732 .\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., \u2026Stojnic, R.\n(2022). Galactica: a large language model for science. ArXiv:2211.09085 .\nTeye, M., Azizpour, H., & Smith, K. (2018). Bayesian uncertainty estimation for batch\nnormalized deep networks. ArXiv:1802.06455 .\nThomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., \u2026 Li, L.-J.\n(2016). Yfcc100m: the new data in multimedia research. Communications of the ACM ,\n59(2), 64\u201373.\nTieleman, T., & Hinton, G. (2012). Divide the gradient by a running average of its recent\nmagnitude. COURSERA:NeuralNetworksforMachineLearning,Lecture6.5-rmsprop .\nTikhonov, A. N., & Arsenin, V. Y. (1977). SolutionsofIll-PosedProblems . W.H. Winston.\nTolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., \u2026 et al.\n(2021).MLP-mixer: anall-MLParchitectureforvision. AdvancesinNeuralInformation\nProcessingSystems ,34.\nTorralba, A., Fergus, R., & Freeman, W. T. (2008). 80 million tiny images: a large data set\nfor nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis\nandMachineIntelligence ,30(11), 1958\u20131970.\nTouvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., & J\u00e9gou, H. (2021). Train-\ningdata-efficientimagetransformers&distillationthroughattention. InternationalCon-\nferenceon MachineLearning (pp. 10347\u201310357).\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., \u2026 et al.\n(2023a). LLaMA: open and efficient foundation language models. ArXiv:2302.13971 .\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., \u2026 et al. (2023b).\nLLaMA 2: open foundation and fine-tuned chat models. ArXiv:2307.09288 .\nTsoumakas, G., &Katakis, I.(2007).Multi-labelclassification: anoverview. International\nJournal of Data Warehousingand Mining ,3(3), 1\u201313.\nTuring, A. (1950). Computing machinery and intelligence. Mind,59(236), 433.\nUijlings, J. R., Van De Sande, K. E., Gevers, T., & Smeulders, A. W. (2013). Selective\nsearch for object recognition. International Journal of Computer Vision ,104(2), 154\u2013\n171.\nVapnik, V. (1995). The Natureof StatisticalLearning Theory . New York: Springer.\nVapnik, V. (1998). StatisticalLearning Theory . New York: John Wiley and Sons.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8f5e2fd-0bff-42eb-8742-7c43413a3ee8": {"__data__": {"id_": "a8f5e2fd-0bff-42eb-8742-7c43413a3ee8", "embedding": null, "metadata": {"page_label": "1107", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7536d56a-0f50-4193-99b2-5211a824a7af", "node_type": "4", "metadata": {"page_label": "1107", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "90d089f638924a9499637480b8f22d7b9c8785c1b040f0e25108e3c6e93b88b5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81d02376-5daa-4180-99b4-310e9610b631", "node_type": "1", "metadata": {"page_label": "1107", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "d5de90418bb4fce2240a6f25dddd8f4db85773905a7df8af81dd19287c98f606", "class_name": "RelatedNodeInfo"}}, "text": "ArXiv:2307.09288 .\nTsoumakas, G., &Katakis, I.(2007).Multi-labelclassification: anoverview. International\nJournal of Data Warehousingand Mining ,3(3), 1\u201313.\nTuring, A. (1950). Computing machinery and intelligence. Mind,59(236), 433.\nUijlings, J. R., Van De Sande, K. E., Gevers, T., & Smeulders, A. W. (2013). Selective\nsearch for object recognition. International Journal of Computer Vision ,104(2), 154\u2013\n171.\nVapnik, V. (1995). The Natureof StatisticalLearning Theory . New York: Springer.\nVapnik, V. (1998). StatisticalLearning Theory . New York: John Wiley and Sons.\nVapnik, V., & Chervonenkis, A. (1964). A note on one class of perceptrons. Automation\nandRemoteControl ,25.\nVapnik, V., & Chervonenkis, A. (1968). Uniform convergence of frequencies of occurence\nof events to their probabilities. Dokl. Akad. NaukSSSR ,181, 915-918.\nVapnik,V.,&Chervonenkis,A.(1971).Ontheuniformconvergenceofrelativefrequencies\nof events to their probabilities. Theory Probab.Appl. ,16(2), 264-281.", "mimetype": "text/plain", "start_char_idx": 2207, "end_char_idx": 3191, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9245306f-beb5-4254-802c-8963c01f79a7": {"__data__": {"id_": "9245306f-beb5-4254-802c-8963c01f79a7", "embedding": null, "metadata": {"page_label": "1108", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f514bc10-07f7-4289-b367-16ad3e66bea9", "node_type": "4", "metadata": {"page_label": "1108", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b9458fcb2f3bbab6a61b5b34c34d546826a0a9028cc17e0328e9fc467525934a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c565a9e-fc17-4e4f-b29a-e4434618c3ea", "node_type": "1", "metadata": {}, "hash": "e3ed56bbc16df2c6758e167f735577fbeb4758d8d1e03c88a6cecb58e32dffe6", "class_name": "RelatedNodeInfo"}}, "text": "1108 REFERENCES\nVapnik, V., & Chervonenkis, A. (1981). The necessary and sufficient conditions for the\nuniform convergence of averages to their expected values. Teoriya Veroyatnostei i Ee\nPrimeneniya ,26(3), 543-564.\nVapnik,V.,&Chervonenkis,A.(1991).Thenecessaryandsufficientconditionsforconsis-\ntency in the empirical risk minimization method. Pattern Recognition and Image Anal-\nysis,1(3), 283-305.\nVapnik, V. N., & Chervonenkis, A. Y. (1974). Ordered risk minimization. Automationand\nRemoteControl ,35, 1226\u20131235, 1403\u20131412.\nVapnik, V. (1992). Principles of risk minimization for learning theory. AdvancesinNeural\nInformation ProcessingSystems (pp. 831\u2013838).\nVapnik, V., Levin, E., & Le Cun, Y. (1994). Measuring the VC-dimension of a learning\nmachine. NeuralComputation ,6(5), 851\u2013876.\nVaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,\u2026Polosukhin,\nI. (2017). Attention is all you need. AdvancesinNeuralInformationProcessingSystems\n(pp. 5998\u20136008).\nWahba, G. (1990). Spline Models forObservationalData . SIAM.\nWaibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. J. (1989). Phoneme recog-\nnition using time-delay neural networks. IEEE Transactions on Acoustics, Speech, and\nSignalProcessing ,37(3), 328\u2013339.\nWang, H., Zhang, A., Zheng, S., Shi, X., Li, M., & Wang, Z. (2022). Removing batch nor-\nmalization boosts adversarial training. International Conference on Machine Learning\n(pp. 23433\u201323445).\nWang, L., Li, M., Liberty, E., & Smola, A. J. (2018). Optimal message scheduling for\naggregation. Networks ,2(3), 2\u20133.\nWang,Q.,Li,B.,Xiao,T.,Zhu,J.,Li,C.,Wong,D.F.,&Chao,L.S.(2019).Learningdeep\ntransformer models for machine translation. Proceedingsofthe57thAnnualMeetingof\ntheAssociation forComputational Linguistics (pp. 1810\u20131822).\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., & Zhou, D. (2023). Self-consistency\nimproves chain of thought reasoning in language models. International Conference on\nLearning Representations .\nWang, Y., Davidson, A., Pan, Y., Wu, Y., Riffel, A., & Owens, J. D. (2016). Gunrock: a\nhigh-performancegraphprocessinglibraryontheGPU. ACMSIGPLANNotices (p.11).\nWarstadt,A.,Singh,A.,&Bowman,S.R.(2019).Neuralnetworkacceptabilityjudgments.\nTransactionsof theAssociation forComputational Linguistics ,7, 625\u2013641.\nWasserman,L.(2013). AllofStatistics: AConciseCourseinStatisticalInference .Springer.\nWatkins, C. J., & Dayan, P. (1992). Q-learning. MachineLearning ,8(3\u20134), 279\u2013292.\nWatson, G. S. (1964). Smooth regression analysis. Sankhy\u0101: TheIndianJournalofStatis-\ntics,Series A , pp. 359\u2013372.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., \u2026 Le, Q. V. (2021).\nFinetuned language models are zero-shot learners. ArXiv:2109.01652 .\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., \u2026 et al. (2022). Emer-\ngent abilities of large language models. ArXiv:2206.07682 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c565a9e-fc17-4e4f-b29a-e4434618c3ea": {"__data__": {"id_": "7c565a9e-fc17-4e4f-b29a-e4434618c3ea", "embedding": null, "metadata": {"page_label": "1108", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f514bc10-07f7-4289-b367-16ad3e66bea9", "node_type": "4", "metadata": {"page_label": "1108", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "b9458fcb2f3bbab6a61b5b34c34d546826a0a9028cc17e0328e9fc467525934a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9245306f-beb5-4254-802c-8963c01f79a7", "node_type": "1", "metadata": {"page_label": "1108", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "4b8b62961f05fafc5557946c950b85f95a80c3b013d13c13fb905e2b23599161", "class_name": "RelatedNodeInfo"}}, "text": "(1992). Q-learning. MachineLearning ,8(3\u20134), 279\u2013292.\nWatson, G. S. (1964). Smooth regression analysis. Sankhy\u0101: TheIndianJournalofStatis-\ntics,Series A , pp. 359\u2013372.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., \u2026 Le, Q. V. (2021).\nFinetuned language models are zero-shot learners. ArXiv:2109.01652 .\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., \u2026 et al. (2022). Emer-\ngent abilities of large language models. ArXiv:2206.07682 .\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., & Zhou, D. (2022). Chain\nof thought prompting elicits reasoning in large language models. ArXiv:2201.11903 .", "mimetype": "text/plain", "start_char_idx": 2385, "end_char_idx": 3030, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a693e7e-99df-4ede-81b6-8dc141f90ef2": {"__data__": {"id_": "5a693e7e-99df-4ede-81b6-8dc141f90ef2", "embedding": null, "metadata": {"page_label": "1109", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "641b07f9-8b08-4fc6-bb19-5fb76f470409", "node_type": "4", "metadata": {"page_label": "1109", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "2ff9389e4e0e7a163179cf4de15982d6052119193a3cad72f68c48bf94bc3861", "class_name": "RelatedNodeInfo"}}, "text": "1109 REFERENCES\nWelling, M., & Teh, Y. W. (2011). Bayesian learning via stochastic gradient Langevin dy-\nnamics.Proceedingsofthe28thInternationalConferenceonMachineLearning(ICML-\n11)(pp. 681\u2013688).\nWengert,R.E.(1964).Asimpleautomaticderivativeevaluationprogram. Communications\noftheACM ,7(8), 463\u2013464.\nWerbos, P. J. (1990). Backpropagation through time: what it does and how to do it. Pro-\nceedingsof theIEEE ,78(10), 1550\u20131560.\nWigner, E. P. (1958). On the distribution of the roots of certain symmetric matrices. Ann.\nMath.(pp. 325\u2013327).\nWilson,A.G.,&Izmailov,P.(2020).Bayesiandeeplearningandaprobabilisticperspective\nof generalization. Advances in Neural Information ProcessingSystems ,33, 4697\u20134708.\nWistuba, M., Rawat, A., & Pedapati, T. (2019). A survey on neural architecture search.\nArXiv:1905.01392[cs.LG] .\nWistuba, M., Schilling, N., & Schmidt-Thieme, L. (2018). Scalable Gaussian process-\nbased transfer surrogates for hyperparameter optimization. MachineLearning ,108, 43\u2013\n78.\nWolpert, D. H., & Macready, W. G. (1995). No free lunch theorems for search . Technical\nReport SFI-TR-95-02-010, Santa Fe Institute.\nWood, F., Gasthaus, J., Archambeau, C., James, L., & Teh, Y. W. (2011). The sequence\nmemoizer. Communicationsof theACM ,54(2), 91\u201398.\nWu, B., Wan, A., Yue, X., Jin, P., Zhao, S., Golmant, N., \u2026 Keutzer, K. (2018). Shift: a\nzero flop, zero parameter alternative to spatial convolutions. Proceedings of the IEEE\nConferenceon ComputerVisionand Pattern Recognition (pp. 9127\u20139135).\nWu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., \u2026 et al. (2016).\nGoogle\u2019s neural machine translation system: bridging the gap between human and ma-\nchine translation. ArXiv:1609.08144 .\nXiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-MNIST: a novel image dataset for\nbenchmarking machine learning algorithms. ArXiv:1708.07747 .\nXiao,L.,Bahri,Y.,Sohl-Dickstein,J.,Schoenholz,S.,&Pennington,J.(2018).Dynamical\nisometry and a mean field theory of CNNs: how to train 10,000-layer vanilla convolu-\ntionalneuralnetworks. InternationalConferenceonMachineLearning (pp.5393\u20135402).\nXie, S., Girshick, R., Doll\u00e1r, P., Tu, Z., & He, K. (2017). Aggregated residual transforma-\ntionsfordeepneuralnetworks. ProceedingsoftheIEEEConferenceonComputerVision\nandPattern Recognition (pp. 1492\u20131500).\nXiong,R.,Yang,Y.,He,D.,Zheng,K.,Zheng,S.,Xing,C.,\u2026Liu,T.(2020).Onlayernor-\nmalization in the transformer architecture. InternationalConferenceonMachineLearn-\ning(pp. 10524\u201310533).\nXiong, W., Wu, L., Alleva, F., Droppo, J., Huang, X., & Stolcke, A. (2018). The Microsoft\n2017conversationalspeechrecognitionsystem. 2018IEEEInternationalConferenceon\nAcoustics,Speechand Signal Processing(ICASSP) (pp. 5934\u20135938).\nYamaguchi, K., Sakamoto, K., Akabane, T., & Fujimoto, Y. (1990). A neural network\nfor speaker-independent isolated word recognition. First International Conference on\nSpokenLanguageProcessing .\nYang, Z., Hu, Z., Deng, Y., Dyer, C., & Smola, A. (2016). Neural machine translation with\nrecurrent attention modeling. ArXiv:1607.05108 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f20d8c72-0584-4d23-ac18-eb554985e933": {"__data__": {"id_": "f20d8c72-0584-4d23-ac18-eb554985e933", "embedding": null, "metadata": {"page_label": "1110", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2277f387-c738-46eb-8fd3-5261f944ae81", "node_type": "4", "metadata": {"page_label": "1110", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "cab4521cc8ed0b6a5a198ecef801a3f54eec41d68c86abc8fbceca644ca3aa1d", "class_name": "RelatedNodeInfo"}}, "text": "1110 REFERENCES\nYang, Z., Moczulski, M., Denil, M., De Freitas, N., Smola, A., Song, L., & Wang, Z.\n(2015).Deepfriedconvnets. ProceedingsoftheIEEEInternationalConferenceonCom-\nputerVision (pp. 1476\u20131483).\nYe,M.,Yin,P.,Lee,W.-C.,&Lee,D.-L.(2011).Exploitinggeographicalinfluenceforcol-\nlaborativepoint-of-interestrecommendation. Proceedingsofthe34thInternationalACM\nSIGIR Conference on Research and Development in Information Retrieval (pp. 325\u2013\n334).\nYou,Y.,Gitman,I.,&Ginsburg,B.(2017).Largebatchtrainingofconvolutionalnetworks.\nArXiv:1708.03888 .\nYu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., \u2026 Wu, Y. (2022). Scaling au-\ntoregressive models for content-rich text-to-image generation. ArXiv:2206.10789 .\nZaheer, M., Reddi, S., Sachan, D., Kale, S., & Kumar, S. (2018). Adaptive methods for\nnonconvexoptimization. AdvancesinNeuralInformationProcessingSystems (pp.9793\u2013\n9803).\nZeiler, M. D. (2012). ADADELTA: an adaptive learning rate method. ArXiv:1212.5701 .\nZeiler, M. D., & Fergus, R. (2013). Stochastic pooling for regularization of deep convolu-\ntional neural networks. ArXiv:1301.3557 .\nZhang, A., Tay, Y., Zhang, S., Chan, A., Luu, A. T., Hui, S. C., & Fu, J. (2021). Beyond\nfully-connected layers with quaternions: parameterization of hypercomplex multiplica-\ntions with 1/n parameters. International Conferenceon Learning Representations .\nZhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2021). Understanding deep\nlearning (still) requires rethinking generalization. Communications of the ACM ,64(3),\n107\u2013115.\nZhang, S., Yao, L., Sun, A., & Tay, Y. (2019). Deep learning based recommender system:\na survey and new perspectives. ACMComputing Surveys ,52(1), 5.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., \u2026 et al. (2022). OPT:\nopen pre-trained transformer language models. ArXiv:2205.01068 .\nZhang, W., Tanida, J., Itoh, K., & Ichioka, Y. (1988). Shift-invariant pattern recognition\nneural network and its optical architecture. Proceedings of Annual Conference of the\nJapanSocietyof Applied Physics .\nZhang, Y., Sun, P., Jiang, Y., Yu, D., Yuan, Z., Luo, P., \u2026 Wang, X. (2021). ByteTrack:\nmulti-object tracking by associating every detection box. ArXiv:2110.06864 .\nZhang, Z., Zhang, A., Li, M., & Smola, A. (2023). Automatic chain of thought prompting\nin large language models. International Conference on Learning Representations .\nZhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., & Smola, A. (2023). Multimodal\nchain-of-thought reasoning in language models. ArXiv:2302.00923 .\nZhao, Z.-Q., Zheng, P., Xu, S.-t., & Wu, X. (2019). Object detection with deep learning:\na review. IEEE Transactions on Neural Networks and Learning Systems ,30(11), 3212\u2013\n3232.\nZhou, D., Sch\u00e4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., \u2026 Chi, E. (2023). Least-\nto-most prompting enables complex reasoning in large language models. International\nConferenceon Learning Representations .\nZhu, J.-Y., Park, T., Isola, P., & Efros, A. A. (2017). Unpaired image-to-image transla-\ntion using cycle-consistent adversarial networks. ProceedingsoftheIEEEInternational\nConferenceon ComputerVision (pp. 2223\u20132232).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3147, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9038e64-d2b0-4162-aaef-1b3dd5c6a8f2": {"__data__": {"id_": "e9038e64-d2b0-4162-aaef-1b3dd5c6a8f2", "embedding": null, "metadata": {"page_label": "1111", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cc9b6c1e-7161-463f-86cd-c63038729c4c", "node_type": "4", "metadata": {"page_label": "1111", "file_name": "DeepDiveIntoDeepLearning.pdf"}, "hash": "a54f43cba9aaa5a8016a7de10c9aaac6254a83e1625f8c9b59a31a527df46eab", "class_name": "RelatedNodeInfo"}}, "text": "1111 REFERENCES\nZhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S.\n(2015). Aligning books and movies: towards story-like visual explanations by watch-\ning movies and reading books. Proceedings of the IEEE International Conference on\nComputerVision (pp. 19\u201327).\nZoph, B., & Le, Q. V. (2016). Neural architecture search with reinforcement learning.\nArXiv:1611.01578 .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 404, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"718eff5e-0d3f-43ac-8226-5232381f4af0": {"node_ids": ["d824f03e-7afe-4bae-bc25-fccfbb343a7e"], "metadata": {"page_label": "i", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "68413b82-f021-4b71-91fe-e2bb6ca0eba6": {"node_ids": ["733d022d-092e-45d9-95a6-aacded131edf"], "metadata": {"page_label": "ii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "84480dbe-b03c-42bf-9b51-3e044e5d48a4": {"node_ids": ["4ac3d829-791e-493b-8b5e-6a90dfdc8c82"], "metadata": {"page_label": "iii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9829986e-2155-4196-a42f-1c6e40ac130f": {"node_ids": ["9071df74-4cbd-46d6-af21-f7fd4b3fc58a"], "metadata": {"page_label": "iv", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "79806eb3-8406-4f54-bdbd-95a12dcf08ed": {"node_ids": ["e88c2583-26da-4339-9d3a-0ab76027e03b"], "metadata": {"page_label": "v", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4e48a309-2246-45e8-802d-53cf0501fb06": {"node_ids": ["51b54416-e865-40dd-b796-b3ae2fa4bd0d"], "metadata": {"page_label": "vi", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f9478ab6-7eaf-4ce6-bbbc-d5258f31b157": {"node_ids": ["3fbce58b-8128-4ce4-b322-170528063ddc"], "metadata": {"page_label": "vii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bc9a497f-1214-4008-a29e-fad59826e416": {"node_ids": ["eadb352f-c537-4734-81b2-392af6f7014a"], "metadata": {"page_label": "viii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4fdae7f2-4ed0-41ad-9971-a8272a8fd4b8": {"node_ids": ["3bd3cd9f-ea0a-4096-aa21-0f6881fa2b2a"], "metadata": {"page_label": "ix", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "020cc405-8012-470b-8ca8-192e424019a4": {"node_ids": ["1f5db6d4-b49a-49e3-a339-d4253926c36d"], "metadata": {"page_label": "x", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2e12f008-0c37-4675-9d4c-25703a56040a": {"node_ids": ["4f2b435c-1f9f-4244-8865-f0ecbcf51166"], "metadata": {"page_label": "xi", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "625db00f-e7c2-4132-8c75-cc7160e87160": {"node_ids": ["a659beeb-9513-49c8-b514-9eccae8ad32b"], "metadata": {"page_label": "xii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1eb5e137-c72c-4e63-b7c2-b7153801d13a": {"node_ids": ["10279548-9563-4846-9cfe-9b14b61999d9"], "metadata": {"page_label": "xiii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e50abfaf-1789-47b9-8d95-1af68955910b": {"node_ids": ["cd5b61ac-21c6-4a55-a278-1974e0e29e92"], "metadata": {"page_label": "xiv", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7956e872-9719-427d-b09e-502e92e1afb8": {"node_ids": ["bb04e546-3cfa-40e9-8be5-2a80074711c5"], "metadata": {"page_label": "xv", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ca9742ea-0801-4e54-93a2-2aa9863274ad": {"node_ids": ["d3c30157-ab36-4772-bfd4-7366d3b329b6"], "metadata": {"page_label": "xvi", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5937be6b-7e42-440c-a403-e75af91ccc9c": {"node_ids": ["9f0ee467-7666-47f5-8fb4-cd7c3d1d2c66"], "metadata": {"page_label": "xvii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f131e00b-603d-4242-a728-fc6ddcd552aa": {"node_ids": ["50240af0-e371-4bd2-bd53-7293a9cad051"], "metadata": {"page_label": "xviii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5c3379b8-6495-43da-81f6-82621a32bb47": {"node_ids": ["590d9183-af52-409b-99e2-28a524a7582b"], "metadata": {"page_label": "xix", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "23adfc25-5664-4baf-ab8a-1beac1691f15": {"node_ids": ["7931a6d6-a775-4d34-bbe4-5f6d0822ba06"], "metadata": {"page_label": "xx", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3a86e926-5a74-4ed2-a224-6ffc6784f2ba": {"node_ids": ["8d6e4149-8f6a-471e-a4e2-58b010c887b7"], "metadata": {"page_label": "xxi", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "755050d1-ae98-4329-a2ac-a91dadf0a1a2": {"node_ids": ["4ddb7105-bfe1-41b8-a8fd-a999a33c51f4"], "metadata": {"page_label": "xxii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8cc005f2-60fa-4609-acec-094babedb29b": {"node_ids": ["54e8887f-8f0d-47ed-8caf-11d09abfe87e"], "metadata": {"page_label": "xxiii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f3e6c603-b7d7-464c-87f7-68992ff22852": {"node_ids": ["1496c6c7-7bef-4855-a34a-225211f5e342"], "metadata": {"page_label": "xxiv", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b39b720a-aba3-401e-9e2c-2097e339cc6f": {"node_ids": ["3eab7099-37aa-4f51-a768-aba28481320f"], "metadata": {"page_label": "xxv", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "16bc8577-167b-416d-b8de-db9bd3ebc3aa": {"node_ids": ["c524c6d5-3fdd-4a1f-a53c-85daa3af1204"], "metadata": {"page_label": "xxvi", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a036a6c9-2a57-4805-92bb-a42b6ebc9dc4": {"node_ids": ["3e435bd9-f4b8-4863-bb5d-666584c289dd"], "metadata": {"page_label": "xxvii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "50d12424-e819-4a2f-8b84-b71f94cc56ef": {"node_ids": ["6abc4124-71a4-4bde-b930-e176e8118d05"], "metadata": {"page_label": "xxviii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f30241e7-7fbc-40a1-be80-fd5120d627ce": {"node_ids": ["416fb828-8a58-461a-837c-24e2a6d72663"], "metadata": {"page_label": "xxix", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ad5b7a7e-e541-4670-b0cf-ceb0f3fc9c15": {"node_ids": ["35f3d01c-a53c-44f9-ae06-c5ff6aa73251"], "metadata": {"page_label": "xxx", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4f2fadff-692c-4d4e-b1ed-8c90d1920f2a": {"node_ids": ["40b49747-41b5-4b32-bf96-12bddc89bf79", "9bd548fe-e6a6-4f90-a147-50f50e795611"], "metadata": {"page_label": "xxxi", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f270e715-0e54-40e6-8bdb-4fc61767b8ab": {"node_ids": ["0b2a8efa-0b87-436a-b681-bddfafed6913"], "metadata": {"page_label": "xxxii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d1893d1a-6cd6-4d84-9764-db71eeaf4acb": {"node_ids": ["dacbf4f4-304e-4819-95fb-2f135c59e6de"], "metadata": {"page_label": "xxxiii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "18327ec2-e894-42cc-9a56-5eadcd04dc1c": {"node_ids": ["641f9f46-a463-457f-93b5-2dd0c1394831"], "metadata": {"page_label": "xxxiv", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "33c1f82e-ff9a-413a-b81f-487249871b1d": {"node_ids": ["a9ec3348-8024-4dcb-9e06-2bce7509ad71"], "metadata": {"page_label": "xxxv", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "725b4510-4255-4142-95f1-94093d6ec3cb": {"node_ids": ["d19c769a-2083-46cb-af7d-b21ae6794916"], "metadata": {"page_label": "xxxvi", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6afe5b7b-8993-4a70-9982-e59a7f594cbf": {"node_ids": ["f379a3e0-3b49-4107-ab83-75744809f415"], "metadata": {"page_label": "xxxvii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "faeec7e9-a99b-43a5-9bc7-90765ce8fa36": {"node_ids": ["a5033a77-aeca-4b6e-ac2a-c4892ae75f89"], "metadata": {"page_label": "xxxviii", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4b1341be-2b39-48fe-81c3-6c4688f41ba4": {"node_ids": ["6d7fcf72-05fc-4066-8d12-7d7ac24c04a8"], "metadata": {"page_label": "xxxix", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "87af99ee-1cc2-4516-bed4-556292d32e37": {"node_ids": ["db467600-1227-47c6-9134-4b3e30cc1a4b"], "metadata": {"page_label": "xl", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d707ff2b-acdc-47e9-b94e-baf71a1ec631": {"node_ids": ["bb8e34ab-e7a6-4b6b-b1c5-9a3fb1acefc6"], "metadata": {"page_label": "1", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9ada9682-d106-4b7e-9d4f-0e8990038a59": {"node_ids": ["b647dc54-6834-49b6-83b1-cb969bb077b3"], "metadata": {"page_label": "2", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ab7fa45c-4770-4bbd-99c6-0160ce218d38": {"node_ids": ["0a2b2869-b579-4b93-a420-73182e14a780"], "metadata": {"page_label": "3", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6e1e3990-cfa5-4bbc-980f-a8e3a7a2b9e9": {"node_ids": ["bf459a42-d58d-4a62-9379-7e579bba9c57"], "metadata": {"page_label": "4", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "268eabe5-f4e3-4f49-9e16-049d211c0a4f": {"node_ids": ["fea18bae-690f-454d-9427-bd3039f7c5be"], "metadata": {"page_label": "5", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7a6ceddd-0d6a-4d15-89d0-3ea9d8c02b9a": {"node_ids": ["1f6a5995-e723-4227-a57a-89c267b10c9a"], "metadata": {"page_label": "6", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "90f29465-ad78-4161-bdd4-4ae10781de61": {"node_ids": ["aa6d42ac-4c18-4dde-b74c-06217b84e8b4"], "metadata": {"page_label": "7", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9b4aba06-fc20-41e8-b5d2-f09aa3342cc2": {"node_ids": ["c36801fb-e233-40ea-b740-562992b9cff5"], "metadata": {"page_label": "8", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4f3ac8ae-3b5c-4aad-aea4-d16630952fae": {"node_ids": ["3f110e45-85d8-4ee6-8215-31ef759867a5"], "metadata": {"page_label": "9", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3f851b04-1eb8-4853-8cb2-6522a9935683": {"node_ids": ["5881026a-a66b-4449-93df-a2eb9e9fd38f"], "metadata": {"page_label": "10", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2d9331a7-06bc-4156-a1cf-c028d1b8f952": {"node_ids": ["01a51abe-c613-4a2f-8b18-eaa5607a0c0f"], "metadata": {"page_label": "11", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "993ecd16-3dbb-4ea7-8382-601df0df9a73": {"node_ids": ["93f3b5b6-01ac-4cd1-8473-ac16d244aa27"], "metadata": {"page_label": "12", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d3353383-62e7-460c-9725-5c306b9e0579": {"node_ids": ["13975d75-bf86-4362-8056-afadc898ab21"], "metadata": {"page_label": "13", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e26e1194-279a-474f-bf68-a760cc5f5fe2": {"node_ids": ["3137bc4c-ddf3-466c-8443-480352a6fc6f"], "metadata": {"page_label": "14", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "30aa02eb-ec59-4612-b2fe-c41afbf31f05": {"node_ids": ["7df582c1-602c-4ae5-9472-b8cdd17100be"], "metadata": {"page_label": "15", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1b3e409d-a989-46f2-a0e8-e68eeb5bed44": {"node_ids": ["f892c4cf-8c0e-4278-bd9c-95a8c6225667"], "metadata": {"page_label": "16", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7f893caf-6c66-4b59-bbd6-636fb8686bea": {"node_ids": ["a586481f-c25e-4ad9-bc02-65108fb45c8c"], "metadata": {"page_label": "17", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0b059111-7d3f-4f17-a0d4-e5c40ca3df67": {"node_ids": ["78b2ab0b-58ff-4e42-b704-d156302e243e"], "metadata": {"page_label": "18", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "71d356a3-d694-40ad-9d24-75e98adb1162": {"node_ids": ["599dbd15-a868-4dfa-81ce-1175022507a9"], "metadata": {"page_label": "19", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f9dcc3c3-a559-4304-9591-3304f007e38e": {"node_ids": ["2bfac92d-e15a-47b8-bbad-cef729e947e9"], "metadata": {"page_label": "20", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "80fde06b-e558-4d1c-8b95-96f704a0c8ed": {"node_ids": ["a66fb34c-e373-4216-90d6-cef2b90fcbed"], "metadata": {"page_label": "21", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "faa5f3df-ccc3-4ce9-9ab7-76660c24116b": {"node_ids": ["c8ad36c6-143f-4a88-b578-a7a3ceaf3953"], "metadata": {"page_label": "22", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "193bd3b5-7552-444a-b940-2d54c3606127": {"node_ids": ["e9da1c07-f300-461e-84cd-c6f7f470907d"], "metadata": {"page_label": "23", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d48f4050-f2ea-43ff-9c61-14a2b05dc1cf": {"node_ids": ["e724e379-5f22-4552-aa6d-c09866d2414d"], "metadata": {"page_label": "24", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8741c4dc-bfa3-4852-b7b1-832e8a24b54f": {"node_ids": ["32f0698d-a8d9-4a9d-a2a4-d60be47a111e"], "metadata": {"page_label": "25", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "adede142-f86b-49fc-b002-f2f3a5c50505": {"node_ids": ["2e1a139e-eb22-4214-ac35-fd7f1ea73c92"], "metadata": {"page_label": "26", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ed028e55-e520-470c-beea-6aafeae3f4a6": {"node_ids": ["6a4afd83-d4e5-4abc-8e56-138e285a0439"], "metadata": {"page_label": "27", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6d1b6549-22b7-4e24-ada5-6a9502eaca5f": {"node_ids": ["15b531d9-d868-42ef-972c-9f35b91b8797"], "metadata": {"page_label": "28", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a07592f2-ca99-4d0c-aaaa-90ab9fbfa598": {"node_ids": ["70443837-b621-4d2d-9952-16940a6111d0"], "metadata": {"page_label": "29", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ff1812b8-79cf-4da5-982e-7ee198ff9407": {"node_ids": ["81f4ac52-a5bf-4771-a1a8-676eb94a99a1"], "metadata": {"page_label": "30", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9734d0af-7af2-4e4d-a5b3-19545f934a24": {"node_ids": ["52a0de85-65fb-4c08-9bce-2569ffeae690"], "metadata": {"page_label": "31", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2612028b-7dd1-4ec1-a359-3a3870825446": {"node_ids": ["bfafcf4f-5410-47a3-b048-8ef5223f6654"], "metadata": {"page_label": "32", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f02f67e8-15b6-4701-8f7f-672b35b4b2da": {"node_ids": ["bc4f408b-8989-401c-ad64-a9ef412fd064"], "metadata": {"page_label": "33", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4008b542-5c38-412d-a8b2-0ec2b53e812d": {"node_ids": ["33ce1c7d-dc1a-4e46-857d-8ea2cd3cf2b6"], "metadata": {"page_label": "34", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "66ec205c-ca01-487c-a425-779b8385ce03": {"node_ids": ["f5e2a343-170a-4139-8156-ce12a9577b72"], "metadata": {"page_label": "35", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6f81479c-1371-4697-ad90-042521adbb4d": {"node_ids": ["4fd92611-7fab-4441-8794-92a5910b24d5"], "metadata": {"page_label": "36", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bba7f5ba-dc84-4b60-83c7-eb40367fc866": {"node_ids": ["0ab73145-2116-4c2e-b60e-f86a33226e5e"], "metadata": {"page_label": "37", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bd52669e-6b5a-49e8-af5f-d8d98e7cdedc": {"node_ids": ["59e90f1d-11f6-4fa6-be21-de0d8e7da99d"], "metadata": {"page_label": "38", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c6150d04-f7c4-4dcf-8c21-87caeda2ea2f": {"node_ids": ["69fb3b61-5fe3-436b-bb81-e079607b99e7"], "metadata": {"page_label": "39", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ee15e2dc-1078-4e4f-a25b-31828a338d6c": {"node_ids": ["b17234be-52da-4b46-a51a-1cb5333a1d5a"], "metadata": {"page_label": "40", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cb7f96df-28be-4cdd-bbec-203a49fc8c6e": {"node_ids": ["0144706d-f33b-4f05-af65-36d213bf4dc2"], "metadata": {"page_label": "41", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0a8981be-7da6-461d-a7a7-fcf1249dce5d": {"node_ids": ["f21c069b-d7e1-413d-9e34-146fab6365b0"], "metadata": {"page_label": "42", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "79e09c87-7906-4c1a-9b88-d93cc0ef91dd": {"node_ids": ["05075055-abf5-447f-b648-c7ee9dfadc97"], "metadata": {"page_label": "43", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ed575f07-71c8-4abc-8fcb-b626e8a8e240": {"node_ids": ["c77e64e3-6c4a-420f-b00a-3edd2cb474f1"], "metadata": {"page_label": "44", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "42c4ac4b-cef1-4d0a-8fce-d4e40198749b": {"node_ids": ["f863320a-4fc3-4296-8916-c7da2406d2fa"], "metadata": {"page_label": "45", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b2399847-6c4d-41d4-bcd1-9721215e597e": {"node_ids": ["c2ed04a1-98db-46e5-9381-634f0f2565ba"], "metadata": {"page_label": "46", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "11d4ef51-b189-4038-9718-c28622a03dbf": {"node_ids": ["a9a90cea-6a16-4cff-a549-447cee2b9d0a"], "metadata": {"page_label": "47", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "787d7e78-b7e6-4afc-a43b-b08a2208eca7": {"node_ids": ["7148b9e3-1f6c-4e4a-b15e-94d4daf221a3"], "metadata": {"page_label": "48", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e5a07e84-b24d-48e3-bdc4-869410dbe674": {"node_ids": ["60ffbdef-f205-4c67-8592-b086ddcdb97f"], "metadata": {"page_label": "49", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b1e526fc-e769-4f49-99e4-3e1247d91071": {"node_ids": ["cf3209ba-d448-4555-880d-53162fe979f9"], "metadata": {"page_label": "50", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "93848c46-d556-4e4e-8e0a-0241df0fa7fc": {"node_ids": ["617bf87d-2e1e-4115-867f-cbae3746b6e4"], "metadata": {"page_label": "51", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "39fc460d-a7b7-4bd7-8ee1-459e7470c5df": {"node_ids": ["b9933bd6-f123-4258-95b8-f385b5513fc1"], "metadata": {"page_label": "52", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e7968dcd-5cd1-405b-8bf7-0d5323556670": {"node_ids": ["7927626e-d9a0-4bbe-bbf8-81369eb40728"], "metadata": {"page_label": "53", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d1bb0de0-db2a-4655-af02-fb158c4efc55": {"node_ids": ["bb2d9984-67b7-4d8a-82b8-91da3e8d5148"], "metadata": {"page_label": "54", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1a032f26-cfb9-483a-b87b-d2fe24205f08": {"node_ids": ["0cad6b4c-576d-4218-8ec2-fa461df72811"], "metadata": {"page_label": "55", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6b4abbce-a384-41fa-8fae-4031aeca7d2a": {"node_ids": ["e421b952-d4f7-48da-9fbd-bcb756e1b7ca"], "metadata": {"page_label": "56", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f8abe648-a4f2-4069-a5f8-4a6f68112cf1": {"node_ids": ["12ad2c3f-fd69-4ab4-b29d-3d46696cbb8e"], "metadata": {"page_label": "57", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e1cda2b2-4e87-4c41-8b20-d709a342f985": {"node_ids": ["ed4790d1-f0ea-4240-ba29-ee1f4c0b5a7f", "f40f765b-1d64-4843-9d95-effd0747b318"], "metadata": {"page_label": "58", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c29adde6-2d1a-4aab-bc2a-c6cad078aafa": {"node_ids": ["f1727c6a-636b-45ca-8997-946b790d86ca"], "metadata": {"page_label": "59", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1f3e80c2-dfac-4a3a-acdc-bf2563ffc259": {"node_ids": ["05b4a805-7dea-4c47-928e-8aeed6583602"], "metadata": {"page_label": "60", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "27e666d4-ce96-485a-b671-d3281a6b8c4f": {"node_ids": ["4b4ddc73-e2c4-427d-a387-a1fb531a33ed"], "metadata": {"page_label": "61", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6d2a4fc0-a7b2-45b4-8cc4-a0e55285a204": {"node_ids": ["d79ded35-b431-4d0d-b983-885f8e797e06"], "metadata": {"page_label": "62", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e8f043b9-8820-458e-aba2-87c03b8ab06e": {"node_ids": ["ed3a7d16-b2a6-453a-b0b8-d93bd8855ec8"], "metadata": {"page_label": "63", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a6d4ada6-5953-431d-954c-d724b8e51203": {"node_ids": ["575e57ca-c73c-4351-b8e9-f2e0bbf09631"], "metadata": {"page_label": "64", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "56155bc6-0e0f-4be7-b8ac-3b92c68e2770": {"node_ids": ["2106326d-5ce7-4f94-af00-bcab066aa183"], "metadata": {"page_label": "65", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "28ceeab6-898e-4fc2-9a56-bb198d4cf3b2": {"node_ids": ["736add59-f22f-4992-a026-4f582564cff0"], "metadata": {"page_label": "66", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "803921da-05a0-4faa-ba0f-829ad3c86c23": {"node_ids": ["4f0428e4-b33d-410f-ad9f-7361b3ebd78d"], "metadata": {"page_label": "67", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cf91f90a-a791-4855-9d9c-a3a341d4ac7d": {"node_ids": ["e0d298e3-4d87-4d4c-949d-783d564177eb"], "metadata": {"page_label": "68", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5e2f61ef-8c36-4930-a332-3d35c3b57010": {"node_ids": ["bffa3e25-34bd-48b2-826e-eb11dd21685f"], "metadata": {"page_label": "69", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "25f44172-c490-46ae-be42-a56b3b190c37": {"node_ids": ["2c3848f2-be86-48a8-b9f2-de60fb766da9"], "metadata": {"page_label": "70", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9d28a70d-bf62-4b79-9a26-c305d73f9dcb": {"node_ids": ["29a3574e-bef1-4177-8feb-021d55c608a2", "9f0aa0db-2a4f-4c48-80fc-150e8d50b22a"], "metadata": {"page_label": "71", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cac64a08-aa40-4215-b4b4-d4913d584a9c": {"node_ids": ["51117917-f541-4cc7-8ecd-4a4e0657e508", "8ba4b377-fb2e-467f-acf2-01cf80f810f8"], "metadata": {"page_label": "72", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "97a80c91-b707-48ee-ab1e-5d5278ecc716": {"node_ids": ["e6edaedb-a003-4b8d-bab2-a55c8ac57c39"], "metadata": {"page_label": "73", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "69ede226-5ed6-4900-8d9f-28e503e72a09": {"node_ids": ["a9a05212-276c-47b7-bffe-24e43ae969c2"], "metadata": {"page_label": "74", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "848d32f0-ee97-43bf-aabd-06716aa69d77": {"node_ids": ["969b1d9c-a631-40ec-b3a1-ce660e929dee", "8666de96-70ca-4cfd-8440-d0f5c0a7f153"], "metadata": {"page_label": "75", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3a8aeaa5-ec81-4fa6-b412-56143a889c20": {"node_ids": ["06f2257d-a580-4a11-a926-8e5f7ec6a48e"], "metadata": {"page_label": "76", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9552ac9c-417b-4475-bb3e-88ab9a6ad5bd": {"node_ids": ["eac017ef-b37f-400a-b548-37df4aff5052"], "metadata": {"page_label": "77", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d11e5500-70e5-4394-960b-8f754e82f6ba": {"node_ids": ["fb5ccd20-5864-46d7-b36d-e3439f394294"], "metadata": {"page_label": "78", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2bae5020-5188-4ec2-ac64-e013c7128923": {"node_ids": ["09aefb07-ba76-4618-8084-06f8842fbcf7"], "metadata": {"page_label": "79", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "45f1c665-9493-4da1-b1d5-86c00104e08e": {"node_ids": ["7974e510-bc8c-4e80-839a-0461fce8a316"], "metadata": {"page_label": "80", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9e55930c-82a8-4720-977e-8af40392cc43": {"node_ids": ["6470f9c7-42a3-4a6b-ace0-c105308081dc"], "metadata": {"page_label": "81", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0a6174e1-6ee8-4abe-b476-7a563e647b47": {"node_ids": ["d6e8128e-529b-4793-a39d-26ac11e59d8b"], "metadata": {"page_label": "82", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2700a2ec-d2c4-42b3-af73-4dce9696eb4a": {"node_ids": ["4c4dd99f-c5cc-42b3-925d-206818b37ba3"], "metadata": {"page_label": "83", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0366dec6-afdd-45a9-922a-34422fc937c7": {"node_ids": ["c8521df9-447c-4cf0-b5f7-9be268d98b2d"], "metadata": {"page_label": "84", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d28071a5-dc63-4768-9fa2-42a47d76b49a": {"node_ids": ["294f9388-866e-4131-ab0c-6aa77ec13715"], "metadata": {"page_label": "85", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "54691a2f-89cb-4fbc-8a2d-6d716e7c6e19": {"node_ids": ["509c1128-7fc7-4729-b016-f13b29faf07e"], "metadata": {"page_label": "86", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d623e7b1-09ec-4870-9c38-647ac09d03f1": {"node_ids": ["cd02a707-db52-45c7-940d-67ff2b765fa8"], "metadata": {"page_label": "87", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "30969199-6085-4578-9221-f1f440c1ac40": {"node_ids": ["f466afd8-29bb-4f73-afb3-385acc39be72"], "metadata": {"page_label": "88", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "97858353-81ad-429d-b869-b24eca3c982d": {"node_ids": ["2876c355-820e-4e31-a68b-e3e831163e1b"], "metadata": {"page_label": "89", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "28b424a1-5b01-4b13-81c1-c0cd1f6ae853": {"node_ids": ["d4e0783a-733b-4775-b0f9-b85c9ec0aaa5"], "metadata": {"page_label": "90", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "864e8da8-17d3-4d13-aa5a-b3a70fa7a20c": {"node_ids": ["eeda7dda-8902-430e-a000-596106f39ab9"], "metadata": {"page_label": "91", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f5a0d173-0573-4744-97b4-f39dd4091f5d": {"node_ids": ["cb341ae3-1545-4440-8aca-473cfc507e78"], "metadata": {"page_label": "92", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9bb7a012-7163-443f-ab82-2ad62999d93a": {"node_ids": ["8da91da0-7a85-4577-b293-3efd2f617353"], "metadata": {"page_label": "93", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b0790221-ca80-4f64-b6ea-69e125aa6c0c": {"node_ids": ["cf7bbc38-4df6-45d5-835b-2989e2d1c95f"], "metadata": {"page_label": "94", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a9cb6792-fd5e-4a72-8726-52d9d4173f2b": {"node_ids": ["e9978a6f-0c1a-4705-8679-7207ffec10d9"], "metadata": {"page_label": "95", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6ceb790f-eaa2-4f07-bd38-bb4386e9833b": {"node_ids": ["bc6804f9-92c5-43d0-beb2-2fb8bca17404"], "metadata": {"page_label": "96", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0e6ee11b-09c9-4283-beb3-dab424a3b7e5": {"node_ids": ["86665916-c47c-43a5-b17b-6b5987b7e8a6"], "metadata": {"page_label": "97", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "569b404b-52eb-45bc-8e55-42687ac31a94": {"node_ids": ["4b634dba-961f-4f7b-a842-4c67be2401c0"], "metadata": {"page_label": "98", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6be64de5-ba13-4db8-9654-11e5f88ef360": {"node_ids": ["89c4e52b-dc97-4b33-990b-32116886e6e7"], "metadata": {"page_label": "99", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1bde34fa-52f4-4540-903a-12463f03952f": {"node_ids": ["bbf8f5f1-bb56-40fa-8ea7-5dc974aca21e"], "metadata": {"page_label": "100", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a4c1b0ff-79d1-4310-953d-e4bf3b9c6b66": {"node_ids": ["a0aa3d46-2046-42af-85c6-94ca5e8e19f6"], "metadata": {"page_label": "101", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "56dde7c9-f983-4dcf-86f0-c60ad35facb4": {"node_ids": ["eafde4f0-0216-4015-ad84-bd56047bc7d5"], "metadata": {"page_label": "102", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "971983bf-9542-4e21-8260-63bff691c029": {"node_ids": ["1dfb7ead-0bb3-4f09-9088-e6dc45de257c"], "metadata": {"page_label": "103", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c5cdc269-d563-481b-8d71-1a0cfcfb3e0f": {"node_ids": ["96d906e9-1754-4878-85b7-63c17c3ad108"], "metadata": {"page_label": "104", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d1e6b0a2-cdda-4bc3-86e0-e204b8181237": {"node_ids": ["319c6702-0378-4b39-a163-0b5f428d362f"], "metadata": {"page_label": "105", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "84050f23-07dc-4fe5-a089-277cd68a87bc": {"node_ids": ["42271eb6-dca6-4e99-acc4-d80224cadcb7"], "metadata": {"page_label": "106", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "99126150-4bad-4350-8ce9-920e7cbd7884": {"node_ids": ["3add66e8-617f-4a99-b0e8-7c1d93cbecac"], "metadata": {"page_label": "107", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "75c67326-a331-4e8d-8832-f162f63b444e": {"node_ids": ["f0cee30a-50d7-4f98-9e40-756259482c1f"], "metadata": {"page_label": "108", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1ca6cce4-e72a-444f-ac2a-43deff0205ad": {"node_ids": ["44351a43-2794-4c66-af29-ddb2480e68a9"], "metadata": {"page_label": "109", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "82bc8d08-c1d9-4a3c-aa47-14524138935f": {"node_ids": ["0f7b4159-17aa-4ce0-9224-598c462466e1"], "metadata": {"page_label": "110", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d2d3d345-8fa6-4374-b0c3-288706de72bc": {"node_ids": ["036deeaa-89ce-4b84-87ae-4b6aec4c3906"], "metadata": {"page_label": "111", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "faba9fee-404d-4c59-874f-f57f0d4703d8": {"node_ids": ["2004dbb0-5b90-45bc-94cc-60e9002ad618"], "metadata": {"page_label": "112", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "804d66b5-09ea-46e4-9561-6ad0cca74770": {"node_ids": ["17be11f2-534d-4e92-891f-8a9549880701"], "metadata": {"page_label": "113", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2fafc2ac-86cb-4ea6-bae7-ace0563ccf3a": {"node_ids": ["73591ad6-2183-4757-870d-aaf315d238f6"], "metadata": {"page_label": "114", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fba1c0eb-4ac1-4887-8978-dcddbf45a5c1": {"node_ids": ["13f15cef-a770-4857-95e0-2f3119d4eb8b"], "metadata": {"page_label": "115", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dcc7dafd-c7b5-43a1-90d1-057288eaac32": {"node_ids": ["0d06b098-2efc-4e00-8176-e3317085491a"], "metadata": {"page_label": "116", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "be20631e-77f0-4d7b-9fbc-f9a796bdb73c": {"node_ids": ["ba999abb-a9de-4d90-8330-975f4054dc3c"], "metadata": {"page_label": "117", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a983d791-6e5b-408b-9c31-46bfd637b3bb": {"node_ids": ["62a8f625-ca96-45c3-9693-dd39c109f849"], "metadata": {"page_label": "118", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6bd754d5-a894-4bfc-aa05-9debd265d1e0": {"node_ids": ["1b81eff3-a9e4-4974-9511-ecf9212c381b"], "metadata": {"page_label": "119", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2493d4d1-7e7c-45b6-b8b0-2733e1314d1e": {"node_ids": ["95f21d77-6be0-46c9-935a-8c42d92c0d94"], "metadata": {"page_label": "120", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cf099bf1-4332-4afb-ad82-a6275a7eb773": {"node_ids": ["c036cec1-d482-4a2c-8bdc-ca84eba44926"], "metadata": {"page_label": "121", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b7274fe8-3501-49ce-8f1c-0bdb2aa36caf": {"node_ids": ["747483d0-42fb-4fba-885f-3d954df4f25a"], "metadata": {"page_label": "122", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b4fd42a2-63c0-436a-95f4-2a50666a1fcf": {"node_ids": ["f48e5092-4abf-4d85-a15a-0d2726f7ac58"], "metadata": {"page_label": "123", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0e0dded5-3b0c-44a5-b22b-7fa94b52cbba": {"node_ids": ["2117805d-9a58-4a7e-b576-004b404a0bec"], "metadata": {"page_label": "124", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2b0f7f56-b153-4d06-8ed2-6ea9deb16314": {"node_ids": ["7b794edc-9173-4211-bf66-fd3045ebbb01"], "metadata": {"page_label": "125", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ae499a2b-cb67-4cf1-9be7-4e4a76a3e268": {"node_ids": ["35db387d-8474-4764-8f7a-523133d79742"], "metadata": {"page_label": "126", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dafbb433-ef7d-40eb-a3d1-4ebb61517bc5": {"node_ids": ["e1b95781-f098-4e55-8cbf-054096b0df62"], "metadata": {"page_label": "127", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "987ba5bb-fb6c-438d-80c0-d289953ccd14": {"node_ids": ["fc6bc613-f563-4787-b42a-22d0e744062a"], "metadata": {"page_label": "128", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a2441bad-0800-4101-8281-e9ad2a57ebfe": {"node_ids": ["02a2fe03-451c-4a9d-870e-7c09af97640c"], "metadata": {"page_label": "129", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3cf8e927-ead8-4e2c-b8e2-936210b5079a": {"node_ids": ["7e0aa599-9a17-4f26-818d-a7656ced225a"], "metadata": {"page_label": "130", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c7616701-0aa2-4f09-b512-aa7d5aa87759": {"node_ids": ["46a4558e-5126-43d4-ac24-16dd16cf64d9"], "metadata": {"page_label": "131", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1b0baee7-0595-43b6-ad90-0ea07d2c0955": {"node_ids": ["5d492a7a-2898-446c-b4bb-58336d18d378"], "metadata": {"page_label": "132", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "31d46ea6-d267-4628-9924-c721964817ef": {"node_ids": ["66869203-ddad-4fd7-af5b-44813d2a5e8b"], "metadata": {"page_label": "133", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "282005e1-8b2e-400f-907e-6911914aebee": {"node_ids": ["b05988b5-4961-49f6-b31f-faf5e330ba32"], "metadata": {"page_label": "134", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d6b007eb-8da9-464c-8176-6de8cd446cb6": {"node_ids": ["207acf91-d020-47df-9576-e8cac033ee35"], "metadata": {"page_label": "135", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ff910460-6fad-4b43-8c95-f24a6fe9f6b6": {"node_ids": ["2c0cce0b-db98-49c1-bb13-3c0c3e230c70"], "metadata": {"page_label": "136", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e9f842b8-ad30-4479-98c5-3845cc22a343": {"node_ids": ["6cf94ed3-8517-40ec-baad-09f03de870f1"], "metadata": {"page_label": "137", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9097fd4f-f17d-43e5-9f3b-05d277ea6ff0": {"node_ids": ["867721e4-aa7a-4afc-8f84-87bd9670a49a"], "metadata": {"page_label": "138", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7f889c2b-dbee-4f51-accc-8ba45db25e5a": {"node_ids": ["998945fc-69b6-4897-8dda-8c3c098c41a9"], "metadata": {"page_label": "139", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d176f800-d9c2-4502-8c4f-4a8ba4b98455": {"node_ids": ["0289e801-99f3-47bf-b3e9-f0ca9829e04e"], "metadata": {"page_label": "140", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "426b8838-6dbd-4ce1-9e62-4a7dd5d25754": {"node_ids": ["8be326e3-1e82-4c2b-b72f-4d2c1893dc31"], "metadata": {"page_label": "141", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "51052632-3339-4f15-8a1b-d50e5d13eb34": {"node_ids": ["6c922f94-d980-4159-88ce-2671031cc20f"], "metadata": {"page_label": "142", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ab13f966-da7f-4b06-b8b9-f67d9caa8262": {"node_ids": ["d0c07562-2367-4994-8642-ec514c193659"], "metadata": {"page_label": "143", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2060e13e-9074-422c-9308-ecbec79611c0": {"node_ids": ["ccb78050-de0b-4ce3-9c79-8c07f7a4c288"], "metadata": {"page_label": "144", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4460bf1d-a326-4c70-927b-fd9e381a6eea": {"node_ids": ["68a061f8-ef31-48f9-8961-08613889e968"], "metadata": {"page_label": "145", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d4296d53-305e-42c1-9629-0491912b9fe0": {"node_ids": ["80c04846-b36e-475c-8109-32ac11cacd27"], "metadata": {"page_label": "146", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d5c0917c-aaa2-4077-bda8-bd5e838abc85": {"node_ids": ["a41a563f-b8a0-4304-a83c-24d02dbebca7"], "metadata": {"page_label": "147", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3dc3e4e8-6507-42bd-ad99-1914b4e246c6": {"node_ids": ["1c2a6668-144c-4172-9d82-50e86c9ef006"], "metadata": {"page_label": "148", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "71b68203-775b-420b-9a36-4a04646551ce": {"node_ids": ["4ef6463d-e027-4487-b5f9-00ec23d1bd01", "6de3f18f-bb5a-496b-bc96-c1218574d6ab"], "metadata": {"page_label": "149", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "43b2b1d8-83c4-48cb-9866-0023673d0dd9": {"node_ids": ["910ca6ab-ef48-4277-989f-7459461e69a6"], "metadata": {"page_label": "150", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ca7fe14a-71f1-4eaf-92a6-9bec7f3e72b2": {"node_ids": ["f0673e0b-28c9-4291-937d-1d3db28782a4"], "metadata": {"page_label": "151", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cddbd3f0-bfbd-4f3f-aef0-876e8aeac72d": {"node_ids": ["508ce776-d7d8-4be4-9c09-13d13ad53257"], "metadata": {"page_label": "152", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "020a85a8-2d42-4824-abfc-d2f43e8d9c90": {"node_ids": ["fcee35f1-d391-405c-990d-b33f8f84f4d0"], "metadata": {"page_label": "153", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "68613df9-b843-4633-85b6-6322b0fdf762": {"node_ids": ["1787ac8b-fba3-40d4-9275-67a7cfc44975"], "metadata": {"page_label": "154", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "32d46f67-6973-434f-aa47-448cacbbfc7c": {"node_ids": ["bbb70f70-ef7e-4984-b68f-82863556fb55"], "metadata": {"page_label": "155", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a0270026-978b-4cd9-9469-10c9a2783bbc": {"node_ids": ["8ace0f10-9738-4194-93bf-3185237d5adf"], "metadata": {"page_label": "156", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "130ca9bf-f780-4912-9725-094e49f23144": {"node_ids": ["fb30c50e-4207-42f5-862e-b28ac7f9eeae"], "metadata": {"page_label": "157", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "eefd56df-5adc-4e44-9a3b-cf43bb829d9c": {"node_ids": ["cdfba9ed-f06e-4608-a36b-0e4811b68b05"], "metadata": {"page_label": "158", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1288119e-1daf-4d9f-bd44-30f7b889d380": {"node_ids": ["bdd044c0-1499-4158-95bb-b9ffa9649cbb"], "metadata": {"page_label": "159", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "109b9958-d9cc-4932-a62c-aa178e3a8102": {"node_ids": ["23260f41-50e1-49e2-a38a-9790e4193406"], "metadata": {"page_label": "160", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b3d735f8-b4b2-44a0-b67b-2acb36a81124": {"node_ids": ["e01b0767-66e1-40e5-8feb-f605a505979d"], "metadata": {"page_label": "161", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ff465812-ef9b-474c-a26d-9b496e5a3f69": {"node_ids": ["188a4439-dbbe-4bee-a792-7200725bc32f"], "metadata": {"page_label": "162", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "64efa05b-1227-45cf-b1e0-f985f402d138": {"node_ids": ["1f8db9e9-0810-432e-8b2c-13fa9c2c4cfc"], "metadata": {"page_label": "163", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "74d213ba-51ce-43d9-9c4d-b33502bebad6": {"node_ids": ["8119c2d5-58cf-42e9-8db3-653f92dad061"], "metadata": {"page_label": "164", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bf5d0c5b-d9fd-497d-ae87-93c4bf9b7db2": {"node_ids": ["26de3d5a-f4ff-4e08-b930-b73df3a5f8ae"], "metadata": {"page_label": "165", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "160b30d5-c10b-4dbb-81f1-dbddc080128a": {"node_ids": ["9707589f-18f7-4e3d-9a23-f4b0fae6279b"], "metadata": {"page_label": "166", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a684543c-2b4e-402c-8e26-134a0476151d": {"node_ids": ["a159fd72-3c5c-4108-a74c-9d82a16ea43e"], "metadata": {"page_label": "167", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f421a9b2-8b56-49da-9723-c20ca39e00d8": {"node_ids": ["efe732cf-652d-4340-b1a5-5a129c547507"], "metadata": {"page_label": "168", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4d0945b2-4bc9-4368-8a51-331d967ac990": {"node_ids": ["4629030f-1eaf-4f74-a973-8c9ff7e6fee6"], "metadata": {"page_label": "169", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e56e9c4c-7541-4cd0-9af1-0a4d62942f41": {"node_ids": ["c4978ed3-e1f1-4cdd-afcb-3f626e5b84c8"], "metadata": {"page_label": "170", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8a07d81d-6212-4c73-a92c-c0d2c08bfdcc": {"node_ids": ["f12838b1-4e79-482a-9104-1e25b6de5975"], "metadata": {"page_label": "171", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0c1b7c52-752b-4294-9cc2-7a5f135261e2": {"node_ids": ["11b75de2-618a-49d3-bce0-1846f4c980d7"], "metadata": {"page_label": "172", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8c4f8a63-5980-41d6-b3f0-979752b4e046": {"node_ids": ["aba8a10a-1cbb-4fa7-8c5a-d03b2388643d"], "metadata": {"page_label": "173", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ced0416e-6f27-44db-850c-26d825932625": {"node_ids": ["f6715a9e-9698-478b-b4cf-ae7a02e07d1e"], "metadata": {"page_label": "174", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9789ce45-2c2e-478f-b6aa-e448fd1f89d7": {"node_ids": ["71841c1d-deb5-4d16-85da-f5057dbe5d65"], "metadata": {"page_label": "175", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "55ee68cb-4244-49e7-b6d0-a9019bd95f3c": {"node_ids": ["0ecc5feb-aceb-42c4-9bef-40e8fc9a30f1"], "metadata": {"page_label": "176", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b55f1c7f-c0bb-43f5-890b-1b4d4c815996": {"node_ids": ["b7588a18-1179-4054-9bc4-aee080d59feb"], "metadata": {"page_label": "177", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7706171d-8a61-4005-8196-ff562817410a": {"node_ids": ["ecb9610f-ff79-456a-b464-af4afe3fc9fd"], "metadata": {"page_label": "178", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fadc733f-1d1a-49d1-9d24-b20379f32cbb": {"node_ids": ["bbfa5a24-5570-40ab-9e88-1070ff3dea05"], "metadata": {"page_label": "179", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e84242c7-f9ba-4b13-84d3-28d371e8cbe7": {"node_ids": ["0487c5ae-0bb0-4326-92c6-8b0c9de1aede"], "metadata": {"page_label": "180", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "28e368dd-8028-423a-a280-5c48f639af16": {"node_ids": ["4fc49fb2-2601-4038-ac8c-54869d048cbe"], "metadata": {"page_label": "181", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a699639e-bca0-4cd5-adb6-417d24d647b0": {"node_ids": ["31b2022b-c507-4d4a-ba85-88aa5fa57d00"], "metadata": {"page_label": "182", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6de7b98d-ff13-4fdd-ab73-f114d214d532": {"node_ids": ["17f799fc-0080-4c79-9532-021d4ed76562"], "metadata": {"page_label": "183", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d0942196-c0f1-4ea6-9fcc-8b32033e34f0": {"node_ids": ["b4192164-4d6b-410d-b76a-78558d81b6b7"], "metadata": {"page_label": "184", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cfeac3a4-8310-4bff-863f-1eab5428c119": {"node_ids": ["02af3693-dec2-4ef2-a225-139e5688eb6d"], "metadata": {"page_label": "185", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4679f4f6-fa1c-4b8b-a31c-3aef76b616fb": {"node_ids": ["b4b472a7-99a1-493c-83d1-d9afbec08591"], "metadata": {"page_label": "186", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f3096a67-e85c-4426-9183-9bce6c436400": {"node_ids": ["5982e40a-f318-4eca-9e3a-e56bf8621ae9"], "metadata": {"page_label": "187", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b01295f9-fbd5-4257-81f8-9f8e0e0d9761": {"node_ids": ["4555d321-a74f-4b1c-9226-3c1f57aaaa83"], "metadata": {"page_label": "188", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6a7cf54a-5c1b-47b2-a818-a8e69ad6c359": {"node_ids": ["7820a1b0-3956-459c-b1e9-6ada6498d8f9"], "metadata": {"page_label": "189", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f33f6653-5afd-4ff4-aef0-85f290e93a03": {"node_ids": ["42398a87-c411-4af3-be09-1d50e5a4822f"], "metadata": {"page_label": "190", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d374bf29-cec2-436d-9cff-ea1d908c4bee": {"node_ids": ["7133b477-be42-46bc-a1cc-1c65119ff934"], "metadata": {"page_label": "191", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b2c2a386-64fc-4042-bd59-0a7e627a16ab": {"node_ids": ["b205836b-b2d8-4f6f-91f9-83795f6eed20"], "metadata": {"page_label": "192", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f4a210c9-ca68-4289-8ba6-61a4979a31a1": {"node_ids": ["70bf1c15-5e06-418c-ba4c-568064428ffa"], "metadata": {"page_label": "193", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9b2696fa-239a-48e2-b1f3-36374ca3ab48": {"node_ids": ["b4b497d6-8070-4b37-bbbe-e642b7f867e0"], "metadata": {"page_label": "194", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "73bc6c90-2e8b-4c4b-9735-fadfe29b721e": {"node_ids": ["ab53b641-576b-4bd8-b5ad-b1aab0a4e94a"], "metadata": {"page_label": "195", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "33646249-555b-4a63-88db-42b1698676d3": {"node_ids": ["5ee8f710-08e9-44f2-8e0d-78a4df39ab1e"], "metadata": {"page_label": "196", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1a519a03-f2c8-4903-9701-574a501fc006": {"node_ids": ["41b2d0ba-bb59-43f9-be98-fbb171e67658"], "metadata": {"page_label": "197", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c6a1889b-6223-4696-bf0d-fb89b957e063": {"node_ids": ["7a8611c5-04ed-4728-80cb-59b1adf3a17a"], "metadata": {"page_label": "198", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cf477d16-c49a-414c-92b7-85d8f83e9791": {"node_ids": ["b134b31d-0896-478e-9f03-182501dfd95b"], "metadata": {"page_label": "199", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6c882f15-c630-4b3f-a944-5637186abcac": {"node_ids": ["dfa37cee-8dfb-4bc4-bde6-07d3e2b5bf80"], "metadata": {"page_label": "200", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e0934978-9309-40ff-96f4-5834ca9c6eef": {"node_ids": ["6ec048b2-c652-46e0-aa89-5632d8fca54b"], "metadata": {"page_label": "201", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "93cb5ba1-df53-493d-91bf-add9213e7d5c": {"node_ids": ["c118c3a9-01f7-4f93-b372-48f394797456"], "metadata": {"page_label": "202", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d1ba9f54-2dd8-4428-b80d-9ec40e9e8ed6": {"node_ids": ["f05aee89-a1c5-43dd-a0d5-d362dcc094b7"], "metadata": {"page_label": "203", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "07904c0b-1395-4ece-9f39-e95cb1463312": {"node_ids": ["2dfd7b78-c397-4997-93c5-13915867be0e"], "metadata": {"page_label": "204", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "61c47e37-f84a-49d8-9ca2-cce1a57b1e9b": {"node_ids": ["3f62d6b9-6613-4a32-bde9-1697596a054a"], "metadata": {"page_label": "205", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9521e2d2-600c-45f3-a51e-17d2e0a66468": {"node_ids": ["b1d91af9-b184-4a8e-8a55-3d41cb14e79c"], "metadata": {"page_label": "206", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3a01a01d-316a-4056-abbe-5c923357eb64": {"node_ids": ["fce3b756-3e14-4188-8246-553d8fd0bfab"], "metadata": {"page_label": "207", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ec9af7d3-c0ff-4bad-89cd-db63a60b8a37": {"node_ids": ["88f04a66-54bf-4987-9a85-aef746ce1545"], "metadata": {"page_label": "208", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "63251683-4b8a-408b-9bb8-2c2476718a2a": {"node_ids": ["a7d2e8d4-15a4-4a8a-97fd-83642ff5fcce"], "metadata": {"page_label": "209", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "91f81465-71ad-4397-8fe4-b85beac190d2": {"node_ids": ["1d7eabca-baf7-449a-b954-5ffbd17905f3"], "metadata": {"page_label": "210", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0e25104d-afc3-4ddb-b038-99e72e084c99": {"node_ids": ["bbeb9871-7723-4e10-84cb-da3dc14b2f2e"], "metadata": {"page_label": "211", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "caef289a-0146-450e-8579-d593be74aa61": {"node_ids": ["91df546e-9d35-4166-b260-2f65c0063dca"], "metadata": {"page_label": "212", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "96c46d1c-f132-422c-ba42-ec1cc9d4a37d": {"node_ids": ["40537af6-6198-4407-b8af-86f3dcc0c020"], "metadata": {"page_label": "213", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "254f834a-a45a-4c20-b76a-8dbd1bc5b0e3": {"node_ids": ["6b45b8b8-c0d3-4b55-a3cf-4ce4916c34be"], "metadata": {"page_label": "214", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e0ec40fb-9a7d-4fd2-924a-7c6b40e7a484": {"node_ids": ["4c314ddf-0317-471e-b4fc-7e8b5bd48497"], "metadata": {"page_label": "215", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5f38b593-f275-492b-9c6e-13c767e4a895": {"node_ids": ["1594afa7-6d8d-4451-abb9-1ae1c0e200a8"], "metadata": {"page_label": "216", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dcd94cf8-f137-4f97-8fd9-7477d7a91e15": {"node_ids": ["b3a331c9-35bc-43bd-97e8-c20deb8c9623"], "metadata": {"page_label": "217", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "68b9aa22-13f6-494e-b3a8-6335e09829eb": {"node_ids": ["42735e49-ae12-4963-9434-79e9e24ece53"], "metadata": {"page_label": "218", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0c85331b-8854-4dab-bdf8-3280c407e252": {"node_ids": ["968c399e-0c32-4318-a444-4df914f18a78"], "metadata": {"page_label": "219", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ab439d72-3acb-4a1f-9886-3b8916fa44b3": {"node_ids": ["2d45e234-135b-4694-aaeb-b61782370124"], "metadata": {"page_label": "220", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f789e0a7-0e28-47ae-ab59-3d0d9f179465": {"node_ids": ["5cbc5313-5de9-42f9-b17a-59fa71e8f6ff"], "metadata": {"page_label": "221", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d1f46888-af6a-478a-9d73-a6bf77e9220b": {"node_ids": ["51c2d5f4-7088-440e-87d2-a4b4fc1c19e0"], "metadata": {"page_label": "222", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "450ca569-6d9f-47c5-9a23-8f041807a11c": {"node_ids": ["2e4b19dc-ba5b-4021-a3f8-b2746542b558"], "metadata": {"page_label": "223", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3b6a2885-1200-4887-83db-4313e1e406c1": {"node_ids": ["a15aa4ae-db5d-43be-83e9-2a94eaa4346e"], "metadata": {"page_label": "224", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fc955964-ce62-4dbc-b97c-8ffe8153c8ea": {"node_ids": ["724fd726-c38f-4352-bc36-99456f123971"], "metadata": {"page_label": "225", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4f924cb6-2415-4d94-89ca-55397bc16c6a": {"node_ids": ["83328087-dd27-4a99-9547-6095b094bcca"], "metadata": {"page_label": "226", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "94b14313-2a41-4826-adab-0e98b7b93609": {"node_ids": ["39fc51ca-2f16-4bc3-a15e-06f1bba31104"], "metadata": {"page_label": "227", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cba11a72-fbbc-4353-a8e7-5295c2e84400": {"node_ids": ["43fe4339-f785-4a70-9c9b-721a95717416"], "metadata": {"page_label": "228", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5e83c011-99f1-4220-ba0a-f14481afd823": {"node_ids": ["a4de1a22-e54e-433b-86cf-deb3f493fc59"], "metadata": {"page_label": "229", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e657479d-ed7e-4588-9e02-315b87303f97": {"node_ids": ["6ea0b164-6607-4970-9c5a-18e0df76e625"], "metadata": {"page_label": "230", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "88cf75b4-4341-4b39-add8-71c8b8d85465": {"node_ids": ["c0cc9fbe-d9f1-4e0a-bf00-d0c9fdd87543"], "metadata": {"page_label": "231", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "18da970b-7bec-43c3-ae49-20589d503edf": {"node_ids": ["c14ea4b0-5afd-488d-b81a-a748eb84949f"], "metadata": {"page_label": "232", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e2481236-722f-4a2f-986d-8f9df3fcf96c": {"node_ids": ["d7227e4c-0a55-4c37-aa59-3592b87c2d75"], "metadata": {"page_label": "233", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1af1e19a-d55f-40a9-b23f-73e4f9fbff71": {"node_ids": ["6bdb2bfc-9dc4-456f-b591-f588cf284bf2"], "metadata": {"page_label": "234", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ee2c0684-bb02-4c30-ae8b-20050ba6dd1d": {"node_ids": ["40dd4549-8e59-4be3-b957-6d41371f357e"], "metadata": {"page_label": "235", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1d9dee5a-cb80-437e-800c-51aeedbbee79": {"node_ids": ["24a75ce5-5b5a-466c-a1e9-29c0c1483f4e", "0f12eff9-ff9f-4b16-94ea-fa40c478e339"], "metadata": {"page_label": "236", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "10274fe2-f588-4446-9cc7-85f785c0bb2c": {"node_ids": ["1ade65fc-7423-4f68-bc86-5fa719f1bb42"], "metadata": {"page_label": "237", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d4699e30-18f2-48c7-a579-769ec7cae559": {"node_ids": ["b4864571-2b3f-4cb6-97d9-f12a5f4eb5cb"], "metadata": {"page_label": "238", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9e4faa9c-a637-4b60-af72-fc6e78cff3d9": {"node_ids": ["0d8224aa-0b3c-462b-9c66-8d7cd3342bfc"], "metadata": {"page_label": "239", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d56510d5-914f-41ac-a9b9-2801aef9772f": {"node_ids": ["44fa7bf1-29f4-45bc-94d4-18ceea75c946"], "metadata": {"page_label": "240", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c8d32618-dd77-46de-88d9-0b268f7f5b89": {"node_ids": ["483f6f18-68bc-4e9f-8959-dc8553ae8a4b"], "metadata": {"page_label": "241", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5b07e079-d31d-4270-882e-316e48ee7913": {"node_ids": ["5a71e983-769c-44d2-bc0c-e2e46829a5c6"], "metadata": {"page_label": "242", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9008f41e-992e-42e7-ac6d-7e25b65e5f43": {"node_ids": ["8638dc00-f234-413b-a28e-33c7fdbef708"], "metadata": {"page_label": "243", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6bf7cb86-07a3-4796-840a-ce9ae4ef2cda": {"node_ids": ["f2ed8b6b-f5f9-4f1e-8f24-ae3b2a5592c3"], "metadata": {"page_label": "244", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "be66d327-b470-4998-8cda-08a4400075a8": {"node_ids": ["ab7bc87b-2e7b-4b0d-af8f-e24f28bcc636"], "metadata": {"page_label": "245", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f5d2b179-533c-412a-9b70-ee3360290f75": {"node_ids": ["f997f261-a64e-40fb-a857-8df56b419cb8"], "metadata": {"page_label": "246", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6578f30c-79e5-4de7-a6fd-cfaeda163f10": {"node_ids": ["02cebaba-a770-4ab2-8a23-2914fa239df3"], "metadata": {"page_label": "247", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6e9595d2-b7e6-4fac-89ce-23ae16b31517": {"node_ids": ["c44f7d26-c882-40dc-b0ce-bbbcfb24a4f1"], "metadata": {"page_label": "248", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2fa136e7-1ad9-4a97-8732-fdba735eca2a": {"node_ids": ["4a0f5c50-c3de-46b1-a256-3c7556c48010"], "metadata": {"page_label": "249", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "219bc4cb-e111-4b42-90d4-bb088057caff": {"node_ids": ["3b4521ee-3459-4c07-91e1-a55bdc18eb18"], "metadata": {"page_label": "250", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6e70b6cc-9731-41dd-a673-82eb52d6204a": {"node_ids": ["3f46cbe0-1b79-4c62-a805-07b054d1f586"], "metadata": {"page_label": "251", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9d115533-87a8-4a49-bcde-d0698f01f95a": {"node_ids": ["b1f4096a-2639-4b71-b403-6a3bab5e4960"], "metadata": {"page_label": "252", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4742d777-1323-4c6c-8a39-d544997464ff": {"node_ids": ["50d74fcd-e380-4383-82bf-7905581bd8e4"], "metadata": {"page_label": "253", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0eede7f1-dbdd-4084-a6ae-3edec85ca30c": {"node_ids": ["a1b14bcc-7f65-4ba5-820b-de94d15bb10c"], "metadata": {"page_label": "254", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7d29ebef-9e59-4f32-83e3-952018e16349": {"node_ids": ["a3c7b034-046c-4505-bd28-358180dce4eb"], "metadata": {"page_label": "255", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5d9e1f79-327b-4349-98ae-0909f1d20eaa": {"node_ids": ["43ca4f5b-6672-4e88-9a19-391df0ee778b"], "metadata": {"page_label": "256", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b1c1a9b5-7673-4e63-9261-279a897f5773": {"node_ids": ["dde949c0-78a0-40da-8ad9-2a3ecb602534"], "metadata": {"page_label": "257", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f7132beb-df11-4b51-8a84-b39cc2c557e3": {"node_ids": ["35ce7004-3c39-4604-90c9-30fb268bd43f"], "metadata": {"page_label": "258", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fe8ebbc1-5be8-46af-b5c8-d9ba47012550": {"node_ids": ["24f0e8eb-c3d8-4f20-a168-f47585aa0345"], "metadata": {"page_label": "259", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "20335c3f-0b91-447b-939e-af2e36e22355": {"node_ids": ["998d537a-a824-425d-b275-0f3a77767c73"], "metadata": {"page_label": "260", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fb3cca11-1b34-438b-a98d-8f55202472af": {"node_ids": ["567cb8f3-cd58-4708-8595-471f3fef1d32"], "metadata": {"page_label": "261", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "066828ac-6f9d-4493-8586-5ab0ab80fca2": {"node_ids": ["cd62ba68-90c0-4918-a4fc-fd3dc2085175"], "metadata": {"page_label": "262", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a66eadd1-079f-4afc-8483-fbdd56f3312c": {"node_ids": ["7a68d825-32a7-48ff-9bb5-31b3fad2a607"], "metadata": {"page_label": "263", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "551d0ed5-7629-4421-8a4f-41ce62de51fb": {"node_ids": ["851ac805-6411-42f1-86fb-8ccfc6d627e1"], "metadata": {"page_label": "264", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d3a9e826-5924-4523-b678-f5a8926c79ed": {"node_ids": ["480d0216-b617-44eb-94bf-09aee77bc2d3"], "metadata": {"page_label": "265", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d16353fc-f3c9-4b37-9757-265c6993b23e": {"node_ids": ["8c372395-a0ef-46b8-afe1-185d2222344d"], "metadata": {"page_label": "266", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "40271db9-e37b-4768-9a86-07238d1bd624": {"node_ids": ["0fd2e343-9acd-4b12-9d36-dd53e6670ffc"], "metadata": {"page_label": "267", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9fb68c1d-a2bd-47b8-be16-f8e32ffab8f2": {"node_ids": ["74b0b8de-94f4-40e6-ab33-ecfd07fa510c"], "metadata": {"page_label": "268", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "afce9df1-82b9-439a-91c2-585166713ccd": {"node_ids": ["b5e21c7e-c5a9-4f7a-85a9-d08d7d90b941"], "metadata": {"page_label": "269", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "385d68ac-6a22-410b-8b61-4f411d2142d2": {"node_ids": ["a36bd654-ffdb-4c54-ad1a-e9784d0f9f9e"], "metadata": {"page_label": "270", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bb998b89-ef7a-4209-8d93-d4330e4498d2": {"node_ids": ["1f98fa6e-2be7-44ff-8b2f-0d90db9e27ef"], "metadata": {"page_label": "271", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "200df845-02fb-424d-80ab-40a2bffa97f3": {"node_ids": ["482613fe-1d7d-4d6e-81f6-a549eea1a91d"], "metadata": {"page_label": "272", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a9af46be-9a60-4040-ac59-7ed70a33a7b2": {"node_ids": ["9eded505-7113-4d6f-aa02-42f709a5da66"], "metadata": {"page_label": "273", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "607f1e33-2605-491b-a23c-b8d81f574279": {"node_ids": ["fbe713bf-768c-4826-b541-800b2333a430"], "metadata": {"page_label": "274", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a744686d-1b9a-4f9d-aff9-2de55375798f": {"node_ids": ["9acd5be2-cb6c-4709-a496-14bbb8ffc34c"], "metadata": {"page_label": "275", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1dcefb27-b1a2-4c71-a453-bfb30aafe29a": {"node_ids": ["89b6a3e9-902b-41f3-8af3-6e4dfc7677dc"], "metadata": {"page_label": "276", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "864edd93-b1d8-4b2b-b4e9-3213cad3ef49": {"node_ids": ["45050730-de8d-4ee4-8af4-7ea0725d9577"], "metadata": {"page_label": "277", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "29aee9ab-d5b9-42e1-89b7-4c02e450513c": {"node_ids": ["334f86ee-0a05-42d7-be55-bd48c6cef81c"], "metadata": {"page_label": "278", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "016b6917-a164-4316-b09c-425ed49c2f2c": {"node_ids": ["a9eb34c8-683b-4bab-83b2-6d393c868de4"], "metadata": {"page_label": "279", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f29676bf-e0a2-465a-9166-f13f8e78abba": {"node_ids": ["cd2c3286-9b73-4433-82bf-1639d30a0533"], "metadata": {"page_label": "280", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a69dcd48-f575-4887-b361-345ad010b03c": {"node_ids": ["f597bd64-b5f4-4af4-9067-4c6ebf724b56"], "metadata": {"page_label": "281", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "31db41e1-2fad-485c-9e9f-55c0cab8bd8e": {"node_ids": ["e593a962-ee8d-4263-9d0a-898ad50ad91d"], "metadata": {"page_label": "282", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1042402b-bbcc-407b-a10e-233a280892e3": {"node_ids": ["a282996b-2ec7-4b94-bec7-ae02483990e8"], "metadata": {"page_label": "283", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6cc5711f-be69-4bd5-ad67-7e64f9cc7552": {"node_ids": ["be6e06db-9ab2-4049-a181-5c00e3bef0fa"], "metadata": {"page_label": "284", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "defd9b5f-5f24-4de5-aabe-73c2fe626903": {"node_ids": ["06d3729f-e058-473b-b7b9-f1bcf1607c22"], "metadata": {"page_label": "285", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e9c09b94-0484-43e5-86ea-a619d09dff35": {"node_ids": ["4983989b-04e5-4f34-a9bc-a18aa3f381fb"], "metadata": {"page_label": "286", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e2c03e3b-0410-4789-856b-2fd854d05840": {"node_ids": ["6f8ea305-5e32-484d-90e0-61a7d3da3311"], "metadata": {"page_label": "287", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1e45ce4f-4856-4da2-8c64-fbeb77e6bf20": {"node_ids": ["9a1e79c6-edbd-4508-b012-3ebdbcee3985"], "metadata": {"page_label": "288", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "73d64630-11d0-4470-8951-80633c25e02b": {"node_ids": ["f1b0203d-0599-4314-b5df-31d85b348f6b"], "metadata": {"page_label": "289", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2224b39a-bf57-4926-9b66-eab46b49fcd7": {"node_ids": ["ea07a59f-b7de-403f-9307-386f55fbd623"], "metadata": {"page_label": "290", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "830f7bff-012f-4563-824e-ee7b19174305": {"node_ids": ["90fbd74a-c31f-4cf4-9967-3effcb9fe875"], "metadata": {"page_label": "291", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0c9329fb-05a7-4a0a-b157-6b2e1a6301fa": {"node_ids": ["2f0df830-fa96-45cb-8308-9aa035c49b7c"], "metadata": {"page_label": "292", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5d5e0954-8283-40bf-95f3-4151f71086c5": {"node_ids": ["d35a654e-01d8-4ded-8751-7408b045e61c"], "metadata": {"page_label": "293", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ab2c582c-4fcb-44f8-8b1d-b004a4f60230": {"node_ids": ["2fa234ca-d799-4869-9cdc-0653cfd177b7"], "metadata": {"page_label": "294", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "be85e1f2-50de-45b6-abb1-53da29a52fa1": {"node_ids": ["ed814763-1e6b-4d9d-8d74-9e10cf909801"], "metadata": {"page_label": "295", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2e3ccea7-4ea6-4b80-85b1-0192b485113c": {"node_ids": ["c6e34e56-6fa3-45bc-8ed2-e89530c71574"], "metadata": {"page_label": "296", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d66172d5-5c87-4774-b04b-9d75d2acd12b": {"node_ids": ["7c8733b6-5f88-4c93-9d8d-66ded4599fe8"], "metadata": {"page_label": "297", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9f4cf67f-16c1-4c00-8f21-148004e01c72": {"node_ids": ["63b66362-2106-4ca8-a5fd-04b48c41ec1a"], "metadata": {"page_label": "298", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f54e4bc0-7397-43c5-9a9c-c2c90f82faa9": {"node_ids": ["cac38340-0975-40a9-9190-14bfcc9ae555"], "metadata": {"page_label": "299", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a82127f1-11be-4b79-ac97-22056bd3b524": {"node_ids": ["c25687ff-5ab1-44c7-b027-7c29d813d957"], "metadata": {"page_label": "300", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a398fe64-1b5b-4eb4-8e1d-a57663a2f83f": {"node_ids": ["ce111cd6-6b11-4ebb-97e7-da8b353e0265"], "metadata": {"page_label": "301", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7dbb6d46-82bb-4b56-aa33-d78c00d7ece2": {"node_ids": ["6ed436ea-8c07-4280-bd4e-b37241178a0a"], "metadata": {"page_label": "302", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5651aed3-a2a3-4170-866c-3f61cdeb7a63": {"node_ids": ["ce52a2ef-9b41-410a-b104-d0f6e6392843"], "metadata": {"page_label": "303", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "798cae7e-23d0-4662-816e-c08708bcdb07": {"node_ids": ["1f755184-ed9b-40f0-9621-f70df9585ce5"], "metadata": {"page_label": "304", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "890f675c-9b03-48e6-8dfa-faaa19476c18": {"node_ids": ["90972da9-91c5-4412-9f88-ce362199d172"], "metadata": {"page_label": "305", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cd796fd0-bd66-4d36-bad0-d6092c6791fb": {"node_ids": ["bfe93013-57a3-445e-a4c8-5ba2d4680089"], "metadata": {"page_label": "306", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6a044a89-72a3-4676-88e5-e6830a53ce30": {"node_ids": ["45465ea9-19e4-4d97-8ca6-791c46b65dd3"], "metadata": {"page_label": "307", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c043c329-0236-4773-b612-e98b3aacce30": {"node_ids": ["579c5922-2e00-40f1-847e-ab65500afeca"], "metadata": {"page_label": "308", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "081ac93b-bc28-46c8-ab99-3d21f40c5b25": {"node_ids": ["4e0b096f-8251-4a24-9ad2-5f38c2e6e3fa"], "metadata": {"page_label": "309", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bf47a3af-25e3-4f13-82b1-e119012a29e3": {"node_ids": ["ada7371e-9776-4c83-bc3e-1377ec6a6699"], "metadata": {"page_label": "310", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1ee68b7f-2c5b-4dc6-8311-460a38e5977c": {"node_ids": ["840024f5-b890-459f-befa-db53fb8f8adc"], "metadata": {"page_label": "311", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "79586157-f9ae-4ec8-8314-1e7bfe7d5c50": {"node_ids": ["4138e68d-08e3-44c0-8e85-14215ddb9385"], "metadata": {"page_label": "312", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9ff34026-3b3b-4004-8dad-7c89304e7d83": {"node_ids": ["ef14457b-712c-4319-a384-bca42703e430"], "metadata": {"page_label": "313", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8f55f959-7b4f-4e17-a63d-1675484569b3": {"node_ids": ["cc6f189e-2359-4374-a859-23f72580b43f"], "metadata": {"page_label": "314", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bcbc1947-aee9-4644-860f-2aa81a9f826d": {"node_ids": ["0789e879-56bd-4479-818e-5f5fb5da83e7"], "metadata": {"page_label": "315", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9480d53f-c80b-42a3-8379-cc06459760cf": {"node_ids": ["9b940e7c-d5ad-44a3-b209-17d04005fd55"], "metadata": {"page_label": "316", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a6767625-e211-40d1-af1c-bcb465387b0f": {"node_ids": ["4d3567e0-f139-448d-844b-6d38f9dcecac"], "metadata": {"page_label": "317", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "14ba5022-b35c-402e-b3d1-c3051c1c3597": {"node_ids": ["c0581543-cc73-4c8f-a235-7b185e47d800"], "metadata": {"page_label": "318", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d0ab9f07-aa64-4025-b27f-055690ac78de": {"node_ids": ["7a336483-247f-4991-87ef-fb975d854adf"], "metadata": {"page_label": "319", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b9f6313e-d215-4074-b111-872baac079ca": {"node_ids": ["085718de-cb0d-4e20-9c6b-3bdd4312a8ed"], "metadata": {"page_label": "320", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b3468217-d6e9-4c03-b2b4-589b7ef1b3db": {"node_ids": ["73dff742-5608-4ff9-a83f-d71571c24847"], "metadata": {"page_label": "321", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "eb3f896c-a985-4cea-9e1f-a097a70bf163": {"node_ids": ["68cd523f-d086-4890-bbe7-d8928ef5f0af"], "metadata": {"page_label": "322", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0b4b7c84-2096-4124-8fea-b55407a030f7": {"node_ids": ["e60b7d92-49be-4e45-a9ba-677bd4426d5e"], "metadata": {"page_label": "323", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2cf87c05-9062-4a49-8c5a-f3b4c730098a": {"node_ids": ["ff05a485-e7a3-4baf-a65f-69e6adf0a2bc"], "metadata": {"page_label": "324", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3bb34d36-b517-472c-9f1b-553bd41084ac": {"node_ids": ["bbdca2d6-7c6a-4c5a-b6e9-d3f931e05f19"], "metadata": {"page_label": "325", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1df2bddc-fcf9-47a8-95a1-abc273c6aaa7": {"node_ids": ["1aede15c-2422-4080-88b9-c8959e8f33e1"], "metadata": {"page_label": "326", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "42a3a1f4-72f2-4ba0-a117-29b27c3bf0f6": {"node_ids": ["b554a9b8-59fb-43f8-bf37-655f523e8e09"], "metadata": {"page_label": "327", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "476a6102-fafe-4a30-a441-61d1d688233a": {"node_ids": ["9ae0cc01-22dc-4ccd-953e-da54bec48fa4"], "metadata": {"page_label": "328", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "46cbdadf-bf54-4e41-a3d5-9b23f1ae87ce": {"node_ids": ["2a6af8f8-d1ae-4e70-a39a-ecc5cfe3a6c9"], "metadata": {"page_label": "329", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0737b0b9-8d92-4191-ba95-1486ef0ba9d2": {"node_ids": ["016891c5-3670-4b8d-88f1-584a87e69449"], "metadata": {"page_label": "330", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c24ad952-0f84-445b-aac9-5d1a63fd9a4d": {"node_ids": ["e04b4435-9a48-4a39-8e94-3ed0dc97449c"], "metadata": {"page_label": "331", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2236f0eb-2457-449a-a621-f767bac4099b": {"node_ids": ["009ed324-e378-4668-afc1-b54c4af139df"], "metadata": {"page_label": "332", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4f8fee51-edb8-4586-abf9-6df2181f773b": {"node_ids": ["fcf9d47a-3372-41c9-9387-96206f510658"], "metadata": {"page_label": "333", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fd63404f-c603-47d4-9230-c8b6f2009b14": {"node_ids": ["b97f0227-b1f4-4b1b-b642-119ed2af03cc"], "metadata": {"page_label": "334", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "896a69bc-3255-4a14-9393-63518a7b9693": {"node_ids": ["c9095183-c80e-45ab-a28a-69aa4593e2b3"], "metadata": {"page_label": "335", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "646b85ff-6473-4fbf-84f9-709631949fa5": {"node_ids": ["16192993-cfc5-490b-827d-800e68238062"], "metadata": {"page_label": "336", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a29d51f8-8480-48b6-91f6-7840d05d0343": {"node_ids": ["80ef2de0-66c0-4c67-8f43-3e902afbe6cb"], "metadata": {"page_label": "337", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bd967916-493b-46fc-b39d-e9d4041bd6c7": {"node_ids": ["06010588-37be-4d91-a2a2-172661c7eb90"], "metadata": {"page_label": "338", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f1fc008e-65cf-4c40-b84d-509f5ca6d561": {"node_ids": ["2ec0c04c-90cc-48e0-9d95-15f9604e38b5"], "metadata": {"page_label": "339", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "633c1e8d-115b-4768-ade8-e94550ad85ce": {"node_ids": ["29c89dbd-6326-4cbd-9a8a-2ded8148268d"], "metadata": {"page_label": "340", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "567e0dc5-5d02-4532-89fe-b97ba870ab92": {"node_ids": ["18213439-c4fe-4178-b0f3-2c378c4d675a"], "metadata": {"page_label": "341", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "58a8000e-a877-4b02-9119-46cc25b53a20": {"node_ids": ["f53c4e68-3d6a-425e-825d-473ae0ccb697"], "metadata": {"page_label": "342", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c3769e97-8951-465f-a04f-14d1296e1c02": {"node_ids": ["430b26d6-616a-4488-926c-2ab730379230"], "metadata": {"page_label": "343", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "86f48024-32fe-41d4-992f-18d4fcfc9b94": {"node_ids": ["38deae6a-6df0-4085-828c-fb5ef4d9b841"], "metadata": {"page_label": "344", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c8227386-cadc-4d1d-a2d0-721ddcfb0fe0": {"node_ids": ["73253272-c53b-4b63-83cf-119ef30bd504"], "metadata": {"page_label": "345", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4cd4b3cd-2e73-487e-929e-abe9947605bb": {"node_ids": ["d825421e-7c53-4858-a460-4e2597330de4"], "metadata": {"page_label": "346", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "390b40e6-70d1-49a7-800e-d2330f7716c7": {"node_ids": ["dd87714c-1909-4881-9083-d8b4255f975c"], "metadata": {"page_label": "347", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "243aafdc-431b-47e3-bd00-1033a755c66d": {"node_ids": ["5c1285ff-c1d9-4940-a2f3-94f1fdaabddd"], "metadata": {"page_label": "348", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "765a9e98-524c-48ed-aae1-2548635bd96a": {"node_ids": ["3972638c-89f1-4e3c-9793-becaa8ad912d"], "metadata": {"page_label": "349", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c1c14f6f-9881-4305-8e47-fffafdf986a7": {"node_ids": ["dbd28fdb-8a33-44dc-a6a1-14d79327f14f"], "metadata": {"page_label": "350", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "907c869d-6af4-400b-a7e2-8902a75233ef": {"node_ids": ["a1c5711d-70d8-47fc-a118-21cd5572200a"], "metadata": {"page_label": "351", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "76565ab8-6625-49bc-b791-b8217bd0dde6": {"node_ids": ["e3c439cf-7e0c-4eb4-aff4-e0c7f5f0a1a0"], "metadata": {"page_label": "352", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3e5ee014-1bc0-4736-a3bc-09a7810bc635": {"node_ids": ["b46c5485-e571-4c1f-9831-720b5c478a20"], "metadata": {"page_label": "353", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "683f59d0-02c0-4b64-9db2-44bf0799aaf6": {"node_ids": ["dd47e34d-7af4-4db6-ad16-cc7a182f4179"], "metadata": {"page_label": "354", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6f7a0035-bd79-4070-8cbe-58855794f9d7": {"node_ids": ["1bb4ab3e-31fb-42f7-94d0-f6ecc748cbde"], "metadata": {"page_label": "355", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9aadcb44-f588-4a6f-b607-186acb4c376d": {"node_ids": ["505c9f14-bf19-4050-b9bc-caf55feec1a8"], "metadata": {"page_label": "356", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cd890bb9-9be8-40fc-a6b8-f335e77bfe44": {"node_ids": ["5530e02b-449e-4a6e-a1e6-6f43ff70fcdb"], "metadata": {"page_label": "357", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9a353861-0601-4f06-b39c-57dabd68afa0": {"node_ids": ["cdc29e24-77ca-4049-ba5b-ea8d771a1e10"], "metadata": {"page_label": "358", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "44cbd1a7-7346-4ff7-a99d-920665f9c0c4": {"node_ids": ["311d43ce-e70a-4574-a47e-cd4828caac9f"], "metadata": {"page_label": "359", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1dd2a4d5-7dfd-4099-bc2d-7415cc52d2ec": {"node_ids": ["a0724728-ca01-49b6-8f87-17154bb6a9aa"], "metadata": {"page_label": "360", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "515e522d-cb4d-461b-8716-ebc1f88fc1fc": {"node_ids": ["99b9c3b0-666c-425f-9a85-60bb66f2b182"], "metadata": {"page_label": "361", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c7d9801e-b41b-4bb0-b55c-3b6f41645198": {"node_ids": ["9b2fc0d0-9a91-49f8-b668-f040b6869c91"], "metadata": {"page_label": "362", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ad9de203-421b-4667-8507-5dbde850858c": {"node_ids": ["d1c82237-8bc1-4c64-ba2f-b52ec7c3c68c", "24f2c4bf-d5f7-4c72-817d-9ac5d97ac74b"], "metadata": {"page_label": "363", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ed82fd56-ca7e-43e4-9ab3-9c44657e6ea2": {"node_ids": ["a2c410c9-0a82-4fda-a12b-0ef0595bf471", "8284f43f-fb3d-453c-9bea-c7b78cb9064a"], "metadata": {"page_label": "364", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0491b36f-d847-4dd7-a9c3-58a8f2f61a89": {"node_ids": ["54467f4d-d8be-46a1-b2be-a628f373b618"], "metadata": {"page_label": "365", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "51d5fa42-8f2c-491d-8e22-1735a0e5455e": {"node_ids": ["e11baa39-e7b3-48f9-a577-a30b95ff3bb6"], "metadata": {"page_label": "366", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "eae95560-c74c-4697-9694-e3ff2e326153": {"node_ids": ["c6b76ca3-b943-4945-bf82-02c63b3a4e45", "e227e0c7-af64-45f5-94bc-65031b194d35"], "metadata": {"page_label": "367", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ea044575-f8a7-4b4b-829d-9477ae6e577f": {"node_ids": ["c4663428-a7ad-41cc-be84-05daf9649fa5"], "metadata": {"page_label": "368", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "15f7c676-5147-48af-aec6-20fad312d9d1": {"node_ids": ["f41c9bf9-fa20-41c6-b2e2-add1d5852427"], "metadata": {"page_label": "369", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "84f5c419-ad8a-4b08-8346-075f0c1a199f": {"node_ids": ["84332a99-4369-4945-9569-4b3c5a4d9af5"], "metadata": {"page_label": "370", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2cca7d5d-5e82-4eb3-99d4-b6e8293cb28e": {"node_ids": ["7d506a5b-812c-41c1-a911-662568a3c373"], "metadata": {"page_label": "371", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b2b8b953-f39b-4755-bfd3-4368039bedc9": {"node_ids": ["29f277f3-6e38-4630-b838-b4325e8c189a"], "metadata": {"page_label": "372", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1ec923d1-3aae-400a-8605-4f388253f05c": {"node_ids": ["17ecd3c8-3de2-4018-be42-5316f92aedf9"], "metadata": {"page_label": "373", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d09f6409-d387-4d39-a3d0-a5338ac8566e": {"node_ids": ["fd5ce960-0a2d-46ab-a3ad-94a279875110"], "metadata": {"page_label": "374", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "119ca212-8a8e-478e-b555-3ac829c68b3d": {"node_ids": ["d7a7287a-7979-492c-8a11-7dc51f0ce9c4"], "metadata": {"page_label": "375", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "430adb4b-bec2-4541-bfac-186e84f3dcc5": {"node_ids": ["d9effaa8-b527-4685-b88a-48f7155fbe82"], "metadata": {"page_label": "376", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dc830c54-edde-4c6c-9f3a-cfeb0515e07b": {"node_ids": ["92e8fddd-9c33-4897-b94c-36d2d2eb357b"], "metadata": {"page_label": "377", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b6c05785-88af-499d-bba4-5818ebced598": {"node_ids": ["a8029e1f-e2b8-4eeb-95fc-49a88e87a1de"], "metadata": {"page_label": "378", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b1506c16-4cac-401a-8980-028809d6ebdd": {"node_ids": ["8ac38a36-8697-4c0c-bf90-e6a55e32edd8"], "metadata": {"page_label": "379", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c4159930-e9bc-4684-9f2d-df448e8b3142": {"node_ids": ["7b129ed1-9ec2-4684-93b5-a1152b40ff69"], "metadata": {"page_label": "380", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "744c407b-0a6c-467c-bbda-66f0f68ea438": {"node_ids": ["e7a5f071-1bdb-4504-8e69-ca8a5f1aba3d"], "metadata": {"page_label": "381", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "15893b6e-9588-4d95-8e60-3c0ade826740": {"node_ids": ["4229dfe0-21a6-4872-9514-e719255e5e1e"], "metadata": {"page_label": "382", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "45c18173-4b74-4433-abd3-d5342778adab": {"node_ids": ["a73dee91-9a96-46fc-8b00-f3a6e862ebe3"], "metadata": {"page_label": "383", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "05bbdacd-b517-4cba-8e22-b263bfb39b04": {"node_ids": ["c18eecdb-b125-425f-aa93-883c579ce57c"], "metadata": {"page_label": "384", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9353f800-dccf-4804-81c6-4a7e0b728082": {"node_ids": ["309ef138-456e-4867-814e-eb7aba692d0d"], "metadata": {"page_label": "385", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3cf4579f-765a-48d9-b24f-84257403da46": {"node_ids": ["ffa10416-ee53-4eda-81a3-d4df7c123079"], "metadata": {"page_label": "386", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "890d97e6-77c5-4b56-93b5-8dc5c23be368": {"node_ids": ["9632a7f2-9afd-480d-90ac-24c788d055f3"], "metadata": {"page_label": "387", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "587c35bf-d49e-453b-bd1d-101813bc3803": {"node_ids": ["b6e67135-e211-4cd5-a81d-fc57dbe3182d"], "metadata": {"page_label": "388", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "65ba714e-d0b1-40a8-b862-70dada25177e": {"node_ids": ["17c21e18-50cd-45da-9068-9d9e67c31ad5"], "metadata": {"page_label": "389", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7780c01d-32d4-4b9e-b885-cde44be1fe18": {"node_ids": ["27110594-a5b8-45ab-9ef4-655d0b145c33"], "metadata": {"page_label": "390", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "16e17059-437f-4dc2-86a6-e56c5bec37e6": {"node_ids": ["2dde1704-9ba0-40ae-ba21-54c27005be6d"], "metadata": {"page_label": "391", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "364ea9bd-3a81-4c52-9a02-8d00ad590cb0": {"node_ids": ["f39c8907-953f-48d6-ba36-2c7c6582b4c8"], "metadata": {"page_label": "392", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "24e42174-6850-4ffa-880e-2838eeede7c7": {"node_ids": ["b2e4feef-306b-49a7-a3c7-b3971169bb83"], "metadata": {"page_label": "393", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8850da52-324e-4616-9b75-40111487b6e4": {"node_ids": ["61425244-fbf8-4090-a9b6-57355549d5e3"], "metadata": {"page_label": "394", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "67a22e51-2b6d-442b-be97-c90f437a1f3f": {"node_ids": ["4fca6872-3f4d-4b2f-b7b2-457c5407f36a"], "metadata": {"page_label": "395", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "620bc0bb-832c-483a-99ec-7851e6154b33": {"node_ids": ["5bdada0f-7628-4949-aafe-e7957356e4f5"], "metadata": {"page_label": "396", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7f1341be-80db-4de4-bff7-636d6c7cd6fc": {"node_ids": ["4923efa5-3aa3-4cb8-9c22-b3a5ddc7ee96"], "metadata": {"page_label": "397", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f1a841b7-bd18-40ad-9be6-310d9c74470a": {"node_ids": ["d035c769-9d32-478c-a883-ce501cd61314"], "metadata": {"page_label": "398", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d51c34ff-d274-434d-a6b6-109da4fff6ae": {"node_ids": ["aae1a597-6ae8-4090-9a1f-de46b6135c13"], "metadata": {"page_label": "399", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2ef67385-f61d-47e6-87bd-29fc4c775555": {"node_ids": ["21508e82-51ca-4f85-b375-af0aa6674294"], "metadata": {"page_label": "400", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "27f77c74-98cd-4841-8e75-0482ac84a378": {"node_ids": ["966e7a9a-f5bb-4755-93d7-96307ca32cb8"], "metadata": {"page_label": "401", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "26394e48-a8b2-469d-bd93-f350850ce6e4": {"node_ids": ["ac52f110-579a-43f1-8d50-0f658cd0f73a"], "metadata": {"page_label": "402", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dc93ac9b-5e8f-40fb-8223-a1bfcc3478e7": {"node_ids": ["1783f64d-549a-48a8-aa95-dc3c228ff8a1"], "metadata": {"page_label": "403", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "20522f53-d63a-488f-9e79-8b40bfc00b25": {"node_ids": ["6b7c6cdf-3d00-44e0-bd62-21a922bb2a37"], "metadata": {"page_label": "404", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "835f5ae4-f5d1-48f3-a9d5-fd1464fcfca9": {"node_ids": ["f89070ba-8c07-420c-b843-0a1fa8ca5efb"], "metadata": {"page_label": "405", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d6cabf4f-e2cf-4834-8c91-54fba7ddb801": {"node_ids": ["2cf00503-d809-43be-a4c6-0abca0d633ae"], "metadata": {"page_label": "406", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "78c4bf7a-6919-42ae-ade7-f5478d491901": {"node_ids": ["10efd3e2-9b49-4348-ad77-a7caee0e0d9a"], "metadata": {"page_label": "407", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "38af870f-8f73-4f26-b2b3-a615dcac6c2b": {"node_ids": ["995d4a4c-de56-440e-bb54-10e1f423ec88"], "metadata": {"page_label": "408", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "26ba1eb2-5253-4e94-aace-aef0d559431b": {"node_ids": ["e379d22a-c04d-4adc-9af5-a3e2604cc116"], "metadata": {"page_label": "409", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2fd6f813-a6fa-4055-9eee-c05395734085": {"node_ids": ["db63c922-ae5a-4c1f-b5e5-81db89bab57f"], "metadata": {"page_label": "410", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9eb559bb-ca03-4dc1-9501-f02b7c9c169d": {"node_ids": ["3509d3de-ec8a-4ee4-b173-969e021decc3"], "metadata": {"page_label": "411", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1732a6f8-01c1-4218-b4f3-99d11cbe957e": {"node_ids": ["8b8fff3a-0596-4258-91b5-c07f19dd94f4"], "metadata": {"page_label": "412", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "507ccc98-5fd9-47ca-ac07-25d22df75645": {"node_ids": ["a52e5ef7-7393-4e6c-a358-3645ce95d1da"], "metadata": {"page_label": "413", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "af1b8f7a-4bdf-4406-b185-4b44a6ccb98a": {"node_ids": ["78fbc8c8-c8e4-4ad4-9ac2-0919d814fd44"], "metadata": {"page_label": "414", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c9f3c7a6-ddd1-47dc-8859-a70e4a4b2be9": {"node_ids": ["b7c0f173-10c3-4700-bdac-279913bfbc8c"], "metadata": {"page_label": "415", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4dda1b94-0c0d-4778-98a9-201045855444": {"node_ids": ["1b96e79c-9da4-4909-ab88-d34788aede01"], "metadata": {"page_label": "416", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2469be29-7572-4342-bb35-7cd3db8c8fbf": {"node_ids": ["6bdc039b-61e1-4663-886d-ef40d5e482ba"], "metadata": {"page_label": "417", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1d072a0b-ba58-4673-8370-1fed05b9976b": {"node_ids": ["47def66b-b3a9-4292-94a8-f1be0f473deb"], "metadata": {"page_label": "418", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0edd96fd-f414-486e-a804-a331afee648c": {"node_ids": ["fd4caa67-7854-493f-a245-c62e4816be1c"], "metadata": {"page_label": "419", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a56e6206-a570-4259-b7be-2abce519d5b0": {"node_ids": ["40638989-acf5-4d36-bbf4-3bec6c732300"], "metadata": {"page_label": "420", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3b063e1e-4251-447a-99b0-7879a71304ac": {"node_ids": ["2087d7ba-b1fd-4b9b-a293-f1394a74db7d"], "metadata": {"page_label": "421", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a0832e99-ac1e-4648-a4ef-e3c891af5858": {"node_ids": ["acc0ead4-870d-413d-839a-277d7691c735"], "metadata": {"page_label": "422", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "19804c2d-938c-4f07-abdc-f027568795fb": {"node_ids": ["a0189eb7-b573-4bdd-8020-c1343ea9deaa"], "metadata": {"page_label": "423", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "26880900-60fc-497e-ba27-1ea178ebade4": {"node_ids": ["856adfe6-5ce1-47ef-9e74-6a4ddd3e4ae9"], "metadata": {"page_label": "424", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9df74982-31d5-4727-a3a2-d2727fc481e9": {"node_ids": ["2a05252e-9a9b-458e-a0d2-0d15b06de29e"], "metadata": {"page_label": "425", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2f63fe20-0c8e-4263-902f-dfbe65a01215": {"node_ids": ["f915f536-434b-42d4-b662-a46cbd1591f1"], "metadata": {"page_label": "426", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e579ddb6-8214-4a51-9332-5ef11f1d8a63": {"node_ids": ["75f54cb3-443c-4d0b-9980-71258992fc3a"], "metadata": {"page_label": "427", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "212411d6-20b1-464b-9afd-f7eca501b16d": {"node_ids": ["8b184368-4eb3-40c4-996f-6d8cee2e63f0"], "metadata": {"page_label": "428", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "24e9b62a-5dec-4e3d-8c08-de41e64ae5f5": {"node_ids": ["7961dc4d-f139-442f-922c-42bbc4321cf6"], "metadata": {"page_label": "429", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2c83ff71-abea-42b8-a0f1-746e89ce16a1": {"node_ids": ["eed0536b-e2b4-4b6b-9d30-b868234ec025"], "metadata": {"page_label": "430", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "87b341da-065a-483a-a356-e7f7c6a64394": {"node_ids": ["0cdd205f-5ca5-498d-82d4-ab8e5120c7f3"], "metadata": {"page_label": "431", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "544eee7c-ffc6-4d3b-8b7b-ad2d93fbd41b": {"node_ids": ["db9f8b9e-d41f-4f13-8040-9fb87cbbca33"], "metadata": {"page_label": "432", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2887db5d-bcf8-48e5-84c9-5a90aa2e0bad": {"node_ids": ["e5199167-e9ff-44e1-8a43-003dbb9c7545"], "metadata": {"page_label": "433", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5c85362d-aad5-4e70-8c34-bf5ad5a43e05": {"node_ids": ["ec5ef2e9-4726-4121-a503-6248dc13bc80"], "metadata": {"page_label": "434", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9e3fceca-e435-43ec-93ed-9ae6e3f9f9f0": {"node_ids": ["834be78d-6808-4f9f-900c-355ab804dab6"], "metadata": {"page_label": "435", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "859c6b72-a850-4e6a-82c1-88f7c26b1ea0": {"node_ids": ["666b28b6-6d43-4c28-8ef5-b8119b98f303"], "metadata": {"page_label": "436", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "307c75f8-8385-40d1-83c1-ca74f5093f70": {"node_ids": ["982917d7-4f1f-4011-9870-fb119b56cd19"], "metadata": {"page_label": "437", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "75e9e62b-1d76-48d5-9964-53d572d0924f": {"node_ids": ["9451c91e-5cd2-4b07-8126-f86cde4ae9a0"], "metadata": {"page_label": "438", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c76e44ec-7081-45d2-bebd-c2e5317f9c34": {"node_ids": ["f62182cc-4cbb-4f43-9149-617063909bf9"], "metadata": {"page_label": "439", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8d57d29f-acc1-4b6d-b012-d95f788a42b7": {"node_ids": ["d01244cc-69c4-42b9-ba91-c9c69a0d0042"], "metadata": {"page_label": "440", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e4bd2813-2a5e-476b-940d-e368a7c7a507": {"node_ids": ["d0e7b778-9862-44a9-afc4-6a809e21a291"], "metadata": {"page_label": "441", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fd36819f-b150-4935-8771-0538b9592b9b": {"node_ids": ["b0e977a7-942e-4647-ae6c-5d6a43848932"], "metadata": {"page_label": "442", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1424a284-25c9-4e1e-9aca-046d0fa037e5": {"node_ids": ["1ed9b3d1-4698-4738-ac04-c1954ff3d573"], "metadata": {"page_label": "443", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b450e992-5a88-4002-b156-6276ca33f482": {"node_ids": ["2ffbdfb1-5a77-4338-80f1-a436b1a728b4"], "metadata": {"page_label": "444", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "17e872b1-0bcd-42e6-9180-c671fb9402ab": {"node_ids": ["859757f8-6754-4869-8e80-db04e9dd71dc"], "metadata": {"page_label": "445", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0f8cd6a3-733b-4aef-bf06-55d41d0ebb3d": {"node_ids": ["d12898bc-f373-4fef-ae23-ebbba1419232"], "metadata": {"page_label": "446", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3a86abe0-878d-4dab-b123-24b3e4e41de4": {"node_ids": ["bdbb2bd8-0c04-4c0e-a0dc-238291db6b1c"], "metadata": {"page_label": "447", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8789bf31-3874-4aff-9f1d-e93894d2f417": {"node_ids": ["cf32acb5-2a3b-4cd0-ad71-da4d36868d8c"], "metadata": {"page_label": "448", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0f724f9a-261a-45dc-868f-4fb6ce8d4b12": {"node_ids": ["1d6bdc77-291b-412a-82be-af198bfb31a3"], "metadata": {"page_label": "449", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b6742c90-e8dd-4046-b535-899db847730e": {"node_ids": ["b06afe50-386d-4829-8a31-fa8075b1293b"], "metadata": {"page_label": "450", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a853a1be-9ead-4e6d-8faa-19baf84a27b4": {"node_ids": ["3700ca56-d9a8-46b5-b814-b14a18fef5c5"], "metadata": {"page_label": "451", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "14112a47-fb6f-4de7-b1ee-d396a86d0133": {"node_ids": ["7a851ed6-52ee-4a10-8339-62860712c3f5"], "metadata": {"page_label": "452", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "66270d69-ecd3-4a9e-8ab0-b0be00805298": {"node_ids": ["7d960554-c8a0-45cd-8f05-dbe4ed527706"], "metadata": {"page_label": "453", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "045d5eb6-8866-4894-9a1b-58923350de99": {"node_ids": ["703b8dd6-de68-4326-bb9a-b6350b57d03d"], "metadata": {"page_label": "454", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "017e18ec-3413-4de4-b398-f372140c8c70": {"node_ids": ["0e7c6a1c-80c8-4f58-8dbb-ef96e4d09b1b"], "metadata": {"page_label": "455", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "70b0aac5-bae7-416a-a552-31ae317f7130": {"node_ids": ["22c72c54-55f9-4422-b0fb-ecc686336282"], "metadata": {"page_label": "456", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "66b6c0f4-7420-44e9-a216-4ee86ed8772a": {"node_ids": ["1bdf9977-0fc6-43a7-aeef-8ca0f6fa08cc"], "metadata": {"page_label": "457", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5ed33ea0-a67f-4b43-ba36-9245588c1218": {"node_ids": ["91f51b77-4c89-4056-9cd1-f44e96c4f8a6"], "metadata": {"page_label": "458", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fa3ffb3e-e43d-4cf6-86da-90269b57abe2": {"node_ids": ["71b0951f-fe18-4862-bb2e-806388d3b5e5"], "metadata": {"page_label": "459", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "45f33713-b6aa-4935-a278-d1a8f94b7a86": {"node_ids": ["0f722215-431d-4bc4-bbdf-346c1150cadc"], "metadata": {"page_label": "460", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "468dbc05-4ddf-4c75-a967-3fd1d3c4a3b7": {"node_ids": ["ca60b4d9-d605-4e42-a7b8-cd7f52694dd7"], "metadata": {"page_label": "461", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "17ed5d61-6115-4619-ad62-d83f972d4364": {"node_ids": ["a9468516-50a6-4005-913a-2b8cf80b51c6"], "metadata": {"page_label": "462", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1d478bcf-346a-4f4c-befd-66ccde15f541": {"node_ids": ["bd92cd31-278a-49a1-a353-fd997a042d06"], "metadata": {"page_label": "463", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "332edaba-6fc6-45f0-97cd-58fab188a06d": {"node_ids": ["3bfc543f-3e0d-4d8c-aa01-9bcb84f54407"], "metadata": {"page_label": "464", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "38e8a702-13cb-4772-b4dc-c87cf9faed3f": {"node_ids": ["6b00b6d6-4faf-4e86-ad4e-543a67605569"], "metadata": {"page_label": "465", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cc49a2ad-5616-4ea4-97de-2711e179ac7c": {"node_ids": ["0d495a40-0072-475f-9582-46f13a42f52e"], "metadata": {"page_label": "466", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e757524d-4f28-45b0-9838-4cca885d8c50": {"node_ids": ["05933c88-bea1-4e6b-aab3-da0a4b47651a"], "metadata": {"page_label": "467", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ceb9deb8-479f-476b-ab84-1b940b71e966": {"node_ids": ["2c14763a-b543-4dac-8aee-def227174b8b"], "metadata": {"page_label": "468", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3b57089e-e78f-464a-8192-4bb3fcec53ab": {"node_ids": ["c1c6ac30-05cd-47b0-af37-b4a5dbc9a137"], "metadata": {"page_label": "469", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0fa3631d-408b-4c3e-b940-357df38437ef": {"node_ids": ["7171afe9-2651-4e5f-8670-ac1d6f7f1227"], "metadata": {"page_label": "470", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ac183fc0-5bad-410d-9e2e-3a168f217883": {"node_ids": ["6e356fba-b60d-421d-ac4b-e161495ad5f5"], "metadata": {"page_label": "471", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8e2ec2e5-1475-4ce7-af5c-cf243f8e56b3": {"node_ids": ["89270b80-2923-4580-ba20-afb22330f146"], "metadata": {"page_label": "472", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "de96f9f4-c76c-49fd-bb9b-9889bf521be9": {"node_ids": ["e8f6d3c1-772f-449d-ac29-edc3f2f8aaf9"], "metadata": {"page_label": "473", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cd6e8b1f-a414-49bc-bf58-61f9ef93468b": {"node_ids": ["c78a67d3-9479-4494-b5e6-a86abf599687"], "metadata": {"page_label": "474", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "139abc9b-b189-4da5-983b-4102e9712266": {"node_ids": ["a314db8f-d50c-4326-8022-5d40d858c2c8"], "metadata": {"page_label": "475", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e68ce08a-c3ae-41ab-bc5d-d0d2513130c8": {"node_ids": ["ef4d319d-9aee-4da8-92d8-501068d9e8b1"], "metadata": {"page_label": "476", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7f4b7344-c284-4406-8be1-4a68d79537ab": {"node_ids": ["13d155d9-2084-4037-a1da-440d699d2760"], "metadata": {"page_label": "477", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7c365f04-87d9-448b-8255-403363fd00ef": {"node_ids": ["82b78fe3-fdf3-462d-8c0d-8a98719bb378", "115ee5cf-493c-49c8-97ec-e875b43b2502"], "metadata": {"page_label": "478", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dd49bda7-f961-472d-bb42-8b127c3103bf": {"node_ids": ["2b6aa26e-9f4e-490a-bb9d-e28f4d9d08ba"], "metadata": {"page_label": "479", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1953d877-8a1e-4b1b-b308-ab5674c29ff3": {"node_ids": ["dd740b46-4c1e-4f68-910b-ad0f1017e51d"], "metadata": {"page_label": "480", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1f2579c0-fe51-4401-8f2b-58cc64ff6c23": {"node_ids": ["b1cef72e-6c1e-462b-9b55-2cf473ffeca9"], "metadata": {"page_label": "481", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cfab5cc6-7a03-4d26-968a-6d8c016655c7": {"node_ids": ["4d24d3b4-ddf7-405d-bab6-d6c54c17adf6"], "metadata": {"page_label": "482", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c9af52b2-d04f-4895-83e1-21ac849d1688": {"node_ids": ["f55a9d5b-7a4b-42b8-9ee6-72568d5db22b"], "metadata": {"page_label": "483", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "426cde84-425e-4b61-99e9-023042bbdd19": {"node_ids": ["518b26b4-b22d-4799-8bea-8eb28a132857"], "metadata": {"page_label": "484", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "516e1e8f-6204-437f-991a-62c2cb08575d": {"node_ids": ["8aa970f8-fcec-440d-af97-16b81bdf624b"], "metadata": {"page_label": "485", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "67e5da83-7725-4af6-a4c1-049aed2f8151": {"node_ids": ["0312e532-d49a-4d7d-b8a6-f48b80cfada4"], "metadata": {"page_label": "486", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e3cf8860-12a1-4ebe-8c91-5087a2184271": {"node_ids": ["0f0de495-1357-4845-ad7c-72f522f1c315"], "metadata": {"page_label": "487", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a7fd15b6-c60a-438c-a6c9-e72f78887474": {"node_ids": ["668666ed-c419-423f-a62d-520a23760612"], "metadata": {"page_label": "488", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f41b4ef6-160b-43a3-832f-453466286a7b": {"node_ids": ["f3123d0a-d1c5-4c46-80d3-222ccc5d5ffd"], "metadata": {"page_label": "489", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5657d12b-0a6a-47c5-b788-f0e2307bdc50": {"node_ids": ["a4311929-6a9c-42e2-bf7b-801dec341307"], "metadata": {"page_label": "490", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f0047b98-d84e-4550-ac43-6a50499d5610": {"node_ids": ["4cf0a829-c6e1-48e8-af19-51c5bbb5fb82"], "metadata": {"page_label": "491", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d9ba1f8c-c362-4d03-9377-4a31f2b61b7c": {"node_ids": ["de58378a-4a53-4bce-a1e4-c082d51b4774"], "metadata": {"page_label": "492", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a9c76e60-c040-4486-9465-08123f8aa914": {"node_ids": ["47f6aa22-3cef-4e04-a3b5-24fb3e679a19"], "metadata": {"page_label": "493", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ec7ede54-4847-4bae-9dc5-590886236728": {"node_ids": ["1b804ccd-355c-42b7-b206-8e1c04d6b3f6"], "metadata": {"page_label": "494", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0625b5ef-df5b-4459-a8d7-913314f6d6b8": {"node_ids": ["ebfd298e-fe6c-499f-9ccd-3faec0475720"], "metadata": {"page_label": "495", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e32cebb8-df3b-49fa-82a9-3c4700c241fc": {"node_ids": ["a99e3950-1ac9-4010-8275-4e3bd1640bd3"], "metadata": {"page_label": "496", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6a268334-8417-4230-95d0-3bd574353cca": {"node_ids": ["a295116b-a653-4f4d-b907-55618a397d5a", "76670990-537d-4735-b4a5-fcde1418c43d"], "metadata": {"page_label": "497", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "448358b8-c6d4-4253-9ec9-79e3f79da6df": {"node_ids": ["42c74a1c-18b9-4522-a0ae-a311c9b2ac60", "eb9ebbf7-681d-4ed5-9bae-ceb88f1d46fe"], "metadata": {"page_label": "498", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "db93a9b8-712c-4c28-947d-25cd3b704785": {"node_ids": ["725a1768-01d5-49ad-a6f0-08747594c5ea"], "metadata": {"page_label": "499", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2244ed92-81c4-4379-a67e-377749487e92": {"node_ids": ["0327a908-0e23-4773-bbcc-da0dbde9dba6"], "metadata": {"page_label": "500", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b7aa6b92-9474-47de-89ee-f40f48c31bb0": {"node_ids": ["2864153e-8de1-45d1-b1e3-ed79efa1677a"], "metadata": {"page_label": "501", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "50cf0158-ef10-401b-ba59-5729777cdfef": {"node_ids": ["ea50337c-dfe9-4320-bb0e-63a3b7e41c49"], "metadata": {"page_label": "502", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "186458d7-dbad-4a6e-a6a6-e6ac1949dd0a": {"node_ids": ["d6f91792-7378-4dee-af67-2e5e9ef0809f"], "metadata": {"page_label": "503", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d9476070-4add-4a0c-9c6d-80b1e3f6ade4": {"node_ids": ["e1671e28-32c0-4dd4-b214-6ec152d851bb"], "metadata": {"page_label": "504", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "826de6dd-2bc5-4667-9e86-98f47b9db291": {"node_ids": ["91648a46-44bb-4961-ac38-4c2a0af334ef"], "metadata": {"page_label": "505", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0fc3d9ed-ea21-4eb6-a4ce-13eeb347714f": {"node_ids": ["bb0379aa-6e9e-47aa-8aeb-9b39488f12e7"], "metadata": {"page_label": "506", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ad54a376-98d7-4605-8568-c5baa0b63502": {"node_ids": ["d57b330d-7b84-4fed-ab88-a39b05199ac3"], "metadata": {"page_label": "507", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e6a52ceb-fbdf-4c17-87ec-424a832afab4": {"node_ids": ["b494b37a-c2e4-4aed-a483-daaebbff3339"], "metadata": {"page_label": "508", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8a4dba34-81d2-43f2-8785-c1faf3602d18": {"node_ids": ["80b0c327-d722-465e-b108-3ce73792be36"], "metadata": {"page_label": "509", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "48bb094f-fbf1-4362-85cd-3dc306d3905e": {"node_ids": ["6ed19751-2b57-4664-b81f-d5a182cb44c5"], "metadata": {"page_label": "510", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "37e60bff-86b3-4035-9306-6324a11ab1d4": {"node_ids": ["774b5dd4-1626-4e5f-b00c-1ff810728790"], "metadata": {"page_label": "511", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "508e586f-4ed6-4ad7-bca8-6914181a85ee": {"node_ids": ["8b0c0a51-8db8-4906-9e79-cdf48ca53e1b"], "metadata": {"page_label": "512", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4aaa9779-ea69-4ec3-b054-8befabc9664d": {"node_ids": ["45cd4fa4-d72c-45f2-9790-7b8ba372fb94"], "metadata": {"page_label": "513", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e8ccf932-3def-4315-983f-c35f7a8be0b1": {"node_ids": ["ad281a15-c177-4155-a6ea-442c1ea0eaf3"], "metadata": {"page_label": "514", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b17d234b-3de3-4989-9f6c-21e935cbaffd": {"node_ids": ["de27b6f4-2199-4a1a-90f3-3bf53c2a7ac8"], "metadata": {"page_label": "515", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5c92d7a3-073d-4feb-b557-efdafb37d046": {"node_ids": ["3a65fdfc-53bd-40fc-a833-88e8728e956c"], "metadata": {"page_label": "516", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fe38761d-cdcd-4257-999c-78d7448f087c": {"node_ids": ["4f9d2610-5406-4f42-8ee3-1897f794e964"], "metadata": {"page_label": "517", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ac932970-4b56-4655-8609-fcad93ff4662": {"node_ids": ["c46b4b6b-f01e-4899-9f39-b46f74a538d3"], "metadata": {"page_label": "518", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b864ceb3-a0dd-4467-8acf-6f7b3e6d0f81": {"node_ids": ["df557906-7bc9-4c37-b375-5e7923984e24"], "metadata": {"page_label": "519", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "da6bdb74-491c-4e3d-9c6c-94a00ec7ca51": {"node_ids": ["8f9bc75d-9915-4f23-b4a9-01065081176b"], "metadata": {"page_label": "520", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ee113b5d-88b8-4936-8585-e0ca52f133f5": {"node_ids": ["93fa176e-9cf9-4354-b0bf-198b8059a8a4"], "metadata": {"page_label": "521", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8d5b8c89-77d8-46de-b6e8-4d36809e55bb": {"node_ids": ["02f150a0-55e3-4a5e-b884-4b89565d3051"], "metadata": {"page_label": "522", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b794af8a-d3da-4f99-a825-497c8d7c8895": {"node_ids": ["b1a70cc4-70b8-4032-bf99-e98e3b512c4e"], "metadata": {"page_label": "523", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "eddb19a5-9349-4aca-afeb-d4184a87cf76": {"node_ids": ["eeb74e30-5716-42cf-9a33-a74d67854db1"], "metadata": {"page_label": "524", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c7d62a26-3881-4daa-a410-139ba68ebbe5": {"node_ids": ["ab275705-6750-4f6f-9d66-3d7effd2c212"], "metadata": {"page_label": "525", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fc6b2b00-0c49-4eb6-8516-3ee5ef7950bd": {"node_ids": ["270d9956-bcca-46f1-b1bf-11a44a42559b"], "metadata": {"page_label": "526", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b1c69d01-cee3-43fe-bac1-f6a736c9653b": {"node_ids": ["9e481b69-5509-4d1f-a550-6fd325450016"], "metadata": {"page_label": "527", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9b724465-a69c-4c19-b8c7-d2b9c33329ea": {"node_ids": ["17e87fec-57f9-4b0c-9905-9d6211dcbb04"], "metadata": {"page_label": "528", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a1c726c9-27a8-46ab-8672-52c69a2b5b76": {"node_ids": ["af846b12-9501-485a-94c3-4c84f6ed9bd5"], "metadata": {"page_label": "529", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7274af2d-02a5-4f48-a3a7-0d7b7050b6a3": {"node_ids": ["5e5bc680-f409-4ef7-bfa5-6249d7565ca8"], "metadata": {"page_label": "530", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "78abfc42-d53c-4bec-bd06-ed1412506650": {"node_ids": ["a0ce8097-c695-41ba-ad6d-900d72d7733a"], "metadata": {"page_label": "531", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5bc8225c-2bb1-4d0d-a78e-36abfda4fc71": {"node_ids": ["38130fa0-c0df-432f-b804-70ecc89990e8"], "metadata": {"page_label": "532", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8e26418c-902e-4c72-aa2d-02d45af7e4c8": {"node_ids": ["9f2bb612-e388-46aa-8194-8b6aaa56c818"], "metadata": {"page_label": "533", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5679fb97-eb15-4f02-bb8d-7c4628433ecb": {"node_ids": ["55321e10-e786-4399-8c77-a8ff6977d2af"], "metadata": {"page_label": "534", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "de9c11ea-b086-4ac9-9dce-68e826004ffe": {"node_ids": ["ef0033be-f40c-423e-b394-18e66f8b86cd"], "metadata": {"page_label": "535", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "35482ce9-2813-4334-aeb4-ed2a6cd74a16": {"node_ids": ["35187c62-7086-4005-9d3e-23dc6973f575"], "metadata": {"page_label": "536", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "509661a8-4ef8-495d-bbd9-be671f4a0185": {"node_ids": ["320f6ce3-69bb-4050-b83b-5671238190ae"], "metadata": {"page_label": "537", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f621cc47-5312-418c-b270-1151e347880d": {"node_ids": ["e0a75e71-0781-46ae-84f8-fb55ed2ac311"], "metadata": {"page_label": "538", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "63e2ed12-14f8-480e-ae4a-2f100494ac8b": {"node_ids": ["1be13b88-df6e-401f-ac76-ec6e1d03e2ad"], "metadata": {"page_label": "539", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "68700776-4698-41dd-b88f-c3d7b77893be": {"node_ids": ["5ce51d43-d267-4af5-ad44-2bbacca68d3b"], "metadata": {"page_label": "540", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "04719050-31d9-4f6f-b79f-cdf83f9eb875": {"node_ids": ["f77e1580-3202-4069-8396-b4d301c9eb63"], "metadata": {"page_label": "541", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "45604538-8ec0-4b22-8efe-e581291900fd": {"node_ids": ["f3a9d673-29be-4d30-9ce1-122a3ebc38bc"], "metadata": {"page_label": "542", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7706c506-7aab-4131-b8fd-7cc38fb6d9b9": {"node_ids": ["36308a0f-39e2-4f05-bcbd-418eec319b20"], "metadata": {"page_label": "543", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6b770969-928a-4557-91cc-3b75cf385d6c": {"node_ids": ["6d317841-d6b4-4d9a-9d91-aea659919974"], "metadata": {"page_label": "544", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1615bf7e-867c-4039-9a78-65cbede6fa37": {"node_ids": ["e37ee13f-6795-4075-b3ab-6daa57838177"], "metadata": {"page_label": "545", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e5bc341a-b7d8-4df2-b0bf-2b6f04d8114d": {"node_ids": ["66e85f99-0c67-41b0-be5d-5979d68087e3"], "metadata": {"page_label": "546", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "690ad7bd-6c9c-4e1d-a8ef-3911b671bf0a": {"node_ids": ["9984db5b-b7c4-48ad-9de7-012bb6960189"], "metadata": {"page_label": "547", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fac7fc97-7b4d-4e5f-88ba-33534d065572": {"node_ids": ["e5d4d179-a075-4fd0-96c2-389d6964f07d"], "metadata": {"page_label": "548", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c4bf5345-77f6-43e9-b79d-21a790aa1ebd": {"node_ids": ["d1934f06-3852-4b7d-a61c-356bc1cb2483"], "metadata": {"page_label": "549", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ce46e09e-73ba-438d-b8bf-95a85710187a": {"node_ids": ["783a9e90-e70b-472b-977a-bee0fdd8ef0f"], "metadata": {"page_label": "550", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "37c84395-2870-4773-8d1e-8e0bd03df537": {"node_ids": ["4d0df072-f17d-4e20-95f6-4889ffc8832e"], "metadata": {"page_label": "551", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b706f7a6-6818-471b-8e59-a68955d784a0": {"node_ids": ["9e4669fb-533f-4142-93ba-d9cf72c3cda0"], "metadata": {"page_label": "552", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "73bd739d-87cc-4c9a-a1db-05fdd5fedaf2": {"node_ids": ["8f4d91e1-4e91-4182-aac8-26c46f018ffb"], "metadata": {"page_label": "553", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "edd683e8-36dd-437d-b3d1-210e8e63af37": {"node_ids": ["1a9697de-950f-44de-81a2-b7c06bf8d755"], "metadata": {"page_label": "554", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c2dba794-2510-4452-b49c-c8653cc43b56": {"node_ids": ["8ebd354d-e48e-4be1-acf7-54a730a1a065"], "metadata": {"page_label": "555", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d8d02010-ef0d-4d79-a340-710361d24a40": {"node_ids": ["89db3279-9a74-4327-8723-d4e708eabda9"], "metadata": {"page_label": "556", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6f6cfdc5-d3c9-47f6-9435-b1066c34f7ce": {"node_ids": ["c34a3bc8-fb03-4163-9904-b87f75e1a492"], "metadata": {"page_label": "557", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "98f68874-6f42-498c-9919-9a2cd3809441": {"node_ids": ["159bbe55-fb8f-43a7-a4a5-a5e4ec4bc2e3"], "metadata": {"page_label": "558", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b8b14827-cf2c-4046-ac29-ad251910988d": {"node_ids": ["734fe262-1096-4705-a8a1-a1bdd0f6366b"], "metadata": {"page_label": "559", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5ac5b7a1-2d51-4b10-95df-633045b4f0e5": {"node_ids": ["06c68378-00f0-40e1-93ba-9ba74a519054"], "metadata": {"page_label": "560", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9c6af9c4-15a0-4449-9e92-5d81a4dad94e": {"node_ids": ["f18da28c-51e1-4828-9eb2-29f1f2a0feff"], "metadata": {"page_label": "561", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fd2a8623-68d2-4d97-96cb-8a119015e1ae": {"node_ids": ["e2417396-627e-4ec8-b0db-0b873c97d11f"], "metadata": {"page_label": "562", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d5ca777d-e9f3-4fe7-9439-d1199975d918": {"node_ids": ["7f4765b9-948a-4531-b929-e23b4713ed4c"], "metadata": {"page_label": "563", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7b022788-9596-4a0f-8a8c-a1b63a64704b": {"node_ids": ["fa543bf2-77e4-47e0-88c6-f18ae73ac5c6"], "metadata": {"page_label": "564", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "68b29de0-926b-462c-b958-f686ee61e541": {"node_ids": ["235932b9-15ba-4d94-b75c-84e35134c9b0"], "metadata": {"page_label": "565", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c82822c6-d9d4-4013-8ebd-a1bb97c792ca": {"node_ids": ["705529df-d07e-4589-8098-3c91d6107682"], "metadata": {"page_label": "566", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7e7f80ed-82dc-4ca0-b969-b4240c770c45": {"node_ids": ["828cba8d-8dc6-441c-8d30-8673b94ec39f"], "metadata": {"page_label": "567", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f4485722-3856-4f6d-ae7f-46363560e2fb": {"node_ids": ["0ba10c19-4cd6-484c-9c63-2e336b42f5a2"], "metadata": {"page_label": "568", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1f605cfc-0e3e-4581-aaee-17390f20445a": {"node_ids": ["d2e8f69f-bd2b-4a1e-a835-42e28276d859"], "metadata": {"page_label": "569", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "27b568b7-1fc0-46f2-bf96-8650bda62a52": {"node_ids": ["03b27506-59c7-414b-9a62-3ac8ce856953"], "metadata": {"page_label": "570", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f72d1ce6-1097-4635-9923-744aa6d44144": {"node_ids": ["699f744c-1efb-405e-ab46-ceed66236686"], "metadata": {"page_label": "571", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "794f82f1-88db-47ab-b47c-42079e9292cd": {"node_ids": ["bd4c3018-ee48-4766-872f-39649b85cb85"], "metadata": {"page_label": "572", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "81d8bfe7-6bc6-4eec-a784-5f6d073b01ae": {"node_ids": ["276f3dd7-abb2-4a06-9f63-f2a29f1afc7d"], "metadata": {"page_label": "573", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a45869e4-360d-451d-8b34-6ead2327eb4f": {"node_ids": ["ca920b6a-bd01-4e88-9aae-bb57565bbbec"], "metadata": {"page_label": "574", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7e1838c6-de90-41ec-bce8-046b2fade43d": {"node_ids": ["7098d7be-8914-4667-9c03-04622aa104f2"], "metadata": {"page_label": "575", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "02bdeffd-9cf4-4f2d-9bbb-641e7ba856f6": {"node_ids": ["cb389878-18f1-4267-8852-fc3e013cf2c6"], "metadata": {"page_label": "576", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "29efb7e4-d9ea-4eb8-bcfb-285aa7fa0115": {"node_ids": ["2fc8f2db-8d87-4812-9933-348645e74b88"], "metadata": {"page_label": "577", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "16eff6d4-b728-40cb-9d17-5492a4d3db27": {"node_ids": ["6c832131-72c3-49ea-aad5-8305fac61540"], "metadata": {"page_label": "578", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2b197b28-e3f9-45ed-bced-90d2a1d64e87": {"node_ids": ["58872b50-a0f7-4fb6-ba8d-207e791c3a4f"], "metadata": {"page_label": "579", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "568eab24-2c2a-4de1-af32-850ff3baba0b": {"node_ids": ["474e4628-d6fb-4f34-b0e2-f9a8b1274c44"], "metadata": {"page_label": "580", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e6e6c0c9-4fd3-4aa7-9ddc-a997aa5a136a": {"node_ids": ["0e8c48c2-16a7-4be9-9f46-84898ecfb370"], "metadata": {"page_label": "581", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7d3978ba-fbef-479f-8fb9-f735eb335f0f": {"node_ids": ["403573a8-ea4a-4b93-8ac4-df6370ddd886"], "metadata": {"page_label": "582", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7ee356cf-f18d-4658-be1d-3eecd39517a0": {"node_ids": ["744885fd-4700-410a-bad7-c3f7bc3f018a"], "metadata": {"page_label": "583", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "62d9fc53-f44c-4af1-9459-75ac958c1f4a": {"node_ids": ["be20fee5-c4ec-4130-96fb-bad5abb096a7"], "metadata": {"page_label": "584", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9d8bcc7d-99ce-4225-b038-e4d772436273": {"node_ids": ["ea2e285e-6b32-41bb-96a9-6089b0137326"], "metadata": {"page_label": "585", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f52bea60-2849-4f3c-bd0b-75b27307e96f": {"node_ids": ["3793f74e-f64e-4348-85a6-26c776d5d76a"], "metadata": {"page_label": "586", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f5385cf2-6400-4ad0-930e-d3759c81f28f": {"node_ids": ["2454e6ca-7c1b-44d0-acf1-3d17236067b2"], "metadata": {"page_label": "587", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a1038074-afb3-4d44-86d9-0bd27efe70e2": {"node_ids": ["5d2454f9-3542-4d87-817c-6c1cb452dd59"], "metadata": {"page_label": "588", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6542d896-6b73-473b-a9ec-7253daf085bf": {"node_ids": ["baf25430-f8b0-40bc-9fe0-02623a72acf1"], "metadata": {"page_label": "589", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b85c8ed2-ac2f-493d-8710-e2bf7fe1c5db": {"node_ids": ["66642baa-b01e-409f-8652-a123639d3c04"], "metadata": {"page_label": "590", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "afc6bf40-77f4-485f-b417-a6df5031ca7b": {"node_ids": ["c58d7660-4fab-44bb-add9-27f8177afe25"], "metadata": {"page_label": "591", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9a32da5a-e4b0-43f8-97e0-7c0a2cf4a8e6": {"node_ids": ["bd0240fe-f10e-4f03-b1a2-ea94732890bc"], "metadata": {"page_label": "592", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c3285bfd-8ddb-4c86-af0e-d32bd0d6c98d": {"node_ids": ["2a4126d3-22e1-4249-afb2-225a9fa318fe"], "metadata": {"page_label": "593", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5201ede0-37a0-4b28-9a30-6a619a089421": {"node_ids": ["b2ba7a9e-8818-4c58-9117-dff8b1eb6848"], "metadata": {"page_label": "594", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "02f2515f-45a9-49ea-8ccc-9abfa8f60b54": {"node_ids": ["141f90ab-c2ff-4353-91f2-bb15abedc1dd"], "metadata": {"page_label": "595", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cd571c8b-3d36-42b6-977d-a6763e091468": {"node_ids": ["f5a6670a-d8e2-4b86-91ba-a238000dc3ef"], "metadata": {"page_label": "596", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6d92b267-b467-4321-8bf1-7a35a3dd2511": {"node_ids": ["fc3aa15d-caff-45d6-af58-701f219c5b97"], "metadata": {"page_label": "597", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4adbe36d-d504-4a00-bfc9-7edef0d7580b": {"node_ids": ["08cc121d-7520-4e8d-a75f-0f157dcbd0bc"], "metadata": {"page_label": "598", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7079f221-2dd2-40e8-b574-f0ba821db952": {"node_ids": ["7f58b013-9f2b-48a9-a38c-4a0d10e9a9cf"], "metadata": {"page_label": "599", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "91d3978d-23ed-4116-921c-a8045e65fd37": {"node_ids": ["5d1503bd-979b-450a-b79b-cfd70c19c97c"], "metadata": {"page_label": "600", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "727a5a3b-4138-41ae-911b-743af3f007bd": {"node_ids": ["b09ce588-62e9-41da-bf9a-a3e29c664f45"], "metadata": {"page_label": "601", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0dda1ba1-7dc8-4a51-bdd0-373731d9da18": {"node_ids": ["3562958f-4766-4dea-b564-00fcc221ddcc"], "metadata": {"page_label": "602", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "46e6e8bb-f018-4f19-be2d-74344601ef18": {"node_ids": ["df019df8-06de-40b2-ad3b-5d55e799fe4f"], "metadata": {"page_label": "603", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dec8a953-54e9-4870-8a75-ca890ceae60c": {"node_ids": ["4e869199-d91f-4abd-9841-7cb53f3acfe0"], "metadata": {"page_label": "604", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ae1446e9-16aa-4ed9-bcd8-3662ba9e36a8": {"node_ids": ["7f03b89b-7596-4b66-a30d-abdded90a372"], "metadata": {"page_label": "605", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5ff1bc0f-9812-4b4f-9e0d-d1504c786d94": {"node_ids": ["314e95de-31ee-473b-8b94-539b4a500bd4"], "metadata": {"page_label": "606", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "23ca34a3-6f46-4ffb-8aab-b38b43d770cc": {"node_ids": ["a0796f4c-7754-4851-a0a9-da343529f29b"], "metadata": {"page_label": "607", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "44292833-a1c8-4799-b97e-bed602ff59c5": {"node_ids": ["97403974-4a13-4e2f-bd52-9abdc013d60f"], "metadata": {"page_label": "608", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d1878d8d-8751-48aa-9f6d-4b42cf115000": {"node_ids": ["24204954-f247-4abf-b85a-2a17fc553d4b"], "metadata": {"page_label": "609", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a2bb13cc-55a1-482b-a220-e4038c6857cf": {"node_ids": ["0afe720a-1989-418c-8728-c59f2604d4df"], "metadata": {"page_label": "610", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4a4aea1f-f7ab-428a-8644-31751fda865e": {"node_ids": ["11e4fd97-41c4-44a1-a78a-3a6324b6e080"], "metadata": {"page_label": "611", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "262c00e1-9ce0-455b-a193-6dbe6b572095": {"node_ids": ["8359c892-d0fa-4c7a-82a6-8c7a1542897b"], "metadata": {"page_label": "612", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "359bd711-5c5d-4d8d-8daa-b337116a5316": {"node_ids": ["9ccd6805-88ab-4279-b96c-86f020514ae4"], "metadata": {"page_label": "613", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d41e905d-f1c5-45a5-8149-92963e0f5e0b": {"node_ids": ["2b2927f5-45ce-4a31-ae8f-2f9c3587fd93"], "metadata": {"page_label": "614", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "edf96e66-a450-4cfe-8470-b59f90525f58": {"node_ids": ["aee3e991-6615-4a6d-9f51-6d3e09bd144d"], "metadata": {"page_label": "615", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3b4a7024-577d-48f9-b074-3a56bfe3f453": {"node_ids": ["9fee0525-b7d6-4464-a8f6-7a6c36a87773"], "metadata": {"page_label": "616", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fb4b9232-e0fd-4ca7-bf8c-3b5550024f88": {"node_ids": ["36853a7d-d51e-4425-96b2-04f3424f080a"], "metadata": {"page_label": "617", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "07556098-21e0-4d69-9ee8-34a69a059a42": {"node_ids": ["857de485-41cc-4094-9320-20939a5cbad7"], "metadata": {"page_label": "618", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1aa3c3c3-b099-4721-9ede-af0e77295c21": {"node_ids": ["804b1b48-6006-4201-9caa-678c69b031d4"], "metadata": {"page_label": "619", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "933df950-0564-4583-98e1-16f9361c55f3": {"node_ids": ["87275339-0bb9-48b1-ac2a-97cfb52a21bf"], "metadata": {"page_label": "620", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "be88b7d8-26c4-4c50-aca0-baae8e319692": {"node_ids": ["9f3c18ba-5c68-45cc-8145-12b68dc2c4a7"], "metadata": {"page_label": "621", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6167ee3f-81a2-4b5d-8fa1-486501fd4fc7": {"node_ids": ["090eda9e-38a8-465a-8498-859013acd003"], "metadata": {"page_label": "622", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bd59607a-61ff-4d76-be0e-acf057d46d33": {"node_ids": ["1a55d121-2e0c-4174-9224-368c74c199c3"], "metadata": {"page_label": "623", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "53bfea96-22e7-447a-a2a5-ea7f4db3fb44": {"node_ids": ["44f9b8c3-a6b9-445b-95d1-ed5c9c8ef8a4"], "metadata": {"page_label": "624", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "27479ff1-2c3e-4715-b37b-914fc1a1e01e": {"node_ids": ["e94a7e1f-0324-4ca3-9438-3a3522b1bc63"], "metadata": {"page_label": "625", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "966f48ab-c8e6-45d3-9941-45aa91d8449f": {"node_ids": ["9817471d-9a7e-4746-abc4-0151edc25779"], "metadata": {"page_label": "626", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "264c24cc-2c56-4f38-a1dd-96b261b31ad6": {"node_ids": ["e5f01008-7f85-42a2-bdcd-84158742819a"], "metadata": {"page_label": "627", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "49e51f27-125c-41f4-ac7b-fc8997c40b9d": {"node_ids": ["2815cdf3-a62d-43cc-9777-467928a74411"], "metadata": {"page_label": "628", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a980f5a8-1f58-4c64-b94d-4a51beb0c9cc": {"node_ids": ["d6b0351d-56cc-47c5-8e11-b6711ee6a7c4"], "metadata": {"page_label": "629", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ca7ee332-c272-494a-8d1a-a4fbb41d91c4": {"node_ids": ["1128b6a0-2285-499e-b14a-038e5091251b"], "metadata": {"page_label": "630", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "55a84ef4-aeba-4c8b-a610-78cf28f6a425": {"node_ids": ["e24853a3-614c-4452-9116-3302db507075"], "metadata": {"page_label": "631", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d146a532-2f3c-49dc-85d3-b25e34d6a592": {"node_ids": ["42e10955-9842-4acd-8f2b-5889104376e3"], "metadata": {"page_label": "632", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1fe4e962-cd2d-4960-ab8e-f097c1d203c4": {"node_ids": ["5876cb2d-c94e-4921-9917-25941f9530c4"], "metadata": {"page_label": "633", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "31484385-bc46-421c-9c38-baeae26bd2b4": {"node_ids": ["2ceaea90-bf72-4740-95f9-92d79f3c778d"], "metadata": {"page_label": "634", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "31601c6f-72a7-41be-be94-382d9fc8b16f": {"node_ids": ["2153a5a5-1056-43bf-9a7c-0eecc3551d8e"], "metadata": {"page_label": "635", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3d794c4e-8b37-4952-b371-8471e23c54cc": {"node_ids": ["84710840-6eb9-4cd3-8442-7923e08d24e0"], "metadata": {"page_label": "636", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6cec1e5e-b4cb-4c62-aca7-10654095f57a": {"node_ids": ["3381bf6e-09d0-4bc7-b056-1614b131ed1e"], "metadata": {"page_label": "637", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "95884c02-e73a-45dd-a107-37f379deb437": {"node_ids": ["8b29dfb1-6729-4bf3-954b-0f6b3fe879a4"], "metadata": {"page_label": "638", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "abfce39d-eb99-4e71-88a0-ceb5722cef89": {"node_ids": ["ee0f81b7-5b9a-44bf-97b7-e89f9fd49920"], "metadata": {"page_label": "639", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b8296b80-18fe-4a90-a4e6-0d05ab4481e6": {"node_ids": ["7c7f3259-2c7f-4f68-9823-235bda659090"], "metadata": {"page_label": "640", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "de5c0c3b-b9b7-4b29-b428-67b61e26490a": {"node_ids": ["a3ceb45b-6da4-4b25-a6b6-b518c2fb52ac"], "metadata": {"page_label": "641", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2d4e2e04-ae67-4b1e-9fd0-6de65f487f74": {"node_ids": ["948d6ee0-2aad-45e7-a939-220a3aa4ee22"], "metadata": {"page_label": "642", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f47de9d4-04d6-4492-8f9e-fada8643d096": {"node_ids": ["b4dbab65-0c5d-4328-9ffb-5163360f8500"], "metadata": {"page_label": "643", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "448cb869-e223-48a6-9466-f465b6af6234": {"node_ids": ["59ce14e2-9c6a-4e15-a9e4-c763084ddb9c"], "metadata": {"page_label": "644", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6c8b6da6-1589-449e-ad61-3103f05c0bc9": {"node_ids": ["6edccddd-adc5-4b4f-8c73-7cb85e9f18c7"], "metadata": {"page_label": "645", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ddf90a3c-417a-4f55-9e18-7c210bd23161": {"node_ids": ["74f0acb7-8822-41f1-b284-cae932aa3acd"], "metadata": {"page_label": "646", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "927e5ccc-9654-4efa-b67d-336e58e045a0": {"node_ids": ["d6ddd1b3-a9de-4253-a39b-4b90e3ad4446"], "metadata": {"page_label": "647", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "adff3466-6057-4fc7-af37-89d64ecc8d78": {"node_ids": ["8c0199ad-7f2f-402f-82ea-41f2f139dd68"], "metadata": {"page_label": "648", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cf288c58-6c63-41c0-b8e9-5923b61e79e7": {"node_ids": ["88f772ee-d8f6-42c0-b44c-8d67c53723af"], "metadata": {"page_label": "649", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ad17cb25-0e69-4f5a-a218-697c597f70db": {"node_ids": ["a2a3f180-9fdf-4c08-bb2e-89ed1dcdc1b2"], "metadata": {"page_label": "650", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ee24f0f3-22e0-4115-ba51-42484dc95d81": {"node_ids": ["6ccf0433-a8d8-4945-b0dd-b6da5f6376fc"], "metadata": {"page_label": "651", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ee8ce06d-e9fb-4f43-87c2-e25b8f16142a": {"node_ids": ["11b22650-43e3-4840-aea7-2345b75b4cc7"], "metadata": {"page_label": "652", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f6433652-fc14-40b7-ae20-cf5bd618af00": {"node_ids": ["e5979a0e-a171-4043-8a90-254604770c94"], "metadata": {"page_label": "653", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7b466cfa-d99a-46a1-83ee-6d3c084760f2": {"node_ids": ["9bc1cabc-b262-42f1-a20a-d1cafbfa15b2"], "metadata": {"page_label": "654", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2c05c809-20e5-4c2c-8f55-119d4bc65ddf": {"node_ids": ["7f995636-794e-411a-bbe0-d907fb5dfd88"], "metadata": {"page_label": "655", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cad1de93-88ca-4e08-8174-8fb3f4ac4477": {"node_ids": ["e6fda905-21ad-4f80-b0c3-f8e3a61e7bac"], "metadata": {"page_label": "656", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "75b5f002-9500-448f-9455-87f3442f42ed": {"node_ids": ["6162a03f-03d6-4b5f-ab67-504eea69f401"], "metadata": {"page_label": "657", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5155a425-bbba-4e0d-b81c-bd73509a22e9": {"node_ids": ["6d680c7f-75c2-4c0f-a2dd-9eddf2700330"], "metadata": {"page_label": "658", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c9b3c287-44aa-4e4f-b1b2-e4708b5f00d5": {"node_ids": ["22addc01-0df5-4ced-8aab-c2f59392b1db"], "metadata": {"page_label": "659", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e2d42b5d-8155-42e0-a13b-da6a6d12d2e3": {"node_ids": ["5e4f4147-16e1-4f8e-a89e-980befb4a411"], "metadata": {"page_label": "660", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cb244484-4e89-46d9-843c-d9357ffe6105": {"node_ids": ["17c90ec9-8504-479e-98cd-df51239686af"], "metadata": {"page_label": "661", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a91fe56b-a7a9-48b3-9dda-24383d6d1ff2": {"node_ids": ["b752b76d-bef6-42e9-868f-2e710bde9440"], "metadata": {"page_label": "662", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2ead6c59-9aad-4c3c-853b-5977a71b2f6a": {"node_ids": ["0f04cef2-36aa-4512-8bd9-c0e50fb6e61e"], "metadata": {"page_label": "663", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "16e4b73a-02cb-45de-b908-9428488e8b02": {"node_ids": ["b885727c-93c2-4f33-8965-bcc04203a70b"], "metadata": {"page_label": "664", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2b5e44a5-e136-438a-b707-13c9677b9d52": {"node_ids": ["4a1d3429-2942-4e2d-bf9f-2e9aa99466c8"], "metadata": {"page_label": "665", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3000855a-a40f-42dd-b782-0e2f718f848d": {"node_ids": ["ce5c7886-f7af-4317-9a1c-e721c3a0caed"], "metadata": {"page_label": "666", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "38742736-69f8-4178-929a-21d9f6fcb7c7": {"node_ids": ["52783869-7f39-449e-8fa2-c04c6872afe7"], "metadata": {"page_label": "667", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "42e697c4-af99-42a5-be6f-8b715043aeaf": {"node_ids": ["bb713ab9-c16e-4cd7-a3a8-6b17312384df"], "metadata": {"page_label": "668", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2c34cf53-539e-4fe6-bc12-8744b8ab45b1": {"node_ids": ["2a9a17ed-518b-4da6-81ca-2f1b3d71b462"], "metadata": {"page_label": "669", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4ba56652-f7fa-436e-bac1-929f49062fb6": {"node_ids": ["1d2e1faf-6ed8-49eb-8db6-2d7d3cb5b03f"], "metadata": {"page_label": "670", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8291bf50-0ba7-49a6-8c13-6c06e12e8af6": {"node_ids": ["08f62a96-ebae-4d52-b107-cb1d1c30dba8"], "metadata": {"page_label": "671", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "df2bc6c0-5550-4a18-98d1-ca0f8acd5c09": {"node_ids": ["0780a937-ec8c-4ed9-8486-7bb7155b732c"], "metadata": {"page_label": "672", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "47485790-4ffe-4c22-af58-6e528b760ead": {"node_ids": ["ddf2788b-bea6-4ace-bcfa-ddeddff86082"], "metadata": {"page_label": "673", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "11215cf3-b9d2-4731-b9fe-7f036853eb80": {"node_ids": ["68321a47-9b7a-4ef3-a774-6b5ee5595047"], "metadata": {"page_label": "674", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c8e817ba-e86b-48d4-ac4e-a1aeffbd1d71": {"node_ids": ["d9d75b1a-2b3a-4167-8533-0b043c9d95b0"], "metadata": {"page_label": "675", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b787ca16-3662-4fc8-be72-4eecca88405f": {"node_ids": ["5cb1281a-9a85-493f-bdd6-37fd180c1e9f"], "metadata": {"page_label": "676", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7c28d313-2532-4c9f-8243-daf2347b7c11": {"node_ids": ["dd163e78-b195-42d2-a50e-5bc7c06f99d8"], "metadata": {"page_label": "677", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "267bec77-a109-448a-9ce2-c9dab2a59cf0": {"node_ids": ["ab21cab0-2221-4684-9657-338b2519d8b6"], "metadata": {"page_label": "678", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a299c54e-0947-4772-a46f-d5d9a57a719a": {"node_ids": ["95390684-5560-4fc4-ad8c-8ec02dc575a1"], "metadata": {"page_label": "679", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "424a7c3f-4cbe-4b51-873f-cb880a91533f": {"node_ids": ["4bd82f63-bb4c-4e84-bf44-6a87939cf60b"], "metadata": {"page_label": "680", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a9644e0d-7967-4a02-aca2-837028f12e99": {"node_ids": ["b78e44ce-8841-43b1-9356-95b875a27223"], "metadata": {"page_label": "681", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "51214602-82da-427b-af0c-be8b40887e07": {"node_ids": ["9f41ad85-ff2f-4e67-b93d-383341400f98"], "metadata": {"page_label": "682", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7c805a67-0a63-4054-9ca6-d9cef8ab4d91": {"node_ids": ["e121fb3b-03ff-42f3-84c9-a5db85f241d8"], "metadata": {"page_label": "683", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "153f8743-c109-4227-8d0e-3cb67514c743": {"node_ids": ["8f422671-7ff5-4473-acdf-c1a5300b06c4"], "metadata": {"page_label": "684", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ce71b5d1-0b1b-40a9-a00a-26fba5fb8b4b": {"node_ids": ["2eae8a1f-7c00-4aff-8c35-1941ffa24651"], "metadata": {"page_label": "685", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4f88ba83-e254-4fe7-8620-46afd37b9966": {"node_ids": ["ee6eeb25-c9b5-4a0a-ac6e-07b6973a45c4"], "metadata": {"page_label": "686", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "85b8a2aa-3b2a-416f-8590-f7f3959d1cf8": {"node_ids": ["105adfb8-c69e-43ab-91ad-21a7dc27f40d"], "metadata": {"page_label": "687", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8176dbfb-831d-43b7-bddc-dd2a10529fd8": {"node_ids": ["e5b35ed3-7cda-4f62-a004-03a375713c5f"], "metadata": {"page_label": "688", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "be5a3c9b-efc9-4b99-a413-3d2c72c0ce96": {"node_ids": ["520eb548-6559-433f-9948-6e73e1b43b30"], "metadata": {"page_label": "689", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9b69b97b-be75-40ee-a84d-807ffa98292b": {"node_ids": ["00a0c6da-b58a-4d7e-ae2d-14e3ebdcd07e"], "metadata": {"page_label": "690", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "88c57ea9-bd30-4bef-8e83-b2b5a37ab8d8": {"node_ids": ["c33684d7-9377-4261-b531-3a085915a23e"], "metadata": {"page_label": "691", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "be8fba07-60f5-4216-8222-bd8a3c56d0e9": {"node_ids": ["e63113c5-e2a9-498a-a0d6-d6bbb19f7875"], "metadata": {"page_label": "692", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e9697af5-4d69-4aff-a3f5-538429ef7550": {"node_ids": ["60087b53-0045-45fe-80a3-94536fc4e761"], "metadata": {"page_label": "693", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "aafd4e9e-d5fa-43c2-9eca-b06a98e65060": {"node_ids": ["67d091fe-dc9b-46ae-b5ba-2df989b15fe4"], "metadata": {"page_label": "694", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4b87256a-30e7-44a9-91ff-f96bf683ae27": {"node_ids": ["9c34ef9b-041e-4cb1-baa3-ce1ba0f59151"], "metadata": {"page_label": "695", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ed46322f-6583-4922-9e4a-01e75987ef46": {"node_ids": ["20ab309f-ff31-4269-9fba-6d4909b01a86"], "metadata": {"page_label": "696", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dbde8267-f37d-4e57-a99b-5ef58957fcfd": {"node_ids": ["603bbae9-6235-40dd-84d0-f939c4110e04", "c0d5bbe9-09f1-4854-9c8d-7cf6ea109deb"], "metadata": {"page_label": "697", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cf3aec92-d71a-4240-afee-bcae965b15d0": {"node_ids": ["0b045def-dc24-4b65-86fb-555504f169f8"], "metadata": {"page_label": "698", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "764f70f0-b2b0-4ce2-9594-5cc7e4b35650": {"node_ids": ["7a62ae3d-f907-4b3c-98a5-d8adea9cf3b1"], "metadata": {"page_label": "699", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c1e645a1-7aac-412a-b4c7-0d1e163e8716": {"node_ids": ["4b7c982d-6e6f-4233-acf0-7a0e66280f28"], "metadata": {"page_label": "700", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e29889e9-a394-44d3-b666-4cc65390941e": {"node_ids": ["9c08bd1c-1a63-4776-a9f2-d5856aa28fcb"], "metadata": {"page_label": "701", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d635aefd-96b4-49b6-8dfe-b7e606bb749f": {"node_ids": ["e6c9566e-8751-4664-8782-d7b72ba64434"], "metadata": {"page_label": "702", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2cbd9874-e1d4-423b-992c-8b2db529cd7f": {"node_ids": ["8c3a1e48-6d17-4190-b9ff-02cc831d3ed5"], "metadata": {"page_label": "703", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "188863b3-daac-4441-a607-fe109b0766dd": {"node_ids": ["5d101822-629a-4eb0-8767-d1a4449ce12f"], "metadata": {"page_label": "704", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bf955db7-1e7a-45a0-aef1-39cf22c1baea": {"node_ids": ["aa31036c-f851-4ec4-ac18-ced0c21ded84"], "metadata": {"page_label": "705", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3f8bbbc3-3ad3-42b7-bc4d-fd4f73f1e647": {"node_ids": ["656ee8c3-a454-49f4-b21f-08c2f6aac575"], "metadata": {"page_label": "706", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a2b965a9-1a4f-4ee9-a1a5-21e15307ae2d": {"node_ids": ["b39e720b-d514-49c1-a485-eb73286fbb7c"], "metadata": {"page_label": "707", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8b394317-0275-45f1-a6f5-ef641760a114": {"node_ids": ["b671dbb3-d1cb-4682-98b8-898df2326874"], "metadata": {"page_label": "708", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6395cf6a-89c4-485e-8465-5c8f2253d8b7": {"node_ids": ["1e5f1d46-3a11-4ff0-8151-b97cbf1ef3bf"], "metadata": {"page_label": "709", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "434751c2-f69d-4511-99ae-c9a2ffd49eb2": {"node_ids": ["472660aa-0262-41e3-b4e3-4990581fa8d6"], "metadata": {"page_label": "710", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "da05a50d-7e02-4334-b061-2bdfe03432b9": {"node_ids": ["e5811d56-5146-4e71-bbfd-decd52533389"], "metadata": {"page_label": "711", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bd801881-de1c-4f9f-8585-8503650f0e76": {"node_ids": ["9b732d77-f709-42c0-a81b-3f2538c64755", "64801aeb-2037-4cc7-b06d-b822589788d2"], "metadata": {"page_label": "712", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6c5a2a59-6761-45d6-9e72-91a950acd176": {"node_ids": ["7db79abb-0693-4028-a608-3deae8cf3ed6"], "metadata": {"page_label": "713", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c85bc06f-c06a-4756-9e85-6ae6bd17e2a7": {"node_ids": ["4d77d70d-92a4-41d5-9174-5172e5d3eee1", "c11f807e-f02c-4039-87b5-b7912935b536"], "metadata": {"page_label": "714", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dee01888-6c66-454e-ac69-9613290d207f": {"node_ids": ["ac79371d-1e1d-4af1-842c-d0843ae4f7f5"], "metadata": {"page_label": "715", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "043e995c-ebc6-41aa-963e-b051cfb39023": {"node_ids": ["35b9af45-a378-4aaf-aa1e-c19782fb6315"], "metadata": {"page_label": "716", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9c88e89d-1572-498c-a162-4b2399be03e4": {"node_ids": ["9fadcd69-b57d-49c1-aa39-8b52110a6b72"], "metadata": {"page_label": "717", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "18e53e85-30c9-4f79-a6ac-cbb67c90bb20": {"node_ids": ["72cf49ee-9743-46cd-afc1-11315b197351"], "metadata": {"page_label": "718", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bf3a0ad1-69a9-4a77-a836-004f10a7feb1": {"node_ids": ["36b1e70d-2fec-476d-8ca0-aa589ecd16c5"], "metadata": {"page_label": "719", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f51033c6-0578-46b9-80c0-b8b9a7736f85": {"node_ids": ["7283e245-0259-4ec4-9fea-80cf06e8cdc3"], "metadata": {"page_label": "720", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f7be9653-0593-489d-abc6-4164e0769f78": {"node_ids": ["95e79758-4a3b-47a1-abaf-a1033d83c3e9"], "metadata": {"page_label": "721", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2b49817c-3133-4373-a1c8-9d4f5f8d908f": {"node_ids": ["aebfe93c-afb2-4d1f-bc1a-f0236e83d72a"], "metadata": {"page_label": "722", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b56ba533-2859-4fc1-8842-0876c945c994": {"node_ids": ["c6ed486b-ecb4-48db-9d74-50f0a167625b"], "metadata": {"page_label": "723", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c70ebe84-e40e-4547-b42f-69e1036482e8": {"node_ids": ["8f51ee76-46d4-4678-af84-8ed3a4b10d74"], "metadata": {"page_label": "724", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c1921b58-16da-468f-afbd-15aa161c8103": {"node_ids": ["c30ea178-e97c-4de9-900a-f8b318d7b678"], "metadata": {"page_label": "725", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "711bd499-e9af-47bf-af3d-d868d7ae49bb": {"node_ids": ["1f772bae-4358-47cd-9437-784c5f917b75"], "metadata": {"page_label": "726", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a691f552-13c7-470e-91b4-835659309171": {"node_ids": ["2e026d2e-9a10-4d60-a3cd-e0873f78b389"], "metadata": {"page_label": "727", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "24ae88e3-bf25-43d3-868d-11372ed0bb7e": {"node_ids": ["4af72307-fad6-45ea-a56e-c9243718a6da"], "metadata": {"page_label": "728", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5ea7bc45-6623-4d87-b6ae-d410bf58478f": {"node_ids": ["7df5a440-856e-405e-99bd-e3943da3ce15"], "metadata": {"page_label": "729", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "84dc5fb7-ecba-479b-9c9d-b186d8d29622": {"node_ids": ["d0b93bdc-e656-4ea0-8ee8-abc701f10e68"], "metadata": {"page_label": "730", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9791e908-04ad-48b0-a9ea-7d90973d0f83": {"node_ids": ["e32bed8e-2fdd-481d-8e4f-16d5f79d0974"], "metadata": {"page_label": "731", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2d3f32ca-e39a-4bbb-b52c-b489c4653ef7": {"node_ids": ["b1bf9c5d-d6cc-4719-9d25-4f25b711a3ac"], "metadata": {"page_label": "732", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3474da39-a95e-4fd0-a2ab-6b7569485ea6": {"node_ids": ["21280591-bb69-497c-a6f2-34ae2d104fc1"], "metadata": {"page_label": "733", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cf62ba83-137b-42a0-8ae8-6e29cae4d7d5": {"node_ids": ["b615f598-7889-4626-a3eb-1e9d4a0375b1"], "metadata": {"page_label": "734", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5ca994dd-546a-4d4e-a130-976f64eaa4ee": {"node_ids": ["db5f2b93-137c-44ce-ba27-931f3ac5a1b5"], "metadata": {"page_label": "735", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "030089ec-2105-4886-ae34-1daa00b518b4": {"node_ids": ["ffffc70c-5201-4fa4-a22d-ecd5005bb512"], "metadata": {"page_label": "736", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fe686b58-d837-432e-b3cd-a6f918c6893f": {"node_ids": ["0d472ffc-d582-42c7-9a0e-5d727d4b5807"], "metadata": {"page_label": "737", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "84472535-029a-4fe2-85b3-6aff6c5030d6": {"node_ids": ["351c368f-bd61-4372-a13e-7b6fbc1f17b4"], "metadata": {"page_label": "738", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7eaf12b8-d3f1-4122-a030-afb3c65c122c": {"node_ids": ["9d686143-7b0a-4530-b6f9-856b4f638512"], "metadata": {"page_label": "739", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3bac9c96-5aa6-4031-b055-e7d034ca1ec2": {"node_ids": ["f787b0c8-5ab5-44af-b1ab-63192b1b2591"], "metadata": {"page_label": "740", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0bed854b-0d52-4e3d-a9cc-215a62cf62b8": {"node_ids": ["425309cf-d435-40f8-95dc-b6c98b585664"], "metadata": {"page_label": "741", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f776f761-4dab-423a-9127-59af5ac54afb": {"node_ids": ["11ec3056-c3a7-4d3e-8b58-44d44e6d8f5b"], "metadata": {"page_label": "742", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "85ef6810-78f9-4ee3-803b-1832f959b972": {"node_ids": ["18bdf931-4694-4cf5-ac01-ed68f0e9d00f"], "metadata": {"page_label": "743", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cec06a64-d579-43f5-8eeb-7d566eefbb09": {"node_ids": ["e0b0a6ae-4ac9-44d4-a9ad-49953cf0c4a8"], "metadata": {"page_label": "744", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0432f765-52ac-46b7-8a64-46993358215e": {"node_ids": ["dcb30c19-f285-40f4-b2ee-9bffa1ccd1ef"], "metadata": {"page_label": "745", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0d3372c6-1931-4ee3-bad5-ef141ce3abd2": {"node_ids": ["0e36d0f2-76f4-4738-9a6f-b19365d0c4d1"], "metadata": {"page_label": "746", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "95426591-5709-442f-8cd2-8abf65a52984": {"node_ids": ["065d99c3-808e-4a13-a2e5-1b237b08d3de"], "metadata": {"page_label": "747", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6065b453-9eba-4f13-916f-941a767417b9": {"node_ids": ["a30f9ca9-727e-46bd-a64c-1c524f941920"], "metadata": {"page_label": "748", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b356567d-d8f2-4cfd-b2dd-9c9113b3039f": {"node_ids": ["51303d9a-7c64-4940-9d2e-fe8656f5aa01"], "metadata": {"page_label": "749", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "11703f22-745c-463d-b53e-eb05835c97bc": {"node_ids": ["b3527a61-ed6e-4de8-8cbd-9cd5b4f07700"], "metadata": {"page_label": "750", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "39c69c99-8687-4d50-a2a8-b4a5d3b50782": {"node_ids": ["0d3dadfa-6703-41e7-971b-57a5b2ef1aa5"], "metadata": {"page_label": "751", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b109ff1f-c244-414c-b811-ceeec2cccaad": {"node_ids": ["e8960da0-1db9-416d-9e30-77ca35ff721f"], "metadata": {"page_label": "752", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "50b099ff-a2fb-4ee8-b292-12c29a73ef64": {"node_ids": ["fe30fa36-116a-46af-8217-5364e41a75d9"], "metadata": {"page_label": "753", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b5a44a25-85bf-411b-8bf2-5e06cac2476e": {"node_ids": ["443af01c-f4ea-47af-8c78-0faab5af3436"], "metadata": {"page_label": "754", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b86d7efe-4a9e-4ee8-a6fa-b700dd73f112": {"node_ids": ["89a6f371-31ee-46f2-92a5-7bb768ce96b2"], "metadata": {"page_label": "755", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "49cb956b-4ce2-4d7b-a872-8b8f03de9b63": {"node_ids": ["0191d8bd-66a8-4914-af7d-8736dce04c78"], "metadata": {"page_label": "756", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0e6cc344-e889-44d2-8e98-166083dd04ab": {"node_ids": ["71b1f9ca-8959-46e8-8ebf-331f0102c9cb"], "metadata": {"page_label": "757", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c61b74a1-7b19-490b-ab97-65784629777e": {"node_ids": ["0c99654e-5608-4c86-b557-56483cc32623"], "metadata": {"page_label": "758", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7839cb72-b817-44da-b6b0-15003fd7aead": {"node_ids": ["f513bbed-6278-49a2-a71a-24b3505019ea"], "metadata": {"page_label": "759", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5db7d381-2daa-4348-ab88-dd1911f16400": {"node_ids": ["6b7ff5ff-9e1a-47bf-9e8a-9971c362adfa"], "metadata": {"page_label": "760", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c031c70d-2e20-48ef-bbf2-03e0041c4461": {"node_ids": ["412732c4-5b34-424a-9ded-76a11e37b52d"], "metadata": {"page_label": "761", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "03ba42ed-82a1-4213-b845-2f336a9ec302": {"node_ids": ["b71f7195-5add-455f-b868-adf835bbd3ad"], "metadata": {"page_label": "762", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "51c66e7f-b309-4cfb-9c21-3335b3889c8c": {"node_ids": ["ee64bb12-3e52-413b-a725-f2056f91b46b"], "metadata": {"page_label": "763", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "588f1e82-afec-4918-b42a-2ff948c3cc10": {"node_ids": ["64a9b03a-71c8-410e-a854-4e2d32ab2337"], "metadata": {"page_label": "764", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8872df19-acec-436c-b456-b8d0c5d4b250": {"node_ids": ["e391b914-9ede-4c6d-84e5-bcf382a9c8d7"], "metadata": {"page_label": "765", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "27cd9a90-cb1d-4ee4-9bb4-ef3c74eacb17": {"node_ids": ["d7445ab1-1718-4f75-b8ea-3ae091754f5c"], "metadata": {"page_label": "766", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ce9258da-39c2-4b61-83f2-89cc5f8ca5e9": {"node_ids": ["e509daa5-5acc-492d-a748-12d3957141f3"], "metadata": {"page_label": "767", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e309113d-9eb5-4b40-8f10-c43fe9c54ef2": {"node_ids": ["b1128b08-8bd9-4477-a44c-a0d0c7e67572"], "metadata": {"page_label": "768", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "602464f7-80eb-4e19-8327-f59b328d6d78": {"node_ids": ["54a5e2a2-ba94-47fb-9b3b-e9ce63feb942"], "metadata": {"page_label": "769", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "991697e3-7fe4-4267-9fa4-feab4a729ea9": {"node_ids": ["9c9f43c1-9cd6-4e28-992a-8cbc3180f8d1"], "metadata": {"page_label": "770", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ecff123b-7552-4248-93e0-8c289547f96d": {"node_ids": ["f7fad0b9-2844-4cb9-bd52-c5d9f147c21a"], "metadata": {"page_label": "771", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "aaa0a4d8-6001-4888-b687-1dd9816c32d6": {"node_ids": ["14869d58-fc89-468b-ac00-95185073a0f9"], "metadata": {"page_label": "772", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "90d22edc-2f3c-4d2d-8950-77d96e519f7a": {"node_ids": ["0dd6b08d-eb79-4b64-9372-652874309ea7"], "metadata": {"page_label": "773", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3fbcd913-a890-4651-b833-e753bca1427b": {"node_ids": ["c190d23c-b1d0-451d-80b7-4d2573b2d88e"], "metadata": {"page_label": "774", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "249417f9-7762-49e9-aec1-a4a04edb652b": {"node_ids": ["01bd7402-96a4-4164-870b-5c9dd0f225ef"], "metadata": {"page_label": "775", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2ee32b91-20d2-46d9-b8ae-688f57246466": {"node_ids": ["29fd5fde-b463-4c33-b288-1189ec085985"], "metadata": {"page_label": "776", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d5557427-3952-46cd-9d61-f5600faad9a4": {"node_ids": ["a2b4e785-9f94-403d-a3d8-01ec399ac9eb"], "metadata": {"page_label": "777", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "beec423b-6d55-4e96-bcb3-b6d24cd12f22": {"node_ids": ["0b15b222-8000-406a-9ce9-bca780217814"], "metadata": {"page_label": "778", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b3ddb4ab-7699-4279-a54e-ef343a82216b": {"node_ids": ["dbc9e6e9-8b61-4b17-868d-55f91d1b332f"], "metadata": {"page_label": "779", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "371143be-00ad-41e1-9555-6c54db6b5d70": {"node_ids": ["5ec2393c-4a87-43ba-8734-e20e4faae9f2"], "metadata": {"page_label": "780", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a1f5bd7d-8c87-40c0-a736-1cbb566acfcf": {"node_ids": ["f1411d93-4bd6-4ad2-a665-58eab8c08e52"], "metadata": {"page_label": "781", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "75f6d77b-b5fb-4a98-9181-63c2c8d6a0a0": {"node_ids": ["e28091a5-55c2-4bf6-858b-5d9168151a8b"], "metadata": {"page_label": "782", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cdc66615-e8f7-454d-adde-c492ab98fec4": {"node_ids": ["0f690db0-6e9f-4ac3-bf2a-66ba64e2745c"], "metadata": {"page_label": "783", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "70d270e3-7220-4e0f-94c9-a7e8051cfc2d": {"node_ids": ["020beea1-5122-43c3-8c61-b94a29d840ea"], "metadata": {"page_label": "784", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9faf1f0c-f20e-45c4-a5fb-f00d2b379b39": {"node_ids": ["1f15aa12-9adb-46c0-9097-d722f80adf07"], "metadata": {"page_label": "785", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b8bbc4af-4a25-4e78-913b-68a40eb2da3f": {"node_ids": ["67cd1bd7-9233-4460-9e78-7ee08dc10107", "8432446f-5322-45ee-854c-36efaa264b3c"], "metadata": {"page_label": "786", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fda93fcd-fd9b-4570-b8c8-4b013c4ffbf7": {"node_ids": ["05b8bcb2-bb0e-42cf-8d1f-108223220ffb"], "metadata": {"page_label": "787", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e333f705-a029-434e-b76f-96a180a3ec34": {"node_ids": ["1ec2911f-0c0a-488c-b41a-61c8310d01a8"], "metadata": {"page_label": "788", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e4bf679b-758e-4449-9934-9decdcd64c53": {"node_ids": ["68a55943-a7b8-4f96-9cd4-97fe6b5d6f60"], "metadata": {"page_label": "789", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b557cdc2-8574-43f1-a84b-ceae39550074": {"node_ids": ["2e345e3e-b3bd-40c6-97e0-9856bed35db2"], "metadata": {"page_label": "790", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "475a5953-2fbc-485d-9a76-a275325c2dd3": {"node_ids": ["984f68a1-5c59-4ab0-955e-1f4ce72d931f"], "metadata": {"page_label": "791", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "eeabf41f-7fa0-4681-ba2a-3676bd7f9385": {"node_ids": ["d12226bf-99f0-4c1b-a956-1f2e0a4e606c", "4c861fe8-96e3-480c-bbba-85e31eb13593"], "metadata": {"page_label": "792", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d2f2c5a3-3ef3-4208-9c97-8b6f48180a05": {"node_ids": ["5c3af813-c686-49f4-bea0-a67affde60f8"], "metadata": {"page_label": "793", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7773128b-4279-4bd5-8144-65c100fd4481": {"node_ids": ["cbcb7c17-f72b-408a-8688-5ddabe10e550"], "metadata": {"page_label": "794", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0b53fdd7-cf28-4b6d-9a44-65a9f35ec628": {"node_ids": ["d4d958a7-9457-413a-9fd3-636f479b6646"], "metadata": {"page_label": "795", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8a9e960d-a1d1-4c7e-b7af-2d9cd6a85549": {"node_ids": ["f7ce98de-95e4-4e73-8b1b-3feb8562c55c"], "metadata": {"page_label": "796", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b9708d7b-2cbb-4ce0-aff5-31fe4187ccbd": {"node_ids": ["bb76bc25-e83e-4a8c-80ee-e1988629b130"], "metadata": {"page_label": "797", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "406a13f3-24ae-4bef-b893-d053451e7b5a": {"node_ids": ["d0d0bbe7-4717-4156-876b-1b542b2a70ca"], "metadata": {"page_label": "798", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7cf12f6b-4a01-4d4c-a1ec-a4527afd1d5b": {"node_ids": ["96a53220-c096-4b50-95fe-93dcae10c664"], "metadata": {"page_label": "799", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4f03000c-d65f-41f7-96a0-619d946bedd3": {"node_ids": ["5b2a49cf-65d9-42f6-8f7e-4ecb68364a03"], "metadata": {"page_label": "800", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "784be029-23c4-47e2-bdf8-5090491c901e": {"node_ids": ["7706b2ab-2b76-4918-82f2-27a4bec76622"], "metadata": {"page_label": "801", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9a8c1c69-39cb-4151-882f-cbb0291d5719": {"node_ids": ["3895357c-aa77-4eca-a240-ebe16c6e2179"], "metadata": {"page_label": "802", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "64175e74-07a9-49a0-8e56-3aa026a55bb2": {"node_ids": ["25ad30e8-57d2-4820-bdc3-fed766c18f44"], "metadata": {"page_label": "803", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "beec1148-fd1f-46bb-a681-21bb55fea26c": {"node_ids": ["d75b9c67-0990-4fa8-8a7f-c6e95606572d"], "metadata": {"page_label": "804", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "48bae3a8-01e6-4887-abb1-ae54676b3cf6": {"node_ids": ["6bd2f278-aee8-4434-bd31-4fd92c885c72", "fb9456ab-509b-4178-b44b-7b0535a777f2"], "metadata": {"page_label": "805", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "246062dc-ef95-400b-abac-45eb0bf6b60d": {"node_ids": ["c8008c2b-d093-4d35-99a6-58770f39a0aa"], "metadata": {"page_label": "806", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a19c1116-e34d-4ae1-9ef6-d954e12bc1b3": {"node_ids": ["a9088e13-8f94-4308-adb5-df414805c66f"], "metadata": {"page_label": "807", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6ce8bdbb-b31c-405b-a012-6c4c5db48cad": {"node_ids": ["fdec6584-db17-495d-8060-e4a074ac7a32"], "metadata": {"page_label": "808", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8e5eab7f-1012-4e31-a6c0-4f897ce5eee2": {"node_ids": ["6985f50c-b4b0-477a-aabc-18ef484e5059"], "metadata": {"page_label": "809", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2f5cbeb6-935c-4c7e-a439-edeef28d34c3": {"node_ids": ["e362f91a-0882-40d9-8cd8-ece14fed4064"], "metadata": {"page_label": "810", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9ee5bb24-c120-43a3-9528-a452917cf703": {"node_ids": ["cfd71b81-5f9d-48cf-86f1-99a2b519efb8", "66b9e61c-d156-4042-824b-83324d37f50e"], "metadata": {"page_label": "811", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "51605275-0f9b-4bc1-b9af-171f8522911f": {"node_ids": ["1e5966ae-30b1-4bb5-8e19-3413171d49e1"], "metadata": {"page_label": "812", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "124870af-5fb8-4ebe-b88c-db7bbd87dab2": {"node_ids": ["69813ee1-f20c-4843-8c2a-def45907f5e9"], "metadata": {"page_label": "813", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "359bcd3f-673d-4481-9332-631ed382ddae": {"node_ids": ["fd71d456-d9c7-4392-87e7-490b559d965b"], "metadata": {"page_label": "814", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6644cc7b-7f6c-49f8-a567-a9032fc20b74": {"node_ids": ["179b325d-5bab-459e-a0fa-d4515753018c", "55892b6f-3b03-4d86-aa18-979028e09910"], "metadata": {"page_label": "815", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1c8a088f-907f-4cd3-a9da-7279e4125309": {"node_ids": ["3abd310c-0878-4a8f-896a-7da87c49c6e5"], "metadata": {"page_label": "816", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a8962609-5222-4c18-b050-d24755f4c61f": {"node_ids": ["5c96a450-ef7d-4096-b67f-5bd8e80edc95"], "metadata": {"page_label": "817", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bd8f1cba-b5d9-4b89-8fc0-bc7fbff84e63": {"node_ids": ["163ba4dd-0169-4ef2-bfb8-c3a318b62152"], "metadata": {"page_label": "818", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a078d418-b645-42b0-bc41-d0b160fbd004": {"node_ids": ["10c5b82a-6940-40fa-9423-30f346f62f9a"], "metadata": {"page_label": "819", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "604fc341-6e0c-4e9e-9b25-38c0ca29f3d9": {"node_ids": ["f23a7dac-1f1c-49df-91ba-bafb743f5333"], "metadata": {"page_label": "820", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2ec51096-01d3-4ed2-88a1-83e80259a523": {"node_ids": ["5df50c6e-d10f-43cb-8791-ba26e34c1b23"], "metadata": {"page_label": "821", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0a41b72f-eb21-4a70-9f48-fa7efccc8866": {"node_ids": ["a410acb2-e91f-4ba1-aae6-36dca069cfd7"], "metadata": {"page_label": "822", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4ffe1006-dd87-4936-a453-7e81e59b4232": {"node_ids": ["74605981-9cf7-4e6a-9992-0b05073d65d7"], "metadata": {"page_label": "823", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1b0a366f-eac6-4b24-bd1e-f195267bc162": {"node_ids": ["d67530fe-05bb-492c-8137-8998b5ebbca6"], "metadata": {"page_label": "824", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "32d20442-5c93-413d-adba-c9221154560f": {"node_ids": ["c307ee79-5254-4d9f-af82-9b67c0de71e1"], "metadata": {"page_label": "825", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "39da3daf-dac6-4def-bedb-9bcecdf5e61c": {"node_ids": ["f116b160-1b0b-4177-b659-0c0790d1580e"], "metadata": {"page_label": "826", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a36e428a-710a-47dc-8317-81cf415ab194": {"node_ids": ["49190b57-77dc-4635-89f9-ffa1a07ed402"], "metadata": {"page_label": "827", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6b1235b0-ce4f-4239-80ed-bf49f662714e": {"node_ids": ["8ea66508-37cc-4c0a-a121-3213ec875673"], "metadata": {"page_label": "828", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2cf657d8-c137-40af-b472-0306010a0b08": {"node_ids": ["c23c5bdf-a5b7-4681-8876-78abeb44ee50"], "metadata": {"page_label": "829", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f5e7b054-13fb-4818-8455-0aa0a204501b": {"node_ids": ["4878d4d0-9dcf-4d59-a113-ce59de5a21a4"], "metadata": {"page_label": "830", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cf1d45f7-7a98-4cfd-9fc5-cc9c550f4904": {"node_ids": ["986cce1c-46ed-4ef7-b82f-de28706cb40a"], "metadata": {"page_label": "831", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "94c21bec-196b-40a1-892d-f12d241167ea": {"node_ids": ["77c0bd3d-6f57-465f-9270-bc7d1f1e00c0"], "metadata": {"page_label": "832", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "31fac131-303b-483f-a23b-4479b48169d4": {"node_ids": ["b721d2cd-c81b-4c99-9699-fa5d6786041a"], "metadata": {"page_label": "833", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a6b0afe5-5335-45b7-870b-94c517864f57": {"node_ids": ["cb8fe440-ee71-46ef-ae48-7b27f9777a28"], "metadata": {"page_label": "834", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5e1d98cd-dda5-4e38-912e-9dfa1afbe47a": {"node_ids": ["5db0efcf-c8de-4053-8828-f4a23c6430a2"], "metadata": {"page_label": "835", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "65db84e1-f92a-45fb-bc28-27bd55d563cd": {"node_ids": ["fc32a593-37a6-4b89-a987-aa526e29baa5"], "metadata": {"page_label": "836", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7338dfe4-7ca9-4526-bcc8-bfb199e92eb5": {"node_ids": ["dd7138b1-f320-42a8-8554-925c09e16009"], "metadata": {"page_label": "837", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7f2e7113-15ed-4db4-981e-09bbc754cd89": {"node_ids": ["1cd795f7-d7aa-42a9-b4b8-41bb723b1f91"], "metadata": {"page_label": "838", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f1ac8b69-9df5-4e79-9a21-4dae840f7a17": {"node_ids": ["8ee354e4-9257-447b-ae48-666606e9a5af"], "metadata": {"page_label": "839", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "110cc1fe-ac50-4d5c-bc9e-3e94486485a4": {"node_ids": ["59fcccaa-b9bf-4754-b54d-4690f97751be"], "metadata": {"page_label": "840", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ed270164-359f-487f-a02d-56f657461b19": {"node_ids": ["b5114c0c-243d-46d2-9027-f3de53cbcf8d"], "metadata": {"page_label": "841", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "56c0033d-29cc-40fe-9cbb-e7b585840e06": {"node_ids": ["769fced0-12b8-43cd-97e8-fa27bc48a22e"], "metadata": {"page_label": "842", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3725b24f-89e2-4b7c-ad3b-1bfffdb29df5": {"node_ids": ["3e5dc8f2-7def-4380-9433-ac7e1c908334"], "metadata": {"page_label": "843", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6e927557-037e-4583-9ac7-66db4dc513ba": {"node_ids": ["63dca23a-ab70-4bd6-a36e-f3a0be20a20b"], "metadata": {"page_label": "844", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "530608eb-cdb1-4fce-872b-6a58f7bb0466": {"node_ids": ["7eb47cb7-6ae8-4c4a-b27a-2e9e8dec25d6"], "metadata": {"page_label": "845", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9d8a0107-2c78-4811-b35a-71a77dd713ef": {"node_ids": ["aaf6458c-af1c-4651-bc36-4c9e4d5ed576"], "metadata": {"page_label": "846", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7d3bb5f7-7610-443f-af36-1cc6c11d6b5f": {"node_ids": ["c8881196-1913-4fcd-bf15-3346ec1f87fd", "2bca6cd1-b02e-4386-8607-9b0734bb8362"], "metadata": {"page_label": "847", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "779ba5bd-e08f-4714-b0c5-43aa1106be38": {"node_ids": ["7101c4a3-15af-479e-91b7-734c641d02de", "f8a58d56-c22e-44c0-b3aa-1c17950e20c2"], "metadata": {"page_label": "848", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e17272c6-7085-4124-be4d-f078d58a8ef6": {"node_ids": ["b96aff72-42bd-4642-967d-fbcaa8044a6e", "a1e3edb6-4574-4892-ba50-d1f4dd8ad966"], "metadata": {"page_label": "849", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "531f88b7-b6a2-4ccc-ac79-064f19d29c1b": {"node_ids": ["a0ff30db-152e-43a7-a7c2-af2889767a56", "d8c0a719-fdfd-4e5a-98bd-ee52a8a2d2f0"], "metadata": {"page_label": "850", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1e037851-65ab-4083-b330-ba70f831a7ce": {"node_ids": ["e2659921-acd7-439e-8d69-f3badfd570a8"], "metadata": {"page_label": "851", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9d288575-1054-4d06-a7e4-482c794c123b": {"node_ids": ["136d7519-4d1e-4301-9825-7527f2f80092"], "metadata": {"page_label": "852", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f035cf80-3eec-4d97-8d95-78b2eebc897b": {"node_ids": ["a24270a9-b8ae-4b89-8799-30a053344137"], "metadata": {"page_label": "853", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e61ece3c-0d5c-4cd7-889c-24d278806d15": {"node_ids": ["ad462851-148b-42d2-a010-23caba5d4321"], "metadata": {"page_label": "854", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2d3ac2a7-90f8-40ad-8b83-c33860f25f7f": {"node_ids": ["b909f588-a3de-45df-8762-3c1426e2a3ef"], "metadata": {"page_label": "855", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b6d65c86-b497-4c6f-8d4b-8f8e3d78fb52": {"node_ids": ["7602450c-d56b-49ee-ab39-6c64047c491c"], "metadata": {"page_label": "856", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "588c75df-6df1-4243-bbcf-ec4da15cbaa2": {"node_ids": ["e1ade370-fc46-43d6-8bf3-257686b1f547"], "metadata": {"page_label": "857", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d151f6aa-271d-46c3-acdd-bd7172340711": {"node_ids": ["01ba77ba-fb95-46e8-be69-ff27d4090e4c"], "metadata": {"page_label": "858", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d3aebd53-d2df-4f54-9f60-d8e8ca347b15": {"node_ids": ["3278088a-a925-4b84-b8a3-89a238104c5d"], "metadata": {"page_label": "859", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4cd67c24-fbe5-41bc-a187-15fc5f828cfc": {"node_ids": ["87b1f151-8445-40ba-85a8-a09fdf56bf7d"], "metadata": {"page_label": "860", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "54e355fc-6c2d-4861-9d88-808af8e9725f": {"node_ids": ["c07c8bb8-5ab0-4946-bce3-e6bbc55b1ac1"], "metadata": {"page_label": "861", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0768b5a9-5cb4-48d4-88ec-ed9a34ed203a": {"node_ids": ["c796b18f-18ac-4d2f-830a-c7324bcd2c55"], "metadata": {"page_label": "862", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9790a890-7e97-49eb-922b-537b4dc0c023": {"node_ids": ["cca303b5-728b-463f-b652-e7be1a4db934"], "metadata": {"page_label": "863", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f9a0f2bf-83b4-4fcb-bff8-bd785e32047e": {"node_ids": ["7a74a2e4-82c5-4a12-89a1-d18eac3cdde6"], "metadata": {"page_label": "864", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "31798e22-345d-431a-96cb-701d39cd51f1": {"node_ids": ["ac9043a9-033c-4fa6-9595-30f20ca0016d"], "metadata": {"page_label": "865", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "698003fc-d2b5-49f0-a816-431af7f5337d": {"node_ids": ["ed743a92-8157-4e05-885d-88fbbcdda53f"], "metadata": {"page_label": "866", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0effb730-ac3e-43fe-9627-820435850fa1": {"node_ids": ["1f562305-f063-46b8-99e4-a5190e1df54c"], "metadata": {"page_label": "867", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e6e68ab4-27fd-45ae-ad8a-e064ed829306": {"node_ids": ["85bb53d9-a52b-43a6-bb47-1a684aeb6b2f"], "metadata": {"page_label": "868", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a2c4ecac-c16e-459d-95e5-0ccd9444b343": {"node_ids": ["97024dc6-f113-4144-9201-04d5090f1167"], "metadata": {"page_label": "869", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f2a23db9-93b8-4661-a0f1-37a4faafc8f4": {"node_ids": ["61549da7-5211-4da7-bc9b-3b26272a5681"], "metadata": {"page_label": "870", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cc824a43-68f6-49d9-93eb-6014ba1848b3": {"node_ids": ["722f6dcd-5131-4050-94d5-a625d3f32955", "60e4271f-b826-4e8d-ad41-8a2954dd5d71"], "metadata": {"page_label": "871", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e36eff1a-6947-4f1b-b943-f7acecdc28e2": {"node_ids": ["2fbcb017-27c6-4248-a54f-d983876c95df", "fba9ba92-2d2d-4908-814f-9efe37937526"], "metadata": {"page_label": "872", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0a47d563-871b-40b8-bfe0-71af3042e0c7": {"node_ids": ["2f1e704a-4097-4c0d-9b2c-d12c0d1aba46", "e8b5786c-e38c-4b64-9220-ebe75f58debb"], "metadata": {"page_label": "873", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dc43ca0c-bd26-4be6-a8a5-c8f2102a86ee": {"node_ids": ["a332080e-9ebd-4cd2-9b69-d2eb838c3362", "ecd1c513-2e12-4122-bdd4-03e6d363e9e6"], "metadata": {"page_label": "874", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6bc13c2a-8305-4858-bb25-259f6f58e99d": {"node_ids": ["a3ac1b31-0ede-450d-beda-33ece7bedca6", "632d3bf8-166a-4d83-9dcb-cc29f0debe84"], "metadata": {"page_label": "875", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8158715b-cef2-4015-9562-b06aaaa04497": {"node_ids": ["251443ec-7dcb-4c95-afcb-4759f06f78ce", "b01c3657-d3f7-4809-8f03-e30950548bbe"], "metadata": {"page_label": "876", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "11672e81-c2ce-4630-90f3-30681ed5d73d": {"node_ids": ["bad50e03-44d3-4963-9db0-3a340a81ecbe"], "metadata": {"page_label": "877", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5204c427-8fd3-4b4d-9141-7e97fb38a56d": {"node_ids": ["b756e03f-a448-44f3-bd46-128035ca97df"], "metadata": {"page_label": "878", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "64fa15ac-c027-4f53-89c9-0c1b5583502f": {"node_ids": ["93d10456-681b-4814-8c67-fdc2ab705ac9"], "metadata": {"page_label": "879", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "27541c14-f639-42c6-bd2b-3e1bfe34bee3": {"node_ids": ["e7a7362e-7807-4628-a066-e150af48a4a1"], "metadata": {"page_label": "880", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "664e8370-87bf-4cbe-9936-e1a4a96f7d10": {"node_ids": ["0ac8ffdc-d304-46c5-9acb-5060ce73d0f2"], "metadata": {"page_label": "881", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "db8c9948-9ca5-4ae9-b2ac-12da3acc3447": {"node_ids": ["15f3d42e-d3ac-42fd-8090-04302d307e3d"], "metadata": {"page_label": "882", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dc775fd5-8062-4090-98a0-b2e3300b9514": {"node_ids": ["cb266fcb-337c-4c10-9ca7-62bf70ff2fd3"], "metadata": {"page_label": "883", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ace730b1-1f39-463d-b1eb-7fd401550204": {"node_ids": ["6d79c9c1-069b-4797-8eaf-c01680bf7c5c"], "metadata": {"page_label": "884", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ef979d4f-4980-46d6-8414-dfeaa0eb1be8": {"node_ids": ["20910cd4-29e2-4d5e-935e-f389b86f469c"], "metadata": {"page_label": "885", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c3757882-d8e2-4e62-bcfc-a2f5f3ac06bd": {"node_ids": ["809f3c92-df81-432f-aa34-97c9af2131ea"], "metadata": {"page_label": "886", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3a85ae34-00d2-4d2d-aca3-0d894760a221": {"node_ids": ["1f6130c1-a12a-4be6-9f0e-afa14a32757d"], "metadata": {"page_label": "887", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8e5339f8-1724-4380-8282-3d6031fcd974": {"node_ids": ["cda350f4-f66b-499c-a7ec-0c6088f77272"], "metadata": {"page_label": "888", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "96edd470-d0a4-4e33-894b-30ca769bc094": {"node_ids": ["2f47f31a-3ee9-43ad-86e7-fe98a4940e8e"], "metadata": {"page_label": "889", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9d8e97e3-178f-42e9-a209-d5ddb5972831": {"node_ids": ["f2d97601-bd16-4f8b-90f6-104b41ccb935"], "metadata": {"page_label": "890", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "01db6d96-bc82-490d-8823-3e677c2905c1": {"node_ids": ["14e35c93-df0e-4104-9f27-bb087aa0ecf4"], "metadata": {"page_label": "891", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a54b8dbe-4d11-4bc0-bd73-f1fdef0876ad": {"node_ids": ["ccf19eb6-f8ec-40ab-9d22-e89afca3131b"], "metadata": {"page_label": "892", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1067cad0-218b-4145-92a1-ef05d72d02f1": {"node_ids": ["1bc7ec52-c798-4997-801a-e49c9e988c28"], "metadata": {"page_label": "893", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0c8f2a83-1d10-4f97-9763-e6a7db2e948f": {"node_ids": ["a0f44d0e-799a-4497-a1df-c42dea6401be"], "metadata": {"page_label": "894", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2e48ac36-3b63-4c9f-b356-0eba1c400395": {"node_ids": ["e9f19840-b05b-41f3-bf86-ad080e9ebccc"], "metadata": {"page_label": "895", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4fbbfba3-46fc-46a7-9c7b-69f5140e7c79": {"node_ids": ["8c94ea6b-6624-441c-a44b-e85e5f4842e2"], "metadata": {"page_label": "896", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7000771f-fa4f-4b34-a811-d9b8ded08d68": {"node_ids": ["95d36ef0-7ab2-4476-b4ad-c07274d50388"], "metadata": {"page_label": "897", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bfd95a86-26e2-4148-b364-865713db1d51": {"node_ids": ["fae7402a-d577-4023-84c4-c1a0ebe10ffd"], "metadata": {"page_label": "898", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "25447448-a2e5-4b64-bfe2-63be4984a1af": {"node_ids": ["2328051a-1fa0-47ff-b173-5106686eb2a4"], "metadata": {"page_label": "899", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "17a136d1-14dd-4305-924f-c5a3579e5699": {"node_ids": ["9e38bb4e-f293-4093-a863-ed4b94dca12e"], "metadata": {"page_label": "900", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "88084533-c476-427c-8d28-250771cfefc1": {"node_ids": ["826174a6-72d6-41ec-854b-654d4a3d4f03"], "metadata": {"page_label": "901", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f19184bc-33fe-4325-a440-1dc077fc720c": {"node_ids": ["4ca65cd8-c9c5-4181-8520-ad31103ebce3"], "metadata": {"page_label": "902", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b0df48ad-3cb4-4ea4-a6d1-cc4211fab607": {"node_ids": ["d369a3f7-414a-4e14-9aed-70c27582456d"], "metadata": {"page_label": "903", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bb6cea9e-2448-4dfe-acaa-9532e1e68cf2": {"node_ids": ["c42f0acf-5c83-4942-989b-cc77266bfd9b"], "metadata": {"page_label": "904", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "655ebe1c-6034-4a99-9087-9f55318f1adc": {"node_ids": ["6f88d2c5-d9df-428d-b946-fd7e7f215871"], "metadata": {"page_label": "905", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cf6c459c-f5d8-48a3-bb57-9a1a481da80f": {"node_ids": ["6b5cd206-2a5b-4ebe-9304-968d60a8d9a4"], "metadata": {"page_label": "906", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1d42eace-8e1d-4d0a-af6c-e3a8da4b7f8c": {"node_ids": ["d593b936-92cd-4459-9b35-ece89cd6177c"], "metadata": {"page_label": "907", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bec61c2e-e6de-4e2b-833d-06eb5deae25b": {"node_ids": ["aa11f417-4fe2-4846-ab61-0f20bcab9492"], "metadata": {"page_label": "908", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dcf74ee0-abdb-45c0-9e06-35a3b586e2cc": {"node_ids": ["7b839114-8ef7-42b1-b4f1-351783de4038"], "metadata": {"page_label": "909", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3bfa176a-1f95-4e51-9b3a-316ce84a22c7": {"node_ids": ["fd4ff1b2-b667-4c9f-863b-f7c781c461e4"], "metadata": {"page_label": "910", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3f8c2413-171e-4f98-8d19-12346440980a": {"node_ids": ["714c0367-e257-48e9-ab67-57cee81a2d05"], "metadata": {"page_label": "911", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c56c00d1-d371-46a7-b3ec-e569cbaa408d": {"node_ids": ["67abb207-16af-4e1d-b5bc-210e4490f591"], "metadata": {"page_label": "912", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "da85d089-01fd-40c4-8a45-99c788db6dc7": {"node_ids": ["1a74591b-165a-45f1-ba40-8f92a5552061"], "metadata": {"page_label": "913", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "58ddc913-545a-4626-88bb-0dab82093ead": {"node_ids": ["ed037b07-5946-47f9-b1b6-c6e585b7fad8"], "metadata": {"page_label": "914", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8bf92c09-77c6-4be5-8579-06705122c6f1": {"node_ids": ["4994a402-6792-4c8b-a35b-d7c8d7582edf"], "metadata": {"page_label": "915", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a350b4db-e07a-41f3-b2f3-a47e9fa9d6b1": {"node_ids": ["53f3f03f-c6c0-45d5-954e-329e7fe23f04"], "metadata": {"page_label": "916", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c87b58e4-1df5-48a4-aa2b-4127c47028ac": {"node_ids": ["ba197a59-a379-4f42-a9b0-bf2df0d229b7"], "metadata": {"page_label": "917", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "78753eb2-0c5c-4969-ac3a-542bf1993ec3": {"node_ids": ["0cb4bd28-addc-47fd-b0ea-57a58e1472ca"], "metadata": {"page_label": "918", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fb00c216-81ba-4c6e-bc60-747457efe92f": {"node_ids": ["67805cde-6ee4-497b-9d96-6f611ad78265"], "metadata": {"page_label": "919", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "508d3bd4-596c-461f-9c79-52a21b0565e3": {"node_ids": ["6e7373b5-b01b-4d75-906d-d82dc989740b"], "metadata": {"page_label": "920", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "dacd34ff-0234-4b63-99dd-4e662e8506b5": {"node_ids": ["24004148-918a-4956-8844-f18279dd1818"], "metadata": {"page_label": "921", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5dd81624-a8cb-4f8b-8721-a22720e67c6c": {"node_ids": ["2ee663ba-ca7b-48d6-b22a-3996d2500b73"], "metadata": {"page_label": "922", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bcc98809-e744-4dbe-9d47-643307221871": {"node_ids": ["1b09708e-dc0a-4045-ae22-67752f0e6a06"], "metadata": {"page_label": "923", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a84966a9-6e4c-4a5a-84b5-57a368c79c36": {"node_ids": ["084b72bb-9a1b-41b1-af62-f9e3c2072c02"], "metadata": {"page_label": "924", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e6629f19-b56d-4506-8c48-e9ad2de85b52": {"node_ids": ["77df227b-fd7b-4820-9305-162fc0f7888f"], "metadata": {"page_label": "925", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "926f1fcf-b6ae-48e5-9a1c-1032cb961b1d": {"node_ids": ["2f4f3e6c-7e05-47ab-88bd-ca2daa0c8e64"], "metadata": {"page_label": "926", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "81487b45-8413-4208-8041-c8aa1f1e063e": {"node_ids": ["b09cbb34-8cd8-4661-b852-7c0fc8f3122a"], "metadata": {"page_label": "927", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "51c128ec-97f0-48f8-a988-147c654b51d1": {"node_ids": ["366cd19e-63b4-4590-8615-829132669bb4", "658a896d-4e6c-4d84-babe-127997a58c4d"], "metadata": {"page_label": "928", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3731a957-0fe7-4560-994c-e56c8892a4aa": {"node_ids": ["b073cf29-47ac-4500-8c23-8d93c9e7f736"], "metadata": {"page_label": "929", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a6886040-9fb9-4382-a229-a5d5cfcf8528": {"node_ids": ["92fef7a1-d40b-4c85-9f10-43706761fefe"], "metadata": {"page_label": "930", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2bb89aa9-f4a4-4aee-b358-eb0db25c2f54": {"node_ids": ["50133199-caee-47d4-a702-34ff49ce8b49"], "metadata": {"page_label": "931", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "834756d3-064a-45f1-a3e8-55f68788240a": {"node_ids": ["e09db88b-e6b2-40a9-9eba-64ac79b4004e"], "metadata": {"page_label": "932", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a1aeb747-1ec0-4b4f-a55b-8e841ae01abc": {"node_ids": ["e2f51572-f016-4a1a-82e6-4e53b68435d6"], "metadata": {"page_label": "933", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7027798f-38c2-413d-bc7a-ebea06b4da2e": {"node_ids": ["615bb5d2-444d-4b63-8b0d-eda0a988d0ec"], "metadata": {"page_label": "934", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "98e00c25-ecb9-470d-bef3-bea7c8fafd91": {"node_ids": ["3c80c011-772b-4d40-9040-950c099266b0"], "metadata": {"page_label": "935", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a4463b90-911f-495e-afa2-52228065ed4d": {"node_ids": ["f1209d05-bdd2-48e8-a0ad-ad144b07f1c7"], "metadata": {"page_label": "936", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "438eab76-7f80-4a5d-9f52-a8cc82a88881": {"node_ids": ["2e590f85-b193-4a1c-b5ab-98e5d1fe727d"], "metadata": {"page_label": "937", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "618b35e2-f2b2-4622-87aa-6a58f48b1d3a": {"node_ids": ["93900cca-5ec3-4e1b-a8c5-f3ed7bc486f1"], "metadata": {"page_label": "938", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "124a7aed-b02e-47bc-876b-63637c59811a": {"node_ids": ["24686375-678e-4794-99ea-f79390176955"], "metadata": {"page_label": "939", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e1669e80-5bf3-4b94-925c-9d6557068806": {"node_ids": ["ea7fb8a8-e8d3-4610-af9a-a0ea2dd76200", "d54636b7-e96a-4c8e-8948-0ddddd27d117"], "metadata": {"page_label": "940", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4a2e1732-fd51-4c0a-8340-1b06f8dc74e7": {"node_ids": ["294056e6-c100-4864-8c30-afe02ef48173", "be308fa9-4744-4784-9e41-2b055da0443c"], "metadata": {"page_label": "941", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f342970e-0e28-4f0c-b773-4d31759ae4b3": {"node_ids": ["b6cf1349-3614-4733-b85b-89eb48dd669f"], "metadata": {"page_label": "942", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5d5f4a4e-86da-463d-9d7b-18e22090acf4": {"node_ids": ["3f751bd9-1ee9-4fce-acc0-20df115a0f42"], "metadata": {"page_label": "943", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3131d840-5cb3-4056-972b-9ed0f6588974": {"node_ids": ["538c2ad7-1046-402a-814e-467b18ff41de"], "metadata": {"page_label": "944", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9c60d7e0-5ce9-4d9b-b874-642cd0715938": {"node_ids": ["3782df88-f77b-4264-893c-b8dabf6f7371"], "metadata": {"page_label": "945", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6593fb55-8dcc-41d1-af99-c1ff1d2685ee": {"node_ids": ["6aedb46f-717f-4b11-b88a-091cbcc56a4d"], "metadata": {"page_label": "946", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5fe9c554-fae6-43a5-b274-712b68c702b3": {"node_ids": ["f58d150a-1790-482b-b36a-2c4480f9e3cf"], "metadata": {"page_label": "947", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "aa20d37c-ddf9-4710-ae59-64d7253f0e77": {"node_ids": ["1b3ad059-49bf-473d-a659-f70a03aff5bd", "1e29e7ff-a941-4280-a840-a095a20f6444"], "metadata": {"page_label": "948", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c410adb9-1403-47d3-859b-c4ad48a30e8f": {"node_ids": ["90f27983-51e9-4dff-ae73-581a5bd0ee32"], "metadata": {"page_label": "949", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "16692cac-39ed-4da8-85cc-43d5493b1090": {"node_ids": ["dd7d0ce9-ded7-4889-a786-3316e372f22a"], "metadata": {"page_label": "950", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "69ddaa41-fbd5-4111-8888-fde555a1cd99": {"node_ids": ["dc60643f-f9fd-49e8-a5e6-5f289fddf027"], "metadata": {"page_label": "951", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c64dd00c-5a71-461b-a7aa-24a07610b74c": {"node_ids": ["7db5ddab-0d7e-41cf-b98f-708becca1aad"], "metadata": {"page_label": "952", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "43b0e7df-4912-4def-90b2-60604db225b6": {"node_ids": ["d5a71e95-442a-4a83-942e-8b08b2dc9830"], "metadata": {"page_label": "953", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ca28fa14-4437-48e2-ae23-9f732ed14586": {"node_ids": ["e239d1ba-cea9-4a53-b51e-5abacd7d6a8e"], "metadata": {"page_label": "954", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bacddf08-1901-4cbc-8828-930cbabc11fd": {"node_ids": ["97b8d7a9-0656-4f20-b577-a1247826d991"], "metadata": {"page_label": "955", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c97ea8d4-f522-4bed-a453-9eb1cb8cfa85": {"node_ids": ["07e28ca8-6908-4963-bb94-3c4153470361"], "metadata": {"page_label": "956", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ec216030-44b3-4a56-8685-bb7fc93f4d2d": {"node_ids": ["3fadfbaf-bbe8-435e-bd03-b5ede69b4b64"], "metadata": {"page_label": "957", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c1d2415a-60aa-4f68-9d93-aa266d0da7be": {"node_ids": ["a7423643-0738-46a7-8d01-4b63fa4b7bb2"], "metadata": {"page_label": "958", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c0669d3f-69d2-4384-af77-e2384c7f6750": {"node_ids": ["f38b6920-b358-482b-87d5-85c675588da0", "c0a62fd9-a516-466e-8c6d-f3a21952a78c"], "metadata": {"page_label": "959", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "71ef1115-d0e5-44e1-a28c-a8a65590dc07": {"node_ids": ["e4b870d2-3473-4d64-af6d-4c7fce205f6f"], "metadata": {"page_label": "960", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "225e3686-c73a-4a0c-8918-a60289e2da04": {"node_ids": ["632b18f6-6fce-457a-b513-a9a6ac1c8f7d"], "metadata": {"page_label": "961", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "df079df9-43ca-4eb6-8a5f-ee9118bda107": {"node_ids": ["582f7517-d580-440f-a65e-459cfc99a4d1"], "metadata": {"page_label": "962", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c0ae27a9-c425-4183-ada0-09aea59be0ba": {"node_ids": ["a6d6af9f-05d8-4977-9194-4dedc4078784"], "metadata": {"page_label": "963", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a0651f0e-0784-437d-ae8c-c7bec54e1752": {"node_ids": ["d308549a-cf83-4b95-90a4-a8300017f518"], "metadata": {"page_label": "964", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b52f1475-6b60-455a-86f1-a3d49ec46177": {"node_ids": ["8ef087df-59fc-48a4-bd99-73dfaed60dad"], "metadata": {"page_label": "965", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3bba584d-9758-4f6e-a11c-5595973d85b3": {"node_ids": ["27ce6b65-4acb-4795-befb-74a42148a334"], "metadata": {"page_label": "966", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7cf99aae-4b90-4823-a1af-72e4dd60279d": {"node_ids": ["197c227d-d7a9-463f-96a6-87925b1079b7"], "metadata": {"page_label": "967", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d67dcd29-2774-498e-95d5-b544c108feae": {"node_ids": ["dbd7f710-f45a-4cff-931e-b6e8407dd47a"], "metadata": {"page_label": "968", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "84273060-b30d-4298-8b78-dff3d59d10aa": {"node_ids": ["7ff618b8-9e78-4eef-b760-9473611787f7"], "metadata": {"page_label": "969", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b3a560e9-2cf1-47e8-9c38-b793af3d8272": {"node_ids": ["9971160d-74af-4728-aa34-0926c603b710"], "metadata": {"page_label": "970", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "df9d599a-bb9c-47ec-ba1f-07091b411110": {"node_ids": ["a6d5edac-2a60-46bf-a1c5-ad1c3fabb8e7"], "metadata": {"page_label": "971", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b5492367-ff90-4338-a4bd-6e51c8408b41": {"node_ids": ["45e271a8-dfb3-434f-b696-4ed8a986bdcc"], "metadata": {"page_label": "972", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0c120afc-9622-4ac3-8db5-de67672b4743": {"node_ids": ["66d6f3d0-4ad1-445b-a986-e779601fafc1"], "metadata": {"page_label": "973", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b2ab75a7-5582-45bd-b8d6-8da6c499d32e": {"node_ids": ["4ca407d9-efb0-489c-8165-ccf7402cf0b6"], "metadata": {"page_label": "974", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7b9a7816-9df1-4087-b9d3-fce837bbb5f0": {"node_ids": ["9e3f3009-4b65-486f-83d9-540d0efc69da"], "metadata": {"page_label": "975", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3700e140-0b5b-49f5-84c0-5188fe804335": {"node_ids": ["9e16bdc3-8b51-409f-be1d-7029b8cd680b"], "metadata": {"page_label": "976", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0a4b4fa7-907d-42d2-9f07-aae78a62ca34": {"node_ids": ["55bba3ab-2576-44ac-bb3f-5cfa7d31e8c6"], "metadata": {"page_label": "977", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b3d53c93-3816-4a3d-b988-89de6678d2a9": {"node_ids": ["6a8c631b-b9d2-4e97-ac9f-75f3bfa35893"], "metadata": {"page_label": "978", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "43291d5d-62f5-499d-8c07-ffdeee836c43": {"node_ids": ["63649d0c-a608-4e13-831f-9ad6be019cf1"], "metadata": {"page_label": "979", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e468b855-cc21-4d80-b909-8200bf17080b": {"node_ids": ["a3e56e09-2cfb-485c-aec3-15fe76030a26"], "metadata": {"page_label": "980", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "83f797ec-4ce8-44b5-89eb-850eb5133e1c": {"node_ids": ["0a2c1165-6a37-4719-9389-543b9c5d68e1"], "metadata": {"page_label": "981", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "58b60538-3f44-4246-b039-bfa7e6c89fb6": {"node_ids": ["e6cb8db1-9369-4c0b-bc7d-9056c48f1a70"], "metadata": {"page_label": "982", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5be7398b-a424-4fe8-b370-8b3f29de85f7": {"node_ids": ["0547c61b-deae-493d-a650-abc29af755a1"], "metadata": {"page_label": "983", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4a264752-044e-40fe-b85f-31e60aad4b47": {"node_ids": ["fba56e42-afc4-44ce-9a92-8e8beed521c7"], "metadata": {"page_label": "984", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4ef10fae-1db0-4059-a410-4269445e3742": {"node_ids": ["dccbc241-4231-4dc0-a06b-e964458bf1c2"], "metadata": {"page_label": "985", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8e8ba577-26c2-482b-8626-160edc2666b8": {"node_ids": ["6f079f40-afe0-4b65-9866-27f79a152bc1"], "metadata": {"page_label": "986", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "19737886-8029-4703-9ad1-9dbe0c77a189": {"node_ids": ["baa2fbaf-78de-4a07-8d2c-41b9fd6d144b"], "metadata": {"page_label": "987", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fdd10dda-4495-45d7-b626-205c73b0e350": {"node_ids": ["00f4552c-96ed-42f0-9c38-f753ce816a45"], "metadata": {"page_label": "988", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3fe14965-f01f-4843-8c32-5de48b389f24": {"node_ids": ["413a6f6b-d2b6-4654-9171-78e0a3075635"], "metadata": {"page_label": "989", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "369c3d17-e714-4d75-96df-0da9758a69c0": {"node_ids": ["43a6c154-d77c-4bba-a247-a6a1aa983001"], "metadata": {"page_label": "990", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "455ce185-23a1-4116-8a54-50d7c7a90082": {"node_ids": ["94cb6485-2b67-4601-bba9-7adb37ae7962"], "metadata": {"page_label": "991", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2411fb60-fa01-4b22-afeb-bfb127e8e749": {"node_ids": ["c5da40fb-97b7-4128-96ff-c568415710a3"], "metadata": {"page_label": "992", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6e7f2396-0cac-40bd-a590-95885dd01ad1": {"node_ids": ["c96a7c2e-19e3-4dbb-b068-68e7c45c2c44"], "metadata": {"page_label": "993", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "24aeb13b-8341-4cb5-86a3-2367cf7d611a": {"node_ids": ["9b90c7bf-fae1-47b2-bf1c-25c78c3999ee"], "metadata": {"page_label": "994", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "030e0f32-38fe-48b1-8cfc-88be95ab76fa": {"node_ids": ["7ca21c7b-703b-4f03-8532-5e5f75773b4f"], "metadata": {"page_label": "995", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2de137e7-5a91-4e95-bc4a-fc3825196fe7": {"node_ids": ["2656542d-47c2-4360-a265-2c4a79b61074"], "metadata": {"page_label": "996", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "31ef0ef6-9099-4b04-9d31-09356ddf80c4": {"node_ids": ["312d428a-4940-4c60-917e-67ddb87cd1db"], "metadata": {"page_label": "997", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9ca373e6-9500-4c0f-bc2c-c7c376545751": {"node_ids": ["1790231f-52d4-4bd3-a063-0cd2707dec6b"], "metadata": {"page_label": "998", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1f14d2b8-2692-425e-a7af-117aa00dc560": {"node_ids": ["65375221-f862-48ef-86da-ec0ef4d2f3c4"], "metadata": {"page_label": "999", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a9d95332-2e83-4b70-b3ef-13d46c8da8bc": {"node_ids": ["eb89a527-8106-4162-9e15-0999c469b114"], "metadata": {"page_label": "1000", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f78015cb-5822-4aba-9634-dc6813c5f4f2": {"node_ids": ["0ae78e98-da02-48b0-b925-7a69c9afc861"], "metadata": {"page_label": "1001", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "88ace39e-f3cb-446f-9cf1-629e1172ce31": {"node_ids": ["6d9059c9-f2a9-4b96-8c99-fb7ea6fa04bb"], "metadata": {"page_label": "1002", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "68b30d78-5beb-4b1b-b8c8-a3eaf7779648": {"node_ids": ["23051ad3-4b2d-4ee6-9dca-488a3dad74f7", "ba79490e-36bf-4af9-b070-a1072bfe6837"], "metadata": {"page_label": "1003", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5d26d6f2-7afc-468b-b9c5-2bf5394a9a98": {"node_ids": ["fb056a27-3fe8-4236-a5e8-5dd2d7712844"], "metadata": {"page_label": "1004", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6e8c0b4b-529c-44de-8d1d-061d4eb83c4c": {"node_ids": ["d6080442-3796-4724-a4a8-5c1749a540ac"], "metadata": {"page_label": "1005", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9f3e69c2-0fbc-4c49-999a-c1f32ee6b5bb": {"node_ids": ["6c731c5c-a58d-4383-8566-cb809b0e3a87"], "metadata": {"page_label": "1006", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ae9e144a-b356-4fd1-a5f2-3cfca09816e7": {"node_ids": ["837717b5-85f6-4c5f-9406-030490cadde1"], "metadata": {"page_label": "1007", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8f59441c-7148-4bb4-82e4-00d48a4b9bac": {"node_ids": ["2c01e35c-fc3d-426c-81c5-3daf593f820d"], "metadata": {"page_label": "1008", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4787e665-1654-4fba-9717-88d6e6213c24": {"node_ids": ["b89a0101-81f4-42cc-9573-e9b1041461f3"], "metadata": {"page_label": "1009", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6098143c-9292-4f57-a499-bd881f4210f5": {"node_ids": ["5d17c222-989c-40b5-97cf-326ddf0127af"], "metadata": {"page_label": "1010", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "174a8d1b-05e5-410d-b995-1b5f7f446daa": {"node_ids": ["71361730-f0d0-4ff9-b186-3854d8baf7fa"], "metadata": {"page_label": "1011", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5aaef3ed-c7a0-4b55-b679-5fad8959b283": {"node_ids": ["d46e4021-3dc8-4bd8-bced-fc4fabda7c3f"], "metadata": {"page_label": "1012", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e0f20ab2-cf4e-43f6-bb0d-a998c38e63f6": {"node_ids": ["4023bfd4-7339-4e80-9557-7544492e1bc5"], "metadata": {"page_label": "1013", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3b4be816-716c-487b-8cb8-1fa3eaf70060": {"node_ids": ["cc19044d-be9d-4f8c-a193-beebd29aeb75"], "metadata": {"page_label": "1014", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "eab24963-c5f1-4db8-b5e4-9a9dcc78bbeb": {"node_ids": ["5ce5ee54-71a2-4630-8537-9bee700c8cd1"], "metadata": {"page_label": "1015", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fd558d3a-bbbd-4469-a5d9-37407de68fbf": {"node_ids": ["0ba3b272-8108-4e27-ad1e-f299a4decb16"], "metadata": {"page_label": "1016", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "1e1692a1-aab8-46fd-870b-ca0de5950f7a": {"node_ids": ["989507b8-d4f6-474b-a2b5-eb7115184462"], "metadata": {"page_label": "1017", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "492f26e7-34b3-45ba-801a-531824a6f307": {"node_ids": ["6fa54a59-ede9-41ec-a26a-dcf5fde958b7"], "metadata": {"page_label": "1018", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "717eefd6-75f1-44ff-8817-5aa70f56b554": {"node_ids": ["d442b39d-325b-4fcc-b611-1451523f279a"], "metadata": {"page_label": "1019", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e0cd431e-ad0c-45f0-a6e5-207c087fe494": {"node_ids": ["dc40fca0-96c3-4e69-a1d2-a09372e7aa11"], "metadata": {"page_label": "1020", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "983ef592-fc28-44b2-a9a8-2ef46879f1ce": {"node_ids": ["55c43a4d-85d5-4aac-a9cf-726606322bbe"], "metadata": {"page_label": "1021", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "86b3dac2-9179-40d3-b464-e76178b124bf": {"node_ids": ["d8d9475c-207f-409d-ad51-efb1dc40ac29"], "metadata": {"page_label": "1022", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f08388fc-5680-40a0-9504-66e18baaaada": {"node_ids": ["d8f2f9b6-7855-430a-aa2b-16bcdc5a8527"], "metadata": {"page_label": "1023", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "126b16cd-8866-4519-979a-fbb79c99a932": {"node_ids": ["6fa8f98d-d005-4856-9539-dbd1693c2a49"], "metadata": {"page_label": "1024", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "61895bb2-23d8-47cc-b4b7-0c05bcfaeecd": {"node_ids": ["ada0f6ff-086d-41d1-a52d-0e458e57050f"], "metadata": {"page_label": "1025", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c5da67f7-cb8a-4822-9aed-5886d8404d15": {"node_ids": ["9b5bb047-bb25-4edd-90f9-ec19cf4a04de"], "metadata": {"page_label": "1026", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bd41028f-590a-49f1-88df-bd062f17fd4a": {"node_ids": ["6aaf7fa0-f7f4-45b8-9c86-fdd98956ff61"], "metadata": {"page_label": "1027", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f7dc9c37-f551-4a6e-a623-4c2e336a84a7": {"node_ids": ["1917c322-83e2-4c3b-b51e-3c3b91d28e63"], "metadata": {"page_label": "1028", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5276800f-96e3-452e-aa87-d9c96a332856": {"node_ids": ["da513750-5241-4602-be80-3b7edd0f3824"], "metadata": {"page_label": "1029", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "02b4f4f3-3929-4107-bcc9-dccc2f6fde59": {"node_ids": ["9e0a11ee-c4f7-41bd-8c2b-e0c2d39a5067"], "metadata": {"page_label": "1030", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "304a7d0f-3c3e-4362-9359-c3d5b5777dd3": {"node_ids": ["fa2eae93-a930-4735-98a0-f208e329207c"], "metadata": {"page_label": "1031", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8537139b-95f0-4aa1-82e6-e6c0686c7b93": {"node_ids": ["ccff6b56-5230-4e30-a58f-203513ee8223"], "metadata": {"page_label": "1032", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6d2e232e-c67b-4d67-818a-ca81529e7f92": {"node_ids": ["c9e3092c-942e-4b5b-9b34-178dc694e178"], "metadata": {"page_label": "1033", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ad734571-2657-4907-8fc1-22e12393e69b": {"node_ids": ["7137ccbf-b9f7-4a9c-ae4c-12fba3c78c38"], "metadata": {"page_label": "1034", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a9c17bef-cfe1-4da0-b79a-9043fa967d4f": {"node_ids": ["b0a40b30-b1bf-4bde-ac31-56f5d404c874"], "metadata": {"page_label": "1035", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5b72fad3-1ac5-426c-8a67-0b062ac2aa24": {"node_ids": ["c60d27eb-6bb5-4363-a8a6-8a5fd3fa3bed"], "metadata": {"page_label": "1036", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "42d8241e-c9a5-4cae-9ebd-7c5168d2703f": {"node_ids": ["699c7f56-58d2-4f9f-b095-5543ad619a8e"], "metadata": {"page_label": "1037", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6de9d35f-0e37-457b-bf6e-b621a00458c9": {"node_ids": ["018101bc-3d92-4323-8f7c-793c858bba83"], "metadata": {"page_label": "1038", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5fb4be06-fa68-46d2-bd2a-60c3500df870": {"node_ids": ["dc4d6639-6fb6-49a1-85bf-670b572da010"], "metadata": {"page_label": "1039", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "097a64d0-a747-4251-8a56-663b7c99f7b9": {"node_ids": ["69778296-dfa3-44b1-b2ae-549b954a6a8f"], "metadata": {"page_label": "1040", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f1adb9f6-d60d-4cf8-9866-cbedc1ef30d1": {"node_ids": ["b449a9b1-836b-4ffe-a656-fa1dcdcfc3c6"], "metadata": {"page_label": "1041", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "15f67db8-59ae-4e03-8fa8-a32d43fb4897": {"node_ids": ["73b08989-3669-4118-9293-d457c2f58d02"], "metadata": {"page_label": "1042", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5f6a8165-b518-43f9-98f7-cbae8632931d": {"node_ids": ["df5138fc-f800-4612-b670-3d169b5b52fe"], "metadata": {"page_label": "1043", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "48fc486b-3e45-4eb8-9cd3-7abf312194f1": {"node_ids": ["175a053d-79e9-4cab-aecd-b33743b00019"], "metadata": {"page_label": "1044", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ce3d2ef9-72fa-4372-bd47-e4c270b094c6": {"node_ids": ["d47acd47-9612-4069-820d-c9f1a8a4305f"], "metadata": {"page_label": "1045", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "80fd7197-223c-41be-92ac-82cff791983c": {"node_ids": ["ab8098c1-8ef6-4b5e-9929-1411d322665b"], "metadata": {"page_label": "1046", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "94629437-77de-469e-b198-a264957172ba": {"node_ids": ["0ae79d6b-9679-4e8a-89f3-0d81a200131c"], "metadata": {"page_label": "1047", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9710fcf5-be7b-4307-995a-a044827c7eae": {"node_ids": ["23b15c56-56d3-4322-8136-070545946120"], "metadata": {"page_label": "1048", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "9a26dbff-c0b5-4f07-8e19-057c9c37ab10": {"node_ids": ["ebfc489f-02b1-49aa-ae19-ef351bd59a80"], "metadata": {"page_label": "1049", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "effc38b6-67cc-4a2d-8120-0c0df34121d4": {"node_ids": ["d817af21-a84b-4bd8-bc0e-530fa27921bd"], "metadata": {"page_label": "1050", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "05a6157c-cb63-4311-8c4b-5842260982e4": {"node_ids": ["ca3122e4-a77e-4c27-9702-50700e9c16c6"], "metadata": {"page_label": "1051", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "560d8476-0eb4-4264-b22a-3fa5bfb876a5": {"node_ids": ["009141f8-67f7-452a-a37d-3b98f4d36440"], "metadata": {"page_label": "1052", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b5277ad7-283f-41e5-9995-d49586d4cb92": {"node_ids": ["3b786500-41b7-4103-abf4-16fe61447225"], "metadata": {"page_label": "1053", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "77c7181f-a596-4142-a757-d270eaccf56b": {"node_ids": ["d0433cd4-9744-4ff2-a556-48e2aaf450e7"], "metadata": {"page_label": "1054", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c767897f-b55a-4b94-bd50-cd45f5603ca8": {"node_ids": ["ab6d3e51-1b31-4bd6-a5d0-ffeb262805ad"], "metadata": {"page_label": "1055", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "bfad893a-dda3-472a-bfd9-5e154c9dc397": {"node_ids": ["eb76921b-bb2d-47ea-83f0-b2d39ff1a5f2"], "metadata": {"page_label": "1056", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5cce5e56-9480-46e8-9baf-d6d0e1f634f3": {"node_ids": ["4c3e8cfa-02f3-4276-b531-df4013d1734e"], "metadata": {"page_label": "1057", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6836bd8c-15dc-4239-864a-2d93e1461e3b": {"node_ids": ["738cc6bf-32c8-4d6c-826b-86c31e6f9e6d"], "metadata": {"page_label": "1058", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3d432e47-783f-4545-b1bd-5139c3c15059": {"node_ids": ["78c3b75b-7a60-4e63-8645-5404d79cd004"], "metadata": {"page_label": "1059", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "853908f1-595d-4280-882e-879d1228ca47": {"node_ids": ["6a43bb76-f705-4e83-8df6-77fe05367747"], "metadata": {"page_label": "1060", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c5877935-558a-4b3d-90f8-5066af04aed0": {"node_ids": ["a940d782-a78b-4fd3-9270-c198e3281230"], "metadata": {"page_label": "1061", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c06221d7-0038-4909-a32a-10beb9601cc5": {"node_ids": ["04a6eded-c1ba-4f6b-9353-d37268b84945"], "metadata": {"page_label": "1062", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ee500273-a3d5-49d8-9acc-091f39ada3a5": {"node_ids": ["40e0c668-5f4b-40c4-b3ff-090f74aee5f9"], "metadata": {"page_label": "1063", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ac812169-2dbe-4ae8-8f5e-4db944318afa": {"node_ids": ["2837d0ff-25fc-44a3-b8a7-a313f811c646"], "metadata": {"page_label": "1064", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "928cc9ae-35f5-4bf8-a84b-6147914d4e8a": {"node_ids": ["f120881f-f240-4591-b8ed-812bd92e1ce4"], "metadata": {"page_label": "1065", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8d16451d-7b2d-4127-a41f-d1430e17378a": {"node_ids": ["2d143041-3a1f-49c1-8b12-9e5982acfcba"], "metadata": {"page_label": "1066", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e2a745d1-c965-4707-b4ab-55713cec3b46": {"node_ids": ["13b12716-f2b1-46d3-923b-7857a3ebde7b"], "metadata": {"page_label": "1067", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e822d38c-1118-46b7-89ed-6fa36726c5d6": {"node_ids": ["39f38b70-3d51-4914-8f57-cb0073e3948e"], "metadata": {"page_label": "1068", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "0f92aa44-848f-46db-ace6-2f909b451715": {"node_ids": ["d4e39efd-ef3d-4f5b-b895-8234df8e531a"], "metadata": {"page_label": "1069", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "33c3afa9-7d09-49dc-b043-fd7549e02697": {"node_ids": ["e16f0be8-ee70-42b7-9c87-b08f5ccffbee"], "metadata": {"page_label": "1070", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ffc8974e-cefa-4a8b-8532-60a43124722f": {"node_ids": ["43d85eab-56aa-404d-8a6c-5f569f0be93f"], "metadata": {"page_label": "1071", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5fbf7d35-020c-4161-bbd9-282f30ff0783": {"node_ids": ["e1b37d06-9524-4968-bff0-c018b96c690f"], "metadata": {"page_label": "1072", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b3d05ef5-22b1-4476-9f6b-29d23d41a99c": {"node_ids": ["81f0d7cc-e154-458c-8fe0-1ab14de17ab3"], "metadata": {"page_label": "1073", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "c385ae98-d182-49d1-af2d-babe1a090f6f": {"node_ids": ["a653b7b8-1908-4ae7-8c5b-fadb23b96c1d"], "metadata": {"page_label": "1074", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "32211d49-2073-48b4-8ff7-1f734f10a6b3": {"node_ids": ["509576d7-b2f8-440b-9dd2-23fe8a5fa0c1"], "metadata": {"page_label": "1075", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3e62bcc8-9676-48c7-b5e7-23c67f8ca9cb": {"node_ids": ["4de06e6a-06ef-4d12-83ba-b9c7253cb552"], "metadata": {"page_label": "1076", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "07fd9c5c-57e0-4e10-bed5-2745789d0e70": {"node_ids": ["a507e63b-8fe6-4935-b765-a29166829d47"], "metadata": {"page_label": "1077", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2c9f70ba-b429-4425-a7b0-faf45f8f87be": {"node_ids": ["062c43cc-3bf7-4a67-a318-11330492a136"], "metadata": {"page_label": "1078", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "18ef0941-93ff-42dc-a734-660d9b930f7c": {"node_ids": ["f3b5f6a5-5ab2-4eeb-9995-51dbc62e6026"], "metadata": {"page_label": "1079", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "656daad1-1fe0-4de6-84cf-c83fc5bb6f24": {"node_ids": ["5372f5d7-2582-47ef-8b71-f1e410b9c3c4"], "metadata": {"page_label": "1080", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "8955b9b8-b58a-41a4-bcfc-55eb3bfd1b93": {"node_ids": ["84319c2f-37eb-49ac-8024-ef05b77288db"], "metadata": {"page_label": "1081", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "397457e8-308a-4713-94ab-883b715a2a67": {"node_ids": ["e29898ed-9490-41ad-96da-6895c9f657be"], "metadata": {"page_label": "1082", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "ab540cf0-82ba-494d-ab73-f5c4e3a97bd9": {"node_ids": ["a002ac36-61a7-44ca-9dde-c6db7b6d6097"], "metadata": {"page_label": "1083", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b9b9a2f5-a75e-492a-90dd-6617e54b0b80": {"node_ids": ["40c0cba8-4942-4aa4-b683-db27a40dd5a9"], "metadata": {"page_label": "1084", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "a51a6fa6-02c9-4e5d-859f-1021e93cabbe": {"node_ids": ["e897a7b9-9bcc-4806-b322-a71bde74d230"], "metadata": {"page_label": "1085", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f62047b8-29c1-4507-aff8-886f2f84387b": {"node_ids": ["8020bdcd-949e-479c-a4cb-4feb516a0d9f"], "metadata": {"page_label": "1086", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7c9250d3-ade2-46d3-b651-687b499fad8f": {"node_ids": ["f00dc602-1d44-45d0-938d-f91f95dc3f09"], "metadata": {"page_label": "1087", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "613e48f1-6a4e-45a9-bd81-6a4cf41dbcb7": {"node_ids": ["b856c401-a290-43ab-bf1f-1509000bb591"], "metadata": {"page_label": "1088", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "d10b4bba-644b-4f1e-8905-26ff299c52c4": {"node_ids": ["3539a9a9-7781-4e12-a8a8-d4941c031bef"], "metadata": {"page_label": "1089", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e4e1e00a-dc67-4746-9699-3add70bf247c": {"node_ids": ["1984ab8a-5c18-423e-9d40-a1a3d9793bee"], "metadata": {"page_label": "1090", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "68d3b583-4b4b-4cf9-9661-d40e639c7d84": {"node_ids": ["6545d33f-26c0-43f0-ad39-5b044e97d1c6", "de1c3eb5-6100-4c2e-88b5-54d9553743d7"], "metadata": {"page_label": "1091", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fefecc41-9720-411d-9e18-8b2087abd80c": {"node_ids": ["889a3660-0620-4518-a83c-b8f692658d97"], "metadata": {"page_label": "1092", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2e2552ba-c063-48e5-9d9d-03148eff3553": {"node_ids": ["77f0f40c-d5f4-4334-ad1c-4c269c4a5382"], "metadata": {"page_label": "1093", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "3b4af78f-dfbb-4a17-ba98-4c9db7e74835": {"node_ids": ["df25089f-6287-4466-9eee-01a4ac93b621"], "metadata": {"page_label": "1094", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "5b66a0c3-0d2b-4040-b753-4ca068f46d45": {"node_ids": ["4d7bca40-c464-4262-a036-8b5b0fe2bfa6"], "metadata": {"page_label": "1095", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "b955caf2-228a-4001-ace2-365da4d78617": {"node_ids": ["ce74b24f-9b5c-4a0f-8800-df3ee01374fa"], "metadata": {"page_label": "1096", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "063e8cf1-f44a-4f0a-a7d6-da2b1b7fdca8": {"node_ids": ["fd33374a-a801-4cdb-9718-bca9893063ad"], "metadata": {"page_label": "1097", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "4aaf6edd-5198-4cd2-964a-7b0955556f5d": {"node_ids": ["d2578d28-073c-4da0-80ae-37f1e17077a4"], "metadata": {"page_label": "1098", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "64608d2b-9958-42f8-b7d1-d31f57b2a7f6": {"node_ids": ["606325b1-1772-4deb-95a6-6da8f9fd3438"], "metadata": {"page_label": "1099", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "55645ccd-a764-4c82-8a5b-d5c1bdb4977f": {"node_ids": ["ae9c5ebe-ccf6-40ef-b4bc-7b1d3979081d"], "metadata": {"page_label": "1100", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "6c9c85b2-4210-453f-ae49-4804109196de": {"node_ids": ["bc24e162-302d-4645-a4e5-2c5a38efda88"], "metadata": {"page_label": "1101", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "e8f083ac-d9bc-4dff-8791-3514d3f1221e": {"node_ids": ["a6a4194a-90d4-4c1c-bc96-ff6dfd17ee81", "cf445afd-d4cc-49e4-973b-2e95bb6d81f3"], "metadata": {"page_label": "1102", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "57a0300a-28b0-4de2-bf3d-482f16b49a6d": {"node_ids": ["9d83e7d1-c2a4-4c7b-9656-b5760bfe2787"], "metadata": {"page_label": "1103", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "fec5a5df-e972-44a8-9721-4a6a5bd71f4c": {"node_ids": ["fd851b36-0457-457b-8c7e-47a2b4f3e81d"], "metadata": {"page_label": "1104", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "512837ee-867d-4121-8de8-ba4ac5dd4ddb": {"node_ids": ["a3866778-7d08-48de-9558-b255ef53cde5"], "metadata": {"page_label": "1105", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "afd9f82c-3eca-4281-b9aa-3d0696d81217": {"node_ids": ["8bf41e91-3dba-4515-888d-6edaabfbbe14"], "metadata": {"page_label": "1106", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "7536d56a-0f50-4193-99b2-5211a824a7af": {"node_ids": ["81d02376-5daa-4180-99b4-310e9610b631", "a8f5e2fd-0bff-42eb-8742-7c43413a3ee8"], "metadata": {"page_label": "1107", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "f514bc10-07f7-4289-b367-16ad3e66bea9": {"node_ids": ["9245306f-beb5-4254-802c-8963c01f79a7", "7c565a9e-fc17-4e4f-b29a-e4434618c3ea"], "metadata": {"page_label": "1108", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "641b07f9-8b08-4fc6-bb19-5fb76f470409": {"node_ids": ["5a693e7e-99df-4ede-81b6-8dc141f90ef2"], "metadata": {"page_label": "1109", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "2277f387-c738-46eb-8fd3-5261f944ae81": {"node_ids": ["f20d8c72-0584-4d23-ac18-eb554985e933"], "metadata": {"page_label": "1110", "file_name": "DeepDiveIntoDeepLearning.pdf"}}, "cc9b6c1e-7161-463f-86cd-c63038729c4c": {"node_ids": ["e9038e64-d2b0-4162-aaef-1b3dd5c6a8f2"], "metadata": {"page_label": "1111", "file_name": "DeepDiveIntoDeepLearning.pdf"}}}}